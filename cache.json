{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model. (arXiv:2203.11199v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11199","description":"<p>Recently, the problem of robustness of pre-trained language models (PrLMs)\nhas received increasing research interest. Latest studies on adversarial\nattacks achieve high attack success rates against PrLMs, claiming that PrLMs\nare not robust. However, we find that the adversarial samples that PrLMs fail\nare mostly non-natural and do not appear in reality. We question the validity\nof current evaluation of robustness of PrLMs based on these non-natural\nadversarial samples and propose an anomaly detector to evaluate the robustness\nof PrLMs with more natural adversarial samples. We also investigate two\napplications of the anomaly detector: (1) In data augmentation, we employ the\nanomaly detector to force generating augmented data that are distinguished as\nnon-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply\nthe anomaly detector to a defense framework to enhance the robustness of PrLMs.\nIt can be used to defend all types of attacks and achieves higher accuracy on\nboth adversarial samples and compliant samples than other defense frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rongzhou Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization. (arXiv:2203.11239v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11239","description":"<p>Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve\nstate-of-the-art performance on many generative NLP tasks. However, such models\npose a great challenge in resource-constrained scenarios owing to their large\nmemory requirements and high latency. To alleviate this issue, we propose to\njointly distill and quantize the model, where knowledge is transferred from the\nfull-precision teacher model to the quantized and distilled low-precision\nstudent model. Empirical analyses show that, despite the challenging nature of\ngenerative tasks, we were able to achieve a 16.5x model footprint compression\nratio with little performance drop relative to the full-precision counterparts\non multiple summarization and QA datasets. We further pushed the limit of\ncompression ratio to 27.7x and presented the performance-efficiency trade-off\nfor generative tasks using pre-trained models. To the best of our knowledge,\nthis is the first work aiming to effectively distill and quantize\nsequence-to-sequence pre-trained models for language generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Classification of Long Documents Using Transformers. (arXiv:2203.11258v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11258","description":"<p>Several methods have been proposed for classifying long textual documents\nusing Transformers. However, there is a lack of consensus on a benchmark to\nenable a fair comparison among different approaches. In this paper, we provide\na comprehensive evaluation of the relative efficacy measured against various\nbaselines and diverse datasets -- both in terms of accuracy as well as time and\nspace overheads. Our datasets cover binary, multi-class, and multi-label\nclassification tasks and represent various ways information is organized in a\nlong text (e.g. information that is critical to making the classification\ndecision is at the beginning or towards the end of the document). Our results\nshow that more complex models often fail to outperform simple baselines and\nyield inconsistent performance across datasets. These findings emphasize the\nneed for future studies to consider comprehensive baselines and datasets that\nbetter represent the task of long document classification to develop robust\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyunji Hayley Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_Y/0/1/0/all/0/1\">Yogarshi Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1\">Kashif Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error. (arXiv:2203.11317v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11317","description":"<p>Discourse analysis allows us to attain inferences of a text document that\nextend beyond the sentence-level. The current performance of discourse models\nis very low on texts outside of the training distribution's coverage,\ndiminishing the practical utility of existing models. There is need for a\nmeasure that can inform us to what extent our model generalizes from the\ntraining to the test sample when these samples may be drawn from distinct\ndistributions. While this can be estimated via distribution shift, we argue\nthat this does not directly correlate with change in the observed error of a\nclassifier (i.e. error-gap). Thus, we propose to use a statistic from the\ntheoretical domain adaptation literature which can be directly tied to\nerror-gap. We study the bias of this statistic as an estimator of error-gap\nboth theoretically and through a large-scale empirical study of over 2400\nexperiments on 6 discourse datasets from domains including, but not limited to:\nnews, biomedical texts, TED talks, Reddit posts, and fiction. Our results not\nonly motivate our proposal and help us to understand its limitations, but also\nprovide insight on the properties of discourse models and datasets which\nimprove performance in domain adaptation. For instance, we find that non-news\ndatasets are slightly easier to transfer to than news datasets when the\ntraining and test sets are very different. Our code and an associated Python\npackage are available to allow practitioners to make more informed model and\ndataset choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Speech Recognition Decoding via Layer Aggregation. (arXiv:2203.11325v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11325","description":"<p>Recently proposed speech recognition systems are designed to predict using\nrepresentations generated by their top layers, employing greedy decoding which\nisolates each timestep from the rest of the sequence. Aiming for improved\nperformance, a beam search algorithm is frequently utilized and a language\nmodel is incorporated to assist with ranking the top candidates. In this work,\nwe experiment with several speech recognition models and find that logits\npredicted using the top layers may hamper beam search from achieving optimal\nresults. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield\nhighly confident predictions, and hypothesize that the predictions are based on\nlocal information and may not take full advantage of the information encoded in\nintermediate layers. To this end, we perform a layer analysis to reveal and\nvisualize how predictions evolve throughout the inference flow. We then propose\na prediction method that aggregates the top M layers, potentially leveraging\nuseful information encoded in intermediate layers and relaxing model\nconfidence. We showcase the effectiveness of our approach via beam search\ndecoding, conducting our experiments on Librispeech test and dev sets and\nachieving WER, and CER reduction of up to 10% and 22%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chazan_S/0/1/0/all/0/1\">Shlomo E. Chazan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Robustness of Offensive Language Classifiers. (arXiv:2203.11331v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11331","description":"<p>Social media platforms are deploying machine learning based offensive\nlanguage classification systems to combat hateful, racist, and other forms of\noffensive speech at scale. However, despite their real-world deployment, we do\nnot yet comprehensively understand the extent to which offensive language\nclassifiers are robust against adversarial attacks. Prior work in this space is\nlimited to studying robustness of offensive language classifiers against\nprimitive attacks such as misspellings and extraneous spaces. To address this\ngap, we systematically analyze the robustness of state-of-the-art offensive\nlanguage classifiers against more crafty adversarial attacks that leverage\ngreedy- and attention-based word selection and context-aware embeddings for\nword replacement. Our results on multiple datasets show that these crafty\nadversarial attacks can degrade the accuracy of offensive language classifiers\nby more than 50% while also being able to preserve the readability and meaning\nof the modified text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rusert_J/0/1/0/all/0/1\">Jonathan Rusert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1\">Zubair Shafiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Padmini Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels. (arXiv:2203.11364v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11364","description":"<p>Pre-trained language models derive substantial linguistic and factual\nknowledge from the massive corpora on which they are trained, and prompt\nengineering seeks to align these models to specific tasks. Unfortunately,\nexisting prompt engineering methods require significant amounts of labeled\ndata, access to model parameters, or both. We introduce a new method for\nselecting prompt templates \\textit{without labeled examples} and\n\\textit{without direct access to the model}. Specifically, over a set of\ncandidate templates, we choose the template that maximizes the mutual\ninformation between the input and the corresponding model output. Across 8\ndatasets representing 7 distinct NLP tasks, we show that when a template has\nhigh mutual information, it also has high accuracy on the task. On the largest\nmodel, selecting prompts with our method gets 90\\% of the way from the average\nprompt accuracy to the best prompt accuracy and requires no ground truth\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sorensen_T/0/1/0/all/0/1\">Taylor Sorensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Alexander Glenn Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_K/0/1/0/all/0/1\">Kyle Jeffrey Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delorey_A/0/1/0/all/0/1\">Alexia Pauline Delorey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_M/0/1/0/all/0/1\">Mahmoud Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulda_N/0/1/0/all/0/1\">Nancy Fulda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language modeling via stochastic processes. (arXiv:2203.11370v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11370","description":"<p>Modern language models can generate high-quality short texts. However, they\noften meander or are incoherent when generating longer texts. These issues\narise from the next-token-only language modeling objective. To address these\nissues, we introduce Time Control (TC), a language model that implicitly plans\nvia a latent stochastic process. TC does this by learning a representation\nwhich maps the dynamics of how text changes in a document to the dynamics of a\nstochastic process of interest. Using this representation, the language model\ncan generate text by first implicitly generating a document plan via a\nstochastic process, and then generating text that is consistent with this\nlatent plan. Compared to domain-specific methods and fine-tuning GPT2 across a\nvariety of text domains, TC improves performance on text infilling and\ndiscourse coherence. On long text generation settings, TC preserves the text\nstructure both in terms of ordering (up to +40% better) and text length\nconsistency (up to +17% better). Human evaluators also prefer TC's output 28.6%\nmore than the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rose E Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Textual Out-of-Domain Detection without In-Domain Labels. (arXiv:2203.11396v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11396","description":"<p>In many real-world settings, machine learning models need to identify user\ninputs that are out-of-domain (OOD) so as to avoid performing wrong actions.\nThis work focuses on a challenging case of OOD detection, where no labels for\nin-domain data are accessible (e.g., no intent labels for the intent\nclassification task). To this end, we first evaluate different language model\nbased approaches that predict likelihood for a sequence of tokens. Furthermore,\nwe propose a novel representation learning based method by combining\nunsupervised clustering and contrastive learning so that better data\nrepresentations for OOD detection can be learned. Through extensive\nexperiments, we demonstrate that this method can significantly outperform\nlikelihood-based methods and can be even competitive to the state-of-the-art\nsupervised approaches with label information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection. (arXiv:2203.11399v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11399","description":"<p>A limitation of current neural dialog models is that they tend to suffer from\na lack of specificity and informativeness in generated responses, primarily due\nto dependence on training data that covers a limited variety of scenarios and\nconveys limited knowledge. One way to alleviate this issue is to extract\nrelevant knowledge from external sources at decoding time and incorporate it\ninto the dialog response. In this paper, we propose a post-hoc\nknowledge-injection technique where we first retrieve a diverse set of relevant\nknowledge snippets conditioned on both the dialog history and an initial\nresponse from an existing dialog model. We construct multiple candidate\nresponses, individually injecting each retrieved snippet into the initial\nresponse using a gradient-based decoding method, and then select the final\nresponse with an unsupervised ranking step. Our experiments in goal-oriented\nand knowledge-grounded dialog settings demonstrate that human annotators judge\nthe outputs from the proposed method to be more engaging and informative\ncompared to responses from prior dialog systems. We further show that\nknowledge-augmentation promotes success in achieving conversational goals in\nboth experimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP 2021 Shared Task: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11400","description":"<p>One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the shared task on Vietnamese MRC at the Eighth Workshop\non Vietnamese Language and Speech Processing (VLSP 2021). This task attracted\n77 participant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the shared task, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% EM and 67.43% F1-score on the private test set.\nThe Vietnamese MRC systems proposed by the top 3 teams use XLM-RoBERTa, a\npowerful pre-trained language model using the transformer architecture. The\nUIT-ViQuAD 2.0 dataset motivates more researchers to explore Vietnamese machine\nreading comprehension, question answering, and question generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tin Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective. (arXiv:2203.11401v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11401","description":"<p>Prior research has discussed and illustrated the need to consider linguistic\nnorms at the community level when studying taboo (hateful/offensive/toxic etc.)\nlanguage. However, a methodology for doing so, that is firmly founded on\ncommunity language norms is still largely absent. This can lead both to biases\nin taboo text classification and limitations in our understanding of the causes\nof bias. We propose a method to study bias in taboo classification and\nannotation where a community perspective is front and center. This is\naccomplished by using special classifiers tuned for each community's language.\nIn essence, these classifiers represent community level language norms. We use\nthese to study bias and find, for example, biases are largest against African\nAmericans (7/10 datasets and all 3 classifiers examined). In contrast to\nprevious papers we also study other communities and find, for example, strong\nbiases against South Asians. In a small scale user study we illustrate our key\nidea which is that common utterances, i.e., those with high alignment scores\nwith a community (community classifier confidence scores) are unlikely to be\nregarded taboo. Annotators who are community members contradict taboo\nclassification decisions and annotations in a majority of instances. This paper\nis a significant step toward reducing false positive taboo decisions that over\ntime harm minority communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_O/0/1/0/all/0/1\">Osama Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusert_J/0/1/0/all/0/1\">Jonathan Rusert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Padmini Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Confidence for Transformer-based Neural Machine Translation. (arXiv:2203.11413v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11413","description":"<p>Confidence estimation aims to quantify the confidence of the model\nprediction, providing an expectation of success. A well-calibrated confidence\nestimate enables accurate failure prediction and proper risk measurement when\ngiven noisy samples and out-of-distribution data in real-world settings.\nHowever, this task remains a severe challenge for neural machine translation\n(NMT), where probabilities from softmax distribution fail to describe when the\nmodel is probably mistaken. To address this problem, we propose an unsupervised\nconfidence estimate learning jointly with the training of the NMT model. We\nexplain confidence as how many hints the NMT model needs to make a correct\nprediction, and more hints indicate low confidence. Specifically, the NMT model\nis given the option to ask for hints to improve translation accuracy at the\ncost of some slight penalty. Then, we approximate their level of confidence by\ncounting the number of hints the model uses. We demonstrate that our learned\nconfidence estimate achieves high accuracy on extensive sentence/word-level\nquality estimation tasks. Analytical results verify that our confidence\nestimate can correctly assess underlying risk in two real-world scenarios: (1)\ndiscovering noisy samples and (2) detecting out-of-domain data. We further\npropose a novel confidence-based instance-specific label smoothing approach\nbased on our learned confidence estimate, which outperforms standard label\nsmoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Abstractive Grounded Summarization of Podcast Transcripts. (arXiv:2203.11425v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11425","description":"<p>Podcasts have recently shown a rapid rise in popularity. Summarization of\npodcast transcripts is of practical benefit to both content providers and\nconsumers. It helps consumers to quickly decide whether they will listen to the\npodcasts and reduces the cognitive load of content providers to write\nsummaries. Nevertheless, podcast summarization faces significant challenges\nincluding factual inconsistencies with respect to the inputs. The problem is\nexacerbated by speech disfluencies and recognition errors in transcripts of\nspoken language. In this paper, we explore a novel abstractive summarization\nmethod to alleviate these challenges. Specifically, our approach learns to\nproduce an abstractive summary while grounding summary segments in specific\nportions of the transcript to allow for full inspection of summary details. We\nconduct a series of analyses of the proposed approach on a large podcast\ndataset and show that the approach can achieve promising results. Grounded\nsummaries bring clear benefits in locating the summary and transcript segments\nthat contain inconsistent information, and hence significantly improve\nsummarization quality in both automatic and human evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-guided Disentangled Tuning for Pretrained Language Models. (arXiv:2203.11431v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11431","description":"<p>Pretrained language models (PLMs) trained on large-scale unlabeled corpus are\ntypically fine-tuned on task-specific downstream datasets, which have produced\nstate-of-the-art results on various NLP tasks. However, the data discrepancy\nissue in domain and scale makes fine-tuning fail to efficiently capture\ntask-specific patterns, especially in the low data regime. To address this\nissue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which\nenhances the generalization of representations by disentangling task-relevant\nsignals from the entangled representations. For a given task, we introduce a\nlearnable confidence model to detect indicative guidance from context, and\nfurther propose a disentangled regularization to mitigate the over-reliance\nproblem. Experimental results on GLUE and CLUE benchmarks show that TDT gives\nconsistently better results than fine-tuning with different PLMs, and extensive\nanalysis demonstrates the effectiveness and robustness of our method. Code is\navailable at https://github.com/lemon0830/TDT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yufan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demo of the Linguistic Field Data Management and Analysis System -- LiFE. (arXiv:2203.11443v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11443","description":"<p>In the proposed demo, we will present a new software - Linguistic Field Data\nManagement and Analysis System - LiFE (https://github.com/kmi-linguistics/life)\n- an open-source, web-based linguistic data management and analysis application\nthat allows for systematic storage, management, sharing and usage of linguistic\ndata collected from the field. The application allows users to store lexical\nitems, sentences, paragraphs, audio-visual content with rich glossing /\nannotation; generate interactive and print dictionaries; and also train and use\nnatural language processing tools and models for various purposes using this\ndata. Since its a web-based application, it also allows for seamless\ncollaboration among multiple persons and sharing the data, models, etc with\neach other.\n</p>\n<p>The system uses the Python-based Flask framework and MongoDB in the backend\nand HTML, CSS and Javascript at the frontend. The interface allows creation of\nmultiple projects that could be shared with the other users. At the backend,\nthe application stores the data in RDF format so as to allow its release as\nLinked Data over the web using semantic web technologies - as of now it makes\nuse of the OntoLex-Lemon for storing the lexical data and Ligt for storing the\ninterlinear glossed text and then internally linking it to the other linked\nlexicons and databases such as DBpedia and WordNet. Furthermore it provides\nsupport for training the NLP systems using scikit-learn and HuggingFace\nTransformers libraries as well as make use of any model trained using these\nlibraries - while the user interface itself provides limited options for tuning\nthe system, an externally-trained model could be easily incorporated within the\napplication; similarly the dataset itself could be easily exported into a\nstandard machine-readable format like JSON or CSV that could be consumed by\nother programs and pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratan_S/0/1/0/all/0/1\">Shyam Ratan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sonal Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11476","description":"<p>Human speakers encode information into raw speech which is then decoded by\nthe listeners. This complex relationship between encoding (production) and\ndecoding (perception) is often modeled separately. Here, we test how decoding\nof lexical and sublexical semantic information can emerge automatically from\nraw speech in unsupervised generative deep convolutional networks that combine\nboth the production and perception principle. We introduce, to our knowledge,\nthe most challenging objective in unsupervised lexical learning: an\nunsupervised network that must learn to assign unique representations for\nlexical items with no direct access to training data. We train several models\n(ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic\nlexical items in the unobserved test data. Strong evidence in favor of lexical\nlearning emerges. The architecture that combines the production and perception\nprinciples is thus able to learn to decode unique information from raw acoustic\ndata in an unsupervised manner without ever accessing real training data. We\npropose a technique to explore lexical and sublexical learned representations\nin the classifier network. The results bear implications for both unsupervised\nspeech synthesis and recognition as well as for unsupervised semantic modeling\nas language models increasingly bypass text and operate from raw acoustics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_Z/0/1/0/all/0/1\">Zhao Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahong_L/0/1/0/all/0/1\">Leng Jiahong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanyu_Z/0/1/0/all/0/1\">Zhao Hanyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_T/0/1/0/all/0/1\">Tang Jie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approaches for Improving the Performance of Fake News Detection in Bangla: Imbalance Handling and Model Stacking. (arXiv:2203.11486v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11486","description":"<p>Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Muzakker Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awosaf_Z/0/1/0/all/0/1\">Zahin Awosaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_M/0/1/0/all/0/1\">Md. Salman Hossan Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvy_A/0/1/0/all/0/1\">Abu Saleh Muhammod Alvy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morol_M/0/1/0/all/0/1\">Md. Kishor Morol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency of Multilingual Pretrained Language Models. (arXiv:2203.11552v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11552","description":"<p>Pretrained language models can be queried for factual knowledge, with\npotential applications in knowledge base acquisition and tasks that require\ninference. However, for that, we need to know how reliable this knowledge is,\nand recent work has shown that monolingual English language models lack\nconsistency when predicting factual knowledge, that is, they fill-in-the-blank\ndifferently for paraphrases describing the same fact. In this paper, we extend\nthe analysis of consistency to a multilingual setting. We introduce a resource,\nmParaRel, and investigate (i) whether multilingual language models such as\nmBERT and XLM-R are more consistent than their monolingual counterparts; and\n(ii) if such models are equally consistent across languages. We find that mBERT\nis as inconsistent as English BERT in English paraphrases, but that both mBERT\nand XLM-R exhibit a high degree of inconsistency in English and even more so\nfor all the other 45 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fierro_C/0/1/0/all/0/1\">Constanza Fierro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis. (arXiv:2203.11562v1 [cs.SD])","link":"http://arxiv.org/abs/2203.11562","description":"<p>Speech synthesis has come a long way as current text-to-speech (TTS) models\ncan now generate natural human-sounding speech. However, most of the TTS\nresearch focuses on using adult speech data and there has been very limited\nwork done on child speech synthesis. This study developed and validated a\ntraining pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models\nusing child speech datasets. This approach adopts a multispeaker TTS retuning\nworkflow to provide a transfer-learning pipeline. A publicly available child\nspeech dataset was cleaned to provide a smaller subset of approximately 19\nhours, which formed the basis of our fine-tuning experiments. Both subjective\nand objective evaluations were performed using a pretrained MOSNet for\nobjective evaluation and a novel subjective framework for mean opinion score\n(MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech\nintelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency.\nObjective evaluation using a pretrained MOSNet showed a strong correlation\nbetween real and synthetic child voices. The final trained model was able to\nsynthesize child-like speech from reference audio samples as short as 5\nseconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiwere_M/0/1/0/all/0/1\">Mariam Yiwere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigioi_D/0/1/0/all/0/1\">Dan Bigioi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucu_H/0/1/0/all/0/1\">Horia Cucu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue. (arXiv:2203.11587v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11587","description":"<p>Context modeling plays a significant role in building multi-turn dialogue\nsystems. In order to make full use of context information, systems can use\nIncomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue\ninto single-turn by merging current utterance and context information into a\nself-contained utterance. However, previous approaches ignore the intent\nconsistency between the original query and rewritten query. The detection of\nomitted or coreferred locations in the original query can be further improved.\nIn this paper, we introduce contrastive learning and multi-task learning to\njointly model the problem. Our method benefits from carefully designed\nself-supervised objectives, which act as auxiliary tasks to capture semantics\nat both sentence-level and token-level. The experiments show that our proposed\nmodel achieves state-of-the-art performance on several public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_T/0/1/0/all/0/1\">Tangjian Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Minghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation. (arXiv:2203.11591v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11591","description":"<p>Pre-training has been adopted in a few of recent works for\nVision-and-Language Navigation (VLN). However, previous pre-training methods\nfor VLN either lack the ability to predict future actions or ignore the\ntrajectory contexts, which are essential for a greedy navigation process. In\nthis work, to promote the learning of spatio-temporal visual-textual\ncorrespondence as well as the agent's capability of decision making, we propose\na novel history-and-order aware pre-training paradigm (HOP) with VLN-specific\nobjectives that exploit the past observations and support future action\nprediction. Specifically, in addition to the commonly used Masked Language\nModeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy\ntasks to model temporal order information: Trajectory Order Modeling (TOM) and\nGroup Order Modeling (GOM). Moreover, our navigation action prediction is also\nenhanced by introducing the task of Action Prediction with History (APH), which\ntakes into account the history visual perceptions. Extensive experimental\nresults on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the\neffectiveness of our proposed method compared against several state-of-the-art\nagents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yanyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Relation-Specific Representations for Few-shot Knowledge Graph Completion. (arXiv:2203.11639v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11639","description":"<p>Recent years have witnessed increasing interest in few-shot knowledge graph\ncompletion (FKGC), which aims to infer unseen query triples for a few-shot\nrelation using a handful of reference triples of the relation. The primary\nfocus of existing FKGC methods lies in learning the relation representations\nthat can reflect the common information shared by the query and reference\ntriples. To this end, these methods learn the embeddings of entities with their\ndirect neighbors, and use the concatenation of the entity embeddings as the\nrelation representations. However, the entity embeddings learned only from\ndirect neighborhoods may have low expressiveness when the entity has sparse\nneighbors or shares a common local neighborhood with other entities. Moreover,\nthe embeddings of two entities are insufficient to represent the semantic\ninformation of their relationship, especially when they have multiple\nrelations. To address these issues, we propose a Relation-Specific Context\nLearning (RSCL) framework, which exploits graph contexts of triples to capture\nthe semantic information of relations and entities simultaneously.\nSpecifically, we first extract graph contexts for each triple, which can\nprovide long-term entity-relation dependencies. To model the graph contexts, we\nthen develop a hierarchical relation-specific learner to learn global and local\nrelation-specific representations for relations by capturing contextualized\ninformation of triples and incorporating local information of entities.\nFinally, we utilize the learned representations to predict the likelihood of\nthe query triples. Experimental results on two public datasets demonstrate that\nRSCL outperforms state-of-the-art FKGC methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook. (arXiv:2203.11669v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11669","description":"<p>Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pranto_P/0/1/0/all/0/1\">Protik Bose Pranto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navid_S/0/1/0/all/0/1\">Syed Zami-Ul-Haque Navid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1\">Protik Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_G/0/1/0/all/0/1\">Gias Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1\">Anindya Iqbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation. (arXiv:2203.11670v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11670","description":"<p>Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiping Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning in Sentiment Analysis. (arXiv:2203.11702v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11702","description":"<p>Aspect-based sentiment analysis (ABSA) task aims to associate a piece of text\nwith a set of aspects and meanwhile infer their respective sentimental\npolarities. Up to now, the state-of-the-art approaches are built upon\nfine-tuning of various pre-trained language models. They commonly aim to learn\nthe aspect-specific representation in the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples.\n</p>\n<p>In this paper, we propose to jointly address aspect categorization and\naspect-based sentiment subtasks in a unified framework. Specifically, we first\nintroduce a simple but effective mechanism that collaborates the semantic and\nsyntactic information to construct auxiliary-sentences for the implicit aspect.\nThen, we encourage BERT to learn the aspect-specific representation in response\nto the automatically constructed auxiliary-sentence instead of the aspect\nitself. Finally, we empirically evaluate the performance of the proposed\nsolution by a comparative study on real benchmark datasets for both ABSA and\nTargeted-ABSA tasks. Our extensive experiments show that it consistently\nachieves state-of-the-art performance in terms of aspect categorization and\naspect-based sentiment across all datasets and the improvement margins are\nconsiderable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murtadha_A/0/1/0/all/0/1\">Ahmed Murtadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Detection, Rapidly React: Unseen Rumors Detection based on Continual Prompt-Tuning. (arXiv:2203.11720v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11720","description":"<p>Since open social platforms allow for a large and continuous flow of\nunverified information, rumors can emerge unexpectedly and spread quickly.\nHowever, existing rumor detection (RD) models often assume the same training\nand testing distributions and cannot cope with the continuously changing social\nnetwork environment. This paper proposes a Continual Prompt-Tuning RD (CPT-RD)\nframework, which avoids catastrophic forgetting of upstream tasks during\nsequential task learning and enables knowledge transfer between domain tasks.\nTo avoid forgetting, we optimize and store task-special soft-prompt for each\ndomain. Furthermore, we also propose several strategies to transfer knowledge\nof upstream tasks to deal with emergencies and a task-conditioned prompt-wise\nhypernetwork (TPHNet) to consolidate past domains, enabling bidirectional\nknowledge transfer. Finally, CPT-RD is evaluated on English and Chinese RD\ndatasets and is effective and efficient compared to state-of-the-art baselines,\nwithout data replay techniques and with only a few parameter tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yuhui Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guoyong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments. (arXiv:2203.11764v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11764","description":"<p>Building on current work on multilingual hate speech (e.g., Ousidhoum et al.\n(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present\nXTREMESPEECH, a new hate speech dataset containing 20,297 social media passages\nfrom Brazil, Germany, India and Kenya. The key novelty is that we directly\ninvolve the affected communities in collecting and annotating the data - as\nopposed to giving companies and governments control over defining and\ncombatting hate speech. This inclusive approach results in datasets more\nrepresentative of actually occurring online speech and is likely to facilitate\nthe removal of the social media content that marginalized communities view as\ncausing the most harm. Based on XTREMESPEECH, we establish novel tasks with\naccompanying baselines, provide evidence that cross-country training is\ngenerally not feasible due to cultural differences between countries and\nperform an interpretability analysis of BERT's predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisiorek_A/0/1/0/all/0/1\">Axel Wisiorek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nann_L/0/1/0/all/0/1\">Leah Nann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbar_H/0/1/0/all/0/1\">Haris Jabbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udupa_S/0/1/0/all/0/1\">Sahana Udupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuetze_H/0/1/0/all/0/1\">Hinrich Schuetze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with Entity Linking. (arXiv:2203.11841v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11841","description":"<p>This paper describes the system proposed by Sabanc{\\i} University Natural\nLanguage Processing Group in the SemEval-2022 MultiCoNER task. We developed an\nunsupervised entity linking pipeline that detects potential entity mentions\nwith the help of Wikipedia and also uses the corresponding Wikipedia context to\nhelp the classifier in finding the named entity type of that mention. Our\nresults showed that our pipeline improved performance significantly, especially\nfor complex entities in low-context settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carik_B/0/1/0/all/0/1\">Buse &#xc7;ar&#x131;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyhan_F/0/1/0/all/0/1\">Fatih Beyhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation. (arXiv:2203.11849v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11849","description":"<p>Recent advances in natural language processing have enabled powerful\nprivacy-invasive authorship attribution. To counter authorship attribution,\nresearchers have proposed a variety of rule-based and learning-based text\nobfuscation approaches. However, existing authorship obfuscation approaches do\nnot consider the adversarial threat model. Specifically, they are not evaluated\nagainst adversarially trained authorship attributors that are aware of\npotential obfuscation. To fill this gap, we investigate the problem of\nadversarial authorship attribution for deobfuscation. We show that\nadversarially trained authorship attributors are able to degrade the\neffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate\nthe effectiveness of adversarial training when the attributor makes incorrect\nassumptions about whether and which obfuscator was used. While there is a a\nclear degradation in attribution accuracy, it is noteworthy that this\ndegradation is still at or above the attribution accuracy of the attributor\nthat is not adversarially trained at all. Our results underline the need for\nstronger obfuscation approaches that are resistant to deobfuscation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wanyue Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusert_J/0/1/0/all/0/1\">Jonathan Rusert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1\">Zubair Shafiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Padmini Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Approach to Understand Mental Health from Reddit: Knowledge-aware Multitask Learning Framework. (arXiv:2203.11856v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11856","description":"<p>Analyzing gender is critical to study mental health (MH) support in CVD\n(cardiovascular disease). The existing studies on using social media for\nextracting MH symptoms consider symptom detection and tend to ignore user\ncontext, disease, or gender. The current study aims to design and evaluate a\nsystem to capture how MH symptoms associated with CVD are expressed differently\nwith the gender on social media. We observe that the reliable detection of MH\nsymptoms expressed by persons with heart disease in user posts is challenging\nbecause of the co-existence of (dis)similar MH symptoms in one post and due to\nvariation in the description of symptoms based on gender. We collect a corpus\nof $150k$ items (posts and comments) annotated using the subreddit labels and\ntransfer learning approaches. We propose GeM, a novel task-adaptive multi-task\nlearning approach to identify the MH symptoms in CVD patients based on gender.\nSpecifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to\ncapture CVD-related MH symptoms. Moreover, it enhances the reliability for\ndifferentiating the gender language in MH symptoms when compared to the\nstate-of-art language models. Our model achieves high (statistically\nsignificant) performance and predicts four labels of MH issues and two gender\nlabels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom\nidentification task and by 2.55% on the gender identification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lokala_U/0/1/0/all/0/1\">Usha Lokala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aseem Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastidar_T/0/1/0/all/0/1\">Triyasha Ghosh Dastidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akthar_M/0/1/0/all/0/1\">Md Shad Akthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panahiazar_M/0/1/0/all/0/1\">Maryam Panahiazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based ensemble for emotion detection. (arXiv:2203.11899v1 [cs.CL])","link":"http://arxiv.org/abs/2203.11899","description":"<p>Detecting emotions in languages is important to accomplish a complete\ninteraction between humans and machines. This paper describes our contribution\nto the WASSA 2022 shared task which handles this crucial task of emotion\ndetection. We have to identify the following emotions: sadness, surprise,\nneutral, anger, fear, disgust, joy based on a given essay text. We are using an\nensemble of ELECTRA and BERT models to tackle this problem achieving an F1\nscore of 62.76%. Our codebase (https://bit.ly/WASSA_shared_task) and our WandB\nproject (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirtane_N/0/1/0/all/0/1\">Neeraja Kirtane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking. (arXiv:2103.00293v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00293","description":"<p>Augmentation of task-oriented dialogues has followed standard methods used\nfor plain-text such as back-translation, word-level manipulation, and\nparaphrasing despite its richly annotated structure. In this work, we introduce\nan augmentation framework that utilizes belief state annotations to match turns\nfrom various dialogues and form new synthetic dialogues in a bottom-up manner.\nUnlike other augmentation strategies, it operates with as few as five examples.\nOur augmentation strategy yields significant improvements when both adapting a\nDST model to a new domain, and when adapting a language model to the DST task,\non evaluations with TRADE and TOD-BERT models. Further analysis shows that our\nmodel performs better on seen values during training, and it is also more\nrobust to unseen values. We conclude that exploiting belief state annotations\nenhances dialogue augmentation and results in improved models in n-shot\ntraining scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksu_T/0/1/0/all/0/1\">Taha Aksu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InFillmore: Frame-Guided Language Generation with Bidirectional Context. (arXiv:2103.04941v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.04941","description":"<p>We propose a structured extension to bidirectional-context conditional\nlanguage generation, or \"infilling,\" inspired by Frame Semantic theory\n(Fillmore, 1976). Guidance is provided through two approaches: (1) model\nfine-tuning, conditioning directly on observed symbolic frames, and (2) a novel\nextension to disjunctive lexically constrained decoding that leverages frame\nsemantic lexical units. Automatic and human evaluations confirm that\nframe-guided generation allows for explicit manipulation of intended infill\nsemantics, with minimal loss in distinguishability from human-generated text.\nOur methods flexibly apply to a variety of use scenarios, and we provide a\ncodebase and interactive demo available from\nhttps://nlp.jhu.edu/demos/infillmore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiefu Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1\">Anton Belyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey. (arXiv:2104.06951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06951","description":"<p>The development of deep learning techniques has allowed Neural Machine\nTranslation (NMT) models to become extremely powerful, given sufficient\ntraining data and training time. However, systems struggle when translating\ntext from a new domain with a distinct style or vocabulary. Fine-tuning on\nin-domain data allows good domain adaptation, but requires sufficient relevant\nbilingual data. Even if this is available, simple fine-tuning can cause\noverfitting to new data and `catastrophic forgetting' of previously learned\nbehaviour.\n</p>\n<p>We concentrate on robust approaches to domain adaptation for NMT,\nparticularly where a system may need to translate across multiple domains. We\ndivide techniques into those revolving around data selection or generation,\nmodel architecture, parameter adaptation procedure, and inference procedure. We\nfinally highlight the benefits of domain adaptation and multi-domain adaptation\ntechniques to other lines of NMT research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_D/0/1/0/all/0/1\">Danielle Saunders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08790","description":"<p>Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.\nfeeling distrust), and behaviorally (e.g. sharing the news with their friends).\nSuch reactions are instantaneous and yet complex, as they rely on factors that\ngo beyond interpreting factual content of news. We propose Misinfo Reaction\nFrames (MRF), a pragmatic formalism for modeling how readers might react to a\nnews headline. In contrast to categorical schema, our free-text dimensions\nprovide a more nuanced way of understanding intent beyond being benign or\nmalicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced\ndataset of reactions to over 25k news headlines focusing on global crises: the\nCovid-19 pandemic, climate change, and cancer. Empirical results confirm that\nit is indeed possible for neural models to predict the prominent patterns of\nreaders' reactions to previously unseen news headlines. Additionally, our user\nstudy shows that displaying machine-generated MRF implications alongside news\nheadlines to readers can increase their trust in real news while decreasing\ntheir trust in misinformation. Our work demonstrates the feasibility and\nimportance of pragmatic inferences on news headlines to help enhance AI-guided\nmisinformation detection and mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pemi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roesner_F/0/1/0/all/0/1\">Franziska Roesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trakhtenbrot's Theorem in Coq: Finite Model Theory through the Constructive Lens. (arXiv:2104.14445v4 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2104.14445","description":"<p>We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. To showcase an application of Trakhtenbrot's theorem, we continue\nour reduction chain with a many-one reduction from FSAT to separation logic.\nAll our results are mechanised in the framework of a growing Coq library of\nsynthetic undecidability proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirst_D/0/1/0/all/0/1\">Dominik Kirst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larchey_Wendling_D/0/1/0/all/0/1\">Dominique Larchey-Wendling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03842","description":"<p>Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning. (arXiv:2105.05557v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05557","description":"<p>Open pit mines left many regions worldwide inhospitable or uninhabitable. To\nput these regions back into use, entire stretches of land must be\nrenaturalized. For the sustainable subsequent use or transfer to a new primary\nuse, many contaminated sites and soil information have to be permanently\nmanaged. In most cases, this information is available in the form of expert\nreports in unstructured data collections or file folders, which in the best\ncase are digitized. Due to size and complexity of the data, it is difficult for\na single person to have an overview of this data in order to be able to make\nreliable statements. This is one of the most important obstacles to the rapid\ntransfer of these areas to after-use. An information-based approach to this\nissue supports fulfilling several Sustainable Development Goals regarding\nenvironment issues, health and climate action. We use a stack of Optical\nCharacter Recognition, Text Classification, Active Learning and Geographic\nInformation System Visualization to effectively mine and visualize this\ninformation. Subsequently, we link the extracted information to geographic\ncoordinates and visualize them using a Geographic Information System. Active\nLearning plays a vital role because our dataset provides no training data. In\ntotal, we process nine categories and actively learn their representation in\nour dataset. We evaluate the OCR, Active Learning and Text Classification\nseparately to report the performance of the system. Active Learning and text\nclassification results are twofold: Whereas our categories about restrictions\nwork sufficient ($&gt;$.85 F1), the seven topic-oriented categories were\ncomplicated for human coders and hence the results achieved mediocre evaluation\nscores ($&lt;$.70 F1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgl_K/0/1/0/all/0/1\">Kim B&#xfc;rgl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annanias_Y/0/1/0/all/0/1\">Yves Annanias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_D/0/1/0/all/0/1\">Daniel Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bender_C/0/1/0/all/0/1\">Christian Bender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mengs_C/0/1/0/all/0/1\">Christoph Mengs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheuermann_G/0/1/0/all/0/1\">Gerik Scheuermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heyer_G/0/1/0/all/0/1\">Gerhard Heyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.16163","description":"<p>Experiments with pre-trained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact tested in the\nexperiment (i.e., the particular instance of the model), it is not always clear\nwhether they hold for the more general procedure which includes the\narchitecture, training data, initialization scheme, and loss function. Recent\nwork has shown that repeating the pre-training process can lead to\nsubstantially different performance, suggesting that an alternate strategy is\nneeded to make principled statements about procedures. To enable researchers to\ndraw more robust conclusions, we introduce the MultiBERTs, a set of 25\nBERT-Base checkpoints, trained with similar hyper-parameters as the original\nBERT model but differing in random weight initialization and shuffling of\ntraining data. We also define the Multi-Bootstrap, a non-parametric bootstrap\nmethod for statistical inference designed for settings where there are multiple\npre-trained models and limited test data. To illustrate our approach, we\npresent a case study of gender bias in coreference resolution, in which the\nMulti-Bootstrap lets us measure effects that may not be detected with a single\ncheckpoint. We release our models and statistical library along with an\nadditional set of 140 intermediate checkpoints captured during pre-training to\nfacilitate research on learning dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1\">Alexander D&#x27;Amour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct speech-to-speech translation with discrete units. (arXiv:2107.05604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05604","description":"<p>We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. We tackle the problem by first applying a\nself-supervised discrete speech encoder on the target speech and then training\na sequence-to-sequence speech-to-unit translation (S2UT) model to predict the\ndiscrete representations of the target speech. When target text transcripts are\navailable, we design a joint speech and text training framework that enables\nthe model to generate dual modality output (speech and text) simultaneously in\nthe same inference pass. Experiments on the Fisher Spanish-English dataset show\nthat the proposed framework yields improvement of 6.7 BLEU compared with a\nbaseline direct S2ST model that predicts spectrogram features. When trained\nwithout any text transcripts, our model performance is comparable to models\nthat predict spectrograms and are trained with text supervision, showing the\npotential of our system for translation between unwritten languages. Audio\nsamples are available at\nhttps://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning When to Translate for Streaming Speech. (arXiv:2109.07368v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07368","description":"<p>How to find proper moments to generate partial sentence translation given a\nstreaming speech input? Existing approaches waiting-and-translating for a fixed\nduration often break the acoustic units in speech, since the boundaries between\nacoustic units in speech are not even. In this paper, we propose MoSST, a\nsimple yet effective method for translating streaming speech content. Given a\nusually long speech sequence, we develop an efficient monotonic segmentation\nmodule inside an encoder-decoder model to accumulate acoustic information\nincrementally and detect proper speech unit boundaries for the input in speech\ntranslation task. Experiments on multiple translation directions of the MuST-C\ndataset show that MoSST outperforms existing methods and achieves the best\ntrade-off between translation quality (BLEU) and latency. Our code is available\nat https://github.com/dqqcasia/mosst.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10274","description":"<p>This work connects language model adaptation with concepts of machine\nlearning theory. We consider a training setup with a large out-of-domain set\nand a small in-domain set. We derive how the benefit of training a model on\neither set depends on the size of the sets and the distance between their\nunderlying distributions. We analyze how out-of-domain pre-training before\nin-domain fine-tuning achieves better generalization than either solution\nindependently. Finally, we present how adaptation techniques based on data\nselection, such as importance sampling, intelligent data selection and\ninfluence functions, can be presented in a common framework which highlights\ntheir similarity and also their subtle differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04655","description":"<p>There is mounting evidence that existing neural network models, in particular\nthe very popular sequence-to-sequence architecture, struggle to systematically\ngeneralize to unseen compositions of seen components. We demonstrate that one\nof the reasons hindering compositional generalization relates to\nrepresentations being entangled. We propose an extension to\nsequence-to-sequence models which encourages disentanglement by adaptively\nre-encoding (at each time step) the source input. Specifically, we condition\nthe source representations on the newly decoded target context which makes it\neasier for the encoder to exploit specialized information for each prediction\nrather than capturing it all in a single forward pass. Experimental results on\nsemantic parsing and machine translation empirically show that our proposal\ndelivers more disentangled representations and better generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07476","description":"<p>Event extraction is typically modeled as a multi-class classification problem\nwhere event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that uses event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\nACE and ERE demonstrate that our approach achieves state-of-the-art performance\non each dataset and significantly outperforms existing methods on zero-shot\nevent extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models. (arXiv:2110.08173v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08173","description":"<p>Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_C/0/1/0/all/0/1\">Charlotte Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C). (arXiv:2110.10780v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10780","description":"<p>While we pay attention to the latest advances in clinical natural language\nprocessing (NLP), we can notice some resistance in the clinical and\ntranslational research community to adopt NLP models due to limited\ntransparency, interpretability, and usability. In this study, we proposed an\nopen natural language processing development framework. We evaluated it through\nthe implementation of NLP algorithms for the National COVID Cohort\nCollaborative (N3C). Based on the interests in information extraction from\nCOVID-19 related clinical notes, our work includes 1) an open data annotation\nprocess using COVID-19 signs and symptoms as the use case, 2) a\ncommunity-driven ruleset composing platform, and 3) a synthetic text data\ngeneration workflow to generate texts for information extraction tasks without\ninvolving human subjects. The corpora were derived from texts from three\ndifferent institutions (Mayo Clinic, University of Kentucky, University of\nMinnesota). The gold standard annotations were tested with a single\ninstitution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,\nand 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,\nrespectively. The study as a consortium effort of the N3C NLP subgroup\ndemonstrates the feasibility of creating a federated NLP algorithm development\nand benchmarking platform to enhance multi-institution clinical NLP study and\nadoption. Although we use COVID-19 as a use case in this effort, our framework\nis general enough to be applied to other domains of interest in clinical NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1\">Andrew Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Sunyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1\">Robert Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Andrew Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_D/0/1/0/all/0/1\">Daniel Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_el_rub_N/0/1/0/all/0/1\">Noor Abu-el-rub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1\">Dalton Schutte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_M/0/1/0/all/0/1\">Masoud Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1\">John D. Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_U/0/1/0/all/0/1\">Umit Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Stephanie S Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H Saltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffter_T/0/1/0/all/0/1\">Thomas Schaffter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfaff_E/0/1/0/all/0/1\">Emily Pfaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chute_C/0/1/0/all/0/1\">Christopher G. Chute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Tim Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1\">Melissa A. Haendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_R/0/1/0/all/0/1\">Rafael Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szolovits_P/0/1/0/all/0/1\">Peter Szolovits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collaborative_N/0/1/0/all/0/1\">National COVID Cohort Collaborative</a> (N3C) <a href=\"http://arxiv.org/find/cs/1/au:+Processing_N/0/1/0/all/0/1\">Natural Language Processing</a> (NLP) <a href=\"http://arxiv.org/find/cs/1/au:+Subgroup/0/1/0/all/0/1\">Subgroup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collaborative_N/0/1/0/all/0/1\">National COVID Cohort Collaborative</a> (N3C)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Review-based Recommenders. (arXiv:2110.14747v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2110.14747","description":"<p>Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Ramses J. Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">Cesar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ditch the Gold Standard: Re-evaluating Conversational Question Answering. (arXiv:2112.08812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08812","description":"<p>Conversational question answering aims to provide natural-language answers to\nusers in information-seeking conversations. Existing conversational QA\nbenchmarks compare models with pre-collected human-human conversations, using\nground-truth answers provided in conversational history. It remains unclear\nwhether we can rely on this static evaluation for model development and whether\ncurrent systems can well generalize to real-world human-machine conversations.\nIn this work, we conduct the first large-scale human evaluation of\nstate-of-the-art conversational QA systems, where human evaluators converse\nwith models and judge the correctness of their answers. We find that the\ndistribution of human machine conversations differs drastically from that of\nhuman-human conversations, and there is a disagreement between human and\ngold-history evaluation in terms of model ranking. We further investigate how\nto improve automatic evaluations, and propose a question rewriting mechanism\nbased on predicted history, which better correlates with human judgments.\nFinally, we analyze the impact of various modeling strategies and discuss\nfuture directions towards building better conversational question answering\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goenka_M/0/1/0/all/0/1\">Manan Goenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03086","description":"<p>Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coster_M/0/1/0/all/0/1\">Mathieu De Coster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreweghe_M/0/1/0/all/0/1\">Mieke Van Herreweghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing. (arXiv:2202.09950v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.09950","description":"<p>The text-based speech editor allows the editing of speech through intuitive\ncutting, copying, and pasting operations to speed up the process of editing\nspeech. However, the major drawback of current systems is that edited speech\noften sounds unnatural due to cut-copy-paste operation. In addition, it is not\nobvious how to synthesize records according to a new word not appearing in the\ntranscript. This paper proposes a novel end-to-end text-based speech editing\nmethod called context-aware mask prediction network (CampNet). The model can\nsimulate the text-based speech editing process by randomly masking part of\nspeech and then predicting the masked region by sensing the speech context. It\ncan solve unnatural prosody in the edited region and synthesize the speech\ncorresponding to the unseen words in the transcript. Secondly, for the possible\noperation of text-based speech editing, we design three text-based operations\nbased on CampNet: deletion, insertion, and replacement. These operations can\ncover various situations of speech editing. Thirdly, to synthesize the speech\ncorresponding to long text in insertion and replacement operations, a\nword-level autoregressive generation method is proposed. Fourthly, we propose a\nspeaker adaptation method using only one sentence for CampNet and explore the\nability of few-shot learning based on CampNet, which provides a new idea for\nspeech forgery tasks. The subjective and objective experiments on VCTK and\nLibriTTS datasets show that the speech editing results based on CampNet are\nbetter than TTS technology, manual editing, and VoCo method. We also conduct\ndetailed ablation experiments to explore the effect of the CampNet structure on\nits performance. Finally, the experiment shows that speaker adaptation with\nonly one sentence can further improve the naturalness of speech. Examples of\ngenerated speech can be found at https://hairuo55.github.io/CampNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.12163","description":"<p>In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, a\nsimple domain adaptation mechanism is introduced to allow adapting an existing\nlanguage identification model to a new domain where the prior language\ndistribution is different. We perform a comparative study of different model\ntopologies under different constraints of model size, and find that\nconformer-base models outperform LSTM and transformer based models. Our\nexperiments also show that attentive temporal pooling and domain adaptation\nsignificantly improve the model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07836","description":"<p>Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References. (arXiv:2203.08928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08928","description":"<p>We consider the problem of pretraining a two-stage open-domain question\nanswering (QA) system (retriever + reader) with strong transfer capabilities.\nThe key challenge is how to construct a large amount of high-quality\nquestion-answer-context triplets without task-specific annotations.\nSpecifically, the triplets should align well with downstream tasks by: (i)\ncovering a wide range of domains (for open-domain applications), (ii) linking a\nquestion to its semantically relevant context with supporting evidence (for\ntraining the retriever), and (iii) identifying the correct answer in the\ncontext (for training the reader). Previous pretraining approaches generally\nfall short of one or more of these requirements. In this work, we automatically\nconstruct a large-scale corpus that meets all three criteria by consulting\nmillions of references cited within Wikipedia. The well-aligned pretraining\nsignals benefit both the retriever and the reader significantly. Our pretrained\nretriever leads to 2%-10% absolute gains in top-20 accuracy. And with our\npretrained reader, the entire system improves by up to 4% in exact match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Dual Read/Write Paths for Simultaneous Machine Translation. (arXiv:2203.09163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09163","description":"<p>Simultaneous machine translation (SiMT) outputs translation while reading\nsource sentence and hence requires a policy to decide whether to wait for the\nnext source word (READ) or generate a target word (WRITE), the actions of which\nform a read/write path. Although the read/write path is essential to SiMT\nperformance, no direct supervision is given to the path in the existing\nmethods. In this paper, we propose a method of dual-path SiMT which introduces\nduality constraints to direct the read/write path. According to duality\nconstraints, the read/write path in source-to-target and target-to-source SiMT\nmodels can be mapped to each other. As a result, the two SiMT models can be\noptimized jointly by forcing their read/write paths to satisfy the mapping.\nExperiments on En-Vi and De-En tasks show that our method can outperform strong\nbaselines under all latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models. (arXiv:2203.10326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10326","description":"<p>We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design artificial\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with an artificial language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTREME-S: Evaluating Cross-lingual Speech Representations. (arXiv:2203.10752v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10752","description":"<p>We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible at\nhttps://hf.co/datasets/google/xtreme_s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozhkov_A/0/1/0/all/0/1\">Anton Lozhkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan Van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2203.10430","description":"<p>Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ren Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. (arXiv:2203.11205v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11205","description":"<p>Mammography, or breast X-ray, is the most widely used imaging modality to\ndetect cancer and other breast diseases. Recent studies have shown that deep\nlearning-based computer-assisted detection and diagnosis (CADe or CADx) tools\nhave been developed to support physicians and improve the accuracy of\ninterpreting mammography. However, most published datasets of mammography are\neither limited on sample size or digitalized from screen-film mammography\n(SFM), hindering the development of CADe and CADx tools which are developed\nbased on full-field digital mammography (FFDM). To overcome this challenge, we\nintroduce VinDr-Mammo - a new benchmark dataset of FFDM for detecting and\ndiagnosing breast cancer and other diseases in mammography. The dataset\nconsists of 5,000 mammography exams, each of which has four standard views and\nis double read with disagreement (if any) being resolved by arbitration. It is\ncreated for the assessment of Breast Imaging Reporting and Data System\n(BI-RADS) and density at the breast level. In addition, the dataset also\nprovides the category, location, and BI-RADS assessment of non-benign findings.\nWe make VinDr-Mammo publicly available on PhysioNet as a new imaging resource\nto promote advances in developing CADe and CADx tools for breast cancer\nscreening.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_K/0/1/0/all/0/1\">Khanh Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_L/0/1/0/all/0/1\">Linh T. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1\">Minh Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">Van Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling. (arXiv:2203.11206v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11206","description":"<p>A fully automated system for interpreting abdominal computed tomography (CT)\nscans with multiple phases of contrast enhancement requires an accurate\nclassification of the phases. This work aims at developing and validating a\nprecise, fast multi-phase classifier to recognize three main types of contrast\nphases in abdominal CT scans. We propose in this study a novel method that uses\na random sampling mechanism on top of deep CNNs for the phase recognition of\nabdominal CT scans of four different phases: non-contrast, arterial, venous,\nand others. The CNNs work as a slice-wise phase prediction, while the random\nsampling selects input slices for the CNN models. Afterward, majority voting\nsynthesizes the slice-wise results of the CNNs, to provide the final prediction\nat scan level. Our classifier was trained on 271,426 slices from 830\nphase-annotated CT scans, and when combined with majority voting on 30% of\nslices randomly chosen from each scan, achieved a mean F1-score of 92.09% on\nour internal test set of 358 scans. The proposed method was also evaluated on 2\nexternal test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were\nannotated by our experts. Although a drop in performance has been observed, the\nmodel performance remained at a high level of accuracy with a mean F1-score of\n76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our\nexperimental results also showed that the proposed method significantly\noutperformed the state-of-the-art 3D approaches while requiring less\ncomputation time for inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dao_B/0/1/0/all/0/1\">Binh T. Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thang V. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging. (arXiv:2203.11209v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11209","description":"<p>The importance of plastic waste recycling is undeniable. In this respect,\ncomputer vision and deep learning enable solutions through the automated\nanalysis of short-wave-infrared hyper-spectral images of plastics. In this\npaper, we offer an exhaustive empirical study to show the importance of\nefficient model selection for resolving the task of hyper-spectral image\nsegmentation of various plastic flakes using deep learning. We assess the\ncomplexity level of generic and specialized models and infer their performance\ncapacity: generic models are often unnecessarily complex. We introduce two\nvariants of a specialized hyper-spectral architecture, PlasticNet, that\noutperforms several well-known segmentation architectures in both performance\nas well as computational complexity. In addition, we shed lights on the\nsignificance of signal pre-processing within the realm of hyper-spectral\nimaging. To complete our contribution, we introduce the largest, most versatile\nhyper-spectral dataset of plastic flakes of four primary polymer types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dijkstra_K/0/1/0/all/0/1\">Klaas Dijkstra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaarsma_F/0/1/0/all/0/1\">Femke Jaarsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkstra_M/0/1/0/all/0/1\">Martin Dijkstra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folkersma_R/0/1/0/all/0/1\">Rudy Folkersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_J/0/1/0/all/0/1\">Jan Jager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1\">Jaap van de Loosdrecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer. (arXiv:2203.11210v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11210","description":"<p>An effective way to model the complex real world is to view the world as a\ncomposition of basic components of objects and transformations. Although humans\nthrough development understand the compositionality of the real world, it is\nextremely difficult to equip robots with such a learning mechanism. In recent\nyears, there has been significant research on autonomously learning\nrepresentations of the world using the deep learning; however, most studies\nhave taken a statistical approach, which requires a large number of training\ndata. Contrary to such existing methods, we take a novel algebraic approach for\nrepresentation learning based on a simpler and more intuitive formulation that\nthe observed world is the combination of multiple independent patterns and\ntransformations that are invariant to the shape of patterns. Since the shape of\npatterns can be viewed as the invariant features against symmetric\ntransformations such as translation or rotation, we can expect that the\npatterns can naturally be extracted by expressing transformations with\nsymmetric Lie group transformers and attempting to reconstruct the scene with\nthem. Based on this idea, we propose a model that disentangles the scenes into\nthe minimum number of basic components of patterns and Lie transformations from\nonly one sequence of images, by introducing the learnable shape-invariant Lie\ngroup transformers as transformation components. Experiments show that given\none sequence of images in which two objects are moving independently, the\nproposed model can discover the hidden distinct objects and multiple\nshape-invariant transformations that constitute the scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takada_T/0/1/0/all/0/1\">T. Takada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimaya_W/0/1/0/all/0/1\">W. Shimaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmura_Y/0/1/0/all/0/1\">Y. Ohmura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1\">Y. Kuniyoshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation. (arXiv:2203.11213v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11213","description":"<p>Glioma is the most common and aggressive brain tumor. Magnetic resonance\nimaging (MRI) plays a vital role to evaluate tumors for the arrangement of\ntumor surgery and the treatment of subsequent procedures. However, the manual\nsegmentation of the MRI image is strenuous, which limits its clinical\napplication. With the development of deep learning, a large number of automatic\nsegmentation methods have been developed, but most of them stay in 2D images,\nwhich leads to subpar performance. Moreover, the serious voxel imbalance\nbetween the brain tumor and the background as well as the different sizes and\nlocations of the brain tumor makes the segmentation of 3D images a challenging\nproblem. Aiming at segmenting 3D MRI, we propose a model for brain tumor\nsegmentation with multiple encoders. The structure contains four encoders and\none decoder. The four encoders correspond to the four modalities of the MRI\nimage, perform one-to-one feature extraction, and then merge the feature maps\nof the four modalities into the decoder. This method reduces the difficulty of\nfeature extraction and greatly improves model performance. We also introduced a\nnew loss function named \"Categorical Dice\", and set different weights for\ndifferent segmented regions at the same time, which solved the problem of voxel\nimbalance. We evaluated our approach using the online BraTS 2020 Challenge\nverification. Our proposed method can achieve promising results in the\nvalidation set compared to the state-of-the-art approaches with Dice scores of\n0.70249, 0.88267, and 0.73864 for the intact tumor, tumor core, and enhanced\ntumor, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Weiji Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkai Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_X/0/1/0/all/0/1\">Xiaobo Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11242","description":"<p>In the last few years, there have been several revolutions in the field of\ndeep learning, mainly headlined by the large impact of Generative Adversarial\nNetworks (GANs). GANs not only provide an unique architecture when defining\ntheir models, but also generate incredible results which have had a direct\nimpact on society. Due to the significant improvements and new areas of\nresearch that GANs have brought, the community is constantly coming up with new\nresearches that make it almost impossible to keep up with the times. Our survey\naims to provide a general overview of GANs, showing the latest architectures,\noptimizations of the loss functions, validation metrics and application areas\nof the most widely recognized variants. The efficiency of the different\nvariants of the model architecture will be evaluated, as well as showing the\nbest application area; as a vital part of the process, the different metrics\nfor evaluating the performance of GANs and the frequently used loss functions\nwill be analyzed. The final objective of this survey is to provide a summary of\nthe evolution and performance of the GANs which are having better results to\nguide future researchers in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_G/0/1/0/all/0/1\">Guillermo Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Edgar Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Alvarez_A/0/1/0/all/0/1\">Alberto D&#xed;az-&#xc1;lvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contribution of Different Handwriting Modalities to Differential Diagnosis of Parkinson's Disease. (arXiv:2203.11269v1 [eess.SP])","link":"http://arxiv.org/abs/2203.11269","description":"<p>In this paper, we evaluate the contribution of different handwriting\nmodalities to the diagnosis of Parkinson's disease. We analyse on-surface\nmovement, in-air movement and pressure exerted on the tablet surface.\nEspecially in-air movement and pressure-based features have been rarely taken\ninto account in previous studies. We show that pressure and in-air movement\nalso possess information that is relevant for the diagnosis of Parkinson's\nDisease (PD) from handwriting. In addition to the conventional kinematic and\nspatio-temporal features, we present a group of the novel features based on\nentropy and empirical mode decomposition of the handwriting signal. The\npresented results indicate that handwriting can be used as biomarker for PD\nproviding classification performance around 89% area under the ROC curve (AUC)\nfor PD classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drot&#xe1;r</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mekyska_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Mekyska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smekal_Z/0/1/0/all/0/1\">Zden&#x11b;k Sm&#xe9;kal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rektorova_I/0/1/0/all/0/1\">Irena Rektorov&#xe1;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masarova_L/0/1/0/all/0/1\">Lucia Masarov&#xe1;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction. (arXiv:2203.11283v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11283","description":"<p>While NeRF has shown great success for neural reconstruction and rendering,\nits limited MLP capacity and long per-scene optimization times make it\nchallenging to model large-scale indoor scenes. In contrast, classical 3D\nreconstruction methods can handle large-scale scenes but do not produce\nrealistic renderings. We propose NeRFusion, a method that combines the\nadvantages of NeRF and TSDF-based fusion techniques to achieve efficient\nlarge-scale reconstruction and photo-realistic rendering. We process the input\nimage sequence to predict per-frame local radiance fields via direct network\ninference. These are then fused using a novel recurrent neural network that\nincrementally reconstructs a global, sparse scene representation in real-time\nat 22 fps. This global volume can be further fine-tuned to boost rendering\nquality. We demonstrate that NeRFusion achieves state-of-the-art quality on\nboth large-scale indoor and small-scale object scenes, with substantially\nfaster reconstruction than NeRF and other recent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoshuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Objective for Learning Disentangled Representations. (arXiv:2203.11284v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11284","description":"<p>Learning representations of images that are invariant to sensitive or\nunwanted attributes is important for many tasks including bias removal and\ncross domain retrieval. Here, our objective is to learn representations that\nare invariant to the domain (sensitive attribute) for which labels are\nprovided, while being informative over all other image attributes, which are\nunlabeled. We present a new approach, proposing a new domain-wise contrastive\nobjective for ensuring invariant representations. This objective crucially\nrestricts negative image pairs to be drawn from the same domain, which enforces\ndomain invariance whereas the standard contrastive objective does not. This\ndomain-wise objective is insufficient on its own as it suffers from shortcut\nsolutions resulting in feature suppression. We overcome this issue by a\ncombination of a reconstruction constraint, image augmentations and\ninitialization with pre-trained weights. Our analysis shows that the choice of\naugmentations is important, and that a misguided choice of augmentations can\nharm the invariance and informativeness objectives. In an extensive evaluation,\nour method convincingly outperforms the state-of-the-art in terms of\nrepresentation invariance, representation informativeness, and training speed.\nFurthermore, we find that in some cases our method can achieve excellent\nresults even without the reconstruction constraint, leading to a much faster\nand resource efficient training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahana_J/0/1/0/all/0/1\">Jonathan Kahana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Network for Future Hand Segmentation from Egocentric Video. (arXiv:2203.11305v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11305","description":"<p>We introduce the novel problem of anticipating a time series of future hand\nmasks from egocentric video. A key challenge is to model the stochasticity of\nfuture head motions, which globally impact the head-worn camera video analysis.\nTo this end, we propose a novel deep generative model -- EgoGAN, which uses a\n3D Fully Convolutional Network to learn a spatio-temporal video representation\nfor pixel-wise visual anticipation, generates future head motion using\nGenerative Adversarial Network (GAN), and then predicts the future hand masks\nbased on the video representation and the generated future head motion. We\nevaluate our method on both the EPIC-Kitchens and the EGTEA Gaze+ datasets. We\nconduct detailed ablation studies to validate the design choices of our\napproach. Furthermore, we compare our method with previous state-of-the-art\nmethods on future image segmentation and show that our method can more\naccurately predict future hand masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenqi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Quantised Neural Networks with STE Variants: the Additive Noise Annealing Algorithm. (arXiv:2203.11323v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11323","description":"<p>Training quantised neural networks (QNNs) is a non-differentiable\noptimisation problem since weights and features are output by piecewise\nconstant functions. The standard solution is to apply the straight-through\nestimator (STE), using different functions during the inference and gradient\ncomputation steps. Several STE variants have been proposed in the literature\naiming to maximise the task accuracy of the trained network. In this paper, we\nanalyse STE variants and study their impact on QNN training. We first observe\nthat most such variants can be modelled as stochastic regularisations of stair\nfunctions; although this intuitive interpretation is not new, our rigorous\ndiscussion generalises to further variants. Then, we analyse QNNs mixing\ndifferent regularisations, finding that some suitably synchronised smoothing of\neach layer map is required to guarantee pointwise compositional convergence to\nthe target discontinuous function. Based on these theoretical insights, we\npropose additive noise annealing (ANA), a new algorithm to train QNNs\nencompassing standard STE and its variants as special cases. When testing ANA\non the CIFAR-10 image classification benchmark, we find that the major impact\non task accuracy is not due to the qualitative shape of the regularisations but\nto the proper synchronisation of the different STE variants used in a network,\nin accordance with the theoretical results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spallanzani_M/0/1/0/all/0/1\">Matteo Spallanzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardi_G/0/1/0/all/0/1\">Gian Paolo Leonardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Matching with Overlapping Attention for Optical Flow Estimation. (arXiv:2203.11335v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11335","description":"<p>Optical flow estimation is a fundamental task in computer vision. Recent\ndirect-regression methods using deep neural networks achieve remarkable\nperformance improvement. However, they do not explicitly capture long-term\nmotion correspondences and thus cannot handle large motions effectively. In\nthis paper, inspired by the traditional matching-optimization methods where\nmatching is introduced to handle large displacements before energy-based\noptimizations, we introduce a simple but effective global matching step before\nthe direct regression and develop a learning-based matching-optimization\nframework, namely GMFlowNet. In GMFlowNet, global matching is efficiently\ncalculated by applying argmax on 4D cost volumes. Additionally, to improve the\nmatching quality, we propose patch-based overlapping attention to extract large\ncontext features. Extensive experiments demonstrate that GMFlowNet outperforms\nRAFT, the most popular optimization-only method, by a large margin and achieves\nstate-of-the-art performance on standard benchmarks. Thanks to the matching and\noverlapping attention, GMFlowNet obtains major improvements on the predictions\nfor textureless regions and large motions. Our code is made publicly available\nat https://github.com/xiaofeng94/GMFlowNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Enyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting Medical Instruments in Minimally Invasive Surgeries using AttentionMask. (arXiv:2203.11358v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11358","description":"<p>Precisely locating and segmenting medical instruments in images of minimally\ninvasive surgeries, medical instrument segmentation, is an essential first step\nfor several tasks in medical image processing. However, image degradations,\nsmall instruments, and the generalization between different surgery types make\nmedical instrument segmentation challenging. To cope with these challenges, we\nadapt the object proposal generation system AttentionMask and propose a\ndedicated post-processing to select promising proposals. The results on the\nROBUST-MIS Challenge 2019 show that our adapted AttentionMask system is a\nstrong foundation for generating state-of-the-art performance. Our evaluation\nin an object proposal generation framework shows that our adapted AttentionMask\nsystem is robust to image degradations, generalizes well to unseen types of\nsurgeries, and copes well with small instruments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1\">Christian Wilms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerlach_A/0/1/0/all/0/1\">Alexander Michael Gerlach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_R/0/1/0/all/0/1\">R&#xfc;diger Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1\">Simone Frintrop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio visual character profiles for detecting background characters in entertainment media. (arXiv:2203.11368v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11368","description":"<p>An essential goal of computational media intelligence is to support\nunderstanding how media stories -- be it news, commercial or entertainment\nmedia -- represent and reflect society and these portrayals are perceived.\nPeople are a central element of media stories. This paper focuses on\nunderstanding the representation and depiction of background characters in\nmedia depictions, primarily movies and TV shows. We define the background\ncharacters as those who do not participate vocally in any scene throughout the\nmovie and address the problem of localizing background characters in videos. We\nuse an active speaker localization system to extract high-confidence\nface-speech associations and generate audio-visual profiles for talking\ncharacters in a movie by automatically clustering them. Using a face\nverification system, we then prune all the face-tracks which match any of the\ngenerated character profiles and obtain the background character face-tracks.\nWe curate a background character dataset which provides annotations for\nbackground character for a set of TV shows, and use it to evaluate the\nperformance of the background character detection framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperShot: Few-Shot Learning by Kernel HyperNetworks. (arXiv:2203.11378v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11378","description":"<p>Few-shot models aim at making predictions using a minimal number of labeled\nexamples from a given task. The main challenge in this area is the one-shot\nsetting where only one element represents each class. We propose HyperShot -\nthe fusion of kernels and hypernetwork paradigm. Compared to reference\napproaches that apply a gradient-based adjustment of the parameters, our model\naims to switch the classification module parameters depending on the task's\nembedding. In practice, we utilize a hypernetwork, which takes the aggregated\ninformation from support data and returns the classifier's parameters\nhandcrafted for the considered problem. Moreover, we introduce the kernel-based\nrepresentation of the support examples delivered to hypernetwork to create the\nparameters of the classification module. Consequently, we rely on relations\nbetween embeddings of the support examples instead of direct feature values\nprovided by the backbone models. Thanks to this approach, our model can adapt\nto highly different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sendera_M/0/1/0/all/0/1\">Marcin Sendera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1\">Marcin Przewi&#x119;&#x17a;likowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanowski_K/0/1/0/all/0/1\">Konrad Karanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1\">Przemys&#x142;aw Spurek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data. (arXiv:2203.11391v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11391","description":"<p>Idiopathic Pulmonary Fibrosis (IPF) is an inexorably progressive fibrotic\nlung disease with a variable and unpredictable rate of progression. CT scans of\nthe lungs inform clinical assessment of IPF patients and contain pertinent\ninformation related to disease progression. In this work, we propose a\nmulti-modal method that uses neural networks and memory banks to predict the\nsurvival of IPF patients using clinical and imaging data. The majority of\nclinical IPF patient records have missing data (e.g. missing lung function\ntests). To this end, we propose a probabilistic model that captures the\ndependencies between the observed clinical variables and imputes missing ones.\nThis principled approach to missing data imputation can be naturally combined\nwith a deep survival analysis model. We show that the proposed framework yields\nsignificantly better survival analysis results than baselines in terms of\nconcordance index and integrated Brier score. Our work also provides insights\ninto novel image-based biomarkers that are linked to mortality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shahin_A/0/1/0/all/0/1\">Ahmed H. Shahin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacob_J/0/1/0/all/0/1\">Joseph Jacob</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barber_D/0/1/0/all/0/1\">David Barber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Convex Objects Image Segmentation via Proximal Alternating Direction Method of Multipliers. (arXiv:2203.11395v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11395","description":"<p>This paper focuses on the issue of image segmentation with convex shape\nprior. Firstly, we use binary function to represent convex object(s). The\nconvex shape prior turns out to be a simple quadratic inequality constraint on\nthe binary indicator function associated with each object. An image\nsegmentation model incorporating convex shape prior into a probability-based\nmethod is proposed. Secondly, a new algorithm is designed to solve involved\noptimization problem, which is a challenging task because of the quadratic\ninequality constraint. To tackle this difficulty, we relax and linearize the\nquadratic inequality constraint to reduce it to solve a sequence of convex\nminimization problems. For each convex problem, an efficient proximal\nalternating direction method of multipliers is developed to solve it. The\nconvergence of the algorithm follows some existing results in the optimization\nliterature. Moreover, an interactive procedure is introduced to improve the\naccuracy of segmentation gradually. Numerical experiments on natural and\nmedical images demonstrate that the proposed method is superior to some\nexisting methods in terms of segmentation accuracy and computational time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shousheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunhai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1\">Xue-Cheng Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real World Dataset for Multi-view 3D Reconstruction. (arXiv:2203.11397v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11397","description":"<p>We present a dataset of 371 3D models of everyday tabletop objects along with\ntheir 320,000 real world RGB and depth images. Accurate annotations of camera\nposes and object poses for each image are performed in a semi-automated fashion\nto facilitate the use of the dataset for myriad 3D applications like shape\nreconstruction, object pose estimation, shape retrieval etc. We primarily focus\non learned multi-view 3D reconstruction due to the lack of appropriate real\nworld benchmark for the task and demonstrate that our dataset can fill that\ngap. The entire annotated dataset along with the source code for the annotation\ntools and evaluation baselines will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_R/0/1/0/all/0/1\">Rakesh Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_M/0/1/0/all/0/1\">Minghao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception. (arXiv:2203.11405v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11405","description":"<p>Self-driving cars must detect vehicles, pedestrians, and other traffic\nparticipants accurately to operate safely. Small, far-away, or highly occluded\nobjects are particularly challenging because there is limited information in\nthe LiDAR point clouds for detecting them. To address this challenge, we\nleverage valuable information from the past: in particular, data collected in\npast traversals of the same scene. We posit that these past data, which are\ntypically discarded, provide rich contextual information for disambiguating the\nabove-mentioned challenging cases. To this end, we propose a novel, end-to-end\ntrainable Hindsight framework to extract this contextual information from past\ntraversals and store it in an easy-to-query data structure, which can then be\nleveraged to aid future 3D object detection of the same scene. We show that\nthis framework is compatible with most modern 3D detection architectures and\ncan substantially improve their average precision on multiple autonomous\ndriving datasets, most notably by more than 300% on the challenging cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yurong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Katie Z Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Mark Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gated Domain-Invariant Feature Disentanglement for Domain Generalizable Object Detection. (arXiv:2203.11432v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11432","description":"<p>For Domain Generalizable Object Detection (DGOD), Disentangled Representation\nLearning (DRL) helps a lot by explicitly disentangling Domain-Invariant\nRepresentations (DIR) from Domain-Specific Representations (DSR). Considering\nthe domain category is an attribute of input data, it should be feasible for\nnetworks to fit a specific mapping which projects DSR into feature channels\nexclusive to domain-specific information, and thus much cleaner disentanglement\nof DIR from DSR can be achieved simply on channel dimension. Inspired by this\nidea, we propose a novel DRL method for DGOD, which is termed Gated\nDomain-Invariant Feature Disentanglement (GDIFD). In GDIFD, a Channel Gate\nModule (CGM) learns to output channel gate signals close to either 0 or 1,\nwhich can mask out the channels exclusive to domain-specific information\nhelpful for domain recognition. With the proposed GDIFD, the backbone in our\nframework can fit the desired mapping easily, which enables the channel-wise\ndisentanglement. In experiments, we demonstrate that our approach is highly\neffective and achieves state-of-the-art DGOD performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haozhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huimin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runfa Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack. (arXiv:2203.11433v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11433","description":"<p>DeepFakes are raising significant social concerns. Although various DeepFake\ndetectors have been developed as forensic countermeasures, these detectors are\nstill vulnerable to attacks. Recently, a few attacks, principally adversarial\nattacks, have succeeded in cloaking DeepFake images to evade detection.\nHowever, these attacks have typical detector-specific designs, which require\nprior knowledge about the detector, leading to poor transferability. Moreover,\nthese attacks only consider simple security scenarios. Less is known about how\neffective they are in high-level scenarios where either the detectors or the\nattacker's knowledge varies. In this paper, we solve the above challenges with\npresenting a novel detector-agnostic trace removal attack for DeepFake\nanti-forensics. Instead of investigating the detector side, our attack looks\ninto the original DeepFake creation pipeline, attempting to remove all\ndetectable natural DeepFake traces to render the fake images more \"authentic\".\nTo implement this attack, first, we perform a DeepFake trace discovery,\nidentifying three discernible traces. Then a trace removal network (TR-Net) is\nproposed based on an adversarial learning framework involving one generator and\nmultiple discriminators. Each discriminator is responsible for one individual\ntrace representation to avoid cross-trace interference. These discriminators\nare arranged in parallel, which prompts the generator to remove various traces\nsimultaneously. To evaluate the attack efficacy, we crafted heterogeneous\nsecurity scenarios where the detectors were embedded with different levels of\ndefense and the attackers' background knowledge of data varies. The\nexperimental results show that the proposed attack can significantly compromise\nthe detection accuracy of six state-of-the-art DeepFake detectors while causing\nonly a negligible loss in visual quality to the original DeepFake samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanlei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning as Multimodal Variational Inference. (arXiv:2203.11437v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11437","description":"<p>This paper proposes a probabilistic extension of SimSiam, a recent\nself-supervised learning (SSL) method. SimSiam trains a model by maximizing the\nsimilarity between image representations of different augmented views of the\nsame image. Although uncertainty-aware machine learning has been getting\ngeneral like deep variational inference, SimSiam and other SSL are\ninsufficiently uncertainty-aware, which could lead to limitations on its\npotential. The proposed extension is to make SimSiam uncertainty-aware based on\nvariational inference. Our main contributions are twofold: Firstly, we clarify\nthe theoretical relationship between non-contrastive SSL and multimodal\nvariational inference. Secondly, we introduce a novel SSL called variational\ninference SimSiam (VI-SimSiam), which incorporates the uncertainty by involving\nspherical posterior distributions. Our experiment shows that VI-SimSiam\noutperforms SimSiam in classification tasks in ImageNette and ImageWoof by\nsuccessfully estimating the representation uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_H/0/1/0/all/0/1\">Hiroki Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1\">Masashi Okada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers. (arXiv:2203.11441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11441","description":"<p>Multi-modal learning has been intensified in recent years, especially for\napplications in facial analysis and action unit detection whilst there still\nexist two main challenges in terms of 1) relevant feature learning for\nrepresentation and 2) efficient fusion for multi-modalities. Recently, there\nare a number of works have shown the effectiveness in utilizing the attention\nmechanism for AU detection, however, most of them are binding the region of\ninterest (ROI) with features but rarely apply attention between features of\neach AU. On the other hand, the transformer, which utilizes a more efficient\nself-attention mechanism, has been widely used in natural language processing\nand computer vision tasks but is not fully explored in AU detection tasks. In\nthis paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)\nmethod for AU detection, which learns AU encoding features representation from\ndifferent modalities by transformer encoder and fuses modalities by another\nfusion transformer module. Multi-head fusion attention is designed in the\nfusion transformer module for the effective fusion of multiple modalities. Our\napproach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,\nand the results are superior to the state-of-the-art algorithms and baseline\nmodels. We further analyze the performance of AU detection from different\nmodalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lijun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11442","description":"<p>This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computation resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\njointly and collaboratively. In detail, AOT employs an identification mechanism\nto associate multiple targets into the same high-dimensional embedding space.\nThus, we can simultaneously process multiple objects' matching and segmentation\ndecoding as efficiently as processing a single object. To sufficiently model\nmulti-object association, a Long Short-Term Transformer (LSTT) is devised to\nconstruct hierarchical matching and propagation. Based on AOT, we further\npropose a more flexible and robust framework, Associating Objects with Scalable\nTransformers (AOST), in which a scalable version of LSTT is designed to enable\nrun-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces\na better layer-wise manner to couple identification and vision embeddings. We\nconduct extensive experiments on multi-object and single-object benchmarks to\nexamine AOT series frameworks. Compared to the state-of-the-art competitors,\nour methods can maintain times of run-time efficiency with superior\nperformance. Notably, we achieve new state-of-the-art performance on three\npopular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test\n(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:\nhttps://github.com/z-x-yang/AOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manipulating UAV Imagery for Satellite Model Training, Calibration and Testing. (arXiv:2203.11447v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11447","description":"<p>Modern livestock farming is increasingly data driven and frequently relies on\nefficient remote sensing to gather data over wide areas. High resolution\nsatellite imagery is one such data source, which is becoming more accessible\nfor farmers as coverage increases and cost falls. Such images can be used to\ndetect and track animals, monitor pasture changes, and understand land use.\nMany of the data driven models being applied to these tasks require ground\ntruthing at resolutions higher than satellites can provide. Simultaneously,\nthere is a lack of available aerial imagery focused on farmland changes that\noccur over days or weeks, such as herd movement. With this goal in mind, we\npresent a new multi-temporal dataset of high resolution UAV imagery which is\nartificially degraded to match satellite data quality. An empirical blurring\nmetric is used to calibrate the degradation process against actual satellite\nimagery of the area. UAV surveys were flown repeatedly over several weeks, for\nspecific farm locations. This 5cm/pixel data is sufficiently high resolution to\naccurately ground truth cattle locations, and other factors such as grass\ncover. From 33 wide area UAV surveys, 1869 patches were extracted and\nartificially degraded using an accurate satellite optical model to simulate\nsatellite data. Geographic patches from multiple time periods are aligned and\npresented as sets, providing a multi-temporal dataset that can be used for\ndetecting changes on farms. The geo-referenced images and 27,853 manually\nannotated cattle labels are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">Jasper Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Cameron Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomax_S/0/1/0/all/0/1\">Sabrina Lomax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_K/0/1/0/all/0/1\">Khalid Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukkarieh_S/0/1/0/all/0/1\">Salah Sukkarieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Textures in Zero-shot Understanding of Fine-Grained Domains. (arXiv:2203.11449v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11449","description":"<p>Textures can be used to describe the appearance of objects in a wide range of\nfine-grained domains. Textures are localized and one can often refer to their\nproperties in a manner that is independent of the object identity. Moreover,\nthere is a rich vocabulary to describe textures corresponding to properties\nsuch as their color, pattern, structure, periodicity, stochasticity, and\nothers. Motivated by this, we study the effectiveness of large-scale language\nand vision models (e.g., CLIP) at recognizing texture attributes in natural\nimages. We first conduct a systematic study of CLIP on texture datasets where\nwe find that it has good coverage for a wide range of texture terms. CLIP can\nalso handle compositional phrases that consist of color and pattern terms\n(e.g., red dots or yellow stripes). We then show how these attributes allow for\nzero-shot fine-grained categorization on existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic Layouts. (arXiv:2203.11453v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11453","description":"<p>Limited by the computational efficiency and accuracy, generating complex 3D\nscenes remains a challenging problem for existing generation networks. In this\nwork, we propose DepthGAN, a novel method of generating depth maps with only\nsemantic layouts as input. First, we introduce a well-designed cascade of\ntransformer blocks as our generator to capture the structural correlations in\ndepth maps, which makes a balance between global feature aggregation and local\nattention. Meanwhile, we propose a cross-attention fusion module to guide edge\npreservation efficiently in depth generation, which exploits additional\nappearance supervision information. Finally, we conduct extensive experiments\non the perspective views of the Structured3d panorama dataset and demonstrate\nthat our DepthGAN achieves superior performance both on quantitative results\nand visual effects in the depth generation task.Furthermore, 3D indoor scenes\ncan be reconstructed by our generated depth maps with reasonable structure and\nspatial coherency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11471","description":"<p>In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute\nhuman pose estimation with calibrated camera. Accurate and generalizable\nabsolute 3D human pose estimation from monocular 2D pose input is an ill-posed\nproblem. To address this challenge, we convert the input from pixel space to 3D\nnormalized rays. This conversion makes our approach robust to camera intrinsic\nparameter changes. To deal with the in-the-wild camera extrinsic parameter\nvariations, Ray3D explicitly takes the camera extrinsic parameters as an input\nand jointly models the distribution between the 3D pose rays and camera\nextrinsic parameters. This novel network design is the key to the outstanding\ngeneralizability of Ray3D approach. To have a comprehensive understanding of\nhow the camera intrinsic and extrinsic parameter variations affect the accuracy\nof absolute 3D key-point localization, we conduct in-depth systematic\nexperiments on three single person 3D benchmarks as well as one synthetic\nbenchmark. These experiments demonstrate that our method significantly\noutperforms existing state-of-the-art models. Our code and the synthetic\ndataset are available at https://github.com/YxZhxn/Ray3D .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenghai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Renliang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Wongun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remember Intentions: Retrospective-Memory-based Trajectory Prediction. (arXiv:2203.11474v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11474","description":"<p>To realize trajectory prediction, most previous methods adopt the\nparameter-based approach, which encodes all the seen past-future instance pairs\ninto model parameters. However, in this way, the model parameters come from all\nseen instances, which means a huge amount of irrelevant seen instances might\nalso involve in predicting the current situation, disturbing the performance.\nTo provide a more explicit link between the current situation and the seen\ninstances, we imitate the mechanism of retrospective memory in neuropsychology\nand propose MemoNet, an instance-based approach that predicts the movement\nintentions of agents by looking for similar scenarios in the training data. In\nMemoNet, we design a pair of memory banks to explicitly store representative\ninstances in the training set, acting as prefrontal cortex in the neural\nsystem, and a trainable memory addresser to adaptively search a current\nsituation with similar instances in the memory bank, acting like basal ganglia.\nDuring prediction, MemoNet recalls previous memory by using the memory\naddresser to index related instances in the memory bank. We further propose a\ntwo-step trajectory prediction system, where the first step is to leverage\nMemoNet to predict the destination and the second step is to fulfill the whole\ntrajectory according to the predicted destinations. Experiments show that the\nproposed MemoNet improves the FDE by 20.3%/10.2%/28.3% from the previous best\nmethod on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has\nthe ability to trace back to specific instances during prediction, promoting\nmore interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weibo Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_Z/0/1/0/all/0/1\">Zhao Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahong_L/0/1/0/all/0/1\">Leng Jiahong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanyu_Z/0/1/0/all/0/1\">Zhao Hanyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_T/0/1/0/all/0/1\">Tang Jie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11481","description":"<p>We introduce AdaMix, an adaptive differentially private algorithm for\ntraining deep neural network classifiers using both private and public image\ndata. While pre-training language models on large public datasets has enabled\nstrong differential privacy (DP) guarantees with minor loss of accuracy, a\nsimilar practice yields punishing trade-offs in vision tasks. A few-shot or\neven zero-shot learning baseline that ignores private data can outperform\nfine-tuning on a large private dataset. AdaMix incorporates few-shot training,\nor cross-modal zero-shot learning, on public data prior to private fine-tuning,\nto improve the trade-off. AdaMix reduces the error increase from the\nnon-private upper bound from the 167-311\\% of the baseline, on average across 6\ndatasets, to 68-92\\% depending on the desired privacy level selected by the\nuser. AdaMix tackles the trade-off arising in visual classification, whereby\nthe most privacy sensitive data, corresponding to isolated points in\nrepresentation space, are also critical for high classification accuracy. In\naddition, AdaMix comes with strong theoretical privacy guarantees and\nconvergence analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1\">Michael Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation. (arXiv:2203.11483v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11483","description":"<p>With the advent of convolutional neural networks, stereo matching algorithms\nhave recently gained tremendous progress. However, it remains a great challenge\nto accurately extract disparities from real-world image pairs taken by\nconsumer-level devices like smartphones, due to practical complicating factors\nsuch as thin structures, non-ideal rectification, camera module inconsistencies\nand various hard-case scenes. In this paper, we propose a set of innovative\ndesigns to tackle the problem of practical stereo matching: 1) to better\nrecover fine depth details, we design a hierarchical network with recurrent\nrefinement to update disparities in a coarse-to-fine manner, as well as a\nstacked cascaded architecture for inference; 2) we propose an adaptive group\ncorrelation layer to mitigate the impact of erroneous rectification; 3) we\nintroduce a new synthetic dataset with special attention to difficult cases for\nbetter generalizing to real-world scenes. Our results not only rank 1st on both\nMiddlebury and ETH3D benchmarks, outperforming existing state-of-the-art\nmethods by a notable margin, but also exhibit high-quality details for\nreal-life photos, which clearly demonstrates the efficacy of our contributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiankun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Pengfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Ziwei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11490","description":"<p>Skin cancer is one of the most common types of malignancy, affecting a large\npopulation and causing a heavy economic burden worldwide. Over the last few\nyears, computer-aided diagnosis has been rapidly developed and make great\nprogress in healthcare and medical practices due to the advances in artificial\nintelligence. However, most studies in skin cancer detection keep pursuing high\nprediction accuracies without considering the limitation of computing resources\non portable devices. In this case, knowledge distillation (KD) has been proven\nas an efficient tool to help improve the adaptability of lightweight models\nunder limited resources, meanwhile keeping a high-level representation\ncapability. To bridge the gap, this study specifically proposes a novel method,\ntermed SSD-KD, that unifies diverse knowledge into a generic KD framework for\nskin diseases classification. Our method models an intra-instance relational\nfeature representation and integrates it with existing KD research. A dual\nrelational knowledge distillation architecture is self-supervisedly trained\nwhile the weighted softened outputs are also exploited to enable the student\nmodel to capture richer knowledge from the teacher model. To demonstrate the\neffectiveness of our method, we conduct experiments on ISIC 2019, a large-scale\nopen-accessed benchmark of skin diseases dermoscopic images. Experiments show\nthat our distilled lightweight model can achieve an accuracy as high as 85% for\nthe classification tasks of 8 different skin diseases with minimal parameters\nand computing requirements. Ablation studies confirm the effectiveness of our\nintra- and inter-instance relational knowledge integration strategy. Compared\nwith state-of-the-art knowledge distillation techniques, the proposed method\ndemonstrates improved performances for multi-diseases classification on the\nlarge-scale dermoscopy database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tim K. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrameHopper: Selective Processing of Video Frames in Detection-driven Real-Time Video Analytics. (arXiv:2203.11493v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11493","description":"<p>Detection-driven real-time video analytics require continuous detection of\nobjects contained in the video frames using deep learning models like YOLOV3,\nEfficientDet. However, running these detectors on each and every frame in\nresource-constrained edge devices is computationally intensive. By taking the\ntemporal correlation between consecutive video frames into account, we note\nthat detection outputs tend to be overlapping in successive frames. Elimination\nof similar consecutive frames will lead to a negligible drop in performance\nwhile offering significant performance benefits by reducing overall computation\nand communication costs. The key technical questions are, therefore, (a) how to\nidentify which frames to be processed by the object detector, and (b) how many\nsuccessive frames can be skipped (called skip-length) once a frame is selected\nto be processed. The overall goal of the process is to keep the error due to\nskipping frames as small as possible. We introduce a novel error vs processing\nrate optimization problem with respect to the object detection task that\nbalances between the error rate and the fraction of frames filtering.\nSubsequently, we propose an off-line Reinforcement Learning (RL)-based\nalgorithm to determine these skip-lengths as a state-action policy of the RL\nagent from a recorded video and then deploy the agent online for live video\nstreams. To this end, we develop FrameHopper, an edge-cloud collaborative video\nanalytics framework, that runs a lightweight trained RL agent on the camera and\npasses filtered frames to the server where the object detection model runs for\na set of applications. We have tested our approach on a number of live videos\ncaptured from real-life scenarios and show that FrameHopper processes only a\nhandful of frames but produces detection results closer to the oracle solution\nand outperforms recent state-of-the-art solutions in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_M/0/1/0/all/0/1\">Md Adnan Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nimi_S/0/1/0/all/0/1\">Sumaiya Tabassum Nimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Yusuf Sarwar Uddin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers. (arXiv:2203.11496v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11496","description":"<p>LiDAR and camera are two important sensors for 3D object detection in\nautonomous driving. Despite the increasing popularity of sensor fusion in this\nfield, the robustness against inferior image conditions, e.g., bad illumination\nand sensor misalignment, is under-explored. Existing fusion methods are easily\naffected by such conditions, mainly due to a hard association of LiDAR points\nand image pixels, established by calibration matrices. We propose TransFusion,\na robust solution to LiDAR-camera fusion with a soft-association mechanism to\nhandle inferior image conditions. Specifically, our TransFusion consists of\nconvolutional backbones and a detection head based on a transformer decoder.\nThe first layer of the decoder predicts initial bounding boxes from a LiDAR\npoint cloud using a sparse set of object queries, and its second decoder layer\nadaptively fuses the object queries with useful image features, leveraging both\nspatial and contextual relationships. The attention mechanism of the\ntransformer enables our model to adaptively determine where and what\ninformation should be taken from the image, leading to a robust and effective\nfusion strategy. We additionally design an image-guided query initialization\nstrategy to deal with objects that are difficult to detect in point clouds.\nTransFusion achieves state-of-the-art performance on large-scale datasets. We\nprovide extensive experiments to demonstrate its robustness against degenerated\nimage quality and calibration errors. We also extend the proposed method to the\n3D tracking task and achieve the 1st place in the leaderboard of nuScenes\ntracking, showing its effectiveness and generalization capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition. (arXiv:2203.11506v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11506","description":"<p>Deep neural networks perform poorly on heavily class-imbalanced datasets.\nGiven the promising performance of contrastive learning, we propose\n$\\mathbf{Re}$balanced $\\mathbf{S}$iamese $\\mathbf{Co}$ntrastive\n$\\mathbf{m}$ining ( $\\mathbf{ResCom}$) to tackle imbalanced recognition. Based\non the mathematical analysis and simulation results, we claim that supervised\ncontrastive learning suffers a dual class-imbalance problem at both the\noriginal batch and Siamese batch levels, which is more serious than long-tailed\nclassification learning. In this paper, at the original batch level, we\nintroduce a class-balanced supervised contrastive loss to assign adaptive\nweights for different classes. At the Siamese batch level, we present a\nclass-balanced queue, which maintains the same number of keys for all classes.\nFurthermore, we note that the contrastive loss gradient with respect to the\ncontrastive logits can be decoupled into the positives and negatives, and easy\npositives and easy negatives will make the contrastive gradient vanish. We\npropose supervised hard positive and negative pairs mining to pick up\ninformative pairs for contrastive computation and improve representation\nlearning. Finally, to approximately maximize the mutual information between the\ntwo views, we propose Siamese Balanced Softmax and joint it with the\ncontrastive loss for one-stage training. ResCom outperforms the previous\nmethods by large margins on multiple long-tailed recognition benchmarks. Our\ncode will be made publicly available at:\nhttps://github.com/dvlab-research/ResCom.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_E/0/1/0/all/0/1\">Eric Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deraining: Where Contrastive Learning Meets Self-similarity. (arXiv:2203.11509v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11509","description":"<p>Image deraining is a typical low-level image restoration task, which aims at\ndecomposing the rainy image into two distinguishable layers: the clean image\nlayer and the rain layer. Most of the existing learning-based deraining methods\nare supervisedly trained on synthetic rainy-clean pairs. The domain gap between\nthe synthetic and real rains makes them less generalized to different real\nrainy scenes. Moreover, the existing methods mainly utilize the property of the\ntwo layers independently, while few of them have considered the mutually\nexclusive relationship between the two layers. In this work, we propose a novel\nnon-local contrastive learning (NLCL) method for unsupervised image deraining.\nConsequently, we not only utilize the intrinsic self-similarity property within\nsamples but also the mutually exclusive property between the two layers, so as\nto better differ the rain layer from the clean image. Specifically, the\nnon-local self-similarity image layer patches as the positives are pulled\ntogether and similar rain layer patches as the negatives are pushed away. Thus\nthe similar positive/negative samples that are close in the original space\nbenefit us to enrich more discriminative representation. Apart from the\nself-similarity sampling strategy, we analyze how to choose an appropriate\nfeature encoder in NLCL. Extensive experiments on different real rainy datasets\ndemonstrate that the proposed method obtains state-of-the-art performance in\nreal deraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuntong_Y/0/1/0/all/0/1\">Ye Yuntong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changfeng_Y/0/1/0/all/0/1\">Yu Changfeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1\">Chang Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xile_Z/0/1/0/all/0/1\">Zhao Xile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxin_Y/0/1/0/all/0/1\">Yan Luxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yonghong_T/0/1/0/all/0/1\">Tian Yonghong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network-based Efficient Dense Point Cloud Generation using Unsigned Distance Fields. (arXiv:2203.11537v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11537","description":"<p>Dense point cloud generation from a sparse or incomplete point cloud is a\ncrucial and challenging problem in 3D computer vision and computer graphics. So\nfar, the existing methods are either computationally too expensive, suffer from\nlimited resolution, or both. In addition, some methods are strictly limited to\nwatertight surfaces -- another major obstacle for a number of applications. To\naddress these issues, we propose a lightweight Convolutional Neural Network\nthat learns and predicts the unsigned distance field for arbitrary 3D shapes\nfor dense point cloud generation using the recently emerged concept of implicit\nfunction learning. Experiments demonstrate that the proposed architecture\nachieves slightly better quality results than the state of the art with 87%\nless model parameters and 40% less GPU memory usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basher_A/0/1/0/all/0/1\">Abol Basher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutellier_J/0/1/0/all/0/1\">Jani Boutellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation. (arXiv:2203.11542v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11542","description":"<p>The COVID-19 pandemic has disrupted various levels of society. The use of\nmasks is essential in preventing the spread of COVID-19 by identifying an image\nof a person using a mask. Although only 23.1% of people use masks correctly,\nArtificial Neural Networks (ANN) can help classify the use of good masks to\nhelp slow the spread of the Covid-19 virus. However, it requires a large\ndataset to train an ANN that can classify the use of masks correctly.\nMaskedFace-Net is a suitable dataset consisting of 137016 digital images with 4\nclass labels, namely Mask, Mask Chin, Mask Mouth Chin, and Mask Nose Mouth.\nMask classification training utilizes Vision Transformers (ViT) architecture\nwith transfer learning method using pre-trained weights on ImageNet-21k, with\nrandom augmentation. In addition, the hyper-parameters of training of 20\nepochs, an Stochastic Gradient Descent (SGD) optimizer with a learning rate of\n0.03, a batch size of 64, a Gaussian Cumulative Distribution (GeLU) activation\nfunction, and a Cross-Entropy loss function are used to be applied on the\ntraining of three architectures of ViT, namely Base-16, Large-16, and Huge-14.\nFurthermore, comparisons of with and without augmentation and transfer learning\nare conducted. This study found that the best classification is transfer\nlearning and augmentation using ViT Huge-14. Using this method on\nMaskedFace-Net dataset, the research reaches an accuracy of 0.9601 on training\ndata, 0.9412 on validation data, and 0.9534 on test data. This research shows\nthat training the ViT model with data augmentation and transfer learning\nimproves classification of the mask usage, even better than convolutional-based\nResidual Network (ResNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahja_H/0/1/0/all/0/1\">Hensel Donato Jahja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutrisno/0/1/0/all/0/1\">Sutrisno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frugal Learning of Virtual Exemplars for Label-Efficient Satellite Image Change Detection. (arXiv:2203.11559v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11559","description":"<p>In this paper, we devise a novel interactive satellite image change detection\nalgorithm based on active learning. The proposed framework is iterative and\nrelies on a question and answer model which asks the oracle (user) questions\nabout the most informative display (subset of critical images), and according\nto the user's responses, updates change detections. The contribution of our\nframework resides in a novel display model which selects the most\nrepresentative and diverse virtual exemplars that adversely challenge the\nlearned change detection functions, thereby leading to highly discriminating\nfunctions in the subsequent iterations of active learning. Extensive\nexperiments, conducted on the challenging task of interactive satellite image\nchange detection, show the superiority of the proposed virtual display model\nagainst the related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschamps_S/0/1/0/all/0/1\">Sebastien Deschamps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement-based frugal learning for satellite image change detection. (arXiv:2203.11564v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11564","description":"<p>In this paper, we introduce a novel interactive satellite image change\ndetection algorithm based on active learning. The proposed approach is\niterative and asks the user (oracle) questions about the targeted changes and\naccording to the oracle's responses updates change detections. We consider a\nprobabilistic framework which assigns to each unlabeled sample a relevance\nmeasure modeling how critical is that sample when training change detection\nfunctions. These relevance measures are obtained by minimizing an objective\nfunction mixing diversity, representativity and uncertainty. These criteria\nwhen combined allow exploring different data modes and also refining change\ndetections. To further explore the potential of this objective function, we\nconsider a reinforcement learning approach that finds the best combination of\ndiversity, representativity and uncertainty, through active learning\niterations, leading to better generalization as corroborated through\nexperiments in interactive satellite image change detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deschamps_S/0/1/0/all/0/1\">Sebastien Deschamps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose CT Image Reconstruction. (arXiv:2203.11565v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11565","description":"<p>The recently proposed sparsifying transform models incur low computational\ncost and have been applied to medical imaging. Meanwhile, deep models with\nnested network structure reveal great potential for learning features in\ndifferent layers. In this study, we propose a network-structured sparsifying\ntransform learning approach for X-ray computed tomography (CT), which we refer\nto as multi-layer clustering-based residual sparsifying transform (MCST)\nlearning. The proposed MCST scheme learns multiple different unitary transforms\nin each layer by dividing each layer's input into several classes. We apply the\nMCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST\nmodel into the regularizer in penalized weighted least squares (PWLS)\nreconstruction. We conducted LDCT reconstruction experiments on XCAT phantom\ndata and Mayo Clinic data and trained the MCST model with 2 (or 3) layers and\nwith 5 clusters in each layer. The learned transforms in the same layer showed\nrich features while additional information is extracted from representation\nresiduals. Our simulation results demonstrate that PWLS-MCST achieves better\nimage reconstruction quality than the conventional FBP method and PWLS with\nedge-preserving (EP) regularizer. It also outperformed recent advanced methods\nlike PWLS with a learned multi-layer residual sparsifying transform prior\n(MARS) and PWLS with a union of learned transforms (ULTRA), especially for\ndisplaying clear edges and preserving subtle details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xikai Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhishen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yong Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1\">Saiprasad Ravishankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Patch Exiting for Scalable Single Image Super-Resolution. (arXiv:2203.11589v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11589","description":"<p>Since the future of computing is heterogeneous, scalability is a crucial\nproblem for single image super-resolution. Recent works try to train one\nnetwork, which can be deployed on platforms with different capacities. However,\nthey rely on the pixel-wise sparse convolution, which is not hardware-friendly\nand achieves limited practical speedup. As image can be divided into patches,\nwhich have various restoration difficulties, we present a scalable method based\non Adaptive Patch Exiting (APE) to achieve more practical speedup.\nSpecifically, we propose to train a regressor to predict the incremental\ncapacity of each layer for the patch. Once the incremental capacity is below\nthe threshold, the patch can exit at the specific layer. Our method can easily\nadjust the trade-off between performance and efficiency by changing the\nthreshold of incremental capacity. Furthermore, we propose a novel strategy to\nenable the network training of our method. We conduct extensive experiments\nacross various backbones, datasets and scaling factors to demonstrate the\nadvantages of our method. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment. (arXiv:2203.11590v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11590","description":"<p>This paper investigates the problem of temporally interpolating dynamic 3D\npoint clouds with large non-rigid deformation. We formulate the problem as\nestimation of point-wise trajectories (i.e., smooth curves) and further reason\nthat temporal irregularity and under-sampling are two major challenges. To\ntackle the challenges, we propose IDEA-Net, an end-to-end deep learning\nframework, which disentangles the problem under the assistance of the\nexplicitly learned temporal consistency. Specifically, we propose a temporal\nconsistency learning module to align two consecutive point cloud frames\npoint-wisely, based on which we can employ linear interpolation to obtain\ncoarse trajectories/in-between frames. To compensate the high-order nonlinear\ncomponents of trajectories, we apply aligned feature embeddings that encode\nlocal geometry properties to regress point-wise increments, which are combined\nwith the coarse estimations. We demonstrate the effectiveness of our method on\nvarious point cloud sequences and observe large improvement over\nstate-of-the-art methods both quantitatively and visually. Our framework can\nbring benefits to 3D motion data acquisition. The source code is publicly\navailable at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yiming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation. (arXiv:2203.11591v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11591","description":"<p>Pre-training has been adopted in a few of recent works for\nVision-and-Language Navigation (VLN). However, previous pre-training methods\nfor VLN either lack the ability to predict future actions or ignore the\ntrajectory contexts, which are essential for a greedy navigation process. In\nthis work, to promote the learning of spatio-temporal visual-textual\ncorrespondence as well as the agent's capability of decision making, we propose\na novel history-and-order aware pre-training paradigm (HOP) with VLN-specific\nobjectives that exploit the past observations and support future action\nprediction. Specifically, in addition to the commonly used Masked Language\nModeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy\ntasks to model temporal order information: Trajectory Order Modeling (TOM) and\nGroup Order Modeling (GOM). Moreover, our navigation action prediction is also\nenhanced by introducing the task of Action Prediction with History (APH), which\ntakes into account the history visual perceptions. Extensive experimental\nresults on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the\neffectiveness of our proposed method compared against several state-of-the-art\nagents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yanyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Negative Pair Generation toward Well-discriminative Feature Space for Face Recognition. (arXiv:2203.11593v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11593","description":"<p>The goal of face recognition (FR) can be viewed as a pair similarity\noptimization problem, maximizing a similarity set $\\mathcal{S}^p$ over positive\npairs, while minimizing similarity set $\\mathcal{S}^n$ over negative pairs.\nIdeally, it is expected that FR models form a well-discriminative feature space\n(WDFS) that satisfies $\\inf{\\mathcal{S}^p} &gt; \\sup{\\mathcal{S}^n}$. With regard\nto WDFS, the existing deep feature learning paradigms (i.e., metric and\nclassification losses) can be expressed as a unified perspective on different\npair generation (PG) strategies. Unfortunately, in the metric loss (ML), it is\ninfeasible to generate negative pairs taking all classes into account in each\niteration because of the limited mini-batch size. In contrast, in\nclassification loss (CL), it is difficult to generate extremely hard negative\npairs owing to the convergence of the class weight vectors to their center.\nThis leads to a mismatch between the two similarity distributions of the\nsampled pairs and all negative pairs. Thus, this paper proposes a unified\nnegative pair generation (UNPG) by combining two PG strategies (i.e., MLPG and\nCLPG) from a unified perspective to alleviate the mismatch. UNPG introduces\nuseful information about negative pairs using MLPG to overcome the CLPG\ndeficiency. Moreover, it includes filtering the similarities of noisy negative\npairs to guarantee reliable convergence and improved performance. Exhaustive\nexperiments show the superiority of UNPG by achieving state-of-the-art\nperformance across recent loss functions on public benchmark datasets. Our code\nand pretrained models are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Junuk Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seonhoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Heung-Seon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joochan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sungbin Son</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Residual Networks for Gaze Mapping on Indian Roads. (arXiv:2203.11611v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11611","description":"<p>In the recent past, greater accessibility to powerful computational resources\nhas enabled progress in the field of Deep Learning and Computer Vision to grow\nby leaps and bounds. This in consequence has lent progress to the domain of\nAutonomous Driving and Navigation Systems. Most of the present research work\nhas been focused on driving scenarios in the European or American roads. Our\npaper draws special attention to the Indian driving context. To this effect, we\npropose a novel architecture, DR-Gaze, which is used to map the driver's gaze\nonto the road. We compare our results with previous works and state-of-the-art\nresults on the DGAZE dataset. Our code will be made publicly available upon\nacceptance of our paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_C/0/1/0/all/0/1\">Chaitanya Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Kshitij Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnoi_S/0/1/0/all/0/1\">Soumya Vishnoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_S/0/1/0/all/0/1\">Sriram Ramanathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution Iterative Feedback Network for Camouflaged Object Detection. (arXiv:2203.11624v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11624","description":"<p>Spotting camouflaged objects that are visually assimilated into the\nbackground is tricky for both object detection algorithms and humans who are\nusually confused or cheated by the perfectly intrinsic similarities between the\nforeground objects and the background surroundings. To tackle this challenge,\nwe aim to extract the high-resolution texture details to avoid the detail\ndegradation that causes blurred vision in edges and boundaries. We introduce a\nnovel HitNet to refine the low-resolution representations by high-resolution\nfeatures in an iterative feedback manner, essentially a global loop-based\nconnection among the multi-scale resolutions. In addition, an iterative\nfeedback loss is proposed to impose more constraints on each feedback\nconnection. Extensive experiments on four challenging datasets demonstrate that\nour \\ourmodel~breaks the performance bottleneck and achieves significant\nimprovements compared with 29 state-of-the-art methods. To address the data\nscarcity in camouflaged scenarios, we provide an application example by\nemploying cross-domain learning to extract the features that can reflect the\ncamouflaged object properties and embed the features into salient objects,\nthereby generating more camouflaged training samples from the diverse salient\nobject datasets The code will be available at\nhttps://github.com/HUuxiaobin/HitNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation. (arXiv:2203.11632v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11632","description":"<p>This paper studies the task of conditional Human Motion Animation (cHMA).\nGiven a source image and a driving video, the model should animate the new\nframe sequence, in which the person in the source image should perform a\nsimilar motion as the pose sequence from the driving video. Despite the success\nof Generative Adversarial Network (GANs) methods in image and video synthesis,\nit is still very challenging to conduct cHMA due to the difficulty in\nefficiently utilizing the conditional guided information such as images or\nposes, and generating images of good visual quality. To this end, this paper\nproposes a novel model of learning to Quantize, Scrabble, and Craft (QS-Craft)\nfor conditional human motion animation. The key novelties come from the newly\nintroduced three key steps: quantize, scrabble and craft. Particularly, our\nQS-Craft employs transformer in its structure to utilize the attention\narchitectures. The guided information is represented as a pose coordinate\nsequence extracted from the driving videos. Extensive experiments on human\nmotion datasets validate the efficacy of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuxin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuelin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Simian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos. (arXiv:2203.11637v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11637","description":"<p>Human actions often induce changes of object states such as \"cutting an\napple\", \"cleaning shoes\" or \"pouring coffee\". In this paper, we seek to\ntemporally localize object states (e.g. \"empty\" and \"full\" cup) together with\nthe corresponding state-modifying actions (\"pouring coffee\") in long uncurated\nvideos with minimal supervision. The contributions of this work are threefold.\nFirst, we develop a self-supervised model for jointly learning state-modifying\nactions together with the corresponding object states from an uncurated set of\nvideos from the Internet. The model is self-supervised by the causal ordering\nsignal, i.e. initial object state $\\rightarrow$ manipulating action\n$\\rightarrow$ end state. Second, to cope with noisy uncurated training data,\nour model incorporates a noise adaptive weighting module supervised by a small\nnumber of annotated still images, that allows to efficiently filter out\nirrelevant videos during training. Third, we collect a new dataset with more\nthan 2600 hours of video and 34 thousand changes of object states, and manually\nannotate a part of this data to validate our approach. Our results demonstrate\nsubstantial improvements over prior work in both action and object\nstate-recognition in video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soucek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Sou&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic State Estimation in Cloth Manipulation Tasks. (arXiv:2203.11647v1 [cs.RO])","link":"http://arxiv.org/abs/2203.11647","description":"<p>Understanding of deformable object manipulations such as textiles is a\nchallenge due to the complexity and high dimensionality of the problem.\nParticularly, the lack of a generic representation of semantic states (e.g.,\n\\textit{crumpled}, \\textit{diagonally folded}) during a continuous manipulation\nprocess introduces an obstacle to identify the manipulation type. In this\npaper, we aim to solve the problem of semantic state estimation in cloth\nmanipulation tasks. For this purpose, we introduce a new large-scale\nfully-annotated RGB image dataset showing various human demonstrations of\ndifferent complicated cloth manipulations. We provide a set of baseline deep\nnetworks and benchmark them on the problem of semantic state estimation using\nour proposed dataset. Furthermore, we investigate the scalability of our\nsemantic state estimation framework in robot monitoring tasks of long and\ncomplex cloth manipulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_G/0/1/0/all/0/1\">Georgies Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Erdal Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borras_J/0/1/0/all/0/1\">J&#xfa;lia Borr&#xe0;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alenya_G/0/1/0/all/0/1\">Guillem Aleny&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Salient Object Detection Using Point Supervison. (arXiv:2203.11652v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11652","description":"<p>Current state-of-the-art saliency detection models rely heavily on large\ndatasets of accurate pixel-wise annotations, but manually labeling pixels is\ntime-consuming and labor-intensive. There are some weakly supervised methods\ndeveloped for alleviating the problem, such as image label, bounding box label,\nand scribble label, while point label still has not been explored in this\nfield. In this paper, we propose a novel weakly-supervised salient object\ndetection method using point supervision. To infer the saliency map, we first\ndesign an adaptive masked flood filling algorithm to generate pseudo labels.\nThen we develop a transformer-based point-supervised saliency detection model\nto produce the first round of saliency maps. However, due to the sparseness of\nthe label, the weakly supervised model tends to degenerate into a general\nforeground detection model. To address this issue, we propose a Non-Salient\nSuppression (NSS) method to optimize the erroneous saliency maps generated in\nthe first round and leverage them for the second round of training. Moreover,\nwe build a new point-supervised dataset (P-DUTS) by relabeling the DUTS\ndataset. In P-DUTS, there is only one labeled point for each salient object.\nComprehensive experiments on five largest benchmark datasets demonstrate our\nmethod outperforms the previous state-of-the-art methods trained with the\nstronger supervision and even surpass several fully supervised state-of-the-art\nmodels. The code is available at: https://github.com/shuyonggao/PSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yangji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11654","description":"<p>Scene graph generation (SGG) aims to extract (subject, predicate, object)\ntriplets in images. Recent works have made a steady progress on SGG, and\nprovide useful tools for high-level vision and language understanding. However,\ndue to the data distribution problems including long-tail distribution and\nsemantic ambiguity, the predictions of current SGG models tend to collapse to\nseveral frequent but uninformative predicates (e.g., \\textit{on}, \\textit{at}),\nwhich limits practical application of these models in downstream tasks. To deal\nwith the problems above, we propose a novel Internal and External Data Transfer\n(IETrans) method, which can be applied in a play-and-plug fashion and expanded\nto large SGG with 1,807 predicate classes. Our IETrans tries to relieve the\ndata distribution problem by automatically creating an enhanced dataset that\nprovides more sufficient and coherent annotations for all predicates. By\ntraining on the transferred dataset, a Neural Motif model doubles the macro\nperformance while maintaining competitive micro performance. The data and code\nfor this paper are publicly available at\n\\url{https://github.com/waxnkw/IETrans-SGG.pytorch}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Self-Supervision for Online Knowledge Distillation. (arXiv:2203.11660v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11660","description":"<p>Recently, researchers have shown an increased interest in the online\nknowledge distillation. Adopting an one-stage and end-to-end training fashion,\nonline knowledge distillation uses aggregated intermediated predictions of\nmultiple peer models for training. However, the absence of a powerful teacher\nmodel may result in the homogeneity problem between group peers, affecting the\neffectiveness of group distillation adversely. In this paper, we propose a\nnovel online knowledge distillation method, \\textbf{C}hannel\n\\textbf{S}elf-\\textbf{S}upervision for Online Knowledge Distillation (CSS),\nwhich structures diversity in terms of input, target, and network to alleviate\nthe homogenization problem. Specifically, we construct a dual-network\nmulti-branch structure and enhance inter-branch diversity through\nself-supervised learning, adopting the feature-level transformation and\naugmenting the corresponding labels. Meanwhile, the dual network structure has\na larger space of independent parameters to resist the homogenization problem\nduring distillation. Extensive quantitative experiments on CIFAR-100 illustrate\nthat our method provides greater diversity than OKDDip and we also give pretty\nperformance improvement, even over the state-of-the-art such as PCL. The\nresults on three fine-grained datasets (StanfordDogs, StanfordCars,\nCUB-200-211) also show the significant generalization capability of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shixiao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_P/0/1/0/all/0/1\">Pan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNNs and Transformers Perceive Hybrid Images Similar to Humans. (arXiv:2203.11678v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11678","description":"<p>Hybrid images is a technique to generate images with two interpretations that\nchange as a function of viewing distance. It has been utilized to study\nmultiscale processing of images by the human visual system. Using 63,000 hybrid\nimages across 10 fruit categories, here we show that predictions of deep\nlearning vision models qualitatively matches with the human perception of these\nimages. Our results provide yet another evidence in support of the hypothesis\nthat Convolutional Neural Networks (CNNs) and Transformers are good at modeling\nthe feedforward sweep of information in the ventral stream of visual cortex.\nCode and data is available at https://github.com/aliborji/hybrid_images.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-attention for ViT-backed Continual Learning. (arXiv:2203.11684v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11684","description":"<p>Continual learning is a longstanding research topic due to its crucial role\nin tackling continually arriving tasks. Up to now, the study of continual\nlearning in computer vision is mainly restricted to convolutional neural\nnetworks (CNNs). However, recently there is a tendency that the newly emerging\nvision transformers (ViTs) are gradually dominating the field of computer\nvision, which leaves CNN-based continual learning lagging behind as they can\nsuffer from severe performance degradation if straightforwardly applied to\nViTs. In this paper, we study ViT-backed continual learning to strive for\nhigher performance riding on recent advances of ViTs. Inspired by mask-based\ncontinual learning methods in CNNs, where a mask is learned per task to adapt\nthe pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,\nattention to self-attention, to adapt a pre-trained ViT to new tasks without\nsacrificing performance on already learned tasks. Unlike prior mask-based\nmethods like Piggyback, where all parameters are associated with corresponding\nmasks, MEAT leverages the characteristics of ViTs and only masks a portion of\nits parameters. It renders MEAT more efficient and effective with less overhead\nand higher accuracy. Extensive experiments demonstrate that MEAT exhibits\nsignificant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%\nabsolute boosts in accuracy. Our code has been released at\nhttps://github.com/zju-vipa/MEAT-TIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Learned Block-Based Image Compression with Block-Level Masked Convolutions and Asymptotic Closed Loop Training. (arXiv:2203.11686v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11686","description":"<p>Learned image compression research has achieved state-of-the-art compression\nperformance with auto-encoder based neural network architectures, where the\nimage is mapped via convolutional neural networks (CNN) into a latent\nrepresentation that is quantized and processed again with CNN to obtain the\nreconstructed image. CNN operate on entire input images. On the other hand,\ntraditional state-of-the-art image and video compression methods process images\nwith a block-by-block processing approach for various reasons. Very recently,\nwork on learned image compression with block based approaches have also\nappeared, which use the auto-encoder architecture on large blocks of the input\nimage and introduce additional neural networks that perform intra/spatial\nprediction and deblocking/post-processing functions. This paper explores an\nalternative learned block-based image compression approach in which neither an\nexplicit intra prediction neural network nor an explicit deblocking neural\nnetwork is used. A single auto-encoder neural network with block-level masked\nconvolutions is used and the block size is much smaller (8x8). By using\nblock-level masked convolutions, each block is processed using reconstructed\nneighboring left and upper blocks both at the encoder and decoder. Hence, the\nmutual information between adjacent blocks is exploited during compression and\neach block is reconstructed using neighboring blocks, resolving the need for\nexplicit intra prediction and deblocking neural networks. Since the explored\nsystem is a closed loop system, a special optimization procedure, the\nasymptotic closed loop design, is used with standard stochastic gradient\ndescent based training. The experimental results indicate competitive image\ncompression performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kamisli_F/0/1/0/all/0/1\">Fatih Kamisli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11692","description":"<p>This manuscript describes the panoptic segmentation method we devised for our\nsubmission to the CONIC challenge at ISBI 2022. Key features of our method are\na weighted loss that we specifically engineered for semantic segmentation of\nhighly imbalanced cell types, and an existing state-of-the art nuclei instance\nsegmentation model, which we combine in a Hovernet-like architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_E/0/1/0/all/0/1\">Elias Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Based Motion Detection for Autonomous Driving. (arXiv:2203.11693v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11693","description":"<p>Motion detection is a fundamental but challenging task for autonomous\ndriving. In particular scenes like highway, remote objects have to be paid\nextra attention for better controlling decision. Aiming at distant vehicles, we\ntrain a neural network model to classify the motion status using optical flow\nfield information as the input. The experiments result in high accuracy,\nshowing that our idea is viable and promising. The trained model also achieves\nan acceptable performance for nearby vehicles. Our work is implemented in\nPyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used.\nVisualization videos are available at\nhttps://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Ka Man Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation. (arXiv:2203.11709v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11709","description":"<p>Recent advances in self-supervised contrastive learning yield good\nimage-level representation, which favors classification tasks but usually\nneglects pixel-level detailed information, leading to unsatisfactory transfer\nperformance to dense prediction tasks such as semantic segmentation. In this\nwork, we propose a pixel-wise contrastive learning method called CP2\n(Copy-Paste Contrastive Pretraining), which facilitates both image- and\npixel-level representation learning and therefore is more suitable for\ndownstream dense prediction tasks. In detail, we copy-paste a random crop from\nan image (the foreground) onto different background images and pretrain a\nsemantic segmentation model with the objective of 1) distinguishing the\nforeground pixels from the background pixels, and 2) identifying the composed\nimages that share the same foreground.Experiments show the strong performance\nof CP2 in downstream semantic segmentation: By finetuning CP2 pretrained models\non PASCAL VOC 2012, we obtain 78.6% mIoU with a ResNet-50 and 79.5% with a\nViT-S.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain. (arXiv:2203.11722v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11722","description":"<p>Digital breast tomosynthesis (DBT) exams should utilize the lowest possible\nradiation dose while maintaining sufficiently good image quality for accurate\nmedical diagnosis. In this work, we propose a convolution neural network (CNN)\nto restore low-dose (LD) DBT projections to achieve an image quality equivalent\nto a standard full-dose (FD) acquisition. The proposed network architecture\nbenefits from priors in terms of layers that were inspired by traditional\nmodel-based (MB) restoration methods, considering a model-based deep learning\napproach, where the network is trained to operate in the variance stabilization\ntransformation (VST) domain. To accurately control the network operation point,\nin terms of noise and blur of the restored image, we propose a loss function\nthat minimizes the bias and matches residual noise between the input and the\noutput. The training dataset was composed of clinical data acquired at the\nstandard FD and low-dose pairs obtained by the injection of quantum noise. The\nnetwork was tested using real DBT projections acquired with a physical\nanthropomorphic breast phantom. The proposed network achieved superior results\nin terms of the mean normalized squared error (MNSE), training time and noise\nspatial correlation compared with networks trained with traditional data-driven\nmethods. The proposed approach can be extended for other medical imaging\napplication that requires LD acquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vimieiro_R/0/1/0/all/0/1\">Rodrigo de Barros Vimieiro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borges_L/0/1/0/all/0/1\">Lucas Rodrigues Borges</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vieira_M/0/1/0/all/0/1\">Marcelo Andrade da Costa Vieira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Anomaly Detection in Medical Images with a Memory-augmented Multi-level Cross-attentional Masked Autoencoder. (arXiv:2203.11725v1 [eess.IV])","link":"http://arxiv.org/abs/2203.11725","description":"<p>Unsupervised anomaly detection (UAD) aims to find anomalous images by\noptimising a detector using a training set that contains only normal images.\nUAD approaches can be based on reconstruction methods, self-supervised\napproaches, and Imagenet pre-trained models. Reconstruction methods, which\ndetect anomalies from image reconstruction errors, are advantageous because\nthey do not rely on the design of problem-specific pretext tasks needed by\nself-supervised approaches, and on the unreliable translation of models\npre-trained from non-medical datasets. However, reconstruction methods may fail\nbecause they can have low reconstruction errors even for anomalous images. In\nthis paper, we introduce a new reconstruction-based UAD approach that addresses\nthis low-reconstruction error issue for anomalous images. Our UAD approach, the\nmemory-augmented multi-level cross-attentional masked autoencoder (MemMC-MAE),\nis a transformer-based approach, consisting of a novel memory-augmented\nself-attention operator for the encoder and a new multi-level cross-attention\noperator for the decoder. MemMC-MAE masks large parts of the input image during\nits reconstruction, reducing the risk that it will produce low reconstruction\nerrors because anomalies are likely to be masked and cannot be reconstructed.\nHowever, when the anomaly is not masked, then the normal patterns stored in the\nencoder's memory combined with the decoder's multi-level cross-attention will\nconstrain the accurate reconstruction of the anomaly. We show that our method\nachieves SOTA anomaly detection and localisation on colonoscopy and Covid-19\nChest X-ray datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W Verjans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography. (arXiv:2203.11726v1 [physics.med-ph])","link":"http://arxiv.org/abs/2203.11726","description":"<p>Left ventricular (LV) function is an important factor in terms of patient\nmanagement, outcome, and long-term survival of patients with heart disease. The\nmost recently published clinical guidelines for heart failure recognise that\nover reliance on only one measure of cardiac function (LV ejection fraction) as\na diagnostic and treatment stratification biomarker is suboptimal. Recent\nadvances in AI-based echocardiography analysis have shown excellent results on\nautomated estimation of LV volumes and LV ejection fraction. However, from\ntime-varying 2-D echocardiography acquisition, a richer description of cardiac\nfunction can be obtained by estimating functional biomarkers from the complete\ncardiac cycle. In this work we propose for the first time an AI approach for\nderiving advanced biomarkers of systolic and diastolic LV function from 2-D\nechocardiography based on segmentations of the full cardiac cycle. These\nbiomarkers will allow clinicians to obtain a much richer picture of the heart\nin health and disease. The AI model is based on the 'nn-Unet' framework and was\ntrained and tested using four different databases. Results show excellent\nagreement between manual and automated analysis and showcase the potential of\nthe advanced systolic and diastolic biomarkers for patient stratification.\nFinally, for a subset of 50 cases, we perform a correlation analysis between\nclinical biomarkers derived from echocardiography and CMR and we show excellent\nagreement between the two modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Ant&#xf3;n</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sidhu_B/0/1/0/all/0/1\">Baldeep S. Sidhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gould_J/0/1/0/all/0/1\">Justin Gould</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Porter_B/0/1/0/all/0/1\">Bradley Porter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Elliott_M/0/1/0/all/0/1\">Mark K. Elliott</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mehta_V/0/1/0/all/0/1\">Vishal Mehta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gu_H/0/1/0/all/0/1\">Haotian Gu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xochicale_M/0/1/0/all/0/1\">Miguel Xochicale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gomez_A/0/1/0/all/0/1\">Alberto Gomez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rinaldi_C/0/1/0/all/0/1\">Christopher A. Rinaldi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cowie_M/0/1/0/all/0/1\">Martin Cowie</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chowienczyk_P/0/1/0/all/0/1\">Phil Chowienczyk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProgressiveMotionSeg: Mutually Reinforced Framework for Event-Based Motion Segmentation. (arXiv:2203.11732v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11732","description":"<p>Dynamic Vision Sensor (DVS) can asynchronously output the events reflecting\napparent motion of objects with microsecond resolution, and shows great\napplication potential in monitoring and other fields. However, the output event\nstream of existing DVS inevitably contains background activity noise (BA noise)\ndue to dark current and junction leakage current, which will affect the\ntemporal correlation of objects, resulting in deteriorated motion estimation\nperformance. Particularly, the existing filter-based denoising methods cannot\nbe directly applied to suppress the noise in event stream, since there is no\nspatial correlation. To address this issue, this paper presents a novel\nprogressive framework, in which a Motion Estimation (ME) module and an Event\nDenoising (ED) module are jointly optimized in a mutually reinforced manner.\nSpecifically, based on the maximum sharpness criterion, ME module divides the\ninput event into several segments by adaptive clustering in a motion\ncompensating warp field, and captures the temporal correlation of event stream\naccording to the clustered motion parameters. Taking temporal correlation as\nguidance, ED module calculates the confidence that each event belongs to real\nactivity events, and transmits it to ME module to update energy function of\nmotion segmentation for noise suppression. The two steps are iteratively\nupdated until stable motion segmentation results are obtained. Extensive\nexperimental results on both synthetic and real datasets demonstrate the\nsuperiority of our proposed approaches against the State-Of-The-Art (SOTA)\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring and Evaluating Image Restoration Potential in Dynamic Scenes. (arXiv:2203.11754v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11754","description":"<p>In dynamic scenes, images often suffer from dynamic blur due to superposition\nof motions or low signal-noise ratio resulted from quick shutter speed when\navoiding motions. Recovering sharp and clean results from the captured images\nheavily depends on the ability of restoration methods and the quality of the\ninput. Although existing research on image restoration focuses on developing\nmodels for obtaining better restored results, fewer have studied to evaluate\nhow and which input image leads to superior restored quality. In this paper, to\nbetter study an image's potential value that can be explored for restoration,\nwe propose a novel concept, referring to image restoration potential (IRP).\nSpecifically, We first establish a dynamic scene imaging dataset containing\ncomposite distortions and applied image restoration processes to validate the\nrationality of the existence to IRP. Based on this dataset, we investigate\nseveral properties of IRP and propose a novel deep model to accurately predict\nIRP values. By gradually distilling and selective fusing the degradation\nfeatures, the proposed model shows its superiority in IRP prediction. Thanks to\nthe proposed model, we are then able to validate how various image restoration\nrelated applications are benefited from IRP prediction. We show the potential\nusages of IRP as a filtering principle to select valuable frames, an auxiliary\nguidance to improve restoration models, and even an indicator to optimize\ncamera settings for capturing better images under dynamic scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shaolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinqiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection. (arXiv:2203.11797v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11797","description":"<p>Deep convolutional neural networks have shown remarkable results on multiple\ndetection tasks. Despite the significant progress, the performance of such\ndetectors are often assessed in public benchmarks under non-realistic\nconditions. Specifically, impact of conventional distortions and processing\noperations such as compression, noise, and enhancement are not sufficiently\nstudied. This paper proposes a rigorous framework to assess performance of\nlearning-based detectors in more realistic situations. An illustrative example\nis shown under deepfake detection context. Inspired by the assessment results,\na data augmentation strategy based on natural image degradation process is\ndesigned, which significantly improves the generalization ability of two\ndeepfake detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruizhi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_T/0/1/0/all/0/1\">Touradj Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11799","description":"<p>Blind-spot network (BSN) and its variants have made significant advances in\nself-supervised denoising. Nevertheless, they are still bound to synthetic\nnoisy inputs due to less practical assumptions like pixel-wise independent\nnoise. Hence, it is challenging to deal with spatially correlated real-world\nnoise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has\nbeen proposed to remove the spatial correlation of real-world noise. However,\nit is not trivial to integrate PD and BSN directly, which prevents the fully\nself-supervised denoising model on real-world images. We propose an Asymmetric\nPD (AP) to address this issue, which introduces different PD stride factors for\ntraining and inference. We systematically demonstrate that the proposed AP can\nresolve inherent trade-offs caused by specific PD stride factors and make BSN\napplicable to practical scenarios. To this end, we develop AP-BSN, a\nstate-of-the-art self-supervised denoising method for real-world sRGB images.\nWe further propose random-replacing refinement, which significantly improves\nthe performance of our AP-BSN without any additional parameters. Extensive\nstudies demonstrate that our method outperforms the other self-supervised and\neven unpaired denoising methods by a large margin, without using any additional\nknowledge, e.g., noise level, regarding the underlying unknown noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions. (arXiv:2203.11807v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11807","description":"<p>Deep convolutional neural networks have achieved exceptional results on\nmultiple detection and recognition tasks. However, the performance of such\ndetectors are often evaluated in public benchmarks under constrained and\nnon-realistic situations. The impact of conventional distortions and processing\noperations found in imaging workflows such as compression, noise, and\nenhancement are not sufficiently studied. Currently, only a few researches have\nbeen done to improve the detector robustness to unseen perturbations. This\npaper proposes a more effective data augmentation scheme based on real-world\nimage degradation process. This novel technique is deployed for deepfake\ndetection tasks and has been evaluated by a more realistic assessment\nframework. Extensive experiments show that the proposed data augmentation\nscheme improves generalization ability to unpredictable data distortions and\nunseen datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_T/0/1/0/all/0/1\">Touradj Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Broad Study of Pre-training for Domain Generalization and Adaptation. (arXiv:2203.11819v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11819","description":"<p>Deep models must learn robust and transferable representations in order to\nperform well on new domains. While domain transfer methods (e.g., domain\nadaptation, domain generalization) have been proposed to learn transferable\nrepresentations across domains, they are typically applied to ResNet backbones\npre-trained on ImageNet. Thus, existing works pay little attention to the\neffects of pre-training on domain transfer tasks. In this paper, we provide a\nbroad study and in-depth analysis of pre-training for domain adaptation and\ngeneralization, namely: network architectures, size, pre-training loss, and\ndatasets. We observe that simply using a state-of-the-art backbone outperforms\nexisting state-of-the-art domain adaptation baselines and set new baselines on\nOffice-Home and DomainNet improving by 10.7\\% and 5.5\\%. We hope that this work\ncan provide more insights for future domain transfer research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Was that so hard? Estimating human classification difficulty. (arXiv:2203.11824v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11824","description":"<p>When doctors are trained to diagnose a specific disease, they learn faster\nwhen presented with cases in order of increasing difficulty. This creates the\nneed for automatically estimating how difficult it is for doctors to classify a\ngiven case. In this paper, we introduce methods for estimating how hard it is\nfor a doctor to diagnose a case represented by a medical image, both when\nground truth difficulties are available for training, and when they are not.\nOur methods are based on embeddings obtained with deep metric learning.\nAdditionally, we introduce a practical method for obtaining ground truth human\ndifficulty for each image case in a dataset using self-assessed certainty. We\napply our methods to two different medical datasets, achieving high Kendall\nrank correlation coefficients, showing that we outperform existing methods by a\nlarge margin on our problem and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hannemose_M/0/1/0/all/0/1\">Morten Rieger Hannemose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundgaard_J/0/1/0/all/0/1\">Josefine Vilsb&#xf8;ll Sundgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ternov_N/0/1/0/all/0/1\">Niels Kvorning Ternov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulsen_R/0/1/0/all/0/1\">Rasmus R. Paulsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1\">Anders Nymark Christensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-View Panorama Image Synthesis. (arXiv:2203.11832v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11832","description":"<p>In this paper, we tackle the problem of synthesizing a ground-view panorama\nimage conditioned on a top-view aerial image, which is a challenging problem\ndue to the large gap between the two image domains with different view-points.\nInstead of learning cross-view mapping in a feedforward pass, we propose a\nnovel adversarial feedback GAN framework named PanoGAN with two key components:\nan adversarial feedback module and a dual branch discrimination strategy.\nFirst, the aerial image is fed into the generator to produce a target panorama\nimage and its associated segmentation map in favor of model training with\nlayout semantics. Second, the feature responses of the discriminator encoded by\nour adversarial feedback module are fed back to the generator to refine the\nintermediate representations, so that the generation performance is continually\nimproved through an iterative generation process. Third, to pursue\nhigh-fidelity and semantic consistency of the generated panorama image, we\npropose a pixel-segmentation alignment mechanism under the dual branch\ndiscrimiantion strategy to facilitate cooperation between the generator and the\ndiscriminator. Extensive experimental results on two challenging cross-view\nimage datasets show that PanoGAN enables high-quality panorama image generation\nwith more convincing details than state-of-the-art approaches. The source code\nand trained models are available at \\url{https://github.com/sswuai/PanoGAN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songsong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1\">Xiao-Yuan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11834","description":"<p>Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caldarola_D/0/1/0/all/0/1\">Debora Caldarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-time Junk Food Recognition System based on Machine Learning. (arXiv:2203.11836v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11836","description":"<p>$ $As a result of bad eating habits, humanity may be destroyed. People are\nconstantly on the lookout for tasty foods, with junk foods being the most\ncommon source. As a consequence, our eating patterns are shifting, and we're\ngravitating toward junk food more than ever, which is bad for our health and\nincreases our risk of acquiring health problems. Machine learning principles\nare applied in every aspect of our lives, and one of them is object recognition\nvia image processing. However, because foods vary in nature, this procedure is\ncrucial, and traditional methods like ANN, SVM, KNN, PLS etc., will result in a\nlow accuracy rate. All of these issues were defeated by the Deep Neural\nNetwork. In this work, we created a fresh dataset of 10,000 data points from 20\njunk food classifications to try to recognize junk foods. All of the data in\nthe data set was gathered using the Google search engine, which is thought to\nbe one-of-a-kind in every way. The goal was achieved using Convolution Neural\nNetwork (CNN) technology, which is well-known for image processing. We achieved\na 98.05\\% accuracy rate throughout the research, which was satisfactory. In\naddition, we conducted a test based on a real-life event, and the outcome was\nextraordinary. Our goal is to advance this research to the next level, so that\nit may be applied to a future study. Our ultimate goal is to create a system\nthat would encourage people to avoid eating junk food and to be\nhealth-conscious. \\keywords{ Machine Learning \\and junk food \\and object\ndetection \\and YOLOv3 \\and custom food dataset.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shifat_S/0/1/0/all/0/1\">Sirajum Munira Shifat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthib_T/0/1/0/all/0/1\">Takitazwar Parthib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyaasa_S/0/1/0/all/0/1\">Sabikunnahar Talukder Pyaasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaity_N/0/1/0/all/0/1\">Nila Maitra Chaity</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Niloy Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morol_M/0/1/0/all/0/1\">Md. Kishor Morol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11853","description":"<p>With rising powerful, low-cost embedded devices, the edge computing has\nbecome an increasingly popular choice. In this paper, we propose a new\nincremental local stochastic gradient descent (SGD) tailored on the Raspberry\nPi to deal with large ImageNet dataset having 1,261,405 images with 1,000\nclasses. The local SGD splits the data block into $k$ partitions using $k$means\nalgorithm and then it learns in the parallel way SGD models in each data\npartition to classify the data locally. The incremental local SGD sequentially\nloads small data blocks of the training dataset to learn local SGD models. The\nnumerical test results on Imagenet dataset show that our incremental local SGD\nalgorithm with the Raspberry Pi 4 is faster and more accurate than the\nstate-of-the-art linear SVM run on a PC Intel(R) Core i7-4790 CPU, 3.6 GHz, 4\ncores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thanh-Nghi Do</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating natural images with direct Patch Distributions Matching. (arXiv:2203.11862v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11862","description":"<p>Many traditional computer vision algorithms generate realistic images by\nrequiring that each patch in the generated image be similar to a patch in a\ntraining image and vice versa. Recently, this classical approach has been\nreplaced by adversarial training with a patch discriminator. The adversarial\napproach avoids the computational burden of finding nearest neighbors of\npatches but often requires very long training times and may fail to match the\ndistribution of patches. In this paper we leverage the recently developed\nSliced Wasserstein Distance and develop an algorithm that explicitly and\nefficiently minimizes the distance between patch distributions in two images.\nOur method is conceptually simple, requires no training and can be implemented\nin a few lines of codes. On a number of image generation tasks we show that our\nresults are often superior to single-image-GANs, require no training, and can\ngenerate high quality images in a few seconds. Our implementation is available\nat https://github.com/ariel415el/GPDM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elnekave_A/0/1/0/all/0/1\">Ariel Elnekave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_Y/0/1/0/all/0/1\">Yair Weiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary DETR with Conditional Matching. (arXiv:2203.11876v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11876","description":"<p>Open-vocabulary object detection, which is concerned with the problem of\ndetecting novel objects guided by natural language, has gained increasing\nattention from the community. Ideally, we would like to extend an\nopen-vocabulary detector such that it can produce bounding box predictions\nbased on user inputs in form of either natural language or exemplar image. This\noffers great flexibility and user experience for human-computer interaction. To\nthis end, we propose a novel open-vocabulary detector based on DETR -- hence\nthe name OV-DETR -- which, once trained, can detect any object given its class\nname or an exemplar image. The biggest challenge of turning DETR into an\nopen-vocabulary detector is that it is impossible to calculate the\nclassification cost matrix of novel classes without access to their labeled\nimages. To overcome this challenge, we formulate the learning objective as a\nbinary matching one between input queries (class name or exemplar image) and\nthe corresponding objects, which learns useful correspondence to generalize to\nunseen queries during testing. For training, we choose to condition the\nTransformer decoder on the input embeddings obtained from a pre-trained\nvision-language model like CLIP, in order to enable matching for both text and\nimage queries. With extensive experiments on LVIS and COCO datasets, we\ndemonstrate that our OV-DETR -- the first end-to-end Transformer-based\nopen-vocabulary detector -- achieves non-trivial improvements over current\nstate of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1\">Yuhang Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Under the Hood of Transformer Networks for Trajectory Forecasting. (arXiv:2203.11878v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11878","description":"<p>Transformer Networks have established themselves as the de-facto\nstate-of-the-art for trajectory forecasting but there is currently no\nsystematic study on their capability to model the motion patterns of people,\nwithout interactions with other individuals nor the social context. This paper\nproposes the first in-depth study of Transformer Networks (TF) and\nBidirectional Transformers (BERT) for the forecasting of the individual motion\nof people, without bells and whistles. We conduct an exhaustive evaluation of\ninput/output representations, problem formulations and sequence modeling,\nincluding a novel analysis of their capability to predict multi-modal futures.\nOut of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are\ntop performers in predicting individual motions, definitely overcoming RNNs and\nLSTMs. Furthermore, they remain within a narrow margin wrt more complex\ntechniques, which include both social interactions and scene contexts. Source\ncode will be released for all conducted experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franco_L/0/1/0/all/0/1\">Luca Franco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Placidi_L/0/1/0/all/0/1\">Leonardo Placidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giuliari_F/0/1/0/all/0/1\">Francesco Giuliari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_I/0/1/0/all/0/1\">Irtiza Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11894","description":"<p>In this work we demonstrate the vulnerability of vision transformers (ViTs)\nto gradient-based inversion attacks. During this attack, the original data\nbatch is reconstructed given model weights and the corresponding gradients. We\nintroduce a method, named GradViT, that optimizes random noise into naturally\nlooking images via an iterative process. The optimization objective consists of\n(i) a loss on matching the gradients, (ii) image prior in the form of distance\nto batch-normalization statistics of a pretrained CNN model, and (iii) a total\nvariation regularization on patches to guide correct recovery locations. We\npropose a unique loss scheduling function to overcome local minima during\noptimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and\nobserve unprecedentedly high fidelity and closeness to the original (hidden)\ndata. During the analysis we find that vision transformers are significantly\nmore vulnerable than previously studied CNNs due to the presence of the\nattention mechanism. Our method demonstrates new state-of-the-art results for\ngradient inversion in both qualitative and quantitative metrics. Project page\nat https://gradvit.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection, Recognition, and Tracking: A Survey. (arXiv:2203.11900v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11900","description":"<p>For humans, object detection, recognition, and tracking are innate. These\nprovide the ability for human to perceive their environment and objects within\ntheir environment. This ability however doesn't translate well in computers. In\nComputer Vision and Multimedia, it is becoming increasingly more important to\ndetect, recognize and track objects in images and/or videos. Many of these\napplications, such as facial recognition, surveillance, animation, are used for\ntracking features and/or people. However, these tasks prove challenging for\ncomputers to do effectively, as there is a significant amount of data to parse\nthrough. Therefore, many techniques and algorithms are needed and therefore\nresearched to try to achieve human like perception. In this literature review,\nwe focus on some novel techniques on object detection and recognition, and how\nto apply tracking algorithms to the detected features to track the objects'\nmovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Song_D/0/1/0/all/0/1\">Dale Chen-Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling faster and more reliable sonographic assessment of gestational age through machine learning. (arXiv:2203.11903v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11903","description":"<p>Fetal ultrasounds are an essential part of prenatal care and can be used to\nestimate gestational age (GA). Accurate GA assessment is important for\nproviding appropriate prenatal care throughout pregnancy and identifying\ncomplications such as fetal growth disorders. Since derivation of GA from\nmanual fetal biometry measurements (head, abdomen, femur) are\noperator-dependent and time-consuming, there have been a number of research\nefforts focused on using artificial intelligence (AI) models to estimate GA\nusing standard biometry images, but there is still room to improve the accuracy\nand reliability of these AI systems for widescale adoption. To improve GA\nestimates, without significant change to provider workflows, we leverage AI to\ninterpret standard plane ultrasound images as well as 'fly-to' ultrasound\nvideos, which are 5-10s videos automatically recorded as part of the standard\nof care before the still image is captured. We developed and validated three AI\nmodels: an image model using standard plane images, a video model using fly-to\nvideos, and an ensemble model (combining both image and video). All three were\nstatistically superior to standard fetal biometry-based GA estimates derived by\nexpert sonographers, the ensemble model has the lowest mean absolute error\n(MAE) compared to the clinical standard fetal biometry (mean difference: -1.51\n$\\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404\nparticipants. We showed that our models outperform standard biometry by a more\nsubstantial margin on fetuses that were small for GA. Our AI models have the\npotential to empower trained operators to estimate GA with higher accuracy\nwhile reducing the amount of time required and user variability in measurement\nacquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chace Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_A/0/1/0/all/0/1\">Angelica Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Christina Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieniek_M/0/1/0/all/0/1\">Marcin Sieniek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">Akib Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jonny Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilgrim_R/0/1/0/all/0/1\">Rory Pilgrim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1\">Katherine Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1\">Daniel Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Shravya Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_R/0/1/0/all/0/1\">Ryan G. Gomes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Predictivity in the Visual Cortex with Gated Recurrent Connections. (arXiv:2203.11910v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11910","description":"<p>Computational models of vision have traditionally been developed in a\nbottom-up fashion, by hierarchically composing a series of straightforward\noperations - i.e. convolution and pooling - with the aim of emulating simple\nand complex cells in the visual cortex, resulting in the introduction of deep\nconvolutional neural networks (CNNs). Nevertheless, data obtained with recent\nneuronal recording techniques support that the nature of the computations\ncarried out in the ventral visual stream is not completely captured by current\ndeep CNN models. To fill the gap between the ventral visual stream and deep\nmodels, several benchmarks have been designed and organized into the\nBrain-Score platform, granting a way to perform multi-layer (V1, V2, V4, IT)\nand behavioral comparisons between the two counterparts. In our work, we aim to\nshift the focus on architectures that take into account lateral recurrent\nconnections, a ubiquitous feature of the ventral visual stream, to devise\nadaptive receptive fields. Through recurrent connections, the input s\nlong-range spatial dependencies can be captured in a local multi-step fashion\nand, as introduced with Gated Recurrent CNNs (GRCNN), the unbounded expansion\nof the neuron s receptive fields can be modulated through the use of gates. In\norder to increase the robustness of our approach and the biological fidelity of\nthe activations, we employ specific data augmentation techniques in line with\nseveral of the scoring benchmarks. Enforcing some form of invariance, through\nheuristics, was found to be beneficial for better neural predictivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azeglio_S/0/1/0/all/0/1\">Simone Azeglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poetto_S/0/1/0/all/0/1\">Simone Poetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aira_L/0/1/0/all/0/1\">Luca Savant Aira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurisso_M/0/1/0/all/0/1\">Marco Nurisso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal Modulation Networks. (arXiv:2203.11926v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11926","description":"<p>In this work, we propose focal modulation network (FocalNet in short), where\nself-attention (SA) is completely replaced by a focal modulation module that is\nmore effective and efficient for modeling token interactions. Focal modulation\ncomprises three components: $(i)$ hierarchical contextualization, implemented\nusing a stack of depth-wise convolutional layers, to encode visual contexts\nfrom short to long ranges at different granularity levels, $(ii)$ gated\naggregation to selectively aggregate context features for each visual token\n(query) based on its content, and $(iii)$ modulation or element-wise affine\ntransformation to fuse the aggregated features into the query vector. Extensive\nexperiments show that FocalNets outperform the state-of-the-art SA counterparts\n(e.g., Swin Transformers) with similar time and memory cost on the tasks of\nimage classification, object detection, and semantic segmentation.\nSpecifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9%\ntop-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains\n86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\\times$224\nand 384$\\times$384, respectively. FocalNets exhibit remarkable superiority when\ntransferred to downstream tasks. For object detection with Mask R-CNN, our\nFocalNet base trained with 1$\\times$ already surpasses Swin trained with\n3$\\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet,\nFocalNet base evaluated at single-scale outperforms Swin evaluated at\nmulti-scale (50.5 v.s. 49.7). These results render focal modulation a favorable\nalternative to SA for effective and efficient visual modeling in real-world\napplications. Code is available at https://github.com/microsoft/FocalNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Distillation by Matching Training Trajectories. (arXiv:2203.11932v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11932","description":"<p>Dataset distillation is the task of synthesizing a small dataset such that a\nmodel trained on the synthetic set will match the test accuracy of the model\ntrained on the full dataset. In this paper, we propose a new formulation that\noptimizes our distilled data to guide networks to a similar state as those\ntrained on real data across many training steps. Given a network, we train it\nfor several iterations on our distilled data and optimize the distilled data\nwith respect to the distance between the synthetically trained parameters and\nthe parameters trained on real data. To efficiently obtain the initial and\ntarget network parameters for large-scale datasets, we pre-compute and store\ntraining trajectories of expert networks trained on the real dataset. Our\nmethod handily outperforms existing methods and also allows us to distill\nhigher-resolution visual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1\">George Cazenavette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tongzhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v1 [cs.LG])","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these harms. Prior proposed bias\nmeasurements lack robustness and feature degradation occurs when mitigating\nbias without access to pretraining data. We address both of these challenges in\nthis paper: First, we evaluate different bias measures and propose the use of\nretrieval metrics to image-text representations via a bias measuring framework.\nSecond, we investigate debiasing methods and show that optimizing for\nadversarial loss via learnable token embeddings minimizes various bias measures\nwithout substantially degrading feature representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from All Vehicles. (arXiv:2203.11934v1 [cs.RO])","link":"http://arxiv.org/abs/2203.11934","description":"<p>In this paper, we present a system to train driving policies from experiences\ncollected not just from the ego-vehicle, but all vehicles that it observes.\nThis system uses the behaviors of other agents to create more diverse driving\nscenarios without collecting additional data. The main difficulty in learning\nfrom other vehicles is that there is no sensor information. We use a set of\nsupervisory tasks to learn an intermediate representation that is invariant to\nthe viewpoint of the controlling vehicle. This not only provides a richer\nsignal at training time but also allows more complex reasoning during\ninference. Learning how all vehicles drive helps predict their behavior at test\ntime and can avoid collisions. We evaluate this system in closed-loop driving\nsimulations. Our system outperforms all prior methods on the public CARLA\nLeaderboard by a wide margin, improving driving score by 25 and route\ncompletion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving\nchallenge. Demo videos are available at https://dotchen.github.io/LAV/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D-OR: Semantic Scene Graphs for OR Domain Modeling. (arXiv:2203.11937v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11937","description":"<p>Surgical procedures are conducted in highly complex operating rooms (OR),\ncomprising different actors, devices, and interactions. To date, only medically\ntrained human experts are capable of understanding all the links and\ninteractions in such a demanding environment. This paper aims to bring the\ncommunity one step closer to automated, holistic and semantic understanding and\nmodeling of OR domain. Towards this goal, for the first time, we propose using\nsemantic scene graphs (SSG) to describe and summarize the surgical scene. The\nnodes of the scene graphs represent different actors and objects in the room,\nsuch as medical staff, patients, and medical equipment, whereas edges are the\nrelationships between them. To validate the possibilities of the proposed\nrepresentation, we create the first publicly available 4D surgical SSG dataset,\n4D-OR, containing ten simulated total knee replacement surgeries recorded with\nsix RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734\nframes and is richly annotated with SSGs, human and object poses, and clinical\nroles. We propose an end-to-end neural network-based SSG generation pipeline,\nwith a rate of success of 0.75 macro F1, indeed being able to infer semantic\nreasoning in the OR. We further demonstrate the representation power of our\nscene graphs by using it for the problem of clinical role prediction, where we\nachieve 0.85 macro F1. The code and dataset will be made available upon\nacceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1\">Ege &#xd6;zsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1\">Ulrich Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"{\\phi}-SfT: Shape-from-Template with a Physics-Based Deformation Model. (arXiv:2203.11938v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11938","description":"<p>Shape-from-Template (SfT) methods estimate 3D surface deformations from a\nsingle monocular RGB camera while assuming a 3D state known in advance (a\ntemplate). This is an important yet challenging problem due to the\nunder-constrained nature of the monocular setting. Existing SfT techniques\npredominantly use geometric and simplified deformation models, which often\nlimits their reconstruction abilities. In contrast to previous works, this\npaper proposes a new SfT approach explaining 2D observations through physical\nsimulations accounting for forces and material properties. Our differentiable\nphysics simulator regularises the surface evolution and optimises the material\nelastic properties such as bending coefficients, stretching stiffness and\ndensity. We use a differentiable renderer to minimise the dense reprojection\nerror between the estimated 3D states and the input images and recover the\ndeformation parameters using an adaptive gradient-based optimisation. For the\nevaluation, we record with an RGB-D camera challenging real surfaces exposed to\nphysical forces with various material properties and textures. Our approach\nsignificantly reduces the 3D reconstruction error compared to multiple\ncompeting methods. For the source code and data, see\nhttps://4dqv.mpi-inf.mpg.de/phi-SfT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kairanda_N/0/1/0/all/0/1\">Navami Kairanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tretschk_E/0/1/0/all/0/1\">Edith Tretschk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v7 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/1904.13216","description":"<p>Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n</p>\n<p>In this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1\">George I Lambrou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Activated Networks. (arXiv:1907.06592v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1907.06592","description":"<p>Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kDecay: Just adding k-decay items on Learning-Rate Schedule to improve Neural Networks. (arXiv:2004.05909v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.05909","description":"<p>Recent work has shown that optimizing the Learning Rate (LR) schedule can be\na very accurate and efficient way to train deep neural networks. We observe\nthat the rate of change (ROC) of LR has correlation with the training process,\nbut how to use this relationship to control the training to achieve the purpose\nof improving accuracy? We propose a new method, k-decay, just add an extra item\nto the commonly used and easy LR schedule(exp, cosine and polynomial), is\neffectively improves the performance of these schedule, also better than the\nstate-of-the-art algorithms of LR shcedule such as SGDR, CLR and AutoLRS. In\nthe k-decay, by adjusting the hyper-parameter \\(k\\), to generate different LR\nschedule, when k increases, the performance is improved. We evaluate the\nk-decay method on CIFAR And ImageNet datasets with different neural networks\n(ResNet, Wide ResNet). Our experiments show that this method can improve on\nmost of them. The accuracy has been improved by 1.08\\% on the CIFAR-10 dataset\nand by 2.07 \\% on the CIFAR-100 dataset. On the ImageNet, accuracy is improved\nby 1.25\\%. Our method is not only a general method to be applied other LR\nShcedule, but also has no additional computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Shape Analysis of Brain Arterial Networks (BAN). (arXiv:2007.04793v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.04793","description":"<p>Structures of brain arterial networks (BANs) - that are complex arrangements\nof individual arteries, their branching patterns, and inter-connectivities -\nplay an important role in characterizing and understanding brain physiology.\nOne would like tools for statistically analyzing the shapes of BANs, i.e.\nquantify shape differences, compare population of subjects, and study the\neffects of covariates on these shapes. This paper mathematically represents and\nstatistically analyzes BAN shapes as elastic shape graphs. Each elastic shape\ngraph is made up of nodes that are connected by a number of 3D curves, and\nedges, with arbitrary shapes. We develop a mathematical representation, a\nRiemannian metric and other geometrical tools, such as computations of\ngeodesics, means and covariances, and PCA for analyzing elastic graphs and\nBANs. This analysis is applied to BANs after separating them into four\ncomponents -- top, bottom, left, and right. This framework is then used to\ngenerate shape summaries of BANs from 92 subjects, and to study the effects of\nage and gender on shapes of BAN components. We conclude that while gender\neffects require further investigation, the age has a clear, quantifiable effect\non BAN shapes. Specifically, we find an increased variance in BAN shapes as age\nincreases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bal_A/0/1/0/all/0/1\">Aditi Basu Bal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needham_T/0/1/0/all/0/1\">Tom Needham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anuj Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A range characterization of the single-quadrant ADRT. (arXiv:2010.05360v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2010.05360","description":"<p>This work characterizes the range of the single-quadrant approximate discrete\nRadon transform (ADRT) of square images. The characterization follows from a\nset of linear constraints on the codomain. We show that for data satisfying\nthese constraints, the exact and fast inversion formula [Rim, Appl. Math. Lett.\n102 106159, 2020] yields a square image in a stable manner. The range\ncharacterization is obtained by first showing that the ADRT is a bijection\nbetween images supported on infinite half-strips, then identifying the linear\nsubspaces that stay finitely supported under the inversion formula.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1\">Weilin Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rim_D/0/1/0/all/0/1\">Donsub Rim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04886","description":"<p>As moving objects always draw more attention of human eyes, the temporal\nmotive information is always exploited complementarily with spatial information\nto detect salient objects in videos. Although efficient tools such as optical\nflow have been proposed to extract temporal motive information, it often\nencounters difficulties when used for saliency detection due to the movement of\ncamera or the partial movement of salient objects. In this paper, we\ninvestigate the complimentary roles of spatial and temporal information and\npropose a novel dynamic spatiotemporal network (DS-Net) for more effective\nfusion of spatiotemporal information. We construct a symmetric two-bypass\nnetwork to explicitly extract spatial and temporal features. A dynamic weight\ngenerator (DWG) is designed to automatically learn the reliability of\ncorresponding saliency branch. And a top-down cross attentive aggregation (CAA)\nprocedure is designed so as to facilitate dynamic complementary aggregation of\nspatiotemporal features. Finally, the features are modified by spatial\nattention with the guidance of coarse saliency map and then go through decoder\npart for final saliency map. Experimental results on five benchmarks VOS,\nDAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method\nachieves superior performance than state-of-the-art algorithms. The source code\nis available at https://github.com/TJUMMG/DS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuting Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11111","description":"<p>In contrast to the generic object, aerial targets are often non-axis aligned\nwith arbitrary orientations having the cluttered surroundings. Unlike the\nmainstreamed approaches regressing the bounding box orientations, this paper\nproposes an effective adaptive points learning approach to aerial object\ndetection by taking advantage of the adaptive points representation, which is\nable to capture the geometric information of the arbitrary-oriented instances.\nTo this end, three oriented conversion functions are presented to facilitate\nthe classification and localization with accurate orientation. Moreover, we\npropose an effective quality assessment and sample assignment scheme for\nadaptive points learning toward choosing the representative oriented reppoints\nsamples during training, which is able to capture the non-axis aligned features\nfrom adjacent objects or background noises. A spatial constraint is introduced\nto penalize the outlier points for roust adaptive learning. Experimental\nresults on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD\nand DIOR-R, demonstrate the efficacy of our proposed approach. The source code\nis availabel at: https://github.com/LiWentomng/OrientedRepPoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kaixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pho(SC)-CTC -- A Hybrid Approach Towards Zero-shot Word Image Recognition. (arXiv:2105.15093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15093","description":"<p>Annotating words in a historical document image archive for word image\nrecognition purpose demands time and skilled human resource (like historians,\npaleographers). In a real-life scenario, obtaining sample images for all\npossible words is also not feasible. However, Zero-shot learning methods could\naptly be used to recognize unseen/out-of-lexicon words in such historical\ndocument images. Based on previous state-of-the-art method for zero-shot word\nrecognition Pho(SC)Net, we propose a hybrid model based on the CTC framework\n(Pho(SC)-CTC) that takes advantage of the rich features learned by Pho(SC)Net\nfollowed by a connectionist temporal classification (CTC) framework to perform\nthe final classification. Encouraging results were obtained on two publicly\navailable historical document datasets and one synthetic handwritten dataset,\nwhich justifies the efficacy of Pho(SC)-CTC and Pho(SC)Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_R/0/1/0/all/0/1\">Ravi Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Anuj Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain Activity. (arXiv:2106.05113v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05113","description":"<p>In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages &amp; depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1\">Guy Gaziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Anytime Learning at Macroscale. (arXiv:2106.09563v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09563","description":"<p>In many practical applications of machine learning data arrives sequentially\nover time in large chunks. Practitioners have then to decide how to allocate\ntheir computational budget in order to obtain the best performance at any point\nin time. Online learning theory for convex optimization suggests that the best\nstrategy is to use data as soon as it arrives. However, this might not be the\nbest strategy when using deep non-linear networks, particularly when these\nperform multiple passes over each chunk of data rendering the overall\ndistribution non i.i.d.. In this paper, we formalize this learning setting in\nthe simplest scenario in which each data chunk is drawn from the same\nunderlying distribution, and make a first attempt at empirically answering the\nfollowing questions: How long should the learner wait before training on the\nnewly arrived chunks? What architecture should the learner adopt? Should the\nlearner increase capacity over time as more data is observed? We probe this\nlearning setting using convolutional neural networks trained on classic\ncomputer vision benchmarks as well as a large transformer model trained on a\nlarge-scale language modeling task. Code is available at\n\\url{www.github.com/facebookresearch/ALMA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1\">Ludovic Denoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00327","description":"<p>Existing deep quantization methods provided an efficient solution for\nlarge-scale image retrieval. However, the significant intra-class variations\nlike pose, illumination, and expressions in face images, still pose a challenge\nfor face image retrieval. In light of this, face image retrieval requires\nsufficiently powerful learning metrics, which are absent in current deep\nquantization works. Moreover, to tackle the growing unseen identities in the\nquery stage, face image retrieval drives more demands regarding model\ngeneralization and system scalability than general image retrieval tasks. This\npaper integrates product quantization with orthonormal constraints into an\nend-to-end deep learning framework to effectively retrieve face images.\nSpecifically, a novel scheme that uses predefined orthonormal vectors as\ncodewords is proposed to enhance the quantization informativeness and reduce\ncodewords' redundancy. A tailored loss function maximizes discriminability\namong identities in each quantization subspace for both the quantized and\noriginal features. An entropy-based regularization term is imposed to reduce\nthe quantization error. Experiments are conducted on four commonly-used face\ndatasets under both seen and unseen identities retrieval settings. Our method\noutperforms all the compared deep hashing/quantization state-of-the-arts under\nboth settings. Results validate the effectiveness of the proposed orthonormal\ncodewords in improving models' standard retrieval performance and\ngeneralization ability. Combing with further experiments on two general image\ndatasets, it demonstrates the broad superiority of our method for scalable\nimage retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09783","description":"<p>In this paper, we focus on a less explored, but more realistic and complex\nproblem of domain adaptation in LiDAR semantic segmentation. There is a\nsignificant drop in performance of an existing segmentation model when training\n(source domain) and testing (target domain) data originate from different LiDAR\nsensors. To overcome this shortcoming, we propose an unsupervised domain\nadaptation framework that leverages unlabeled target domain data for\nself-supervision, coupled with an unpaired mask transfer strategy to mitigate\nthe impact of domain shifts. Furthermore, we introduce the gated adapter module\nwith a small number of parameters into the network to account for target\ndomain-specific information. Experiments adapting from both real-to-real and\nsynthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the\nsignificant improvement over prior arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1\">Mrigank Rochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1\">Shubhra Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1\">Eduardo R. Corral-Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabatchian_A/0/1/0/all/0/1\">Amir Nabatchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07917","description":"<p>A formal autism diagnosis can be an inefficient and lengthy process. Families\nmay wait months or longer before receiving a diagnosis for their child despite\nevidence that earlier intervention leads to better treatment outcomes. Digital\ntechnologies which detect the presence of behaviors related to autism can scale\naccess to pediatric diagnoses. This work aims to demonstrate the feasibility of\ndeep learning technologies for detecting hand flapping from unstructured home\nvideos as a first step towards validating whether models and digital\ntechnologies can be leveraged to aid with autism diagnoses. We used the\nSelf-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand\nflapping, head banging, and spinning exhibited by children. From all the hand\nflapping videos, we extracted 100 positive and control videos of hand flapping,\neach between 2 to 5 seconds in duration. Utilizing both\nlandmark-driven-approaches and MobileNet V2's pretrained convolutional layers,\nour highest performing model achieved a testing F1 score of 84% (90% precision\nand 80% recall) when evaluating with 5-fold cross validation 100 times. This\nwork provides the first step towards developing precise deep learning methods\nfor activity detection of autism-related behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Out-of-Distribution Detection Based on the Pre-trained Model CLIP. (arXiv:2109.02748v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02748","description":"<p>In an out-of-distribution (OOD) detection problem, samples of known\nclasses(also called in-distribution classes) are used to train a special\nclassifier. In testing, the classifier can (1) classify the test samples of\nknown classes to their respective classes and also (2) detect samples that do\nnot belong to any of the known classes (i.e., they belong to some unknown or\nOOD classes). This paper studies the problem of zero-shot\nout-of-distribution(OOD) detection, which still performs the same two tasks in\ntesting but has no training except using the given known class names. This\npaper proposes a novel yet simple method (called ZOC) to solve the problem. ZOC\nbuilds on top of the recent advances in zero-shot classification through\nmulti-modal representation learning. It first extends the pre-trained\nlanguage-vision model CLIP by training a text-based image description generator\non top of CLIP. In testing, it uses the extended model to generate candidate\nunknown class names for each test sample and computes a confidence score based\non both the known class names and candidate unknown class names for zero-shot\nOOD detection. Experimental results on 5 benchmark datasets for OOD detection\ndemonstrate that ZOC outperforms the baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Hashing for Image Retrieval. (arXiv:2109.12564v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12564","description":"<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet. The code is\nreleased at \\url{https://github.com/shivram1987/VisionTransformerHashing}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei-Ta Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Efficient Multi-Agent Cooperative Visual Exploration. (arXiv:2110.05734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05734","description":"<p>We tackle the problem of cooperative visual exploration where multiple agents\nneed to jointly explore unseen regions as fast as possible based on visual\nsignals. Classical planning-based methods often suffer from expensive\ncomputation overhead at each step and a limited expressiveness of complex\ncooperation strategy. By contrast, reinforcement learning (RL) has recently\nbecome a popular paradigm for tackling this challenge due to its modeling\ncapability of arbitrarily complex strategies and minimal inference overhead. In\nthis paper, we extend the state-of-the-art single-agent visual navigation\nmethod, Active Neural SLAM (ANS), to the multi-agent setting by introducing a\nnovel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages\na transformer-based architecture, Spatial-TeamFormer, which effectively\ncaptures spatial relations and intra-agent interactions via hierarchical\nspatial self-attentions. In addition, we also implement a few multi-agent\nenhancements to process local information from each agent for an aligned\nspatial representation and more precise planning. Finally, we perform policy\ndistillation to extract a meta policy to significantly improve the\ngeneralization capability of final policy. We call this overall solution,\nMulti-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms\nclassical planning-based baselines for the first time in a photo-realistic 3D\nsimulator, Habitat. Code and videos can be found at\nhttps://sites.google.com/view/maans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are we ready for a new paradigm shift? A Survey on Visual Deep MLP. (arXiv:2111.04060v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04060","description":"<p>Recently, the proposed deep MLP models have stirred up a lot of interest in\nthe vision community. Historically, the availability of larger datasets\ncombined with increased computing capacity leads to paradigm shifts. This\nreview paper provides detailed discussions on whether MLP can be a new paradigm\nfor computer vision. We compare the intrinsic connections and differences\nbetween convolution, self-attention mechanism, and Token-mixing MLP in detail.\nAdvantages and limitations of Token-mixing MLP are provided, followed by\ncareful analysis of recent MLP-like variants, from module design to network\narchitecture, and their applications. In the GPU era, the locally and globally\nweighted summations are the current mainstreams, represented by the convolution\nand self-attention mechanism, as well as MLP. We suggest the further\ndevelopment of paradigm to be considered alongside the next-generation\ncomputing devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Linmi Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet. (arXiv:2111.04739v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.04739","description":"<p>Accurate retinal vessel segmentation is an important task for many\ncomputer-aided diagnosis systems. Yet, it is still a challenging problem due to\nthe complex vessel structures of an eye. Numerous vessel segmentation methods\nhave been proposed recently, however more research is needed to deal with poor\nsegmentation of thin and tiny vessels. To address this, we propose a new deep\nlearning pipeline combining the efficiency of residual dense net blocks and,\nresidual squeeze and excitation blocks. We validate experimentally our approach\non three datasets and show that our pipeline outperforms current state of the\nart techniques on the sensitivity metric relevant to assess capture of small\nvessels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karaali_A/0/1/0/all/0/1\">Ali Karaali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sexton_D/0/1/0/all/0/1\">Donal J. Sexton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08918","description":"<p>Recent works with an implicit neural function shed light on representing\nimages in arbitrary resolution. However, a standalone multi-layer perceptron\nshows limited performance in learning high-frequency components. In this paper,\nwe propose a Local Texture Estimator (LTE), a dominant-frequency estimator for\nnatural images, enabling an implicit function to capture fine details while\nreconstructing images in a continuous manner. When jointly trained with a deep\nsuper-resolution (SR) architecture, LTE is capable of characterizing image\ntextures in 2D Fourier space. We show that an LTE-based neural function\nachieves favorable performance against existing deep SR methods within an\narbitrary-scale factor. Furthermore, we demonstrate that our implementation\ntakes the shortest running time compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyong Hwan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCurrents: Learning Implicit Representations of Shapes with Boundaries. (arXiv:2111.09383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09383","description":"<p>Recent techniques have been successful in reconstructing surfaces as level\nsets of learned functions (such as signed distance fields) parameterized by\ndeep neural networks. Many of these methods, however, learn only closed\nsurfaces and are unable to reconstruct shapes with boundary curves. We propose\na hybrid shape representation that combines explicit boundary curves with\nimplicit learned interiors. Using machinery from geometric measure theory, we\nparameterize currents using deep networks and use stochastic gradient descent\nto solve a minimal surface problem. By modifying the metric according to target\ngeometry coming, e.g., from a mesh or point cloud, we can use this approach to\nrepresent arbitrary surfaces, learning implicitly defined shapes with\nexplicitly defined boundary curves. We further demonstrate learning families of\nshapes jointly parameterized by boundary curves and latent codes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palmer_D/0/1/0/all/0/1\">David Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_D/0/1/0/all/0/1\">Dmitriy Smirnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Stephanie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chern_A/0/1/0/all/0/1\">Albert Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10502","description":"<p>In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline that splits the joint task into independent stages, or fuse\n2D and 3D information in an \"early-fusion\" or \"late-fusion\" manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, called CamLiFlow. It consists of 2D and 3D branches with multiple\nbidirectional connections between them in specific layers. Different from\nprevious work, we apply a point-based 3D branch to better extract the geometric\nfeatures and design a symmetric learnable operator to fuse dense image features\nand sparse point features. Experiments show that CamLiFlow achieves better\nperformance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow\nbenchmark, outperforming the previous art with 1/7 parameters. Code is\navailable at https://github.com/MCG-NJU/CamLiFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12448","description":"<p>Learning a disentangled, interpretable, and structured latent representation\nin 3D generative models of faces and bodies is still an open problem. The\nproblem is particularly acute when control over identity features is required.\nIn this paper, we propose an intuitive yet effective self-supervised approach\nto train a 3D shape variational autoencoder (VAE) which encourages a\ndisentangled latent representation of identity features. Curating the\nmini-batch generation by swapping arbitrary features across different shapes\nallows to define a loss function leveraging known differences and similarities\nin the latent representations. Experimental results conducted on 3D meshes show\nthat state-of-the-art methods for latent disentanglement are not able to\ndisentangle identity features of faces and bodies. Our proposed method properly\ndecouples the generation of such features while maintaining good representation\nand reconstruction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACPL: Anti-curriculum Pseudo-labelling for Semi-supervised Medical Image Classification. (arXiv:2111.12918v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12918","description":"<p>Effective semi-supervised learning (SSL) in medical image analysis (MIA) must\naddress two challenges: 1) work effectively on both multi-class (e.g., lesion\nclassification) and multi-label (e.g., multiple-disease diagnosis) problems,\nand 2) handle imbalanced learning (because of the high variance in disease\nprevalence). One strategy to explore in SSL MIA is based on the pseudo\nlabelling strategy, but it has a few shortcomings. Pseudo-labelling has in\ngeneral lower accuracy than consistency learning, it is not specifically\ndesigned for both multi-class and multi-label problems, and it can be\nchallenged by imbalanced learning. In this paper, unlike traditional methods\nthat select confident pseudo label by threshold, we propose a new SSL\nalgorithm, called anti-curriculum pseudo-labelling (ACPL), which introduces\nnovel techniques to select informative unlabelled samples, improving training\nbalance and allowing the model to work for both multi-label and multi-class\nproblems, and to estimate pseudo labels by an accurate ensemble of classifiers\n(improving pseudo label accuracy). We run extensive experiments to evaluate\nACPL on two public medical image classification benchmarks: Chest X-Ray14 for\nthorax disease multi-label classification and ISIC2018 for skin lesion\nmulti-class classification. Our method outperforms previous SOTA SSL methods on\nboth datasets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-Ensemble for Semantic Segmentation. (arXiv:2111.13280v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13280","description":"<p>Ensemble of predictions is known to perform better than individual\npredictions taken separately. However, for tasks that require heavy\ncomputational resources, e.g. semantic segmentation, creating an ensemble of\nlearners that needs to be trained separately is hardly tractable. In this work,\nwe propose to leverage the performance boost offered by ensemble methods to\nenhance the semantic segmentation, while avoiding the traditional heavy\ntraining cost of the ensemble. Our self-ensemble approach takes advantage of\nthe multi-scale features set produced by feature pyramid network methods to\nfeed independent decoders, thus creating an ensemble within a single model.\nSimilar to the ensemble, the final prediction is the aggregation of the\nprediction made by each learner. In contrast to previous works, our model can\nbe trained end-to-end, alleviating the traditional cumbersome multi-stage\ntraining of ensembles. Our self-ensemble approach outperforms the current\nstate-of-the-art on the benchmark datasets Pascal Context and COCO-Stuff-10K\nfor semantic segmentation and is competitive on ADE20K and Cityscapes. Code is\npublicly available at github.com/WalBouss/SenFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1\">Walid Bousselham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thibault_G/0/1/0/all/0/1\">Guillaume Thibault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagano_L/0/1/0/all/0/1\">Lucas Pagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machireddy_A/0/1/0/all/0/1\">Archana Machireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_J/0/1/0/all/0/1\">Joe Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Young Hwan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xubo Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Annotator Preference and Stochastic Annotation Error for Medical Image Segmentation. (arXiv:2111.13410v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13410","description":"<p>Manual annotation of medical images is highly subjective, leading to\ninevitable and huge annotation biases. Deep learning models may surpass human\nperformance on a variety of tasks, but they may also mimic or amplify these\nbiases. Although we can have multiple annotators and fuse their annotations to\nreduce stochastic errors, we cannot use this strategy to handle the bias caused\nby annotators' preferences. In this paper, we highlight the issue of\nannotator-related biases on medical image segmentation tasks, and propose a\nPreference-involved Annotation Distribution Learning (PADL) framework to\naddress it from the perspective of disentangling an annotator's preference from\nstochastic errors using distribution learning so as to produce not only a meta\nsegmentation but also the segmentation possibly made by each annotator. Under\nthis framework, a stochastic error modeling (SEM) module estimates the meta\nsegmentation map and average stochastic error map, and a series of human\npreference modeling (HPM) modules estimate each annotator's segmentation and\nthe corresponding stochastic error. We evaluated our PADL framework on two\nmedical image benchmarks with different imaging modalities, which have been\nannotated by multiple medical professionals, and achieved promising performance\non all five medical image segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zehui Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shishuai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDR: Self-Supervised Image Denoising via Iterative Data Refinement. (arXiv:2111.14358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14358","description":"<p>The lack of large-scale noisy-clean image pairs restricts supervised\ndenoising methods' deployment in actual applications. While existing\nunsupervised methods are able to learn image denoising without ground-truth\nclean images, they either show poor performance or work under impractical\nsettings (e.g., paired noisy images). In this paper, we present a practical\nunsupervised image denoising method to achieve state-of-the-art denoising\nperformance. Our method only requires single noisy images and a noise model,\nwhich is easily accessible in practical raw image denoising. It performs two\nsteps iteratively: (1) Constructing a noisier-noisy dataset with random noise\nfrom the noise model; (2) training a model on the noisier-noisy dataset and\nusing the trained model to refine noisy images to obtain the targets used in\nthe next round. We further approximate our full iterative method with a fast\nalgorithm for more efficient training while keeping its original high\nperformance. Experiments on real-world, synthetic, and correlated noise show\nthat our proposed unsupervised denoising approach has superior performances\nover existing unsupervised methods and competitive performance with supervised\nmethods. In addition, we argue that existing denoising datasets are of low\nquality and contain only a small number of scenes. To evaluate raw image\ndenoising performance in real-world applications, we build a high-quality raw\nimage dataset SenseNoise-500 that contains 500 real-life scenes. The dataset\ncan serve as a strong benchmark for better evaluating raw image denoising. Code\nand dataset will be released at https://github.com/zhangyi-3/IDR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dasong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1\">Ka Lung Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02857","description":"<p>In a point cloud sequence, 3D object tracking aims to predict the location\nand orientation of an object in the current search point cloud given a template\npoint cloud. Motivated by the success of transformers, we propose Point\nTracking TRansformer (PTTR), which efficiently predicts high-quality 3D\ntracking results in a coarse-to-fine manner with the help of transformer\noperations. PTTR consists of three novel designs. 1) Instead of random\nsampling, we design Relation-Aware Sampling to preserve relevant points to\ngiven templates during subsampling. 2) Furthermore, we propose a Point Relation\nTransformer (PRT) consisting of a self-attention and a cross-attention module.\nThe global self-attention operation captures long-range dependencies to enhance\nencoded point features for the search area and the template, respectively.\nSubsequently, we generate the coarse tracking results by matching the two sets\nof point features via cross-attention. 3) Based on the coarse tracking results,\nwe employ a novel Prediction Refinement Module to obtain the final refined\nprediction. In addition, we create a large-scale point cloud single object\ntracking benchmark based on the Waymo Open Dataset. Extensive experiments show\nthat PTTR achieves superior point cloud tracking in both accuracy and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yueru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03596","description":"<p>Event cameras are novel bio-inspired sensors, which asynchronously capture\npixel-level intensity changes in the form of \"events\". Due to their sensing\nmechanism, event cameras have little to no motion blur, a very high temporal\nresolution and require significantly less power and memory than traditional\nframe-based cameras. These characteristics make them a perfect fit to several\nreal-world applications such as egocentric action recognition on wearable\ndevices, where fast camera motion and limited power challenge traditional\nvision sensors. However, the ever-growing field of event-based vision has, to\ndate, overlooked the potential of event cameras in such applications. In this\npaper, we show that event data is a very valuable modality for egocentric\naction recognition. To do so, we introduce N-EPIC-Kitchens, the first\nevent-based camera extension of the large-scale EPIC-Kitchens dataset. In this\ncontext, we propose two strategies: (i) directly processing event-camera data\nwith traditional video-processing architectures (E$^2$(GO)) and (ii) using\nevent-data to distill optical flow information (E$^2$(GO)MO). On our proposed\nbenchmark, we show that event data provides a comparable performance to RGB and\noptical flow, yet without any additional flow computation at deploy time, and\nan improved performance of up to 4% with respect to RGB only information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goletto_G/0/1/0/all/0/1\">Gabriele Goletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusso_E/0/1/0/all/0/1\">Emanuele Gusso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaping Visual Representations with Attributes for Few-Shot Learning. (arXiv:2112.06398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06398","description":"<p>Few-shot recognition aims to recognize novel categories under low-data\nregimes. Some recent few-shot recognition methods introduce auxiliary semantic\nmodality, i.e., category attribute information, into representation learning,\nwhich enhances the feature discrimination and improves the recognition\nperformance. Most of these existing methods only consider the attribute\ninformation of support set while ignoring the query set, resulting in a\npotential loss of performance. In this letter, we propose a novel\nattribute-shaped learning (ASL) framework, which can jointly perform query\nattributes generation and discriminative visual representation learning for\nfew-shot recognition. Specifically, a visual-attribute generator (VAG) is\nconstructed to predict the attributes of queries. By leveraging the attributes\ninformation, an attribute-visual attention module (AVAM) is designed, which can\nadaptively utilize attributes and visual representations to learn more\ndiscriminative features. Under the guidance of attribute modality, our method\ncan learn enhanced semantic-aware representation for classification.\nExperiments demonstrate that our method can achieve competitive results on CUB\nand SUN benchmarks. Our source code is available at:\n\\url{https://github.com/chenhaoxing/ASL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huaxiong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaohui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt for Continual Learning. (arXiv:2112.08654v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.08654","description":"<p>The mainstream paradigm behind continual learning has been to adapt the model\nparameters to non-stationary data distributions, where catastrophic forgetting\nis the central challenge. Typical methods rely on a rehearsal buffer or known\ntask identity at test time to retrieve learned knowledge and address\nforgetting, while this work presents a new paradigm for continual learning that\naims to train a more succinct memory system without accessing task identity at\ntest time. Our method learns to dynamically prompt (L2P) a pre-trained model to\nlearn tasks sequentially under different task transitions. In our proposed\nframework, prompts are small learnable parameters, which are maintained in a\nmemory space. The objective is to optimize prompts to instruct the model\nprediction and explicitly manage task-invariant and task-specific knowledge\nwhile maintaining model plasticity. We conduct comprehensive experiments under\npopular image classification benchmarks with different challenging continual\nlearning settings, where L2P consistently outperforms prior state-of-the-art\nmethods. Surprisingly, L2P achieves competitive results against rehearsal-based\nmethods even without a rehearsal buffer and is directly applicable to\nchallenging task-agnostic continual learning. Source code is available at\nhttps://github.com/google-research/l2p.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1\">Jennifer Dy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12329","description":"<p>We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as\nqueries in Transformer decoders and dynamically updates them layer-by-layer.\nUsing box coordinates not only helps using explicit positional priors to\nimprove the query-to-feature similarity and eliminate the slow training\nconvergence issue in DETR, but also allows us to modulate the positional\nattention map using the box width and height information. Such a design makes\nit clear that queries in DETR can be implemented as performing soft ROI pooling\nlayer-by-layer in a cascade manner. As a result, it leads to the best\nperformance on MS-COCO benchmark among the DETR-like detection models under the\nsame setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50\nepochs. We also conducted extensive experiments to confirm our analysis and\nverify the effectiveness of our methods. Code is available at\n\\url{https://github.com/SlongLiu/DAB-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00455","description":"<p>Hierarchical semantic structures naturally exist in an image dataset, in\nwhich several semantically relevant image clusters can be further integrated\ninto a larger cluster with coarser-grained semantics. Capturing such structures\nwith image representations can greatly benefit the semantic understanding on\nvarious downstream tasks. Existing contrastive representation learning methods\nlack such an important model capability. In addition, the negative pairs used\nin these methods are not guaranteed to be semantically distinct, which could\nfurther hamper the structural correctness of learned image representations. To\ntackle these limitations, we propose a novel contrastive learning framework\ncalled Hierarchical Contrastive Selective Coding (HCSC). In this framework, a\nset of hierarchical prototypes are constructed and also dynamically updated to\nrepresent the hierarchical semantic structures underlying the data in the\nlatent space. To make image representations better fit such semantic\nstructures, we employ and further improve conventional instance-wise and\nprototypical contrastive learning via an elaborate pair selection scheme. This\nscheme seeks to select more diverse positive pairs with similar semantics and\nmore precise negative pairs with truly distinct semantics. On extensive\ndownstream tasks, we verify the superior performance of HCSC over\nstate-of-the-art contrastive methods, and the effectiveness of major model\ncomponents is proved by plentiful analytical studies. We build a comprehensive\nmodel zoo in Sec. D. Our source code and model weights are available at\nhttps://github.com/gyfastas/HCSC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A hybrid 2-stage vision transformer for artificial intelligence-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies: a diagnostic tool for guiding gastric cancer treatment. (arXiv:2202.08510v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08510","description":"<p>Gastric endoscopic screening is an effective way to decide appropriate\ngastric cancer (GC) treatment at an early stage, reducing GC-associated\nmortality rate. Although artificial intelligence (AI) has brought a great\npromise to assist pathologist to screen digitalized whole slide images,\nautomatic classification systems for guiding proper GC treatment based on\nclinical guideline are still lacking. We propose an AI system classifying 5\nclasses of GC histology, which can be perfectly matched to general GC treatment\nguidance. The AI system was designed to mimic the way pathologist understand\nslides through multi-scale self-attention mechanism using a 2-stage Vision\nTransformer network. The AI system performance was evaluated on 876 internal\nendoscopic slides and 336 external endoscopic slides from clinical cohort. We\nfurther evaluated practical usability of the AI system on observation of\nAI-assisted 6 pathologist performance. The AI system demonstrates clinical\ncapability by achieving class-average diagnostic sensitivity of above 85% for\nboth internal and external cohort analysis. Furthermore, AI-assisted\npathologists showed significantly improved diagnostic sensitivity by 10% within\n18% saved screening time compared to human pathologists (p-values of 0.006 and\n0.030, respectively). The reliable performance of the AI system in multi-center\ncohort testing and its clinical applicability demonstrate that AI-assisted\nendoscopic CG screening would help reduce the workload of limited pathologists.\nFurthermore, the AI system has a great potential for providing presumptive\npathologic opinion for deciding proper treatment for early GC patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1\">Yujin Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_G/0/1/0/all/0/1\">Go Eun Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Hee Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeo_M/0/1/0/all/0/1\">Min-Kyung Yeo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"If you could see me through my eyes\": Predicting Pedestrian Perception. (arXiv:2202.13981v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13981","description":"<p>Pedestrians are particularly vulnerable road users in urban traffic. With the\narrival of autonomous driving, novel technologies can be developed specifically\nto protect pedestrians. We propose a machine learning toolchain to train\nartificial neural networks as models of pedestrian behavior. In a preliminary\nstudy, we use synthetic data from simulations of a specific pedestrian crossing\nscenario to train a variational autoencoder and a long short-term memory\nnetwork to predict a pedestrian's future visual perception. We can accurately\npredict a pedestrian's future perceptions within relevant time horizons. By\niteratively feeding these predicted frames into these networks, they can be\nused as simulations of pedestrians as indicated by our results. Such trained\nnetworks can later be used to predict pedestrian behaviors even from the\nperspective of the autonomous car. Another future extension will be to re-train\nthese networks with real-world video data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petzold_J/0/1/0/all/0/1\">Julian Petzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahby_M/0/1/0/all/0/1\">Mostafa Wahby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stark_F/0/1/0/all/0/1\">Franek Stark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrje_U/0/1/0/all/0/1\">Ulrich Behrje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamann_H/0/1/0/all/0/1\">Heiko Hamann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01057","description":"<p>Online action detection has attracted increasing research interests in recent\nyears. Current works model historical dependencies and anticipate the future to\nperceive the action evolution within a video segment and improve the detection\naccuracy. However, the existing paradigm ignores category-level modeling and\ndoes not pay sufficient attention to efficiency. Considering a category, its\nrepresentative frames exhibit various characteristics. Thus, the category-level\nmodeling can provide complimentary guidance to the temporal dependencies\nmodeling. This paper develops an effective exemplar-consultation mechanism that\nfirst measures the similarity between a frame and exemplary frames, and then\naggregates exemplary features based on the similarity weights. This is also an\nefficient mechanism, as both similarity measurement and feature aggregation\nrequire limited computations. Based on the exemplar-consultation mechanism, the\nlong-term dependencies can be captured by regarding historical frames as\nexemplars, while the category-level modeling can be achieved by regarding\nrepresentative frames from a category as exemplars. Due to the complementarity\nfrom the category-level modeling, our method employs a lightweight architecture\nbut achieves new high performance on three benchmarks. In addition, using a\nspatio-temporal network to tackle video frames, our method makes a good\ntrade-off between effectiveness and efficiency. Code is available at\nhttps://github.com/VividLe/Online-Action-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Le Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanFormer: a Transformer Based Model for Pan-sharpening. (arXiv:2203.02916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02916","description":"<p>Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)\nimage from a low-resolution (LR) multi-spectral (MS) image and its\ncorresponding panchromatic (PAN) image acquired by a same satellite. Inspired\nby a new fashion in recent deep learning community, we propose a novel\nTransformer based model for pan-sharpening. We explore the potential of\nTransformer in image feature extraction and fusion. Following the successful\ndevelopment of vision transformers, we design a two-stream network with the\nself-attention to extract the modality-specific features from the PAN and MS\nmodalities and apply a cross-attention module to merge the spectral and spatial\nfeatures. The pan-sharpened image is produced from the enhanced fused features.\nExtensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our\nTransformer based model achieves impressive results and outperforms many\nexisting CNN based methods, which shows the great potential of introducing\nTransformer to the pan-sharpening task. Codes are available at\nhttps://github.com/zhysora/PanFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huanyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual MLP for Scoring Sport. (arXiv:2203.03990v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03990","description":"<p>Figure skating scoring is a challenging task because it requires judging\nplayers' technical moves as well as coordination with the background music.\nPrior learning-based work cannot solve it well for two reasons: 1) each move in\nfigure skating changes quickly, hence simply applying traditional frame\nsampling will lose a lot of valuable information, especially in a 3-5 minutes\nlasting video, so an extremely long-range representation learning is necessary;\n2) prior methods rarely considered the critical audio-visual relationship in\ntheir models. Thus, we introduce a multimodal MLP architecture, named\nSkating-Mixer. It extends the MLP-Mixer-based framework into a multimodal\nfashion and effectively learns long-term representations through our designed\nmemory recurrent unit (MRU). Aside from the model, we also collected a\nhigh-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8\ntypes of programs with 7 different rating metrics, overtaking other datasets in\nboth quantity and diversity. Experiments show the proposed method outperforms\nSOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In\naddition, we include an analysis applying our method to recent competitions\nthat occurred in Beijing 2022 Winter Olympic Games, proving our method has\nstrong robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tiantian Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuantai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-invariant 3D Adversarial Point Clouds. (arXiv:2203.04041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04041","description":"<p>Adversary and invisibility are two fundamental but conflict characters of\nadversarial perturbations. Previous adversarial attacks on 3D point cloud\nrecognition have often been criticized for their noticeable point outliers,\nsince they just involve an \"implicit constrain\" like global distance loss in\nthe time-consuming optimization to limit the generated noise. While point cloud\nis a highly structured data format, it is hard to constrain its perturbation\nwith a simple loss or metric properly. In this paper, we propose a novel\nPoint-Cloud Sensitivity Map to boost both the efficiency and imperceptibility\nof point perturbations. This map reveals the vulnerability of point cloud\nrecognition models when encountering shape-invariant adversarial noises. These\nnoises are designed along the shape surface with an \"explicit constrain\"\ninstead of extra distance loss. Specifically, we first apply a reversible\ncoordinate transformation on each point of the point cloud input, to reduce one\ndegree of point freedom and limit its movement on the tangent plane. Then we\ncalculate the best attacking direction with the gradients of the transformed\npoint cloud obtained on the white-box model. Finally we assign each point with\na non-negative score to construct the sensitivity map, which benefits both\nwhite-box adversarial invisibility and black-box query-efficiency extended in\nour work. Extensive evaluations prove that our method can achieve the superior\nperformance on various point cloud recognition models, with its satisfying\nadversarial imperceptibility and strong resistance to different point cloud\ndefense settings. Our code is available at: https://github.com/shikiw/SI-Adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abandoning the Bayer-Filter to See in the Dark. (arXiv:2203.04042v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04042","description":"<p>Low-light image enhancement - a pervasive but challenging problem, plays a\ncentral role in enhancing the visibility of an image captured in a poor\nillumination environment. Due to the fact that not all photons can pass the\nBayer-Filter on the sensor of the color camera, in this work, we first present\na De-Bayer-Filter simulator based on deep neural networks to generate a\nmonochrome raw image from the colored raw image. Next, a fully convolutional\nnetwork is proposed to achieve the low-light image enhancement by fusing\ncolored raw data with synthesized monochrome raw data. Channel-wise attention\nis also introduced to the fusion process to establish a complementary\ninteraction between features from colored and monochrome raw images. To train\nthe convolutional networks, we propose a dataset with monochrome and color raw\npairs named Mono-Colored Raw paired dataset (MCR) collected by using a\nmonochrome camera without Bayer-Filter and a color camera with Bayer-Filter.\nThe proposed pipeline take advantages of the fusion of the virtual monochrome\nand the color raw images and our extensive experiments indicate that\nsignificant improvement can be achieved by leveraging raw sensor data and\ndata-driven learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_X/0/1/0/all/0/1\">Xingbo Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Wanyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_Z/0/1/0/all/0/1\">Zhihui Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiewen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teoh_A/0/1/0/all/0/1\">Andrew Beng Jin Teoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Density-Aware Voxels for LiDAR 3D Object Detection. (arXiv:2203.05662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05662","description":"<p>LiDAR has become one of the primary 3D object detection sensors in autonomous\ndriving. However, LiDAR's diverging point pattern with increasing distance\nresults in a non-uniform sampled point cloud ill-suited to discretized\nvolumetric feature extraction. Current methods either rely on voxelized point\nclouds or use inefficient farthest point sampling to mitigate detrimental\neffects caused by density variation but largely ignore point density as a\nfeature and its predictable relationship with distance from the LiDAR sensor.\nOur proposed solution, Point Density-Aware Voxel network (PDV), is an\nend-to-end two stage LiDAR 3D object detection architecture that is designed to\naccount for these point density variations. PDV efficiently localizes voxel\nfeatures from the 3D sparse convolution backbone through voxel point centroids.\nThe spatially localized voxel features are then aggregated through a\ndensity-aware RoI grid pooling module using kernel density estimation (KDE) and\nself-attention with point density positional encoding. Finally, we exploit\nLiDAR's point density to distance relationship to refine our final bounding box\nconfidences. PDV outperforms all state-of-the-art methods on the Waymo Open\nDataset and achieves competitive results on the KITTI dataset. We provide a\ncode release for PDV which is available at https://github.com/TRAILab/PDV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jordan S. K. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuai_T/0/1/0/all/0/1\">Tianshu Kuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven L. Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06558","description":"<p>Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08344","description":"<p>We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08481","description":"<p>Visual grounding, i.e., localizing objects in images according to natural\nlanguage queries, is an important topic in visual language understanding. The\nmost effective approaches for this task are based on deep learning, which\ngenerally require expensive manually labeled image-query or patch-query pairs.\nTo eliminate the heavy dependence on human annotations, we present a novel\nmethod, named Pseudo-Q, to automatically generate pseudo language queries for\nsupervised training. Our method leverages an off-the-shelf object detector to\nidentify visual objects from unlabeled images, and then language queries for\nthese objects are obtained in an unsupervised fashion with a pseudo-query\ngeneration module. Then, we design a task-related query prompt module to\nspecifically tailor generated pseudo language queries for visual grounding\ntasks. Further, in order to fully capture the contextual relationships between\nimages and language queries, we develop a visual-language model equipped with\nmulti-level cross-modality attention mechanism. Extensive experimental results\ndemonstrate that our method has two notable benefits: (1) it can reduce human\nannotation costs significantly, e.g., 31% on RefCOCO without degrading original\nmodel's performance under the fully supervised setting, and (2) without bells\nand whistles, it achieves superior or comparable performance compared to\nstate-of-the-art weakly-supervised visual grounding methods on all the five\ndatasets we have experimented. Code is available at\nhttps://github.com/LeapLabTHU/Pseudo-Q.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongchen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow. (arXiv:2203.08652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08652","description":"<p>Deep Implicit Functions (DIFs) represent 3D geometry with continuous signed\ndistance functions learned through deep neural nets. Recently DIFs-based\nmethods have been proposed to handle shape reconstruction and dense point\ncorrespondences simultaneously, capturing semantic relationships across shapes\nof the same class by learning a DIFs-modeled shape template. These methods\nprovide great flexibility and accuracy in reconstructing 3D shapes and\ninferring correspondences. However, the point correspondences built from these\nmethods do not intrinsically preserve the topology of the shapes, unlike\nmesh-based template matching methods. This limits their applications on 3D\ngeometries where underlying topological structures exist and matter, such as\nanatomical structures in medical images. In this paper, we propose a new model\ncalled Neural Diffeomorphic Flow (NDF) to learn deep implicit shape templates,\nrepresenting shapes as conditional diffeomorphic deformations of templates,\nintrinsically preserving shape topologies. The diffeomorphic deformation is\nrealized by an auto-decoder consisting of Neural Ordinary Differential Equation\n(NODE) blocks that progressively map shapes to implicit templates. We conduct\nextensive experiments on several medical image organ segmentation datasets to\nevaluate the effectiveness of NDF on reconstructing and aligning shapes. NDF\nachieves consistently state-of-the-art organ shape reconstruction and\nregistration results in both accuracy and quality. The source code is publicly\navailable at https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction. (arXiv:2203.08657v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08657","description":"<p>Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality\nthat aims to recover objects or scene parts outside the field of view from\nmeasurements of light that is indirectly scattered off a directly visible,\ndiffuse wall. Despite recent advances in acquisition and reconstruction\ntechniques, the well-posedness of the problem at large, and the recoverability\nof objects and their shapes in particular, remains an open question. The\ncommonly employed Fermat path criterion is rather conservative with this\nregard, as it classifies some surfaces as unrecoverable, although they\ncontribute to the signal.\n</p>\n<p>In this paper, we use a simpler necessary criterion for an opaque surface\npatch to be recoverable. Such piece of surface must be directly visible from\nsome point on the wall, and it must occlude the space behind itself. Inspired\nby recent advances in neural implicit representations, we devise a new\nrepresentation and reconstruction technique for NLoS scenes that unifies the\ntreatment of recoverability with the reconstruction itself. Our approach, which\nwe validate on various synthetic and experimental datasets, exhibits\ninteresting properties. Unlike memory-inefficient volumetric representations,\nours allows to infer adaptively tessellated surfaces from time-of-flight\nmeasurements of moderate resolution. It can further recover features beyond the\nFermat path criterion, and it is robust to significant amounts of\nself-occlusion. We believe that this is the first time that these properties\nhave been achieved in one system that, as an additional benefit, is trainable\nand hence suited for data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grau_J/0/1/0/all/0/1\">Javier Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plack_M/0/1/0/all/0/1\">Markus Plack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haehn_P/0/1/0/all/0/1\">Patrick Haehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Michael Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullin_M/0/1/0/all/0/1\">Matthias Hullin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know your sensORs -- A Modality Study For Surgical Action Classification. (arXiv:2203.08674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08674","description":"<p>The surgical operating room (OR) presents many opportunities for automation\nand optimization. Videos from various sources in the OR are becoming\nincreasingly available. The medical community seeks to leverage this wealth of\ndata to develop automated methods to advance interventional care, lower costs,\nand improve overall patient outcomes. Existing datasets from OR room cameras\nare thus far limited in size or modalities acquired, leaving it unclear which\nsensor modalities are best suited for tasks such as recognizing surgical action\nfrom videos. This study demonstrates that surgical action recognition\nperformance can vary depending on the image modalities used. We perform a\nmethodical analysis on several commonly available sensor modalities, presenting\ntwo fusion approaches that improve classification performance. The analyses are\ncarried out on a set of multi-view RGB-D video recordings of 18 laparoscopic\nprocedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastian_L/0/1/0/all/0/1\">Lennart Bastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiliger_C/0/1/0/all/0/1\">Christian Heiliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcz_K/0/1/0/all/0/1\">Konrad Karcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1\">Ulrich Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-directional Object-context Prioritization Learning for Saliency Ranking. (arXiv:2203.09416v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09416","description":"<p>The saliency ranking task is recently proposed to study the visual behavior\nthat humans would typically shift their attention over different objects of a\nscene based on their degrees of saliency. Existing approaches focus on learning\neither object-object or object-scene relations. Such a strategy follows the\nidea of object-based attention in Psychology, but it tends to favor those\nobjects with strong semantics (e.g., humans), resulting in unrealistic saliency\nranking. We observe that spatial attention works concurrently with object-based\nattention in the human visual recognition system. During the recognition\nprocess, the human spatial attention mechanism would move, engage, and\ndisengage from region to region (i.e., context to context). This inspires us to\nmodel the region-level interactions, in addition to the object-level reasoning,\nfor saliency ranking. To this end, we propose a novel bi-directional method to\nunify spatial attention and object-based attention for saliency ranking. Our\nmodel includes two novel modules: (1) a selective object saliency (SOS) module\nthat models objectbased attention via inferring the semantic representation of\nthe salient object, and (2) an object-context-object relation (OCOR) module\nthat allocates saliency ranks to objects by jointly modeling the object-context\nand context-object interactions of the salient objects. Extensive experiments\nshow that our approach outperforms existing state-of-theart methods. Our code\nand pretrained model are available at https://github.com/GrassBro/OCOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09481","description":"<p>Denoising diffusion probabilistic models are a promising new class of\ngenerative models that are competitive with GANs on perceptual metrics. In this\npaper, we explore their potential for sequentially generating video. Inspired\nby recent advances in neural video compression, we use denoising diffusion\nmodels to stochastically generate a residual to a deterministic next-frame\nprediction. We compare this approach to two sequential VAE and two GAN\nbaselines on four datasets, where we test the generated frames for perceptual\nquality and forecasting accuracy against ground truth frames. We find\nsignificant improvements in terms of perceptual quality on all data and\nimprovements in terms of frame forecasting for complex high-resolution videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prakhar Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09645","description":"<p>Local feature matching is a computationally intensive task at the subpixel\nlevel. While detector-based methods coupled with feature descriptors struggle\nin low-texture scenes, CNN-based methods with a sequential extract-to-match\npipeline, fail to make use of the matching capacity of the encoder and tend to\noverburden the decoder for matching. In contrast, we propose a novel\nhierarchical extract-and-match transformer, termed as MatchFormer. Inside each\nstage of the hierarchical encoder, we interleave self-attention for feature\nextraction and cross-attention for feature matching, enabling a human-intuitive\nextract-and-match scheme. Such a match-aware encoder releases the overloaded\ndecoder and makes the model highly efficient. Further, combining self- and\ncross-attention on multi-scale features in a hierarchical architecture improves\nmatching robustness, particularly in low-texture indoor scenes or with less\noutdoor training data. Thanks to such a strategy, MatchFormer is a multi-win\nsolution in efficiency, robustness, and precision. Compared to the previous\nbest method in indoor pose estimation, our lite MatchFormer has only 45%\nGFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The\nlarge MatchFormer reaches state-of-the-art on four different benchmarks,\nincluding indoor pose estimation (ScanNet), outdoor pose estimation\n(MegaDepth), homography estimation and image matching (HPatch), and visual\nlocalization (InLoc). Code will be made publicly available at\nhttps://github.com/jamycheung/MatchFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10489","description":"<p>As convolution has empowered many smart applications, dynamic convolution\nfurther equips it with the ability to adapt to diverse inputs. However, the\nstatic and dynamic convolutions are either layout-agnostic or\ncomputation-heavy, making it inappropriate for layout-specific applications,\ne.g., face recognition and medical image segmentation. We observe that these\napplications naturally exhibit the characteristics of large intra-image\n(spatial) variance and small cross-image variance. This observation motivates\nour efficient translation variant convolution (TVConv) for layout-aware visual\nprocessing. Technically, TVConv is composed of affinity maps and a\nweight-generating block. While affinity maps depict pixel-paired relationships\ngracefully, the weight-generating block can be explicitly overparameterized for\nbetter training while maintaining efficient inference. Although conceptually\nsimple, TVConv significantly improves the efficiency of the convolution and can\nbe readily plugged into various network architectures. Extensive experiments on\nface recognition show that TVConv reduces the computational cost by up to 3.1x\nand improves the corresponding throughput by 2.3x while maintaining a high\naccuracy compared to the depthwise convolution. Moreover, for the same\ncomputation cost, we boost the mean accuracy by up to 4.21%. We also conduct\nexperiments on the optic disc/cup segmentation task and obtain better\ngeneralization performance, which helps mitigate the critical data scarcity\nissue. Code is available at https://github.com/JierunChen/TVConv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jierun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianlang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Weipeng Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Sangtae Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10492","description":"<p>Recently self-supervised representation learning has drawn considerable\nattention from the scene text recognition community. Different from previous\nstudies using contrastive learning, we tackle the issue from an alternative\nperspective, i.e., by formulating the representation learning scheme in a\ngenerative manner. Typically, the neighboring image patches among one text line\ntend to have similar styles, including the strokes, textures, colors, etc.\nMotivated by this common sense, we augment one image patch and use its\nneighboring patch as guidance to recover itself. Specifically, we propose a\nSimilarity-Aware Normalization (SimAN) module to identify the different\npatterns and align the corresponding styles from the guiding patch. In this\nway, the network gains representation capability for distinguishing complex\npatterns such as messy strokes and cluttered backgrounds. Experiments show that\nthe proposed SimAN significantly improves the representation quality and\nachieves promising performance. Moreover, we surprisingly find that our\nself-supervised generative network has impressive potential for data synthesis,\ntext image editing, and font interpolation, which suggests that the proposed\nSimAN has a wide range of practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10562","description":"<p>Image signal processors (ISPs) are historically grown legacy software systems\nfor reconstructing color images from noisy raw sensor measurements. They are\nusually composited of many heuristic blocks for denoising, demosaicking, and\ncolor restoration. Color reproduction in this context is of particular\nimportance, since the raw colors are often severely distorted, and each smart\nphone manufacturer has developed their own characteristic heuristics for\nimproving the color rendition, for example of skin tones and other visually\nimportant colors.\n</p>\n<p>In recent years there has been strong interest in replacing the historically\ngrown ISP systems with deep learned pipelines. Much progress has been made in\napproximating legacy ISPs with such learned models. However, so far the focus\nof these efforts has been on reproducing the structural features of the images,\nwith less attention paid to color rendition.\n</p>\n<p>Here we present CRISPnet, the first learned ISP model to specifically target\ncolor rendition accuracy relative to a complex, legacy smart phone ISP. We\nachieve this by utilizing both image metadata (like a legacy ISP would), as\nwell as by learning simple global semantics based on image classification --\nsimilar to what a legacy ISP does to determine the scene type. We also\ncontribute a new ISP image dataset consisting of both high dynamic range\nmonitor data, as well as real-world data, both captured with an actual cell\nphone ISP pipeline under a variety of lighting conditions, exposure times, and\ngain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Matheus Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10636","description":"<p>We propose a trainable Image Signal Processing (ISP) framework that produces\nDSLR quality images given RAW images captured by a smartphone. To address the\ncolor misalignments between training image pairs, we employ a color-conditional\nISP network and optimize a novel parametric color mapping between each input\nRAW and reference DSLR image. During inference, we predict the target color\nimage by designing a color prediction network with efficient Global Context\nTransformer modules. The latter effectively leverage global information to\nlearn consistent color and tone mappings. We further propose a robust masked\naligned loss to identify and discard regions with inaccurate motion estimation\nduring training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,\nconsisting of weakly paired phone RAW and DSLR sRGB images. We extensively\nevaluate our method, setting a new state-of-the-art on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Samarth Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation. (arXiv:2203.10739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10739","description":"<p>Sparsely annotated semantic segmentation (SASS) aims to train a segmentation\nnetwork with coarse-grained (i.e., point-, scribble-, and block-wise)\nsupervisions, where only a small proportion of pixels are labeled in each\nimage. In this paper, we propose a novel tree energy loss for SASS by providing\nsemantic guidance for unlabeled pixels. The tree energy loss represents images\nas minimum spanning trees to model both low-level and high-level pair-wise\naffinities. By sequentially applying these affinities to the network\nprediction, soft pseudo labels for unlabeled pixels are generated in a\ncoarse-to-fine manner, achieving dynamic online self-training. The tree energy\nloss is effective and easy to be incorporated into existing frameworks by\ncombining it with a traditional segmentation loss. Compared with previous SASS\nmethods, our method requires no multistage training strategies, alternating\noptimization procedures, additional supervised data, or time-consuming\npost-processing while outperforming them in all SASS settings. Code is\navailable at https://github.com/megvii-research/TreeEnergyLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Vision Transformers: Combining Improvements in Metric Learning. (arXiv:2203.10833v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10833","description":"<p>Metric learning aims to learn a highly discriminative model encouraging the\nembeddings of similar classes to be close in the chosen metrics and pushed\napart for dissimilar ones. The common recipe is to use an encoder to extract\nembeddings and a distance-based loss function to match the representations --\nusually, the Euclidean distance is utilized. An emerging interest in learning\nhyperbolic data embeddings suggests that hyperbolic geometry can be beneficial\nfor natural data. Following this line of work, we propose a new\nhyperbolic-based model for metric learning. At the core of our method is a\nvision transformer with output embeddings mapped to hyperbolic space. These\nembeddings are directly optimized using modified pairwise cross-entropy loss.\nWe evaluate the proposed model with six different formulations on four datasets\nachieving the new state-of-the-art performance. The source code is available at\nhttps://github.com/htdt/hyp_metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ermolov_A/0/1/0/all/0/1\">Aleksandr Ermolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirvakhabova_L/0/1/0/all/0/1\">Leyla Mirvakhabova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1\">Valentin Khrulkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational ergonomics for task delegation in Human-Robot Collaboration: spatiotemporal adaptation of the robot to the human through contactless gesture recognition. (arXiv:2203.11007v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.11007","description":"<p>The high prevalence of work-related musculoskeletal disorders (WMSDs) could\nbe addressed by optimizing Human-Robot Collaboration (HRC) frameworks for\nmanufacturing applications. In this context, this paper proposes two hypotheses\nfor ergonomically effective task delegation and HRC. The first hypothesis\nstates that it is possible to quantify ergonomically professional tasks using\nmotion data from a reduced set of sensors. Then, the most dangerous tasks can\nbe delegated to a collaborative robot. The second hypothesis is that by\nincluding gesture recognition and spatial adaptation, the ergonomics of an HRC\nscenario can be improved by avoiding needless motions that could expose\noperators to ergonomic risks and by lowering the physical effort required of\noperators. An HRC scenario for a television manufacturing process is optimized\nto test both hypotheses. For the ergonomic evaluation, motion primitives with\nknown ergonomic risks were modeled for their detection in professional tasks\nand to estimate a risk score based on the European Assembly Worksheet (EAWS). A\nDeep Learning gesture recognition module trained with egocentric television\nassembly data was used to complement the collaboration between the human\noperator and the robot. Additionally, a skeleton-tracking algorithm provided\nthe robot with information about the operator's pose, allowing it to spatially\nadapt its motion to the operator's anthropometrics. Three experiments were\nconducted to determine the effect of gesture recognition and spatial adaptation\non the operator's range of motion. The rate of spatial adaptation was used as a\nkey performance indicator (KPI), and a new KPI for measuring the reduction in\nthe operator's motion is presented in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olivas_Padilla_B/0/1/0/all/0/1\">Brenda Elizabeth Olivas-Padilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanagiotou_D/0/1/0/all/0/1\">Dimitris Papanagiotou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senteri_G/0/1/0/all/0/1\">Gavriela Senteri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manitsaris_S/0/1/0/all/0/1\">Sotiris Manitsaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_A/0/1/0/all/0/1\">Alina Glushkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11156","description":"<p>In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2010.06255","description":"<p>Aerial tracking, which has exhibited its omnipresent dedication and splendid\nperformance, is one of the most active applications in the remote sensing\nfield. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,\nequipped with a visual tracking approach, has been widely used in aviation,\nnavigation, agriculture,transportation, and public security, etc. As is\nmentioned above, the UAV-based aerial tracking platform has been gradually\ndeveloped from research to practical application stage, reaching one of the\nmain aerial remote sensing technologies in the future. However, due to the\nreal-world onerous situations, e.g., harsh external challenges, the vibration\nof the UAV mechanical structure (especially under strong wind conditions), the\nmaneuvering flight in complex environment, and the limited computation\nresources onboard, accuracy, robustness, and high efficiency are all crucial\nfor the onboard tracking methods. Recently, the discriminative correlation\nfilter (DCF)-based trackers have stood out for their high computational\nefficiency and appealing robustness on a single CPU, and have flourished in the\nUAV visual tracking community. In this work, the basic framework of the\nDCF-based trackers is firstly generalized, based on which, 23 state-of-the-art\nDCF-based trackers are orderly summarized according to their innovations for\nsolving various issues. Besides, exhaustive and quantitative experiments have\nbeen extended on various prevailing UAV tracking benchmarks, i.e., UAV123,\nUAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903\nframes in total. The experiments show the performance, verify the feasibility,\nand demonstrate the current challenges of DCF-based trackers onboard UAV\ntracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fuling Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Geng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}