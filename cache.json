{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Position-wise optimizer: A nature-inspired optimization algorithm. (arXiv:2204.05312v1 [cs.NE])","link":"http://arxiv.org/abs/2204.05312","description":"<p>The human nervous system utilizes synaptic plasticity to solve optimization\nproblems. Previous studies have tried to add the plasticity factor to the\ntraining process of artificial neural networks, but most of those models\nrequire complex external control over the network or complex novel rules. In\nthis manuscript, a novel nature-inspired optimization algorithm is introduced\nthat imitates biological neural plasticity. Furthermore, the model is tested on\nthree datasets and the results are compared with gradient descent optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valizadeh_A/0/1/0/all/0/1\">Amir Valizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Streaming End-to-End Speech Translation with Neural Transducers. (arXiv:2204.05352v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05352","description":"<p>Neural transducers have been widely used in automatic speech recognition\n(ASR). In this paper, we introduce it to streaming end-to-end speech\ntranslation (ST), which aims to convert audio signals to texts in other\nlanguages directly. Compared with cascaded ST that performs ASR followed by\ntext-based machine translation (MT), the proposed Transformer transducer\n(TT)-based ST model drastically reduces inference latency, exploits speech\ninformation, and avoids error propagation from ASR to MT. To improve the\nmodeling capacity, we propose attention pooling for the joint network in TT. In\naddition, we extend TT-based ST to multilingual ST, which generates texts of\nmultiple languages at the same time. Experimental results on a large-scale 50\nthousand (K) hours pseudo-labeled training set show that TT-based ST not only\nsignificantly reduces inference time but also outperforms non-streaming\ncascaded ST for English-German translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis. (arXiv:2204.05356v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05356","description":"<p>Sentiment analysis is an important task in natural language processing. In\nrecent works, pre-trained language models are often used to achieve\nstate-of-the-art results, especially when training data is scarce. It is common\nto fine-tune on the downstream task, usually by adding task-specific layers on\ntop of the model. In this paper, we focus on aspect-based sentiment analysis,\nwhich involves extracting aspect term, category, and predicting their\ncorresponding polarities. In particular, we are interested in few-shot\nsettings. We propose to reformulate the extraction and prediction tasks into\nthe sequence generation task, using a generative language model with\nunidirectional attention (GPT2 is used unless stated otherwise). This way, the\nmodel learns to accomplish the tasks via language generation without the need\nof training task-specific layers. Our evaluation results on the single-task\npolarity prediction show that our approach outperforms the previous\nstate-of-the-art (based on BERT) on average performance by a large margins in\nfew-shot and full-shot settings. More importantly, our generative approach\nsignificantly reduces the model variance caused by low-resource data. We\nfurther demonstrate that the proposed generative language model can handle\njoint and multi-task settings, unlike previous work. We observe that the\nproposed sequence generation method achieves further improved performances on\npolarity prediction when the model is trained via joint and multi-task\nsettings. Further evaluation on similar sentiment analysis datasets, SST-2,\nSST- and OOS intent detection validates the superiority and noise robustness of\ngenerative language model in few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_Asl_E/0/1/0/all/0/1\">Ehsan Hosseini-Asl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Speech-Text Pre-training for Speech Translation and Recognition. (arXiv:2204.05409v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05409","description":"<p>We describe a method to jointly pre-train speech and text in an\nencoder-decoder modeling framework for speech translation and recognition. The\nproposed method incorporates four self-supervised and supervised subtasks for\ncross modality learning. A self-supervised speech subtask leverages unlabelled\nspeech data, and a (self-)supervised text to text subtask makes use of abundant\ntext training data. Two auxiliary supervised speech tasks are included to unify\nspeech and text modeling space. Our contribution lies in integrating linguistic\ninformation from the text corpus into the speech pre-training. Detailed\nanalysis reveals learning interference among subtasks. Two pre-training\nconfigurations for speech translation and recognition, respectively, are\npresented to alleviate subtask interference. Our experiments show the proposed\nmethod can effectively fuse speech and text information into one model. It\nachieves between 1.7 and 2.3 BLEU improvement above the state of the art on the\nMuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the\nLibrispeech speech recognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Decoding with Controlled Patience. (arXiv:2204.05424v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05424","description":"<p>Text generation with beam search has proven successful in a wide range of\napplications. The commonly-used implementation of beam decoding follows a first\ncome, first served heuristic: it keeps a set of already completed sequences\nover time steps and stops when the size of this set reaches the beam size. We\nintroduce a patience factor, a simple modification to this decoding algorithm,\nthat generalizes the stopping criterion and provides flexibility to the depth\nof search. Extensive empirical results demonstrate that the patience factor\nimproves decoding performance of strong pretrained models on news text\nsummarization and machine translation over diverse language pairs, with a\nnegligible inference slowdown. Our approach only modifies one line of code and\ncan be thus readily incorporated in any implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoTEx: Explaining Model Decisions with Prototype Tensors. (arXiv:2204.05426v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05426","description":"<p>We present ProtoTEx, a novel white-box NLP classification architecture based\non prototype networks. ProtoTEx faithfully explains model decisions based on\nprototype tensors that encode latent clusters of training examples. At\ninference time, classification decisions are based on the distances between the\ninput text and the prototype tensors, explained via the training examples most\nsimilar to the most influential prototypes. We also describe a novel\ninterleaved training algorithm that effectively handles classes characterized\nby the absence of indicative features. On a propaganda detection task, ProtoTEx\naccuracy matches BART-large and exceeds BERT-large with the added benefit of\nproviding faithful explanations. A user study also shows that prototype-based\nexplanations help non-experts to better recognize propaganda in online news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chitrank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1\">Venelin Kovatchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05428","description":"<p>Most evaluations of attribution methods focus on the English language. In\nthis work, we present a multilingual approach for evaluating attribution\nmethods for the Natural Language Inference (NLI) task in terms of plausibility\nand faithfulness properties. First, we introduce a novel cross-lingual strategy\nto measure faithfulness based on word alignments, which eliminates the\npotential downsides of erasure-based evaluations. We then perform a\ncomprehensive evaluation of attribution methods, considering different output\nmechanisms and aggregation methods. Finally, we augment the XNLI dataset with\nhighlight-based explanations, providing a multilingual NLI dataset with\nhighlights, which may support future exNLP studies. Our results show that\nattribution methods performing best for plausibility and faithfulness are\ndifferent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1\">Kerem Zaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification. (arXiv:2204.05459v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05459","description":"<p>Existing approaches to mitigate demographic biases evaluate on monolingual\ndata, however, multilingual data has not been examined. In this work, we treat\nthe gender as domains (e.g., male vs. female) and present a standard domain\nadaptation model to reduce the gender bias and improve performance of text\nclassifiers under multilingual settings. We evaluate our approach on two text\nclassification tasks, hate speech detection and rating prediction, and\ndemonstrate the effectiveness of our approach with three fair-aware baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorrectSpeech: A Fully Automated System for Speech Correction and Accent Reduction. (arXiv:2204.05460v1 [eess.AS])","link":"http://arxiv.org/abs/2204.05460","description":"<p>This study extends our previous work on text-based speech editing to\ndeveloping a fully automated system for speech correction and accent reduction.\nConsider the application scenario that a recorded speech audio contains certain\nerrors, e.g., inappropriate words, mispronunciations, that need to be\ncorrected. The proposed system, named CorrectSpeech, performs the correction in\nthree steps: recognizing the recorded speech and converting it into\ntime-stamped symbol sequence, aligning recognized symbol sequence with target\ntext to determine locations and types of required edit operations, and\ngenerating the corrected speech. Experiments show that the quality and\nnaturalness of corrected speech depend on the performance of speech recognition\nand alignment modules, as well as the granularity level of editing operations.\nThe proposed system is evaluated on two corpora: a manually perturbed version\nof VCTK and L2-ARCTIC. The results demonstrate that our system is able to\ncorrect mispronunciation and reduce accent in speech recordings. Audio samples\nare available online for demonstration\nhttps://daxintan-cuhk.github.io/CorrectSpeech/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_N/0/1/0/all/0/1\">Nianzu Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset. (arXiv:2204.05483v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05483","description":"<p>Dialog systems must be capable of incorporating new skills via updates over\ntime in order to reflect new use cases or deployment scenarios. Similarly,\ndevelopers of such ML-driven systems need to be able to add new training data\nto an already-existing dataset to support these new skills. In intent\nclassification systems, problems can arise if training data for a new skill's\nintent overlaps semantically with an already-existing intent. We call such\ncases collisions. This paper introduces the task of intent collision detection\nbetween multiple datasets for the purposes of growing a system's skillset. We\nintroduce several methods for detecting collisions, and evaluate our methods on\nreal datasets that exhibit collisions. To highlight the need for intent\ncollision detection, we show that model performance suffers if new data is\nadded in such a way that does not arbitrate colliding intents. Finally, we use\ncollision detection to construct and benchmark a new dataset, Redwood, which is\ncomposed of 451 ntent categories from 13 original intent classification\ndatasets, making it the largest publicly available intent classification\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection. (arXiv:2204.05488v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05488","description":"<p>Hope Speech Detection, a task of recognizing positive expressions, has made\nsignificant strides recently. However, much of the current works focus on model\ndevelopment without considering the issue of inherent imbalance in the data.\nOur work revisits this issue in hope-speech detection by introducing focal\nloss, data augmentation, and pre-processing strategies. Accordingly, we find\nthat introducing focal loss as part of Multilingual-BERT's (M-BERT) training\nprocess mitigates the effect of class imbalance and improves overall F1-Macro\nby 0.11. At the same time, contextual and back-translation-based word\naugmentation with M-BERT improves results by 0.10 over baseline despite\nimbalance. Finally, we show that overlapping word removal based on\npre-processing, though simple, improves F1-Macro by 0.28. In due process, we\npresent detailed studies depicting various behaviors of each of these\nstrategies and summarize key findings from our empirical results for those\ninterested in getting the most out of M-BERT for hope speech detection under\nreal-world conditions of data imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LekshmiAmmal_H/0/1/0/all/0/1\">Hariharan RamakrishnaIyer LekshmiAmmal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikiran_M/0/1/0/all/0/1\">Manikandan Ravikiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisha_G/0/1/0/all/0/1\">Gayathri Nisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balamuralidhar_N/0/1/0/all/0/1\">Navyasree Balamuralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1\">Adithya Madhusoodanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madasamy_A/0/1/0/all/0/1\">Anand Kumar Madasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05511","description":"<p>Fact verification (FV) is a challenging task which aims to verify a claim\nusing multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.\nMost existing approaches follow a three-step pipeline framework, including\ndocument retrieval, sentence retrieval and claim verification. High-quality\nevidences provided by the first two steps are the foundation of the effective\nreasoning in the last step. Despite being important, high-quality evidences are\nrarely studied by existing works for FV, which often adopt the off-the-shelf\nmodels to retrieve relevant documents and sentences in an\n\"index-retrieve-then-rank\" fashion. This classical approach has clear drawbacks\nas follows: i) a large document index as well as a complicated search process\nis required, leading to considerable memory and computational overhead; ii)\nindependent scoring paradigms fail to capture the interactions among documents\nand sentences in ranking; iii) a fixed number of sentences are selected to form\nthe final evidence set. In this work, we propose \\textit{GERE}, the first\nsystem that retrieves evidences in a generative fashion, i.e., generating the\ndocument titles as well as evidence sentence identifiers. This enables us to\nmitigate the aforementioned technical issues since: i) the memory and\ncomputational cost is greatly reduced because the document index is eliminated\nand the heavy ranking process is replaced by a light generative process; ii)\nthe dependency between documents and that between sentences could be captured\nvia sequential generation process; iii) the generative formulation allows us to\ndynamically select a precise set of relevant evidences for each claim. The\nexperimental results on the FEVER dataset show that GERE achieves significant\nimprovements over the state-of-the-art baselines, with both time-efficiency and\nmemory-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Faithfulness Metrics for Model Interpretability Methods. (arXiv:2204.05514v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05514","description":"<p>Interpretation methods to reveal the internal reasoning processes behind\nmachine learning models have attracted increasing attention in recent years. To\nquantify the extent to which the identified interpretations truly reflect the\nintrinsic decision-making mechanisms, various faithfulness evaluation metrics\nhave been proposed. However, we find that different faithfulness metrics show\nconflicting preferences when comparing different interpretations. Motivated by\nthis observation, we aim to conduct a comprehensive and comparative study of\nthe widely adopted faithfulness metrics. In particular, we introduce two\nassessment dimensions, namely diagnosticity and time complexity. Diagnosticity\nrefers to the degree to which the faithfulness metric favours relatively\nfaithful interpretations over randomly generated ones, and time complexity is\nmeasured by the average number of model forward passes. According to the\nexperimental results, we find that sufficiency and comprehensiveness metrics\nhave higher diagnosticity and lower time complexity than the other faithfulness\nmetric\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chun Sik Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Huanqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Guanqing Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05515","description":"<p>Compared with unimodal data, multimodal data can provide more features to\nhelp the model analyze the sentiment of data. Previous research works rarely\nconsider token-level feature fusion, and few works explore learning the common\nfeatures related to sentiment in multimodal data to help the model fuse\nmultimodal features. In this paper, we propose a Contrastive Learning and\nMulti-Layer Fusion (CLMLF) method for multimodal sentiment detection.\nSpecifically, we first encode text and image to obtain hidden representations,\nand then use a multi-layer fusion module to align and fuse the token-level\nfeatures of text and image. In addition to the sentiment analysis task, we also\ndesigned two contrastive learning tasks, label based contrastive learning and\ndata based contrastive learning tasks, which will help the model learn common\nfeatures related to sentiment in multimodal data. Extensive experiments\nconducted on three publicly available multimodal datasets demonstrate the\neffectiveness of our approach for multimodal sentiment detection compared with\nexisting methods. The codes are available for use at\nhttps://github.com/Link-Li/CLMLF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trigger-GNN: A Trigger-Based Graph Neural Network for Nested Named Entity Recognition. (arXiv:2204.05518v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05518","description":"<p>Nested named entity recognition (NER) aims to identify the entity boundaries\nand recognize categories of the named entities in a complex hierarchical\nsentence. Some works have been done using character-level, word-level, or\nlexicon-level based models. However, such researches ignore the role of the\ncomplementary annotations. In this paper, we propose a trigger-based graph\nneural network (Trigger-GNN) to leverage the nested NER. It obtains the\ncomplementary annotation embeddings through entity trigger encoding and\nsemantic matching, and tackle nested entity utilizing an efficient graph\nmessage passing architecture, aggregation-update mode. We posit that using\nentity triggers as external annotations can add in complementary supervision\nsignals on the whole sentences. It helps the model to learn and generalize more\nefficiently and cost-effectively. Experiments show that the Trigger-GNN\nconsistently outperforms the baselines on four public NER datasets, and it can\neffectively alleviate the nested NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yuan Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_F/0/1/0/all/0/1\">Fanyang Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Annotation of Therapeutic Working Alliance in Psychotherapy. (arXiv:2204.05522v1 [q-bio.NC])","link":"http://arxiv.org/abs/2204.05522","description":"<p>The therapeutic working alliance is an important predictor of the outcome of\nthe psychotherapy treatment. In practice, the working alliance is estimated\nfrom a set of scoring questionnaires in an inventory that both the patient and\nthe therapists fill out. In this work, we propose an analytical framework of\ndirectly inferring the therapeutic working alliance from the natural language\nwithin the psychotherapy sessions in a turn-level resolution with deep\nembeddings such as the Doc2Vec and SentenceBERT models. The transcript of each\npsychotherapy session can be transcribed and generated in real-time from the\nsession speech recordings, and these embedded dialogues are compared with the\ndistributed representations of the statements in the working alliance\ninventory. We demonstrate, in a real-world dataset with over 950 sessions of\npsychotherapy treatments in anxiety, depression, schizophrenia and suicidal\npatients, the effectiveness of this method in mapping out trajectories of\npatient-therapist alignment and the interpretability that can offer insights in\nclinical psychiatry. We believe such a framework can be provide timely feedback\nto the therapist regarding the quality of the conversation in interview\nsessions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image. (arXiv:2204.05533v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05533","description":"<p>This study investigates how fake news uses a thumbnail for a news article\nwith a focus on whether a news article's thumbnail represents the news content\ncorrectly. A news article shared with an irrelevant thumbnail can mislead\nreaders into having a wrong impression of the issue, especially in social media\nenvironments where users are less likely to click the link and consume the\nentire content. We propose to capture the degree of semantic incongruity in the\nmultimodal relation by using the pretrained CLIP representation. From a\nsource-level analysis, we found that fake news employs a more incongruous image\nto the main content than general news. Going further, we attempted to detect\nnews articles with image-text incongruity. Evaluation experiments suggest that\nCLIP-based methods can successfully detect news articles in which the thumbnail\nis semantically irrelevant to news text. This study contributes to the research\nby providing a novel view on tackling online fake news and misinformation. Code\nand datasets are available at\nhttps://github.com/ssu-humane/fake-news-thumbnail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyewon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1\">Yejun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not always about you: Prioritizing community needs when developing endangered language technology. (arXiv:2204.05541v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05541","description":"<p>Languages are classified as low-resource when they lack the quantity of data\nnecessary for training statistical and machine learning tools and models.\nCauses of resource scarcity vary but can include poor access to technology for\ndeveloping these resources, a relatively small population of speakers, or a\nlack of urgency for collecting such resources in bilingual populations where\nthe second language is high-resource. As a result, the languages described as\nlow-resource in the literature are as different as Finnish on the one hand,\nwith millions of speakers using it in every imaginable domain, and Seneca, with\nonly a small-handful of fluent speakers using the language primarily in a\nrestricted domain. While issues stemming from the lack of resources necessary\nto train models unite this disparate group of languages, many other issues cut\nacross the divide between widely-spoken low resource languages and endangered\nlanguages. In this position paper, we discuss the unique technological,\ncultural, practical, and ethical challenges that researchers and indigenous\nspeech community members face when working together to develop language\ntechnology to support endangered language documentation and revitalization. We\nreport the perspectives of language teachers, Master Speakers and elders from\nindigenous communities, as well as the point of view of academics. We describe\nan ongoing fruitful collaboration and make recommendations for future\npartnerships between academic researchers and language community stakeholders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zoey Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_C/0/1/0/all/0/1\">Crystal Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatcher_R/0/1/0/all/0/1\">Richard Hatcher Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudhommeaux_E/0/1/0/all/0/1\">Emily Prud&#x27;hommeaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition. (arXiv:2204.05544v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05544","description":"<p>Recent years have witnessed the improving performance of Chinese Named Entity\nRecognition (NER) from proposing new frameworks or incorporating word lexicons.\nHowever, the inner composition of entity mentions in character-level Chinese\nNER has been rarely studied. Actually, most mentions of regular types have\nstrong name regularity. For example, entities end with indicator words such as\n\"company\" or \"bank\" usually belong to organization. In this paper, we propose a\nsimple but effective method for investigating the regularity of entity spans in\nChinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).\nSpecifically, the proposed model consists of two branches: a regularity-aware\nmodule and a regularityagnostic module. The regularity-aware module captures\nthe internal regularity of each span for better entity type prediction, while\nthe regularity-agnostic module is employed to locate the boundary of entities\nand relieve the excessive attention to span regularity. An orthogonality space\nis further constructed to encourage two modules to extract different aspects of\nregularity features. To verify the effectiveness of our method, we conduct\nextensive experiments on three benchmark datasets and a practical medical\ndataset. The experimental results show that our RICON significantly outperforms\nprevious state-of-the-art methods, including various lexicon-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yingjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering. (arXiv:2204.05555v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05555","description":"<p>Price Per Unit (PPU) is an essential information for consumers shopping on\ne-commerce websites when comparing products. Finding total quantity in a\nproduct is required for computing PPU, which is not always provided by the\nsellers. To predict total quantity, all relevant quantities given in a product\nattributes such as title, description and image need to be inferred correctly.\nWe formulate this problem as a question-answering (QA) task rather than named\nentity recognition (NER) task for fact extraction. In our QA approach, we first\npredict the unit of measure (UoM) type (e.g., volume, weight or count), that\nformulates the desired question (e.g., \"What is the total volume?\") and then\nuse this question to find all the relevant answers. Our model architecture\nconsists of two subnetworks for the two subtasks: a classifier to predict UoM\ntype (or the question) and an extractor to extract the relevant quantities. We\nuse a deep character-level CNN architecture for both subtasks, which enables\n(1) easy expansion to new stores with similar alphabets, (2) multi-span\nanswering due to its span-image architecture and (3) easy deployment by keeping\nmodel-inference latency low. Our QA approach outperforms rule-based methods by\n34.4% in precision and also BERT-based fact extraction approach in all stores\nglobally, with largest precision lift of 10.6% in the US store.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arici_T/0/1/0/all/0/1\">Tarik Arici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Kushal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceker_H/0/1/0/all/0/1\">Hayreddin &#xc7;eker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saladi_A/0/1/0/all/0/1\">Anoop S V K K Saladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutar_I/0/1/0/all/0/1\">Ismail Tutar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting. (arXiv:2204.05610v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05610","description":"<p>Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in\nproducing rational and factual responses. However, to establish long-term\nrelationships with users, the KDG model needs the capability to generate\nresponses in a desired style or attribute. Thus, we study a new problem:\nStylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two\nchallenges: (1) How to train a SKDG model where no &lt;context, knowledge,\nstylized response&gt; triples are available. (2) How to cohere with context and\npreserve the knowledge when generating a stylized response. In this paper, we\npropose a novel disentangled template rewriting (DTR) method which generates\nresponses via combing disentangled style templates (from monolingual stylized\ncorpus) and content templates (from KDG corpus). The entire framework is\nend-to-end differentiable and learned without supervision. Extensive\nexperiments on two benchmarks indicate that DTR achieves a significant\nimprovement on all evaluation metrics compared with previous state-of-the-art\nstylized dialogue generation methods. Besides, DTR achieves comparable\nperformance with the state-of-the-art KDG methods in standard KDG evaluation\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jian Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR in German: A Detailed Error Analysis. (arXiv:2204.05617v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05617","description":"<p>The amount of freely available systems for automatic speech recognition (ASR)\nbased on neural networks is growing steadily, with equally increasingly\nreliable predictions. However, the evaluation of trained models is typically\nexclusively based on statistical metrics such as WER or CER, which do not\nprovide any insight into the nature or impact of the errors produced when\npredicting transcripts from speech input. This work presents a selection of ASR\nmodel architectures that are pretrained on the German language and evaluates\nthem on a benchmark of diverse test datasets. It identifies cross-architectural\nprediction errors, classifies those into categories and traces the sources of\nerrors per category back into training data as well as other sources. Finally,\nit discusses solutions in order to create qualitatively better training\ndatasets and more robust ASR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1\">Johannes Wirth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1\">Rene Peinl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks. (arXiv:2204.05626v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05626","description":"<p>In this paper, we study the challenging instance-wise vision-language tasks,\nwhere the free-form language is required to align with the objects instead of\nthe whole image. To address these tasks, we propose X-DETR, whose architecture\nhas three major components: an object detector, a language encoder, and\nvision-language alignment. The vision and language streams are independent\nuntil the end and they are aligned using an efficient dot-product operation.\nThe whole network is trained end-to-end, such that the detector is optimized\nfor the vision-language tasks instead of an off-the-shelf component. To\novercome the limited size of paired object-language annotations, we leverage\nother weak types of supervision to expand the knowledge coverage. This simple\nyet effective architecture of X-DETR shows good accuracy and fast speeds for\nmultiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection\nof 1.2K categories at ~20 frames per second without using any LVIS annotation\nduring training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gukyeong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1\">Erhan Bas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiomify -- Building a Collocation-supplemented Reverse Dictionary of English Idioms with Word2Vec for non-native learners. (arXiv:2204.05634v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05634","description":"<p>The aim of idiomify is to build a collocation-supplemented reverse dictionary\nof idioms for the non-native learners of English. We aim to do so because the\nreverse dictionary could help the non-natives explore idioms on demand, and the\ncollocations could also guide them on using idioms more adequately. The\ncornerstone of the project is a reliable way of mining idioms from corpora,\nwhich is however a challenge because idioms extensively vary in forms. We\ntackle this by automatically deriving matching rules from their base forms. We\nuse Point-wise Mutual Inclusion (PMI), Term Frequency - Inverse Document\nFrequency (TF-IDF) to model collocations, since both of them are popular metric\nfor pairwise significance. We also try Term Frequency (TF) as the baseline\nmodel. As for implementing the reverse-dictionary, three approaches could be\ntaken: inverted index, graphs and distributional semantics. We choose to take\nthe last approach and implement the reverse dictionary with Word2Vec, because\nit is the most flexible approach of all and Word2Vec is a simple yet strong\nbaseline. Evaluating the methods has revealed rooms for improvement. We learn\nthat we can better identify idioms with the help of slop, wildcard and\nreordering techniques. We also learn that we can get the best of both PMI and\nTF-IDF if we use machine learning to find the sweet spot. Lastly, We learn that\nIdiomify could be further improved with a mixture of inverted index and\ndistributional semantics approach. The limits aside, the proposed methods are\nfeasible, and their benefits to the non-natives are apparent, which therefore\ncan be used to aid the non-natives in acquiring English idioms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eu-Bin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creativity in translation: machine translation as a constraint for literary texts. (arXiv:2204.05655v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05655","description":"<p>This article presents the results of a study involving the translation of a\nshort story by Kurt Vonnegut from English to Catalan and Dutch using three\nmodalities: machine-translation (MT), post-editing (PE) and translation without\naid (HT). Our aim is to explore creativity, understood to involve novelty and\nacceptability, from a quantitative perspective. The results show that HT has\nthe highest creativity score, followed by PE, and lastly, MT, and this is\nunanimous from all reviewers. A neural MT system trained on literary data does\nnot currently have the necessary capabilities for a creative translation; it\nrenders literal solutions to translation problems. More importantly, using MT\nto post-edit raw output constrains the creativity of translators, resulting in\na poorer translation often not fit for publication, according to experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arenas_A/0/1/0/all/0/1\">Ana Guerberof Arenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks. (arXiv:2204.05660v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05660","description":"<p>Given the ubiquitous nature of numbers in text, reasoning with numbers to\nperform simple calculations is an important skill of AI systems. While many\ndatasets and models have been developed to this end, state-of-the-art AI\nsystems are brittle; failing to perform the underlying mathematical reasoning\nwhen they appear in a slightly different scenario. Drawing inspiration from\nGLUE that was proposed in the context of natural language understanding, we\npropose NumGLUE, a multi-task benchmark that evaluates the performance of AI\nsystems on eight different tasks, that at their core require simple arithmetic\nunderstanding. We show that this benchmark is far from being solved with neural\nmodels including state-of-the-art large-scale language models performing\nsignificantly worse than humans (lower by 46.4%). Further, NumGLUE promotes\nsharing knowledge across tasks, especially those with limited training data as\nevidenced by the superior performance (average gain of 3.4% on each task) when\na model is jointly trained on all the tasks as opposed to task-specific\nmodeling. Finally, we hope that NumGLUE will encourage systems that perform\nrobust and general arithmetic reasoning within language, a first step towards\nbeing able to perform more complex mathematical reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arindam Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1\">Bhavdeep Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured. (arXiv:2204.05673v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05673","description":"<p>Transformer-based models are now predominant in NLP. They outperform\napproaches based on static models in many respects. This success has in turn\nprompted research that reveals a number of biases in the language models\ngenerated by transformers. In this paper we utilize this research on biases to\ninvestigate to what extent transformer-based language models allow for\nextracting knowledge about object relations (X occurs in Y; X consists of Z;\naction A involves using X). To this end, we compare contextualized models with\ntheir static counterparts. We make this comparison dependent on the application\nof a number of similarity measures and classifiers. Our results are threefold:\nFirstly, we show that the models combined with the different similarity\nmeasures differ greatly in terms of the amount of knowledge they allow for\nextracting. Secondly, our results suggest that similarity measures perform much\nworse than classifier-based approaches. Thirdly, we show that, surprisingly,\nstatic models perform almost as well as contextualized models -- in some cases\neven better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henlein_A/0/1/0/all/0/1\">Alexander Henlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1\">Alexander Mehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Financial Causality Extraction. (arXiv:2204.05674v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05674","description":"<p>Causality represents the foremost relation between events in financial\ndocuments such as financial news articles, financial reports. Each financial\ncausality contains a cause span and an effect span. Previous works proposed\nsequence labeling approaches to solve this task. But sequence labeling models\nfind it difficult to extract multiple causalities and overlapping causalities\nfrom the text segments. In this paper, we explore a generative approach for\ncausality extraction using the encoder-decoder framework and pointer networks.\nWe use a causality dataset from the financial domain, \\textit{FinCausal}, for\nour experiments and our proposed framework achieves very competitive\nperformance on this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Soumya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butala_Y/0/1/0/all/0/1\">Yash Butala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1\">Koustuv Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Losses for One-Class Textual Anomaly Detection. (arXiv:2204.05695v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05695","description":"<p>Current deep learning methods for anomaly detection in text rely on\nsupervisory signals in inliers that may be unobtainable or bespoke\narchitectures that are difficult to tune. We study a simpler alternative:\nfine-tuning Transformers on the inlier data with self-supervised objectives and\nusing the losses as an anomaly score. Overall, the self-supervision approach\noutperforms other methods under various anomaly detection scenarios, improving\nthe AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by\n22.8% on average. Additionally, the optimal objective and resultant learnt\nrepresentation depend on the type of downstream anomaly. The separability of\nanomalies and inliers signals that a representation is more effective for\ndetecting semantic anomalies, whilst the presence of narrow feature directions\nsignals a representation that is effective for detecting syntactic anomalies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_K/0/1/0/all/0/1\">Kimberly T. Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_T/0/1/0/all/0/1\">Toby Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1\">Lewis D. Griffin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change. (arXiv:2204.05717v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05717","description":"<p>Morphological and syntactic changes in word usage (as captured, e.g., by\ngrammatical profiles) have been shown to be good predictors of a word's meaning\nchange. In this work, we explore whether large pre-trained contextualised\nlanguage models, a common tool for lexical semantic change detection, are\nsensitive to such morphosyntactic changes. To this end, we first compare the\nperformance of grammatical profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, covering 7 languages, and then combine\nthe two approaches in ensembles to assess their complementarity. Our results\nshow that ensembling grammatical profiles with XLM-R improves semantic change\ndetection performance for most datasets and languages. This indicates that\nlanguage models do not fully cover the fine-grained morphological and syntactic\nsignals that are explicitly represented in grammatical profiles.\n</p>\n<p>An interesting exception are the test sets where the time spans under\nanalysis are much longer than the time gap between them (for example,\ncentury-long spans with a one-year gap between them). Morphosyntactic change is\nslow so grammatical profiles do not detect in such cases. In contrast, language\nmodels, thanks to their access to lexical information, are able to detect fast\ntopical changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1\">Lidia Pivovarova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposed Meta-Learning for Few-Shot Named Entity Recognition. (arXiv:2204.05751v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05751","description":"<p>Few-shot named entity recognition (NER) systems aim at recognizing\nnovel-class named entities based on only a few labeled examples. In this paper,\nwe present a decomposed meta-learning approach which addresses the problem of\nfew-shot NER by sequentially tackling few-shot span detection and few-shot\nentity typing using meta-learning. In particular, we take the few-shot span\ndetection as a sequence labeling problem and train the span detector by\nintroducing the model-agnostic meta-learning (MAML) algorithm to find a good\nmodel parameter initialization that could fast adapt to new entity classes. For\nfew-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced\nprototypical networks to find a good embedding space that can better\ndistinguish text span representations from different entity classes. Extensive\nexperiments on various benchmarks show that our approach achieves superior\nperformance over prior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tingting Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Express in Knowledge-Grounded Conversation. (arXiv:2204.05805v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05805","description":"<p>Grounding dialogue generation by extra knowledge has shown great potentials\ntowards building a system capable of replying with knowledgeable and engaging\nresponses. Existing studies focus on how to synthesize a response with proper\nknowledge, yet neglect that the same knowledge could be expressed differently\nby speakers even under the same context. In this work, we mainly consider two\naspects of knowledge expression, namely the structure of the response and style\nof the content in each part. We therefore introduce two sequential latent\nvariables to represent the structure and the content style respectively. We\npropose a segmentation-based generation model and optimize the model by a\nvariational approach to discover the underlying pattern of knowledge expression\nin a response. Evaluation results on two benchmarks indicate that our model can\nlearn the structure style defined by a few examples and generate responses in\ndesired content style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tingchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages. (arXiv:2204.05814v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05814","description":"<p>Accuracy of English-language Question Answering (QA) systems has improved\nsignificantly in recent years with the advent of Transformer-based models\n(e.g., BERT). These models are pre-trained in a self-supervised fashion with a\nlarge English text corpus and further fine-tuned with a massive English QA\ndataset (e.g., SQuAD). However, QA datasets on such a scale are not available\nfor most of the other languages. Multi-lingual BERT-based models (mBERT) are\noften used to transfer knowledge from high-resource languages to low-resource\nlanguages. Since these models are pre-trained with huge text corpora containing\nmultiple languages, they typically learn language-agnostic embeddings for\ntokens from different languages. However, directly training an mBERT-based QA\nsystem for low-resource languages is challenging due to the paucity of training\ndata. In this work, we augment the QA samples of the target language using\ntranslation and transliteration into other languages and use the augmented data\nto fine-tune an mBERT-based QA model, which is already pre-trained in English.\nExperiments on the Google ChAII dataset show that fine-tuning the mBERT model\nwith translations from the same language family boosts the question-answering\nperformance, whereas the performance degrades in the case of cross-language\nfamilies. We further show that introducing a contrastive loss between the\ntranslated question-context feature pairs during the fine-tuning process,\nprevents such degradation with cross-lingual family translations and leads to\nmarginal improvement. The code for this work is available at\nhttps://github.com/gokulkarthik/mucot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehlot_A/0/1/0/all/0/1\">Abhishek Singh Gehlot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1\">Sahal Shaji Mullappilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework. (arXiv:2204.05819v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05819","description":"<p>Entity recognition is a fundamental task in understanding document images.\nTraditional sequence labeling frameworks treat the entity types as class IDs\nand rely on extensive data and high-quality annotations to learn semantics\nwhich are typically expensive in practice. In this paper, we aim to build an\nentity recognition model requiring only a few shots of annotated document\nimages. To overcome the data limitation, we propose to leverage the label\nsurface names to better inform the model of the target entity type semantics\nand also embed the labels into the spatial embedding space to capture the\nspatial correspondence between regions and labels. Specifically, we go beyond\nsequence labeling and develop a novel label-aware seq2seq framework, LASER. The\nproposed model follows a new labeling scheme that generates the label surface\nnames word-by-word explicitly after generating the entities. During training,\nLASER refines the label semantics by updating the label surface name\nrepresentations and also strengthens the label-region correlation. In this way,\nLASER recognizes the entities from document images through both semantic and\nlayout correspondence. Extensive experiments on two benchmark datasets\ndemonstrate the superiority of LASER under the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?. (arXiv:2204.05832v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05832","description":"<p>Large pretrained Transformer language models have been shown to exhibit\nzero-shot generalization, i.e. they can perform a wide variety of tasks that\nthey were not explicitly trained on. However, the architectures and pretraining\nobjectives used across state-of-the-art models differ significantly, and there\nhas been limited systematic comparison of these factors. In this work, we\npresent a large-scale evaluation of modeling choices and their impact on\nzero-shot generalization. In particular, we focus on text-to-text models and\nexperiment with three model architectures (causal/non-causal decoder-only and\nencoder-decoder), trained with two different pretraining objectives\n(autoregressive and masked language modeling), and evaluated with and without\nmultitask prompted finetuning. We train models with over 5 billion parameters\nfor more than 170 billion tokens, thereby increasing the likelihood that our\nconclusions will transfer to even larger scales. Our experiments show that\ncausal decoder-only models trained on an autoregressive language modeling\nobjective exhibit the strongest zero-shot generalization after purely\nunsupervised pretraining. However, models with non-causal visibility on their\ninput trained with a masked language modeling objective followed by multitask\nfinetuning perform the best among our experiments. We therefore consider the\nadaptation of pretrained models across architectures and objectives. We find\nthat pretrained non-causal decoder models can be adapted into performant\ngenerative causal decoder models, using autoregressive language modeling as a\ndownstream task. Furthermore, we find that pretrained causal decoder models can\nbe efficiently adapted into non-causal decoder models, ultimately achieving\ncompetitive performance after multitask finetuning. Code and checkpoints are\navailable at https://github.com/bigscience-workshop/architecture-objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution in Literary Texts. (arXiv:2204.05836v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05836","description":"<p>We present the Project Dialogism Novel Corpus, or PDNC, an annotated dataset\nof quotations for English literary texts. PDNC contains annotations for 35,978\nquotations across 22 full-length novels, and is by an order of magnitude the\nlargest corpus of its kind. Each quotation is annotated for the speaker,\naddressees, type of quotation, referring expression, and character mentions\nwithin the quotation text. The annotated attributes allow for a comprehensive\nevaluation of models of quotation attribution and coreference for literary\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_A/0/1/0/all/0/1\">Adam Hammond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirst_G/0/1/0/all/0/1\">Graeme Hirst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. (arXiv:2204.05862v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05862","description":"<p>We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1\">Neel Nanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies. (arXiv:2204.05879v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05879","description":"<p>Generating factual, long-form text such as Wikipedia articles raises three\nkey challenges: how to gather relevant evidence, how to structure information\ninto well-formed text, and how to ensure that the generated text is factually\ncorrect. We address these by developing a model for English text that uses a\nretrieval mechanism to identify relevant supporting information on the web and\na cache-based pre-trained encoder-decoder to generate long-form biographies\nsection by section, including citation information. To assess the impact of\navailable web evidence on the output text, we compare the performance of our\napproach when generating biographies about women (for which less information is\navailable on the web) vs. biographies generally. To this end, we curate a\ndataset of 1,500 biographies about women. We analyze our generated text to\nunderstand how differences in available web evidence data affect generation. We\nevaluate the factuality, fluency, and quality of the generated texts using\nautomatic metrics and human evaluation. We hope that these techniques can be\nused as a starting point for human writers, to aid in reducing the complexity\ninherent in the creation of long-form, factual text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardent_C/0/1/0/all/0/1\">Claire Gardent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking. (arXiv:2204.05895v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05895","description":"<p>In a task-oriented dialogue system, Dialogue State Tracking (DST) keeps track\nof all important information by filling slots with values given through the\nconversation. Existing methods generally rely on a predefined set of values and\nstruggle to generalise to previously unseen slots in new domains. In this\npaper, we propose a multi-domain and multi-lingual dialogue state tracker in a\nneural reading comprehension approach. Our approach fills the slot values using\nspan prediction, where the values are extracted from the dialogue itself. With\na novel training strategy and an independent domain classifier, empirical\nresults demonstrate that our model is a domain-scalable and open-vocabulary\nmodel that achieves 53.2% Joint Goal Accuracy (JGA) on MultiWOZ 2.1. We show\nits competitive transferability by zero-shot domain-adaptation experiments on\nMultiWOZ 2.1 with an average JGA of 31.6% for five domains. In addition, it\nachieves cross-lingual transfer with state-of-the-art zero-shot results, 64.9%\nJGA from English to German and 68.6% JGA from English to Italian on WOZ 2.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dialogue Policy Transformer for Continual Reinforcement Learning. (arXiv:2204.05928v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05928","description":"<p>Continual learning is one of the key components of human learning and a\nnecessary requirement of artificial intelligence. As dialogue can potentially\nspan infinitely many topics and tasks, a task-oriented dialogue system must\nhave the capability to continually learn, dynamically adapting to new\nchallenges while preserving the knowledge it already acquired. Despite the\nimportance, continual reinforcement learning of the dialogue policy has\nremained largely unaddressed. The lack of a framework with training protocols,\nbaseline models and suitable metrics, has so far hindered research in this\ndirection. In this work we fill precisely this gap, enabling research in\ndialogue policy optimisation to go from static to dynamic learning. We provide\na continual learning algorithm, baseline architectures and metrics for\nassessing continual learning models. Moreover, we propose the dynamic dialogue\npolicy transformer (DDPT), a novel dynamic architecture that can integrate new\nknowledge seamlessly, is capable of handling large state spaces and obtains\nsignificant zero-shot performance when being exposed to unseen domains, without\nany growth in network parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-Chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Logical Event Schemas From Pre-Trained Language Models. (arXiv:2204.05939v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05939","description":"<p>We present NESL (the Neuro-Episodic Schema Learner), an event schema learning\nsystem that combines large language models, FrameNet parsing, a powerful\nlogical representation of language, and a set of simple behavioral schemas\nmeant to bootstrap the learning process. In lieu of a pre-made corpus of\nstories, our dataset is a continuous feed of \"situation samples\" from a\npre-trained language model, which are then parsed into FrameNet frames, mapped\ninto simple behavioral schemas, and combined and generalized into complex,\nhierarchical schemas for a variety of everyday scenarios. We show that careful\nsampling from the language model can help emphasize stereotypical properties of\nsituations and de-emphasize irrelevant details, and that the resulting schemas\nspecify situations more comprehensively than those learned by other systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawley_L/0/1/0/all/0/1\">Lane Lawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_L/0/1/0/all/0/1\">Lenhart Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05953","description":"<p>Sign language recognition and translation first uses a recognition module to\ngenerate glosses from sign language videos and then employs a translation\nmodule to translate glosses into spoken sentences. Most existing works focus on\nthe recognition step, while paying less attention to sign language translation.\nIn this work, we propose a task-aware instruction network, namely TIN-SLT, for\nsign language translation, by introducing the instruction module and the\nlearning-based feature fuse strategy into a Transformer network. In this way,\nthe pre-trained model's language ability can be well explored and utilized to\nfurther boost the translation performance. Moreover, by exploring the\nrepresentation space of sign language glosses and target spoken language, we\npropose a multi-level data augmentation scheme to adjust the data distribution\nof the training set. We conduct extensive experiments on two challenging\nbenchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method\noutperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code\nis published at https://github.com/yongcaoplus/TIN-SLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Long Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengdao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_H/0/1/0/all/0/1\">Hwang Kai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantified Reproducibility Assessment of NLP Results. (arXiv:2204.05961v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05961","description":"<p>This paper describes and tests a method for carrying out quantified\nreproducibility assessment (QRA) that is based on concepts and definitions from\nmetrology. QRA produces a single score estimating the degree of reproducibility\nof a given system and evaluation measure, on the basis of the scores from, and\ndifferences between, different reproductions. We test QRA on 18 system and\nevaluation measure combinations (involving diverse NLP tasks and types of\nevaluation), for each of which we have the original results and one to seven\nreproduction results. The proposed QRA method produces\ndegree-of-reproducibility scores that are comparable across multiple\nreproductions not only of the same, but of different original studies. We find\nthat the proposed method facilitates insights into causes of variation between\nreproductions, and allows conclusions to be drawn about what changes to system\nand/or evaluation design might lead to improved reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1\">Maja Popovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering material information using hierarchical Reformer model on financial regulatory filings. (arXiv:2204.05979v1 [q-fin.ST])","link":"http://arxiv.org/abs/2204.05979","description":"<p>Most applications of machine learning for finance are related to forecasting\ntasks for investment decisions. Instead, we aim to promote a better\nunderstanding of financial markets with machine learning techniques. Leveraging\nthe tremendous progress in deep learning models for natural language\nprocessing, we construct a hierarchical Reformer ([15]) model capable of\nprocessing a large document level dataset, SEDAR, from canadian financial\nregulatory filings. Using this model, we show that it is possible to predict\ntrade volume changes using regulatory filings. We adapt the pretraining task of\nHiBERT ([36]) to obtain good sentence level representations using a large\nunlabelled document dataset. Finetuning the model to successfully predict trade\nvolume changes indicates that the model captures a view from financial markets\nand processing regulatory filings is beneficial. Analyzing the attention\npatterns of our model reveals that it is able to detect some indications of\nmaterial information without explicit training, which is highly relevant for\ninvestors and also for the market surveillance mandate of financial regulators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Mercier_F/0/1/0/all/0/1\">Francois Mercier</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Narsimhan_M/0/1/0/all/0/1\">Makesh Narsimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem. (arXiv:2204.05990v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05990","description":"<p>We propose an autoregressive entity linking model, that is trained with two\nauxiliary tasks, and learns to re-rank generated samples at inference time. Our\nproposed novelties address two weaknesses in the literature. First, a recent\nmethod proposes to learn mention detection and then entity candidate selection,\nbut relies on predefined sets of candidates. We use encoder-decoder\nautoregressive entity linking in order to bypass this need, and propose to\ntrain mention detection as an auxiliary task instead. Second, previous work\nsuggests that re-ranking could help correct prediction errors. We add a new,\nauxiliary task, match prediction, to learn re-ranking. Without the use of a\nknowledge base or candidate sets, our model sets a new state of the art in two\nbenchmark datasets of entity linking: COMETA in the biomedical domain, and\nAIDA-CoNLL in the news domain. We show through ablation studies that each of\nthe two auxiliary tasks increases performance, and that re-ranking is an\nimportant factor to the increase. Finally, our low-resource experimental\nresults suggest that performance on the main task benefits from the knowledge\nlearned by the auxiliary tasks, and not just from the additional training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mrini_K/0/1/0/all/0/1\">Khalil Mrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05991","description":"<p>Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sanjay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">Will Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Entity Disambiguation with BERT. (arXiv:1909.00426v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.00426","description":"<p>We propose a global entity disambiguation (ED) model based on BERT. To\ncapture global contextual information for ED, our model treats not only words\nbut also entities as input tokens, and solves the task by sequentially\nresolving mentions to their referent entities and using resolved entities as\ninputs at each step. We train the model using a large entity-annotated corpus\nobtained from Wikipedia. We achieve new state-of-the-art results on five\nstandard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The\nsource code and model checkpoint are available at\nhttps://github.com/studio-ousia/luke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washio_K/0/1/0/all/0/1\">Koki Washio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1\">Hiroyuki Shindo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Represent Programs with Heterogeneous Graphs. (arXiv:2012.04188v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2012.04188","description":"<p>Program source code contains complex structure information, which can be\nrepresented in structured data forms like trees or graphs. To acquire the\nstructural information in source code, most existing researches use abstract\nsyntax trees (AST). A group of works add additional edges to ASTs to convert\nsource code into graphs and use graph neural networks to learn representations\nfor program graphs. Although these works provide additional control or data\nflow information to ASTs for downstream tasks, they neglect an important aspect\nof structure information in AST itself: the different types of nodes and edges.\nIn ASTs, different nodes contain different kinds of information like variables\nor control flow, and the relation between a node and all its children can also\nbe different.\n</p>\n<p>To address the information of node and edge types, we bring the idea of\nheterogeneous graphs to learning on source code and present a new formula of\nbuilding heterogeneous program graphs from ASTs with additional type\ninformation for nodes and edges. We use the ASDL grammar of programming\nlanguage to define the node and edge types of program graphs. Then we use\nheterogeneous graph neural networks to learn on these graphs. We evaluate our\napproach on two tasks: code comment generation and method naming. Both tasks\nrequire reasoning on the semantics of complete code snippets. Experiment\nresults show that our approach outperforms baseline models, including\nhomogeneous graph-based models, showing that leveraging the type information of\nnodes and edges in program graphs can help in learning program semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kechi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huangzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection. (arXiv:2012.10289v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.10289","description":"<p>Hate speech is a challenging issue plaguing the online social media. While\nbetter models for hate speech detection are continuously being developed, there\nis little research on the bias and interpretability aspects of hate speech. In\nthis paper, we introduce HateXplain, the first benchmark hate speech dataset\ncovering multiple aspects of the issue. Each post in our dataset is annotated\nfrom three different perspectives: the basic, commonly used 3-class\nclassification (i.e., hate, offensive or normal), the target community (i.e.,\nthe community that has been the victim of hate speech/offensive speech in the\npost), and the rationales, i.e., the portions of the post on which their\nlabelling decision (as hate, offensive or normal) is based. We utilize existing\nstate-of-the-art models and observe that even models that perform very well in\nclassification do not score high on explainability metrics like model\nplausibility and faithfulness. We also observe that models, which utilize the\nhuman rationales for training, perform better in reducing unintended bias\ntowards target communities. We have made our code and dataset public at\nhttps://github.com/punyajoy/HateXplain\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01250","description":"<p>The popularity of pretrained language models in natural language processing\nsystems calls for a careful evaluation of such models in down-stream tasks,\nwhich have a higher potential for societal impact. The evaluation of such\nsystems usually focuses on \\textit{accuracy measures}. Our findings in this\npaper call for attention to be paid to \\textit{fairness measures} as well.\nThrough the analysis of more than a dozen pretrained language models of varying\nsizes on two toxic text classification tasks (English), we demonstrate that\nfocusing on accuracy measures alone can lead to models with wide variation in\nfairness characteristics. Specifically, we observe that fairness can vary even\nmore than accuracy with increasing training data size and different random\ninitializations. At the same time, we find that little of the fairness\nvariation is explained by model size, despite claims in the literature. To\nimprove model fairness without retraining, we show that two post-processing\nmethods developed for structured, tabular data can be successfully applied to a\nrange of pretrained language models. \\textit{Warning: This paper contains\nsamples of offensive text.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dennis Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. (arXiv:2109.03777v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03777","description":"<p>Graph neural networks have triggered a resurgence of graph-based text\nclassification methods, defining today's state of the art. We show that a wide\nmulti-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent\ngraph-based models TextGCN and HeteGCN in an inductive text classification\nsetting and is comparable with HyperGAT. Moreover, we fine-tune a\nsequence-based BERT and a lightweight DistilBERT model, which both outperform\nall state-of-the-art models. These results question the importance of synthetic\ngraphs used in modern text classifiers. In terms of efficiency, DistilBERT is\nstill twice as large as our BoW-based wide MLP, while graph-based models like\nTextGCN require setting up an $\\mathcal{O}(N^2)$ graph, where $N$ is the\nvocabulary plus corpus size. Finally, since Transformers need to compute\n$\\mathcal{O}(L^2)$ attention weights with sequence length $L$, the MLP models\nshow higher training and inference speeds on datasets with long sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STraTA: Self-Training with Task Augmentation for Better Few-shot Learning. (arXiv:2109.06270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06270","description":"<p>Despite their recent successes in tackling many NLP tasks, large-scale\npre-trained language models do not perform as well in few-shot settings where\nonly a handful of training examples are available. To address this shortcoming,\nwe propose STraTA, which stands for Self-Training with Task Augmentation, an\napproach that builds on two key ideas for effective leverage of unlabeled data.\nFirst, STraTA uses task augmentation, a novel technique that synthesizes a\nlarge amount of data for auxiliary-task fine-tuning from target-task unlabeled\ntexts. Second, STraTA performs self-training by further fine-tuning the strong\nbase model created by task augmentation on a broad distribution of\npseudo-labeled data. Our experiments demonstrate that STraTA can substantially\nimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on the\nSST-2 sentiment dataset, STraTA, with only 8 training examples per class,\nachieves comparable results to standard fine-tuning with 67K training examples.\nOur analyses reveal that task augmentation and self-training are both\ncomplementary and independently effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Grady Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning via Language Model In-context Tuning. (arXiv:2110.07814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07814","description":"<p>The goal of meta-learning is to learn to adapt to a new task with only a few\nlabeled examples. To tackle this problem in NLP, we propose $\\textit{in-context\ntuning}$, which recasts adaptation and prediction as a simple sequence\nprediction problem: to form the input sequence, we concatenate the task\ninstruction, the labeled examples, and the target input to predict; to\nmeta-train the model to learn from in-context examples, we fine-tune a\npre-trained language model (LM) to predict the target label from the input\nsequences on a collection of tasks.\n</p>\n<p>We benchmark our method on two collections of text classification tasks: LAMA\nand BinaryClfs. Compared to first-order MAML which adapts the model with\ngradient descent, our method better leverages the inductive bias of LMs to\nperform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROC\nscore on BinaryClfs, with increasing advantage w.r.t. model size. Compared to\nnon-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning\ndirectly learns to learn from in-context examples. On BinaryClfs, in-context\ntuning improves the average AUC-ROC score by an absolute $10\\%$, and reduces\nthe variance with respect to example ordering by 6x and example choices by 2x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech Recognition using Frequency Masking. (arXiv:2112.01821v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.01821","description":"<p>Automatic speech recognition (ASR) models are prevalent, particularly in\napplications for voice navigation and voice control of domestic appliances. The\ncomputational core of ASRs are deep neural networks (DNNs) that have been shown\nto be susceptible to adversarial perturbations; easily misused by attackers to\ngenerate malicious outputs. To help test the security and robustnesss of ASRS,\nwe propose techniques that generate blackbox (agnostic to the DNN), untargeted\nadversarial attacks that are portable across ASRs. This is in contrast to\nexisting work that focuses on whitebox targeted attacks that are time consuming\nand lack portability.\n</p>\n<p>Our techniques generate adversarial attacks that have no human audible\ndifference by manipulating the audio signal using a psychoacoustic model that\nmaintains the audio perturbations below the thresholds of human perception. We\nevaluate portability and effectiveness of our techniques using three popular\nASRs and two input audio datasets using the metrics - Word Error Rate (WER) of\noutput transcription, Similarity to original audio, attack Success Rate on\ndifferent ASRs and Detection score by a defense system. We found our\nadversarial attacks were portable across ASRs, not easily detected by a\nstate-of-the-art defense system, and had significant difference in output\ntranscriptions while sounding similar to original audio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1\">Ajitha Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenIE: Generative Information Extraction. (arXiv:2112.08340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08340","description":"<p>Structured and grounded representation of text is typically formalized by\nclosed information extraction, the problem of extracting an exhaustive set of\n(subject, relation, object) triplets that are consistent with a predefined set\nof entities and relations from a knowledge base schema. Most existing works are\npipelines prone to error accumulation, and all approaches are only applicable\nto unrealistically small numbers of entities and relations. We introduce GenIE\n(generative information extraction), the first end-to-end autoregressive\nformulation of closed information extraction. GenIE naturally exploits the\nlanguage knowledge from the pre-trained transformer by autoregressively\ngenerating relations and entities in textual form. Thanks to a new bi-level\nconstrained generation strategy, only triplets consistent with the predefined\nknowledge base schema are produced. Our experiments show that GenIE is\nstate-of-the-art on closed information extraction, generalizes from fewer\ntraining data points than baselines, and scales to a previously unmanageable\nnumber of entities and relations. With this work, closed information extraction\nbecomes practical in realistic scenarios, providing new opportunities for\ndownstream tasks. Finally, this work paves the way towards a unified end-to-end\napproach to the core tasks of information extraction. Code, data and models\navailable at https://github.com/epfl-dlab/GenIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey in Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06414","description":"<p>In recent years, it has been seen that deep neural networks are lacking\nrobustness and are likely to break in case of adversarial perturbations in\ninput data. Strong adversarial attacks are proposed by various authors for\ncomputer vision and Natural Language Processing (NLP). As a counter-effort,\nseveral defense mechanisms are also proposed to save these networks from\nfailing. In contrast with image data, generating adversarial attacks and\ndefending these models is not easy in NLP because of the discrete nature of the\ntext data. However, numerous methods for adversarial defense are proposed of\nlate, for different NLP tasks such as text classification, named entity\nrecognition, natural language inferencing, etc. These methods are not just used\nfor defending neural networks from adversarial attacks, but also used as a\nregularization mechanism during training, saving the model from overfitting.\nThe proposed survey is an attempt to review different methods proposed for\nadversarial defenses in NLP in the recent past by proposing a novel taxonomy.\nThis survey also highlights the fragility of the advanced deep neural networks\nin NLP and the challenges in defending them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M.Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering. (arXiv:2203.06942v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06942","description":"<p>To alleviate the data scarcity problem in training question answering\nsystems, recent works propose additional intermediate pre-training for dense\npassage retrieval (DPR). However, there still remains a large discrepancy\nbetween the provided upstream signals and the downstream question-passage\nrelevance, which leads to less improvement. To bridge this gap, we propose the\nHyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever\nwith the text relevance induced by hyperlink-based topology within Web\ndocuments. We demonstrate that the hyperlink-based structures of dual-link and\nco-mention can provide effective relevance signals for large-scale pre-training\nthat better facilitate downstream passage retrieval. We investigate the\neffectiveness of our approach across a wide range of open-domain QA datasets\nunder zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The\nexperiments show our HLP outperforms the BM25 by up to 7 points as well as\nother pre-training methods by more than 10 points in terms of top-20 retrieval\naccuracy under the zero-shot scenario. Furthermore, HLP significantly\noutperforms other pre-training methods under the other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1\">Ke Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Enrui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian approach to translators' reliability assessment. (arXiv:2203.07135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07135","description":"<p>Translation Quality Assessment (TQA) is a process conducted by human\ntranslators and is widely used, both for estimating the performance of\n(increasingly used) Machine Translation, and for finding an agreement between\ntranslation providers and their customers. While translation scholars are aware\nof the importance of having a reliable way to conduct the TQA process, it seems\nthat there is limited literature that tackles the issue of reliability with a\nquantitative approach. In this work, we consider the TQA as a complex process\nfrom the point of view of physics of complex systems and approach the\nreliability issue from the Bayesian paradigm. Using a dataset of translation\nquality evaluations (in the form of error annotations), produced entirely by\nthe Professional Translation Service Provider Translated SRL, we compare two\nBayesian models that parameterise the following features involved in the TQA\nprocess: the translation difficulty, the characteristics of the translators\ninvolved in producing the translation, and of those assessing its quality - the\nreviewers. We validate the models in an unsupervised setting and show that it\nis possible to get meaningful insights into translators even with just one\nreview per translation; subsequently, we extract information like translators'\nskills and reviewers' strictness, as well as their consistency in their\nrespective roles. Using this, we show that the reliability of reviewers cannot\nbe taken for granted even in the case of expert translators: a translator's\nexpertise can induce a cognitive bias when reviewing a translation produced by\nanother translator. The most expert translators, however, are characterised by\nthe highest level of consistency, both in translating and in assessing the\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miccheli_M/0/1/0/all/0/1\">Marco Miccheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leban_A/0/1/0/all/0/1\">Andrej Leban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tacchella_A/0/1/0/all/0/1\">Andrea Tacchella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaccaria_A/0/1/0/all/0/1\">Andrea Zaccaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzilli_D/0/1/0/all/0/1\">Dario Mazzilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bratieres_S/0/1/0/all/0/1\">S&#xe9;bastien Brati&#xe8;res</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender and Racial Stereotype Detection in Legal Opinion Word Embeddings. (arXiv:2203.13369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13369","description":"<p>Studies have shown that some Natural Language Processing (NLP) systems encode\nand replicate harmful biases with potential adverse ethical effects in our\nsociety. In this article, we propose an approach for identifying gender and\nracial stereotypes in word embeddings trained on judicial opinions from U.S.\ncase law. Embeddings containing stereotype information may cause harm when used\nby downstream systems for classification, information extraction, question\nanswering, or other machine learning systems used to build legal research\ntools. We first explain how previously proposed methods for identifying these\nbiases are not well suited for use with word embeddings trained on legal\nopinion text. We then propose a domain adapted method for identifying gender\nand racial biases in the legal domain. Our analyses using these methods suggest\nthat racial and gender biases are encoded into word embeddings trained on legal\nopinions. These biases are not mitigated by exclusion of historical data, and\nappear across multiple large topical areas of the law. Implications for\ndownstream systems that use legal opinion word embeddings and suggestions for\npotential mitigation strategies based on our observations are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matthews_S/0/1/0/all/0/1\">Sean Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudzina_J/0/1/0/all/0/1\">John Hudzina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepehr_D/0/1/0/all/0/1\">Dawn Sepehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning. (arXiv:2203.15526v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.15526","description":"<p>Automated Audio captioning (AAC) is a cross-modal task that generates natural\nlanguage to describe the content of input audio. Most prior works usually\nextract single-modality acoustic features and are therefore sub-optimal for the\ncross-modal decoding task. In this work, we propose a novel AAC system called\nCLIP-AAC to learn interactive cross-modality representation with both acoustic\nand textual information. Specifically, the proposed CLIP-AAC introduces an\naudio-head and a text-head in the pre-trained encoder to extract audio-text\ninformation. Furthermore, we also apply contrastive learning to narrow the\ndomain difference by learning the correspondence between the audio signal and\nits paired captions. Experimental results show that the proposed CLIP-AAC\napproach surpasses the best baseline by a significant margin on the Clotho\ndataset in terms of NLP evaluation metrics. The ablation study indicates that\nboth the pre-trained model and contrastive learning contribute to the\nperformance gain of the AAC model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Heqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaofeng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16027","description":"<p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and\nprovides performance lift by adapting unlabelled data to downstream task.\nUnfortunately, existing adaptations mainly involve deterministic rules that\ncannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze\nanswer extraction method used in TAPT that is extendable for adaptation on any\ncloze-style machine reading comprehension (MRC) downstream tasks. We experiment\non multiple-choice cloze-style MRC tasks, and show that Clozer performs\nsignificantly better compared to the oracle and state-of-the-art in escalating\nTAPT effectiveness in lifting model performance, and prove that Clozer is able\nto recognize the gold answers independently of any heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Su Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding. (arXiv:2204.01450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01450","description":"<p>Grounding temporal video segments described in natural language queries\neffectively and efficiently is a crucial capability needed in\nvision-and-language fields. In this paper, we deal with the fast video temporal\ngrounding (FVTG) task, aiming at localizing the target segment with high speed\nand favorable accuracy. Most existing approaches adopt elaborately designed\ncross-modal interaction modules to improve the grounding performance, which\nsuffer from the test-time bottleneck. Although several common space-based\nmethods enjoy the high-speed merit during inference, they can hardly capture\nthe comprehensive and explicit relations between visual and textual modalities.\nIn this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a\ncommonsense-aware cross-modal alignment (CCA) framework, which incorporates\ncommonsense-guided visual and text representations into a complementary common\nspace for fast video temporal grounding. Specifically, the commonsense concepts\nare explored and exploited by extracting the structural semantic information\nfrom a language corpus. Then, a commonsense-aware interaction module is\ndesigned to obtain bridged visual and text features by utilizing the learned\ncommonsense concepts. Finally, to maintain the original semantic information of\ntextual queries, a cross-modal complementary common space is optimized to\nobtain matching scores for performing FVTG. Extensive results on two\nchallenging benchmarks show that our CCA method performs favorably against\nstate-of-the-arts while running at high speed. Our code is available at\nhttps://github.com/ZiyueWu59/CCA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shucheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Panoptic, Instance and Semantic Relations: A Relational Context Encoder to Enhance Panoptic Segmentation. (arXiv:2204.05370v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05370","description":"<p>This paper presents a novel framework to integrate both semantic and instance\ncontexts for panoptic segmentation. In existing works, it is common to use a\nshared backbone to extract features for both things (countable classes such as\nvehicles) and stuff (uncountable classes such as roads). This, however, fails\nto capture the rich relations among them, which can be utilized to enhance\nvisual understanding and segmentation performance. To address this shortcoming,\nwe propose a novel Panoptic, Instance, and Semantic Relations (PISR) module to\nexploit such contexts. First, we generate panoptic encodings to summarize key\nfeatures of the semantic classes and predicted instances. A Panoptic Relational\nAttention (PRA) module is then applied to the encodings and the global feature\nmap from the backbone. It produces a feature map that captures 1) the relations\nacross semantic classes and instances and 2) the relations between these\npanoptic categories and spatial features. PISR also automatically learns to\nfocus on the more important instances, making it robust to the number of\ninstances used in the relational attention module. Moreover, PISR is a general\nmodule that can be applied to any existing panoptic segmentation architecture.\nThrough extensive evaluations on panoptic segmentation benchmarks like\nCityscapes, COCO, and ADE20K, we show that PISR attains considerable\nimprovements over existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyojin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Debasmit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrepalli_R/0/1/0/all/0/1\">Risheek Garrepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space. (arXiv:2204.05376v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05376","description":"<p>Despite the surge of deep learning in the past decade, some users are\nskeptical to deploy these models in practice due to their black-box nature.\nSpecifically, in the medical space where there are severe potential\nrepercussions, we need to develop methods to gain confidence in the models'\ndecisions. To this end, we propose a novel medical imaging generative\nadversarial framework, medXGAN (medical eXplanation GAN), to visually explain\nwhat a medical classifier focuses on in its binary predictions. By encoding\ndomain knowledge of medical images, we are able to disentangle anatomical\nstructure and pathology, leading to fine-grained visualization through latent\ninterpolation. Furthermore, we optimize the latent space such that\ninterpolation explains how the features contribute to the classifier's output.\nOur method outperforms baselines such as Gradient-Weighted Class Activation\nMapping (Grad-CAM) and Integrated Gradients in localization and explanatory\nability. Additionally, a combination of the medXGAN with Integrated Gradients\ncan yield explanations more robust to noise. The code is available at:\nhttps://github.com/avdravid/medXGAN_explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dravid_A/0/1/0/all/0/1\">Amil Dravid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiffers_F/0/1/0/all/0/1\">Florian Schiffers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05381","description":"<p>Self-supervised learning (SSL) has attracted much interest in remote sensing\nand earth observation due to its ability to learn task-agnostic representations\nwithout human annotation. While most of the existing SSL works in remote\nsensing utilize ConvNet backbones and focus on a single modality, we explore\nthe potential of vision transformers (ViTs) for joint SAR-optical\nrepresentation learning. Based on DINO, a state-of-the-art SSL algorithm that\ndistills knowledge from two augmented views of an input image, we combine SAR\nand optical imagery by concatenating all channels to a unified input.\nSubsequently, we randomly mask out channels of one modality as a data\naugmentation strategy. While training, the model gets fed optical-only,\nSAR-only, and SAR-optical image pairs learning both inner- and intra-modality\nrepresentations. Experimental results employing the BigEarthNet-MM dataset\ndemonstrate the benefits of both, the ViT backbones and the proposed multimodal\nSSL algorithm DINO-MM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Few-Shot Part Segmentation using Coarse Supervision. (arXiv:2204.05393v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05393","description":"<p>A significant bottleneck in training deep networks for part segmentation is\nthe cost of obtaining detailed annotations. We propose a framework to exploit\ncoarse labels such as figure-ground masks and keypoint locations that are\nreadily available for some categories to improve part segmentation models. A\nkey challenge is that these annotations were collected for different tasks and\nwith different labeling styles and cannot be readily mapped to the part labels.\nTo this end, we propose to jointly learn the dependencies between labeling\nstyles and the part segmentation model, allowing us to utilize supervision from\ndiverse labels. To evaluate our approach we develop a benchmark on the\nCaltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines\nbased on multi-task learning, semi-supervised learning, and competitive methods\nrelying on loss functions manually designed to exploit sparse-supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_O/0/1/0/all/0/1\">Oindrila Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zezhou Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Adversarial Explanations with Grad-CAM. (arXiv:2204.05427v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05427","description":"<p>Gradient-weighted Class Activation Mapping (Grad- CAM), is an example-based\nexplanation method that provides a gradient activation heat map as an\nexplanation for Convolution Neural Network (CNN) models. The drawback of this\nmethod is that it cannot be used to generalize CNN behaviour. In this paper, we\npresent a novel method that extends Grad-CAM from example-based explanations to\na method for explaining global model behaviour. This is achieved by introducing\ntwo new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in\nDissimilarity (VID), for model generalization. These metrics are computed by\ncomparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of\nthe Grad-CAM generated heatmap for samples from the original test set and\nsamples from the adversarial test set. For our experiment, we study adversarial\nattacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models\nsuch as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM).\nWe then compute the metrics MOD and VID for the automatic face recognition\n(AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the\nregion highlighted in the Grad-CAM heatmap, reflecting its participation to the\ndecision making, across all models under adversarial attacks. The proposed\nmethod can be used to understand adversarial attacks and explain the behaviour\nof black box CNN models for image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmay Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trehan_U/0/1/0/all/0/1\">Utkarsh Trehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallat_K/0/1/0/all/0/1\">Khawla Mallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugelay_J/0/1/0/all/0/1\">Jean-Luc Dugelay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Approach to Adversarial Robustness in Few-shot Image Classification. (arXiv:2204.05432v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05432","description":"<p>Few-shot image classification, where the goal is to generalize to tasks with\nlimited labeled data, has seen great progress over the years. However, the\nclassifiers are vulnerable to adversarial examples, posing a question regarding\ntheir generalization capabilities. Recent works have tried to combine\nmeta-learning approaches with adversarial training to improve the robustness of\nfew-shot classifiers. We show that a simple transfer-learning based approach\ncan be used to train adversarially robust few-shot classifiers. We also present\na method for novel classification task based on calibrating the centroid of the\nfew-shot category towards the base classes. We show that standard adversarial\ntraining on base categories along with calibrated centroid-based classifier in\nthe novel categories, outperforms or is on-par with state-of-the-art advanced\nmethods on standard benchmarks for few-shot learning. Our method is simple,\neasy to scale, and with little effort can lead to robust few-shot classifiers.\nCode is available here: \\url{https://github.com/UCDvision/Simple_few_shot.git}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_A/0/1/0/all/0/1\">Akshayvarun Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glass Segmentation with RGB-Thermal Image Pairs. (arXiv:2204.05453v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05453","description":"<p>This paper proposes a new glass segmentation method utilizing paired RGB and\nthermal images. Due to the large difference between the transmission property\nof visible light and that of the thermal energy through the glass where most\nglass is transparent to the visible light but opaque to thermal energy, glass\nregions of a scene are made more distinguishable with a pair of RGB and thermal\nimages than solely with an RGB image. To exploit such a unique property, we\npropose a neural network architecture that effectively combines an RGB-thermal\nimage pair with a new multi-modal fusion module based on attention, and\nintegrate CNN and transformer to extract local features and long-range\ndependencies, respectively. As well, we have collected a new dataset containing\n5551 RGB-thermal image pairs with ground-truth segmentation annotations. The\nqualitative and quantitative evaluations demonstrate the effectiveness of the\nproposed approach on fusing RGB and thermal data for glass segmentation. Our\ncode and data are available at\nhttps://github.com/Dong-Huo/RGB-T-Glass-Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Multimodal Transformers Robust to Missing Modality?. (arXiv:2204.05454v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05454","description":"<p>Multimodal data collected from the real world are often imperfect due to\nmissing modalities. Therefore multimodal models that are robust against\nmodal-incomplete data are highly preferred. Recently, Transformer models have\nshown great success in processing multimodal data. However, existing work has\nbeen limited to either architecture designs or pre-training strategies; whether\nTransformer models are naturally robust against missing-modal data has rarely\nbeen investigated. In this paper, we present the first-of-its-kind work to\ncomprehensively investigate the behavior of Transformers in the presence of\nmodal-incomplete data. Unsurprising, we find Transformer models are sensitive\nto missing modalities while different modal fusion strategies will\nsignificantly affect the robustness. What surprised us is that the optimal\nfusion strategy is dataset dependent even for the same Transformer model; there\ndoes not exist a universal strategy that works in general cases. Based on these\nfindings, we propose a principle method to improve the robustness of\nTransformer models by automatically searching for an optimal fusion strategy\nregarding input data. Experimental validations on three benchmarks support the\nsuperior performance of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengmeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Testuggine_D/0/1/0/all/0/1\">Davide Testuggine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-Of-Distribution Detection In Unsupervised Continual Learning. (arXiv:2204.05462v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05462","description":"<p>Unsupervised continual learning aims to learn new tasks incrementally without\nrequiring human annotations. However, most existing methods, especially those\ntargeted on image classification, only work in a simplified scenario by\nassuming all new data belong to new tasks, which is not realistic if the class\nlabels are not provided. Therefore, to perform unsupervised continual learning\nin real life applications, an out-of-distribution detector is required at\nbeginning to identify whether each new data corresponds to a new task or\nalready learned tasks, which still remains under-explored yet. In this work, we\nformulate the problem for Out-of-distribution Detection in Unsupervised\nContinual Learning (OOD-UCL) with the corresponding evaluation protocol. In\naddition, we propose a novel OOD detection method by correcting the output bias\nat first and then enhancing the output confidence for in-distribution data\nbased on task discriminativeness, which can be applied directly without\nmodifying the learning procedures and objectives of continual learning. Our\nmethod is evaluated on CIFAR-100 dataset by following the proposed evaluation\nprotocol and we show improved performance compared with existing OOD detection\nmethods under the unsupervised continual learning scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud. (arXiv:2204.05481v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05481","description":"<p>Place recognition or loop closure detection is one of the core components in\na full SLAM system. In this paper, aiming at strengthening the relevancy of\nlocal neighboring points and the contextual dependency among global points\nsimultaneously, we investigate the exploitation of transformer-based network\nfor feature extraction, and propose a Hierarchical Transformer for Place\nRecognition (HiTPR). The HiTPR consists of four major parts: point cell\ngeneration, short-range transformer (SRT), long-range transformer (LRT) and\nglobal descriptor aggregation. Specifically, the point cloud is initially\ndivided into a sequence of small cells by downsampling and nearest neighbors\nsearching. In the SRT, we extract the local feature for each point cell. While\nin the LRT, we build the global dependency among all of the point cells in the\nwhole point cloud. Experiments on several standard benchmarks demonstrate the\nsuperiority of the HiTPR in terms of average recall rate, achieving 93.71% at\ntop 1% and 86.63% at top 1 on the Oxford RobotCar dataset for example.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhixing Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hui Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Graph Matching for Modification Similarity Applied to Electronic Document Comparison. (arXiv:2204.05486v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05486","description":"<p>In this paper, we present a novel neural graph matching approach applied to\ndocument comparison. Document comparison is a common task in the legal and\nfinancial industries. In some cases, the most important differences may be the\naddition or omission of words, sentences, clauses, or paragraphs. However, it\nis a challenging task without recording or tracing whole edited process. Under\nmany temporal uncertainties, we explore the potentiality of our approach to\nproximate the accurate comparison to make sure which element blocks have a\nrelation of edition with others. In beginning, we apply a document layout\nanalysis that combining traditional and modern technics to segment layout in\nblocks of various types appropriately. Then we transform this issue to a\nproblem of layout graph matching with textual awareness. About graph matching,\nit is a long-studied problem with a broad range of applications. However,\ndifferent from previous works focusing on visual images or structural layout,\nwe also bring textual features into our model for adapting this domain.\nSpecifically, based on the electronic document, we introduce an encoder to deal\nwith the visual presentation decoding from PDF. Additionally, because the\nmodifications can cause the inconsistency of document layout analysis between\nmodified documents and the blocks can be merged and split, Sinkhorn divergence\nis adopted in our graph neural approach, which tries to overcome both these\nissues with many-to-many block matching. We demonstrate this on two categories\nof layouts, as follows., legal agreement and scientific articles, collected\nfrom our real-case datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-Fang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chiching Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning with Noisy Labels. (arXiv:2204.05494v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05494","description":"<p>Few-shot learning (FSL) methods typically assume clean support sets with\naccurately labeled samples when training on novel classes. This assumption can\noften be unrealistic: support sets, no matter how small, can still include\nmislabeled samples. Robustness to label noise is therefore essential for FSL\nmethods to be practical, but this problem surprisingly remains largely\nunexplored. To address mislabeled samples in FSL settings, we make several\ntechnical contributions. (1) We offer simple, yet effective, feature\naggregation methods, improving the prototypes used by ProtoNet, a popular FSL\ntechnique. (2) We describe a novel Transformer model for Noisy Few-Shot\nLearning (TraNFS). TraNFS leverages a transformer's attention mechanism to\nweigh mislabeled versus correct samples. (3) Finally, we extensively test these\nmethods on noisy versions of MiniImageNet and TieredImageNet. Our results show\nthat TraNFS is on-par with leading FSL methods on clean support sets, yet\noutperforms them, by far, in the presence of label noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kevin J Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangrej_S/0/1/0/all/0/1\">Samrudhdhi B. Rangrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_V/0/1/0/all/0/1\">Vladan Petrovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-aware Location Regression Network for Temporal Video Grounding. (arXiv:2204.05499v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05499","description":"<p>The key to successful grounding for video surveillance is to understand a\nsemantic phrase corresponding to important actors and objects. Conventional\nmethods ignore comprehensive contexts for the phrase or require heavy\ncomputation for multiple phrases. To understand comprehensive contexts with\nonly one semantic phrase, we propose Position-aware Location Regression Network\n(PLRN) which exploits position-aware features of a query and a video.\nSpecifically, PLRN first encodes both the video and query using positional\ninformation of words and video segments. Then, a semantic phrase feature is\nextracted from an encoded query with attention. The semantic phrase feature and\nencoded video are merged and made into a context-aware feature by reflecting\nlocal and global contexts. Finally, PLRN predicts start, end, center, and width\nvalues of a grounding boundary. Our experiments show that PLRN achieves\ncompetitive performance over existing methods with less computation time and\nmemory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunoh Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jin Young Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoupleFace: Relation Matters for Face Recognition Distillation. (arXiv:2204.05502v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05502","description":"<p>Knowledge distillation is an effective method to improve the performance of a\nlightweight neural network (i.e., student model) by transferring the knowledge\nof a well-performed neural network (i.e., teacher model), which has been widely\napplied in many computer vision tasks, including face recognition.\nNevertheless, the current face recognition distillation methods usually utilize\nthe Feature Consistency Distillation (FCD) (e.g., L2 distance) on the learned\nembeddings extracted by the teacher and student models for each sample, which\nis not able to fully transfer the knowledge from the teacher to the student for\nface recognition. In this work, we observe that mutual relation knowledge\nbetween samples is also important to improve the discriminative ability of the\nlearned representation of the student model, and propose an effective face\nrecognition distillation method called CoupleFace by additionally introducing\nthe Mutual Relation Distillation (MRD) into existing distillation framework.\nSpecifically, in MRD, we first propose to mine the informative mutual\nrelations, and then introduce the Relation-Aware Distillation (RAD) loss to\ntransfer the mutual relation knowledge of the teacher model to the student\nmodel. Extensive experimental results on multiple benchmark datasets\ndemonstrate the effectiveness of our proposed CoupleFace for face recognition.\nMoreover, based on our proposed CoupleFace, we have won the first place in the\nICCV21 Masked Face Recognition Challenge (MS1M track).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haoyu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSOINet: Feature-Space Optimization-Inspired Network for Image Compressive Sensing. (arXiv:2204.05503v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05503","description":"<p>In recent years, deep learning-based image compressive sensing (ICS) methods\nhave achieved brilliant success. Many optimization-inspired networks have been\nproposed to bring the insights of optimization algorithms into the network\nstructure design and have achieved excellent reconstruction quality with low\ncomputational complexity. But they keep the information flow in pixel space as\ntraditional algorithms by updating and transferring the image in pixel space,\nwhich does not fully use the information in the image features. In this paper,\nwe propose the idea of achieving information flow phase by phase in feature\nspace and design a Feature-Space Optimization-Inspired Network (dubbed FSOINet)\nto implement it by mapping both steps of proximal gradient descent algorithm\nfrom pixel space to feature space. Moreover, the sampling matrix is learned\nend-to-end with other network parameters. Experiments show that the proposed\nFSOINet outperforms the existing state-of-the-art methods by a large margin\nboth quantitatively and qualitatively. The source code is available on\nhttps://github.com/cwjjun/FSOINet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenjun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunling Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-Agent. (arXiv:2204.05513v1 [cs.RO])","link":"http://arxiv.org/abs/2204.05513","description":"<p>Focusing on the task of point-to-point navigation for an autonomous driving\nvehicle, we propose a novel deep learning model trained with end-to-end and\nmulti-task learning manners to perform both perception and control tasks\nsimultaneously. The model is used to drive the ego vehicle safely by following\na sequence of routes defined by the global planner. The perception part of the\nmodel is used to encode high-dimensional observation data provided by an RGBD\ncamera while performing semantic segmentation, semantic depth cloud (SDC)\nmapping, and traffic light state and stop sign prediction. Then, the control\npart decodes the encoded features along with additional information provided by\nGPS and speedometer to predict waypoints that come with a latent feature space.\nFurthermore, two agents are employed to process these outputs and make a\ncontrol policy that determines the level of steering, throttle, and brake as\nthe final action. The model is evaluated on CARLA simulator with various\nscenarios made of normal-adversarial situations and different weathers to mimic\nreal-world conditions. In addition, we do a comparative study with some recent\nmodels to justify the performance in multiple aspects of driving. Moreover, we\nalso conduct an ablation study on SDC mapping and multi-agent to understand\ntheir roles and behavior. As a result, our model achieves the highest driving\nscore even with fewer parameters and computation load. To support future\nstudies, we share our codes at\nhttps://github.com/oskarnatan/end-to-end-driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Natan_O/0/1/0/all/0/1\">Oskar Natan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1\">Jun Miura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation. (arXiv:2204.05525v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05525","description":"<p>Although vision transformers (ViTs) have achieved great success in computer\nvision, the heavy computational cost hampers their applications to dense\nprediction tasks such as semantic segmentation on mobile devices. In this\npaper, we present a mobile-friendly architecture named \\textbf{To}ken\n\\textbf{P}yramid Vision Trans\\textbf{former} (\\textbf{TopFormer}). The proposed\n\\textbf{TopFormer} takes Tokens from various scales as input to produce\nscale-aware semantic features, which are then injected into the corresponding\ntokens to augment the representation. Experimental results demonstrate that our\nmethod significantly outperforms CNN- and ViT-based networks across several\nsemantic segmentation datasets and achieves a good trade-off between accuracy\nand latency. On the ADE20K dataset, TopFormer achieves 5\\% higher accuracy in\nmIoU than MobileNetV3 with lower latency on an ARM-based mobile device.\nFurthermore, the tiny version of TopFormer achieves real-time inference on an\nARM-based mobile device with competitive results. The code and models are\navailable at: https://github.com/hustvl/TopFormer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guozhong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05532","description":"<p>While significant progress has been made in deep video denoising, it remains\nvery challenging for exploiting historical and future frames. Bidirectional\nrecurrent networks (BiRNN) have exhibited appealing performance in several\nvideo restoration tasks. However, BiRNN is intrinsically offline because it\nuses backward recurrent modules to propagate from the last to current frames,\nwhich causes high latency and large memory consumption. To address the offline\nissue of BiRNN, we present a novel recurrent network consisting of forward and\nlook-ahead recurrent modules for unidirectional video denoising. Particularly,\nlook-ahead module is an elaborate forward module for leveraging information\nfrom near-future frames. When denoising the current frame, the hidden features\nby forward and look-ahead recurrent modules are combined, thereby making it\nfeasible to exploit both historical and near-future frames. Due to the scene\nmotion between non-neighboring frames, border pixels missing may occur when\nwarping look-ahead feature from near-future frame to current frame, which can\nbe largely alleviated by incorporating forward warping and border enlargement.\nExperiments show that our method achieves state-of-the-art performance with\nconstant latency and memory consumption. The source code and pre-trained models\nwill be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-set Text Recognition via Character-Context Decoupling. (arXiv:2204.05535v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05535","description":"<p>The open-set text recognition task is an emerging challenge that requires an\nextra capability to cognize novel characters during evaluation. We argue that a\nmajor cause of the limited performance for current methods is the confounding\neffect of contextual information over the visual information of individual\ncharacters. Under open-set scenarios, the intractable bias in contextual\ninformation can be passed down to visual information, consequently impairing\nthe classification performance. In this paper, a Character-Context Decoupling\nframework is proposed to alleviate this problem by separating contextual\ninformation and character-visual information. Contextual information can be\ndecomposed into temporal information and linguistic information. Here, temporal\ninformation that models character order and word length is isolated with a\ndetached temporal attention module. Linguistic information that models n-gram\nand other linguistic statistics is separated with a decoupled context anchor\nmechanism. A variety of quantitative and qualitative experiments show that our\nmethod achieves promising performance on open-set, zero-shot, and close-set\ntext recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night. (arXiv:2204.05538v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05538","description":"<p>The semantic segmentation of nighttime scenes is a challenging problem that\nis key to impactful applications like self-driving cars. Yet, it has received\nlittle attention compared to its daytime counterpart. In this paper, we propose\nNightLab, a novel nighttime segmentation framework that leverages multiple deep\nlearning models imbued with night-aware features to yield State-of-The-Art\n(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab\ncontains models at two levels of granularity, i.e. image and regional, and each\nlevel is composed of light adaptation and segmentation modules. Given a\nnighttime image, the image level model provides an initial segmentation\nestimate while, in parallel, a hardness detection module identifies regions and\ntheir surrounding context that need further analysis. A regional level model\nfocuses on these difficult regions to provide a significantly improved\nsegmentation. All the models in NightLab are trained end-to-end using a set of\nproposed night-aware losses without handcrafted heuristics. Extensive\nexperiments on the NightCity and BDD100K datasets show NightLab achieves SoTA\nperformance compared to concurrent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xueqing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiaochen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content and Style Aware Generation of Text-line Images for Handwriting Recognition. (arXiv:2204.05539v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05539","description":"<p>Handwritten Text Recognition has achieved an impressive performance in public\nbenchmarks. However, due to the high inter- and intra-class variability between\nhandwriting styles, such recognizers need to be trained using huge volumes of\nmanually labeled training data. To alleviate this labor-consuming problem,\nsynthetic data produced with TrueType fonts has been often used in the training\nloop to gain volume and augment the handwriting style variability. However,\nthere is a significant style bias between synthetic and real data which hinders\nthe improvement of recognition performance. To deal with such limitations, we\npropose a generative method for handwritten text-line images, which is\nconditioned on both visual appearance and textual content. Our method is able\nto produce long text-line samples with diverse handwriting styles. Once\nproperly trained, our method can also be adapted to new target data by only\naccessing unlabeled text-line images to mimic handwritten styles and produce\nimages with any textual content. Extensive experiments have been done on making\nuse of the generated samples to boost Handwritten Text Recognition performance.\nBoth qualitative and quantitative results demonstrate that the proposed\napproach outperforms the current state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Lei Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1\">Pau Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinol_M/0/1/0/all/0/1\">Mar&#xe7;al Rusi&#xf1;ol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Mauricio Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reliable Image Outpainting: Learning Structure-Aware Multimodal Fusion with Depth Guidance. (arXiv:2204.05543v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05543","description":"<p>Image outpainting technology generates visually reasonable content regardless\nof authenticity, making it unreliable to serve for practical applications even\nthough introducing additional modalities eg. the sketch. Since sparse depth\nmaps are widely captured in robotics and autonomous systems, together with RGB\nimages, we combine the sparse depth in the image outpainting task to provide\nmore reliable performance. Concretely, we propose a Depth-Guided Outpainting\nNetwork (DGONet) to model the feature representations of different modalities\ndifferentially and learn the structure-aware cross-modal fusion. To this end,\ntwo components are designed to implement: 1) The Multimodal Learning Module\nproduces unique depth and RGB feature representations from the perspectives of\ndifferent modal characteristics. 2) The Depth Guidance Fusion Module leverages\nthe complete depth modality to guide the establishment of RGB contents by\nprogressive multimodal feature fusion. Furthermore, we specially design an\nadditional constraint strategy consisting of Cross-modal Loss and Edge Loss to\nenhance ambiguous contours and expedite reliable content generation. Extensive\nexperiments on KITTI demonstrate our superiority over the state-of-the-art\nmethods with more reliable content generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Undoing the Damage of Label Shift for Cross-domain Semantic Segmentation. (arXiv:2204.05546v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05546","description":"<p>Existing works typically treat cross-domain semantic segmentation (CDSS) as a\ndata distribution mismatch problem and focus on aligning the marginal\ndistribution or conditional distribution. However, the label shift issue is\nunfortunately overlooked, which actually commonly exists in the CDSS task, and\noften causes a classifier bias in the learnt model. In this paper, we give an\nin-depth analysis and show that the damage of label shift can be overcome by\naligning the data conditional distribution and correcting the posterior\nprobability. To this end, we propose a novel approach to undo the damage of the\nlabel shift problem in CDSS. In implementation, we adopt class-level feature\nalignment for conditional distribution alignment, as well as two simple yet\neffective methods to rectify the classifier bias from source to target by\nremolding the classifier predictions. We conduct extensive experiments on the\nbenchmark datasets of urban scenes, including GTA5 to Cityscapes and SYNTHIA to\nCityscapes, where our proposed approach outperforms previous methods by a large\nmargin. For instance, our model equipped with a self-training strategy reaches\n59.3% mIoU on GTA5 to Cityscapes, pushing to a new state-of-the-art. The code\nwill be available at https://github.com/manmanjun/Undoing UDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jiale Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1\">Tong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lixin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization. (arXiv:2204.05547v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05547","description":"<p>Recent Knowledge distillation (KD) studies show that different manually\ndesigned schemes impact the learned results significantly. Yet, in KD,\nautomatically searching an optimal distillation scheme has not yet been well\nexplored. In this paper, we propose DistPro, a novel framework which searches\nfor an optimal KD process via differentiable meta-learning. Specifically, given\na pair of student and teacher networks, DistPro first sets up a rich set of KD\nconnection from the transmitting layers of the teacher to the receiving layers\nof the student, and in the meanwhile, various transforms are also proposed for\ncomparing feature maps along its pathway for the distillation. Then, each\ncombination of a connection and a transform choice (pathway) is associated with\na stochastic weighting process which indicates its importance at every step\nduring the distillation. In the searching stage, the process can be effectively\nlearned through our proposed bi-level meta-optimization strategy. In the\ndistillation stage, DistPro adopts the learned processes for knowledge\ndistillation, which significantly improves the student accuracy especially when\nfaster training is required. Lastly, we find the learned processes can be\ngeneralized between similar tasks and networks. In our experiments, DistPro\nproduces state-of-the-art (SoTA) accuracy under varying number of learning\nepochs on popular datasets, i.e. CIFAR100 and ImageNet, which demonstrate the\neffectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xueqing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dawei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compact Model Training by Low-Rank Projection with Energy Transfer. (arXiv:2204.05566v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05566","description":"<p>Low-rankness plays an important role in traditional machine learning, but is\nnot so popular in deep learning. Most previous low-rank network compression\nmethods compress the networks by approximating pre-trained models and\nre-training. However, optimal solution in the Euclidean space may be quite\ndifferent from the one in the low-rank manifold. A well pre-trained model is\nnot a good initialization for the model with low-rank constraint. Thus, the\nperformance of low-rank compressed network degrades significantly. Compared to\nother network compression methods such as pruning, low-rank methods attracts\nless attention in recent years. In this paper, we devise a new training method,\nlow-rank projection with energy transfer (LRPET), that trains low-rank\ncompressed networks from scratch and achieves competitive performance. First,\nwe propose to alternately perform stochastic gradient descent training and\nprojection onto the low-rank manifold. This asymptotically approaches the\noptimal solution in the low-rank manifold. Compared to re-training on compact\nmodel, this enables fully utilization of model capacity since solution space is\nrelaxed back to Euclidean space after projection. Second, the matrix energy\n(the sum of squares of singular values) reduction caused by projection is\ncompensated by energy transfer. We uniformly transfer the energy of the pruned\nsingular values to the remaining ones. We theoretically show that energy\ntransfer eases the trend of gradient vanishing caused by projection.\nComprehensive experiment on CIFAR-10 and ImageNet have justified that our\nmethod is superior to other low-rank compression methods and also outperforms\nrecent state-of-the-art pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kailing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection. (arXiv:2204.05575v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05575","description":"<p>Autonomous driving faces great safety challenges for a lack of global\nperspective and the limitation of long-range perception capabilities. It has\nbeen widely agreed that vehicle-infrastructure cooperation is required to\nachieve Level 5 autonomy. However, there is still NO dataset from real\nscenarios available for computer vision researchers to work on\nvehicle-infrastructure cooperation-related problems. To accelerate computer\nvision research and innovation for Vehicle-Infrastructure Cooperative\nAutonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first\nlarge-scale, multi-modality, multi-view dataset from real scenarios for VICAD.\nDAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames\nare captured from real scenes with 3D annotations. The Vehicle-Infrastructure\nCooperative 3D Object Detection problem (VIC3D) is introduced, formulating the\nproblem of collaboratively locating and identifying 3D objects using sensory\ninputs from both vehicle and infrastructure. In addition to solving traditional\n3D object detection problems, the solution of VIC3D needs to consider the\ntemporal asynchrony problem between vehicle and infrastructure sensors and the\ndata transmission cost between them. Furthermore, we propose Time Compensation\nLate Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark\nbased on DAIR-V2X. Find data, code, and more up-to-date information at\nhttps://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haibao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yizhen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Mao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yiyi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zebang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenglong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jirui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zaiqing Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection. (arXiv:2204.05585v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05585","description":"<p>Convolutional neural networks (CNNs) are good at extracting contexture\nfeatures within certain receptive fields, while transformers can model the\nglobal long-range dependency features. By absorbing the advantage of\ntransformer and the merit of CNN, Swin Transformer shows strong feature\nrepresentation ability. Based on it, we propose a cross-modality fusion model\nSwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin\nTransformer to extract the hierarchical features, boosted by attention\nmechanism to bridge the gap between two modalities, and guided by edge\ninformation to sharp the contour of salient object. To be specific, two-stream\nSwin Transformer encoder first extracts multi-modality features, and then\nspatial alignment and channel re-calibration module is presented to optimize\nintra-level cross-modality features. To clarify the fuzzy boundary, edge-guided\ndecoder achieves inter-level cross-modality fusion under the guidance of edge\nfeatures. The proposed model outperforms the state-of-the-art models on RGB-D\nand RGB-T datasets, showing that it provides more insight into the\ncross-modality complementarity task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yacheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review. (arXiv:2204.05591v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05591","description":"<p>Glaucoma is a leading cause of irreversible vision impairment globally and\ncases are continuously rising worldwide. Early detection is crucial, allowing\ntimely intervention which can prevent further visual field loss. To detect\nglaucoma, examination of the optic nerve head via fundus imaging can be\nperformed, at the centre of which is the assessment of the optic cup and disc\nboundaries. Fundus imaging is non-invasive and low-cost; however, the image\nexamination relies on subjective, time-consuming, and costly expert\nassessments. A timely question to ask is can artificial intelligence mimic\nglaucoma assessments made by experts. Namely, can artificial intelligence\nautomatically find the boundaries of the optic cup and disc (providing a\nso-called segmented fundus image) and then use the segmented image to identify\nglaucoma with high accuracy. We conducted a comprehensive review on artificial\nintelligence-enabled glaucoma detection frameworks that produce and use\nsegmented fundus images. We found 28 papers and identified two main approaches:\n1) logical rule-based frameworks, based on a set of simplistic decision rules;\nand 2) machine learning/statistical modelling based frameworks. We summarise\nthe state-of-art of the two approaches and highlight the key hurdles to\novercome for artificial intelligence-enabled glaucoma detection frameworks to\nbe translated into clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coan_L/0/1/0/all/0/1\">Lauren Coan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_K/0/1/0/all/0/1\">Krishna Adithya Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyaya_S/0/1/0/all/0/1\">Swati Upadhyaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czanner_S/0/1/0/all/0/1\">Silvester Czanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1\">Rengaraj Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willoughby_C/0/1/0/all/0/1\">Colin E. Willoughby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavitha_S/0/1/0/all/0/1\">Srinivasan Kavitha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czanner_G/0/1/0/all/0/1\">Gabriela Czanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Equity of Nuclear Norm Maximization in Unsupervised Domain Adaptation. (arXiv:2204.05596v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05596","description":"<p>Nuclear norm maximization has shown the power to enhance the transferability\nof unsupervised domain adaptation model (UDA) in an empirical scheme. In this\npaper, we identify a new property termed equity, which indicates the balance\ndegree of predicted classes, to demystify the efficacy of nuclear norm\nmaximization for UDA theoretically. With this in mind, we offer a new\ndiscriminability-and-equity maximization paradigm built on squares loss, such\nthat predictions are equalized explicitly. To verify its feasibility and\nflexibility, two new losses termed Class Weighted Squares Maximization (CWSM)\nand Normalized Squares Maximization (NSM), are proposed to maximize both\npredictive discriminability and equity, from the class level and the sample\nlevel, respectively. Importantly, we theoretically relate these two novel\nlosses (i.e., CWSM and NSM) to the equity maximization under mild conditions,\nand empirically suggest the importance of the predictive equity in UDA.\nMoreover, it is very efficient to realize the equity constraints in both\nlosses. Experiments of cross-domain image classification on three popular\nbenchmark datasets show that both CWSM and NSM contribute to outperforming the\ncorresponding counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenju Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Long Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengzhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baoyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperDet3D: Learning a Scene-conditioned 3D Object Detector. (arXiv:2204.05599v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05599","description":"<p>A bathtub in a library, a sink in an office, a bed in a laundry room -- the\ncounter-intuition suggests that scene provides important prior knowledge for 3D\nobject detection, which instructs to eliminate the ambiguous detection of\nsimilar objects. In this paper, we propose HyperDet3D to explore\nscene-conditioned prior knowledge for 3D object detection. Existing methods\nstrive for better representation of local elements and their relations without\nscene-conditioned knowledge, which may cause ambiguity merely based on the\nunderstanding of individual points and object candidates. Instead, HyperDet3D\nsimultaneously learns scene-agnostic embeddings and scene-specific knowledge\nthrough scene-conditioned hypernetworks. More specifically, our HyperDet3D not\nonly explores the sharable abstracts from various 3D scenes, but also adapts\nthe detector to the given scene at test time. We propose a discriminative\nMulti-head Scene-specific Attention (MSA) module to dynamically control the\nlayer parameters of the detector conditioned on the fusion of scene-conditioned\nknowledge. Our HyperDet3D achieves state-of-the-art results on the 3D object\ndetection benchmark of the ScanNet and SUN RGB-D datasets. Moreover, through\ncross-dataset evaluation, we show the acquired scene-conditioned prior\nknowledge still takes effect when facing 3D scenes with domain gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open-Set Object Detection and Discovery. (arXiv:2204.05604v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05604","description":"<p>With the human pursuit of knowledge, open-set object detection (OSOD) has\nbeen designed to identify unknown objects in a dynamic world. However, an issue\nwith the current setting is that all the predicted unknown objects share the\nsame category as \"unknown\", which require incremental learning via a\nhuman-in-the-loop approach to label novel classes. In order to address this\nproblem, we present a new task, namely Open-Set Object Detection and Discovery\n(OSODD). This new task aims to extend the ability of open-set object detectors\nto further discover the categories of unknown objects based on their visual\nappearance without human effort. We propose a two-stage method that first uses\nan open-set object detector to predict both known and unknown objects. Then, we\nstudy the representation of predicted objects in an unsupervised manner and\ndiscover new categories from the set of unknown objects. With this method, a\ndetector is able to detect objects belonging to known classes and define novel\ncategories for objects of unknown classes with minimal supervision. We show the\nperformance of our model on the MS-COCO dataset under a thorough evaluation\nprotocol. We hope that our work will promote further research towards a more\nrobust real-world detection system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regression or Classification? Reflection on BP prediction from PPG data using Deep Neural Networks in the scope of practical applications. (arXiv:2204.05605v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05605","description":"<p>Photoplethysmographic (PPG) signals offer diagnostic potential beyond heart\nrate analysis or blood oxygen level monitoring. In the recent past, research\nfocused extensively on non-invasive PPG-based approaches to blood pressure (BP)\nestimation. These approaches can be subdivided into regression and\nclassification methods. The latter assign PPG signals to predefined BP\nintervals that represent clinically relevant ranges. The former predict\nsystolic (SBP) and diastolic (DBP) BP as continuous variables and are of\nparticular interest to the research community. However, the reported accuracies\nof BP regression methods vary widely among publications with some authors even\nquestioning the feasibility of PPG-based BP regression altogether. In our work,\nwe compare BP regression and classification approaches. We argue that BP\nclassification might provide diagnostic value that is equivalent to regression\nin many clinically relevant scenarios while being similar or even superior in\nterms of performance. We compare several established neural architectures using\npublicly available PPG data for SBP regression and classification with and\nwithout personalization using subject-specific data. We found that\nclassification and regression models perform similar before personalization.\nHowever, after personalization, the accuracy of classification based methods\noutperformed regression approaches. We conclude that BP classification might be\npreferable over BP regression in certain scenarios where a coarser segmentation\nof the BP range is sufficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schrumpf_F/0/1/0/all/0/1\">Fabian Schrumpf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serdack_P/0/1/0/all/0/1\">Paul Rudi Serdack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_M/0/1/0/all/0/1\">Mirco Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Predictive Learning from Videos. (arXiv:2204.05624v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05624","description":"<p>Predictive learning ideally builds the world model of physical processes in\none or more given environments. Typical setups assume that we can collect data\nfrom all environments at all times. In practice, however, different prediction\ntasks may arrive sequentially so that the environments may change persistently\nthroughout the training procedure. Can we develop predictive learning\nalgorithms that can deal with more realistic, non-stationary physical\nenvironments? In this paper, we study a new continual learning problem in the\ncontext of video prediction, and observe that most existing methods suffer from\nsevere catastrophic forgetting in this setup. To tackle this problem, we\npropose the continual predictive learning (CPL) approach, which learns a\nmixture world model via predictive experience replay and performs test-time\nadaptation with non-parametric task inference. We construct two new benchmarks\nbased on RoboNet and KTH, in which different tasks correspond to different\nphysical robotic environments or human actions. Our approach is shown to\neffectively mitigate forgetting and remarkably outperform the na\\\"ive\ncombinations of previous art in video prediction and continual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wendong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Siyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks. (arXiv:2204.05626v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05626","description":"<p>In this paper, we study the challenging instance-wise vision-language tasks,\nwhere the free-form language is required to align with the objects instead of\nthe whole image. To address these tasks, we propose X-DETR, whose architecture\nhas three major components: an object detector, a language encoder, and\nvision-language alignment. The vision and language streams are independent\nuntil the end and they are aligned using an efficient dot-product operation.\nThe whole network is trained end-to-end, such that the detector is optimized\nfor the vision-language tasks instead of an off-the-shelf component. To\novercome the limited size of paired object-language annotations, we leverage\nother weak types of supervision to expand the knowledge coverage. This simple\nyet effective architecture of X-DETR shows good accuracy and fast speeds for\nmultiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection\nof 1.2K categories at ~20 frames per second without using any LVIS annotation\nduring training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gukyeong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1\">Erhan Bas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Register a Live onto a Liver ? Partial Matching in the Space of Varifolds. (arXiv:2204.05665v1 [eess.IV])","link":"http://arxiv.org/abs/2204.05665","description":"<p>Partial shapes correspondences is a problem that often occurs in computer\nvision (occlusion, evolution in time...). In medical imaging, data may come\nfrom different modalities and be acquired under different conditions which\nleads to variations in shapes and topologies. In this paper we use an\nasymmetric data dissimilarity term applicable to various geometric shapes like\nsets of curves or surfaces, assessing the embedding of a shape into another one\nwithout relying on correspondences. It is designed as a data attachment for the\nLarge Deformation Diffeomorphic Metric Mapping (LDDMM) framework, allowing to\ncompute a meaningful deformation of one shape onto a subset of the other. We\nrefine it in order to control the resulting non-rigid deformations and provide\nconsistent deformations of the shapes along with their ambient space. We show\nthat partial matching can be used for robust multi-modal liver registration\nbetween a Computed Tomography (CT) volume and a Cone Beam Computed Tomography\n(CBCT) volume. The 3D imaging of the patient CBCT at point of care that we call\nlive is truncated while the CT pre-intervention provides a full visualization\nof the liver. The proposed method allows the truncated surfaces from CBCT to be\naligned non-rigidly, yet realistically, with surfaces from CT with an average\ndistance of 2.6mm(+/- 2.2). The generated deformations extend consistently to\nthe liver volume, and are evaluated on points of interest for the physicians,\nwith an average distance of 5.8mm (+/- 2.7) for vessels bifurcations and 5.13mm\n(+/- 2.5) for tumors landmarks. Such multi-modality volumes registrations would\nhelp the physicians in the perspective of navigating their tools in the\npatient's anatomy to locate structures that are hardly visible in the CBCT used\nduring their procedures. Our code is available at\nhttps://github.com/plantonsanti/PartialMatchingVarifolds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Antonsanti_P/0/1/0/all/0/1\">Pierre-Louis Antonsanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benseghir_T/0/1/0/all/0/1\">Thomas Benseghir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jugnon_V/0/1/0/all/0/1\">Vincent Jugnon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosn_M/0/1/0/all/0/1\">Mario Ghosn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chassat_P/0/1/0/all/0/1\">Perrine Chassat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaltenmark_I/0/1/0/all/0/1\">Ir&#xe8;ne Kaltenmark</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glaunes_J/0/1/0/all/0/1\">Joan Glaun&#xe8;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-Stream Joint Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2204.05666v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05666","description":"<p>The Zero-Shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging task\nbecause of the large domain gap between sketches and natural images as well as\nthe semantic inconsistency between seen and unseen categories. Previous\nliterature bridges seen and unseen categories by semantic embedding, which\nrequires prior knowledge of the exact class names and additional extraction\nefforts. And most works reduce domain gap by mapping sketches and natural\nimages into a common high-level space using constructed sketch-image pairs,\nwhich ignore the unpaired information between images and sketches. To address\nthese issues, in this paper, we propose a novel Three-Stream Joint Training\nNetwork (3JOIN) for the ZS-SBIR task. To narrow the domain differences between\nsketches and images, we extract edge maps for natural images and treat them as\na bridge between images and sketches, which have similar content to images and\nsimilar style to sketches. For exploiting a sufficient combination of sketches,\nnatural images, and edge maps, a novel three-stream joint training network is\nproposed. In addition, we use a teacher network to extract the implicit\nsemantics of the samples without the aid of other semantics and transfer the\nlearned knowledge to unseen classes. Extensive experiments conducted on two\nreal-world datasets demonstrate the superiority of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu-Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen-Duo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin-Shun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DeformRS: Certifying Spatial Deformations on Point Clouds. (arXiv:2204.05687v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05687","description":"<p>3D computer vision models are commonly used in security-critical applications\nsuch as autonomous driving and surgical robotics. Emerging concerns over the\nrobustness of these models against real-world deformations must be addressed\npractically and reliably. In this work, we propose 3DeformRS, a method to\ncertify the robustness of point cloud Deep Neural Networks (DNNs) against\nreal-world deformations. We developed 3DeformRS by building upon recent work\nthat generalized Randomized Smoothing (RS) from pixel-intensity perturbations\nto vector-field deformations. In particular, we specialized RS to certify DNNs\nagainst parameterized deformations (e.g. rotation, twisting), while enjoying\npractical computational costs. We leverage the virtues of 3DeformRS to conduct\na comprehensive empirical study on the certified robustness of four\nrepresentative point cloud DNNs on two datasets and against seven different\ndeformations. Compared to previous approaches for certifying point cloud DNNs,\n3DeformRS is fast, scales well with point cloud size, and provides\ncomparable-to-better certificates. For instance, when certifying a plain\nPointNet against a 3{\\deg} z-rotation on 1024-point clouds, 3DeformRS grants a\ncertificate 3x larger and 20x faster than previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S%2E_G/0/1/0/all/0/1\">Gabriel P&#xe9;rez S.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Resolution for Selfie Biometrics: Introduction and Application to Face and Iris. (arXiv:2204.05688v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05688","description":"<p>The lack of resolution has a negative impact on the performance of\nimage-based biometrics. Many applications which are becoming ubiquitous in\nmobile devices do not operate in a controlled environment, and their\nperformance significantly drops due to the lack of pixel resolution. While many\ngeneric super-resolution techniques have been studied to restore low-resolution\nimages for biometrics, the results obtained are not always as desired. Those\ngeneric methods are usually aimed to enhance the visual appearance of the\nscene. However, producing an overall visual enhancement of biometric images\ndoes not necessarily correlate with a better recognition performance. Such\ntechniques are designed to restore generic images and therefore do not exploit\nthe specific structure found in biometric images (e.g. iris or faces), which\ncauses the solution to be sub-optimal. For this reason, super-resolution\ntechniques have to be adapted for the particularities of images from a specific\nbiometric modality. In recent years, there has been an increased interest in\nthe application of super-resolution to different biometric modalities, such as\nface iris, gait or fingerprint. This chapter presents an overview of recent\nadvances in super-resolution reconstruction of face and iris images, which are\nthe two prevalent modalities in selfie biometrics. We also provide experimental\nresults using several state-of-the-art reconstruction algorithms, demonstrating\nthe benefits of using super-resolution to improve the quality of face and iris\nimages prior to classification. In the reported experiments, we study the\napplication of super-resolution to face and iris images captured in the visible\nrange, using experimental setups that represent well the selfie biometrics\nscenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrugia_R/0/1/0/all/0/1\">Reuben A. Farrugia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Anomaly and Change Detection with Multivariate Gaussianization. (arXiv:2204.05699v1 [cs.LG])","link":"http://arxiv.org/abs/2204.05699","description":"<p>Anomaly detection is a field of intense research. Identifying low probability\nevents in data/images is a challenging problem given the high-dimensionality of\nthe data, especially when no (or little) information about the anomaly is\navailable a priori. While plenty of methods are available, the vast majority of\nthem do not scale well to large datasets and require the choice of some (very\noften critical) hyperparameters. Therefore, unsupervised and computationally\nefficient detection methods become strictly necessary. We propose an\nunsupervised method for detecting anomalies and changes in remote sensing\nimages by means of a multivariate Gaussianization methodology that allows to\nestimate multivariate densities accurately, a long-standing problem in\nstatistics and machine learning. The methodology transforms arbitrarily complex\nmultivariate data into a multivariate Gaussian distribution. Since the\ntransformation is differentiable, by applying the change of variables formula\none can estimate the probability at any point of the original domain. The\nassumption is straightforward: pixels with low estimated probability are\nconsidered anomalies. Our method can describe any multivariate distribution,\nmakes an efficient use of memory and computational resources, and is\nparameter-free. We show the efficiency of the method in experiments involving\nboth anomaly detection and change detection in different remote sensing image\nsets. Results show that our approach outperforms other linear and nonlinear\nmethods in terms of detection power in both anomaly and change detection\nscenarios, showing robustness and scalability to dimensionality and sample\nsizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padron_Hidalgo_J/0/1/0/all/0/1\">Jos&#xe9; A. Padr&#xf3;n-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1\">Gustau Camps-Valls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Roots: Reconstructing Large and Complex Cranial Defects using an Image-based Statistical Shape Model. (arXiv:2204.05703v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05703","description":"<p>Designing implants for large and complex cranial defects is a challenging\ntask, even for professional designers. Current efforts on automating the design\nprocess focused mainly on convolutional neural networks (CNN), which have\nproduced state-of-the-art results on reconstructing synthetic defects. However,\nexisting CNN-based methods have been difficult to translate to clinical\npractice in cranioplasty, as their performance on complex and irregular cranial\ndefects remains unsatisfactory. In this paper, a statistical shape model (SSM)\nbuilt directly on the segmentation masks of the skulls is presented. We\nevaluate the SSM on several cranial implant design tasks, and the results show\nthat, while the SSM performs suboptimally on synthetic defects compared to\nCNN-based approaches, it is capable of reconstructing large and complex defects\nwith only minor manual corrections. The quality of the resulting implants is\nexamined and assured by experienced neurosurgeons. In contrast, CNN-based\napproaches, even with massive data augmentation, fail or produce\nless-than-satisfactory implants for these cases. Codes are publicly available\nat https://github.com/Jianningli/ssm\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1\">David G. Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1\">Michele R. Aizenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GARF: Gaussian Activated Radiance Fields for High Fidelity Reconstruction and Pose Estimation. (arXiv:2204.05735v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05735","description":"<p>Despite Neural Radiance Fields (NeRF) showing compelling results in\nphotorealistic novel views synthesis of real-world scenes, most existing\napproaches require accurate prior camera poses. Although approaches for jointly\nrecovering the radiance field and camera pose exist (BARF), they rely on a\ncumbersome coarse-to-fine auxiliary positional embedding to ensure good\nperformance. We present Gaussian Activated neural Radiance Fields (GARF), a new\npositional embedding-free neural radiance field architecture - employing\nGaussian activations - that outperforms the current state-of-the-art in terms\nof high fidelity reconstruction and pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chng_S/0/1/0/all/0/1\">Shin-Fang Chng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherrah_J/0/1/0/all/0/1\">Jamie Sherrah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LifeLonger: A Benchmark for Continual Disease Classification. (arXiv:2204.05737v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05737","description":"<p>Deep learning models have shown a great effectiveness in recognition of\nfindings in medical images. However, they cannot handle the ever-changing\nclinical environment, bringing newly annotated medical data from different\nsources. To exploit the incoming streams of data, these models would benefit\nlargely from sequentially learning from new samples, without forgetting the\npreviously obtained knowledge. In this paper we introduce LifeLonger, a\nbenchmark for continual disease classification on the MedMNIST collection, by\napplying existing state-of-the-art continual learning methods. In particular,\nwe consider three continual learning scenarios, namely, task and class\nincremental learning and the newly defined cross-domain incremental learning.\nTask and class incremental learning of diseases address the issue of\nclassifying new samples without re-training the models from scratch, while\ncross-domain incremental learning addresses the issue of dealing with datasets\noriginating from different institutions while retaining the previously obtained\nknowledge. We perform a thorough analysis of the performance and examine how\nthe well-known challenges of continual learning, such as the catastrophic\nforgetting exhibit themselves in this setting. The encouraging results\ndemonstrate that continual learning has a major potential to advance disease\nclassification and to produce a more robust and efficient learning framework\nfor clinical settings. The code repository, data partitions and baseline\nresults for the complete benchmark will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1\">Mohammad Mahdi Derakhshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonsbeek_T/0/1/0/all/0/1\">Tom van Sonsbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the Proximity of Adversarial Examples to Class Manifolds in Deep Networks. (arXiv:2204.05764v1 [cs.LG])","link":"http://arxiv.org/abs/2204.05764","description":"<p>Deep neural networks achieve remarkable performance in multiple fields.\nHowever, after proper training they suffer from an inherent vulnerability\nagainst adversarial examples (AEs). In this work we shed light on inner\nrepresentations of the AEs by analysing their activations on the hidden layers.\nWe test various types of AEs, each crafted using a specific norm constraint,\nwhich affects their visual appearance and eventually their behavior in the\ntrained networks. Our results in image classification tasks (MNIST and\nCIFAR-10) reveal qualitative differences between the individual types of AEs,\nwhen comparing their proximity to the class-specific manifolds on the inner\nrepresentations. We propose two methods that can be used to compare the\ndistances to class-specific manifolds, regardless of the changing dimensions\nthroughout the network. Using these methods, we consistently confirm that some\nof the adversarials do not necessarily leave the proximity of the manifold of\nthe correct class, not even in the last hidden layer of the neural network.\nNext, using UMAP visualisation technique, we project the class activations to\n2D space. The results indicate that the activations of the individual AEs are\nentangled with the activations of the test set. This, however, does not hold\nfor a group of crafted inputs called the rubbish class. We also confirm the\nentanglement of adversarials with the test set numerically using the soft\nnearest neighbour loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pocos_S/0/1/0/all/0/1\">&#x160;tefan P&#xf3;co&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckova_I/0/1/0/all/0/1\">Iveta Be&#x10d;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1\">Igor Farka&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GORDA: Graph-based ORientation Distribution Analysis of SLI scatterometry Patterns of Nerve Fibres. (arXiv:2204.05776v1 [eess.IV])","link":"http://arxiv.org/abs/2204.05776","description":"<p>Scattered Light Imaging (SLI) is a novel approach for microscopically\nrevealing the fibre architecture of unstained brain sections. The measurements\nare obtained by illuminating brain sections from different angles and measuring\nthe transmitted (scattered) light under normal incidence. The evaluation of\nscattering profiles commonly relies on a peak picking technique and feature\nextraction from the peaks, which allows quantitative determination of parallel\nand crossing in-plane nerve fibre directions for each image pixel. However, the\nestimation of the 3D orientation of the fibres cannot be assessed with the\ntraditional methodology. We propose an unsupervised learning approach using\nspherical convolutions for estimating the 3D orientation of neural fibres,\nresulting in a more detailed interpretation of the fibre orientation\ndistributions in the brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vaca_E/0/1/0/all/0/1\">Esteban Vaca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menzel_M/0/1/0/all/0/1\">Miriam Menzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amunts_K/0/1/0/all/0/1\">Katrin Amunts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Axer_M/0/1/0/all/0/1\">Markus Axer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dickscheid_T/0/1/0/all/0/1\">Timo Dickscheid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with impured training data. (arXiv:2204.05778v1 [eess.IV])","link":"http://arxiv.org/abs/2204.05778","description":"<p>The detection of lesions in magnetic resonance imaging (MRI)-scans of human\nbrains remains challenging, time-consuming and error-prone. Recently,\nunsupervised anomaly detection (UAD) methods have shown promising results for\nthis task. These methods rely on training data sets that solely contain healthy\nsamples. Compared to supervised approaches, this significantly reduces the need\nfor an extensive amount of labeled training data. However, data labelling\nremains error-prone. We study how unhealthy samples within the training data\naffect anomaly detection performance for brain MRI-scans. For our evaluations,\nwe consider three publicly available data sets and use autoencoders (AE) as a\nwell-established baseline method for UAD. We systematically evaluate the effect\nof impured training data by injecting different quantities of unhealthy samples\nto our training set of healthy samples from T1-weighted MRI-scans. We evaluate\na method to identify falsely labeled samples directly during training based on\nthe reconstruction error of the AE. Our results show that training with impured\ndata decreases the UAD performance notably even with few falsely labeled\nsamples. By performing outlier removal directly during training based on the\nreconstruction-loss, we demonstrate that falsely labeled data can be detected\nand removed to mitigate the effect of falsely labeled data. Overall, we\nhighlight the importance of clean data sets for UAD in brain MRI and\ndemonstrate an approach for detecting falsely labeled data directly during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1\">Finn Behrendt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1\">Marcel Bengs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rogge_F/0/1/0/all/0/1\">Frederik Rogge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kruger_J/0/1/0/all/0/1\">Julia Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Opfer_R/0/1/0/all/0/1\">Roland Opfer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Breast Cancer Classification via Hypercomplex Neural Networks. (arXiv:2204.05798v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05798","description":"<p>Traditionally, deep learning-based methods for breast cancer classification\nperform a single-view analysis. However, radiologists simultaneously analyze\nall four views that compose a mammography exam, owing to the correlations\ncontained in mammography views, which present crucial information for\nidentifying tumors. In light of this, some studies have started to propose\nmulti-view methods. Nevertheless, in such existing architectures, mammogram\nviews are processed as independent images by separate convolutional branches,\nthus losing correlations among them. To overcome such limitations, in this\npaper we propose a novel approach for multi-view breast cancer classification\nbased on parameterized hypercomplex neural networks. Thanks to hypercomplex\nalgebra properties, our networks are able to model, and thus leverage, existing\ncorrelations between the different views that comprise a mammogram exam, thus\nmimicking the reading process performed by clinicians. As a consequence, the\nproposed method is able to handle the information of a patient altogether\nwithout breaking the multi-view nature of the exam. Starting from the proposed\nhypercomplex approach, we define architectures designed to process two-view\nexams, namely PHResNets, and four-view exams, i.e., PHYSEnet and PHYSBOnet,\nwith the ability to grasp inter-view correlations in a wide range of clinical\nuse cases. Through an extensive experimental evaluation conducted with two\npublicly available datasets, CBIS-DDSM and INbreast, we demonstrate that our\nparameterized hypercomplex models clearly outperform real-valued counterparts\nand also state-of-the-art methods, proving that breast cancer classification\nbenefits from the proposed multi-view architecture. Full code and pretrained\nmodels for complete reproducibility of our experiments are freely available at:\nhttps://github.com/ispamm/PHBreast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_E/0/1/0/all/0/1\">Eleonora Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valleriani_M/0/1/0/all/0/1\">Martina Valleriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVOPS Benchmark: Evaluation of Plane Segmentation from RGBD and LiDAR Data. (arXiv:2204.05799v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05799","description":"<p>This paper provides the EVOPS dataset for plane segmentation from 3D data,\nboth from RGBD images and LiDAR point clouds (PC). We have designed two\nannotation methodologies (RGBD and LiDAR) running on well-known and widely-used\ndatasets and we have provided a complete set of benchmarking tools including\npoint, planes and segmentation metrics. The data includes a total number of 10k\nRGBD and 7K LiDAR frames over different selected scenes which consist of high\nquality segmented planes. The experiments report quality of SOTA methods for\nRGBD plane segmentation on our annotated data. All labeled data and benchmark\ntools used have been made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iarosh_D/0/1/0/all/0/1\">Dmitrii Iarosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukushkin_D/0/1/0/all/0/1\">Denis Kukushkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncharov_N/0/1/0/all/0/1\">Nikolai Goncharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokeev_P/0/1/0/all/0/1\">Pavel Mokeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1\">Arthur Saliou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework. (arXiv:2204.05819v1 [cs.CL])","link":"http://arxiv.org/abs/2204.05819","description":"<p>Entity recognition is a fundamental task in understanding document images.\nTraditional sequence labeling frameworks treat the entity types as class IDs\nand rely on extensive data and high-quality annotations to learn semantics\nwhich are typically expensive in practice. In this paper, we aim to build an\nentity recognition model requiring only a few shots of annotated document\nimages. To overcome the data limitation, we propose to leverage the label\nsurface names to better inform the model of the target entity type semantics\nand also embed the labels into the spatial embedding space to capture the\nspatial correspondence between regions and labels. Specifically, we go beyond\nsequence labeling and develop a novel label-aware seq2seq framework, LASER. The\nproposed model follows a new labeling scheme that generates the label surface\nnames word-by-word explicitly after generating the entities. During training,\nLASER refines the label semantics by updating the label surface name\nrepresentations and also strengthens the label-region correlation. In this way,\nLASER recognizes the entities from document images through both semantic and\nlayout correspondence. Extensive experiments on two benchmark datasets\ndemonstrate the superiority of LASER under the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Cross-Attention-Driven Spatial-Spectral Graph Convolutional Network for Hyperspectral Image Classification. (arXiv:2204.05823v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05823","description":"<p>Recently, graph convolutional networks (GCNs) have been developed to explore\nspatial relationship between pixels, achieving better classification\nperformance of hyperspectral images (HSIs). However, these methods fail to\nsufficiently leverage the relationship between spectral bands in HSI data. As\nsuch, we propose an adaptive cross-attention-driven spatial-spectral graph\nconvolutional network (ACSS-GCN), which is composed of a spatial GCN (Sa-GCN)\nsubnetwork, a spectral GCN (Se-GCN) subnetwork, and a graph cross-attention\nfusion module (GCAFM). Specifically, Sa-GCN and Se-GCN are proposed to extract\nthe spatial and spectral features by modeling correlations between spatial\npixels and between spectral bands, respectively. Then, by integrating attention\nmechanism into information aggregation of graph, the GCAFM, including three\nparts, i.e., spatial graph attention block, spectral graph attention block, and\nfusion block, is designed to fuse the spatial and spectral features and\nsuppress noise interference in Sa-GCN and Se-GCN. Moreover, the idea of the\nadaptive graph is introduced to explore an optimal graph through back\npropagation during the training process. Experiments on two HSI data sets show\nthat the proposed method achieves better performance than other classification\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jin-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wen-Shuai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Negative Replay for Continual Learning. (arXiv:2204.05842v1 [cs.LG])","link":"http://arxiv.org/abs/2204.05842","description":"<p>Learning continually is a key aspect of intelligence and a necessary ability\nto solve many real-life problems. One of the most effective strategies to\ncontrol catastrophic forgetting, the Achilles' heel of continual learning, is\nstoring part of the old data and replaying them interleaved with new\nexperiences (also known as the replay approach). Generative replay, which is\nusing generative models to provide replay patterns on demand, is particularly\nintriguing, however, it was shown to be effective mainly under simplified\nassumptions, such as simple scenarios and low-dimensional data. In this paper,\nwe show that, while the generated data are usually not able to improve the\nclassification accuracy for the old classes, they can be effective as negative\nexamples (or antagonists) to better learn the new classes, especially when the\nlearning experiences are small and contain examples of just one or few classes.\nThe proposed approach is validated on complex class-incremental and\ndata-incremental continual learning scenarios (CORe50 and ImageNet-1000)\ncomposed of high-dimensional data and a large number of training experiences: a\nsetup where existing generative replay approaches usually fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graffieti_G/0/1/0/all/0/1\">Gabriele Graffieti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1\">Davide Maltoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_L/0/1/0/all/0/1\">Lorenzo Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Compositional Embeddings for Multimodal Image Retrieval. (arXiv:2204.05845v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05845","description":"<p>Existing works in image retrieval often consider retrieving images with one\nor two query inputs, which do not generalize to multiple queries. In this work,\nwe investigate a more challenging scenario for composing multiple multimodal\nqueries in image retrieval. Given an arbitrary number of query images and (or)\ntexts, our goal is to retrieve target images containing the semantic concepts\nspecified in multiple multimodal queries. To learn an informative embedding\nthat can flexibly encode the semantics of various queries, we propose a novel\nmultimodal probabilistic composer (MPC). Specifically, we model input images\nand texts as probabilistic embeddings, which can be further composed by a\nprobabilistic composition rule to facilitate image retrieval with multiple\nmultimodal queries. We propose a new benchmark based on the MS-COCO dataset and\nevaluate our model on various setups that compose multiple images and (or) text\nqueries for multimodal image retrieval. Without bells and whistles, we show\nthat our probabilistic model formulation significantly outperforms existing\nrelated methods on multimodal image retrieval while generalizing well to query\nwith different amounts of inputs given in arbitrary visual and (or) textual\nmodalities. Code is available here: https://github.com/andreineculai/MPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neculai_A/0/1/0/all/0/1\">Andrei Neculai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCMS: Motion Forecasting with Dual Consistency and Multi-Pseudo-Target Supervision. (arXiv:2204.05859v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05859","description":"<p>We present a novel framework for motion forecasting with Dual Consistency\nConstraints and Multi-Pseudo-Target supervision. The motion forecasting task\npredicts future trajectories of vehicles by incorporating spatial and temporal\ninformation from the past. A key design of DCMS is the proposed Dual\nConsistency Constraints that regularize the predicted trajectories under\nspatial and temporal perturbation during the training stage. In addition, we\ndesign a novel self-ensembling scheme to obtain accurate pseudo targets to\nmodel the multi-modality in motion forecasting through supervision with\nmultiple targets explicitly, namely Multi-Pseudo-Target supervision. Our\nexperimental results on the Argoverse motion forecasting benchmark show that\nDCMS significantly outperforms the state-of-the-art methods, achieving 1st\nplace on the leaderboard. We also demonstrate that our proposed strategies can\nbe incorporated into other motion forecasting approaches as general training\nschemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Maosheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiamiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xunnong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tongyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic keypoint-based pose estimation from single RGB frames. (arXiv:2204.05864v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05864","description":"<p>This paper presents an approach to estimating the continuous 6-DoF pose of an\nobject from a single RGB image. The approach combines semantic keypoints\npredicted by a convolutional network (convnet) with a deformable shape model.\nUnlike prior investigators, we are agnostic to whether the object is textured\nor textureless, as the convnet learns the optimal representation from the\navailable training-image data. Furthermore, the approach can be applied to\ninstance- and class-based pose recovery. Additionally, we accompany our main\npipeline with a technique for semi-automatic data generation from unlabeled\nvideos. This procedure allows us to train the learnable components of our\nmethod with minimal manual intervention in the labeling process. Empirically,\nwe show that our approach can accurately recover the 6-DoF object pose for both\ninstance- and class-based scenarios even against a cluttered background. We\napply our approach both to several, existing, large-scale datasets - including\nPASCAL3D+, LineMOD-Occluded, YCB-Video, and TUD-Light - and, using our labeling\npipeline, to a new dataset with novel object classes that we introduce here.\nExtensive empirical evaluations show that our approach is able to provide pose\nestimation results comparable to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osteen_P/0/1/0/all/0/1\">Philip R. Osteen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1\">Georgios Pavlakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaney_K/0/1/0/all/0/1\">Kenneth Chaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_W/0/1/0/all/0/1\">Wyatt Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Event Camera-based Odometry for Planetary Robots. (arXiv:2204.05880v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05880","description":"<p>Due to their resilience to motion blur and high robustness in low-light and\nhigh dynamic range conditions, event cameras are poised to become enabling\nsensors for vision-based exploration on future Mars helicopter missions.\nHowever, existing event-based visual-inertial odometry (VIO) algorithms either\nsuffer from high tracking errors or are brittle, since they cannot cope with\nsignificant depth uncertainties caused by an unforeseen loss of tracking or\nother effects. In this work, we introduce EKLT-VIO, which addresses both\nlimitations by combining a state-of-the-art event-based frontend with a\nfilter-based backend. This makes it both accurate and robust to uncertainties,\noutperforming event- and frame-based VIO algorithms on challenging benchmarks\nby 32%. In addition, we demonstrate accurate performance in hover-like\nconditions (outperforming existing event-based methods) as well as high\nrobustness in newly collected Mars-like and high-dynamic-range sequences, where\nexisting frame-based methods fail. In doing so, we show that event-based VIO is\nthe way forward for vision-based exploration on Mars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahlknecht_F/0/1/0/all/0/1\">Florian Mahlknecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_J/0/1/0/all/0/1\">Jeremy Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockenbauer_F/0/1/0/all/0/1\">Friedrich M. Rockenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrell_B/0/1/0/all/0/1\">Benjamin Morrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaune_J/0/1/0/all/0/1\">Jeff Delaune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisCUIT: Visual Auditor for Bias in CNN Image Classifier. (arXiv:2204.05899v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05899","description":"<p>CNN image classifiers are widely used, thanks to their efficiency and\naccuracy. However, they can suffer from biases that impede their practical\napplications. Most existing bias investigation techniques are either\ninapplicable to general image classification tasks or require significant user\nefforts in perusing all data subgroups to manually specify which data\nattributes to inspect. We present VisCUIT, an interactive visualization system\nthat reveals how and why a CNN classifier is biased. VisCUIT visually\nsummarizes the subgroups on which the classifier underperforms and helps users\ndiscover and characterize the cause of the underperformances by revealing image\nconcepts responsible for activating neurons that contribute to\nmisclassifications. VisCUIT runs in modern browsers and is open-source,\nallowing people to easily access and extend the tool to other model\narchitectures and datasets. VisCUIT is available at the following public demo\nlink: https://poloclub.github.io/VisCUIT. A video demo is available at\nhttps://youtu.be/eNDbSyM4R_4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_D/0/1/0/all/0/1\">Duen Horng</a> (Polo) <a href=\"http://arxiv.org/find/cs/1/au:+Chau/0/1/0/all/0/1\">Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Distribution Learning for Generalizable Multi-source Person Re-identification. (arXiv:2204.05903v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05903","description":"<p>Person re-identification (Re-ID) is a critical technique in the video\nsurveillance system, which has achieved significant success in the supervised\nsetting. However, it is difficult to directly apply the supervised model to\narbitrary unseen domains due to the domain gap between the available source\ndomains and unseen target domains. In this paper, we propose a novel label\ndistribution learning (LDL) method to address the generalizable multi-source\nperson Re-ID task (i.e., there are multiple available source domains, and the\ntesting domain is unseen during training), which aims to explore the relation\nof different classes and mitigate the domain-shift across different domains so\nas to improve the discrimination of the model and learn the domain-invariant\nfeature, simultaneously. Specifically, during the training process, we produce\nthe label distribution via the online manner to mine the relation information\nof different classes, thus it is beneficial for extracting the discriminative\nfeature. Besides, for the label distribution of each class, we further revise\nit to give more and equal attention to the other domains that the class does\nnot belong to, which can effectively reduce the domain gap across different\ndomains and obtain the domain-invariant feature. Furthermore, we also give the\ntheoretical analysis to demonstrate that the proposed method can effectively\ndeal with the domain-shift issue. Extensive experiments on multiple benchmark\ndatasets validate the effectiveness of the proposed method and show that the\nproposed method can outperform the state-of-the-art methods. Besides, further\nanalysis also reveals the superiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Forgery Detection via Guided Adversarial Interpolation. (arXiv:2204.05905v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05905","description":"<p>Realistic visual media synthesis is becoming a critical societal issue with\nthe surge of face manipulation models; new forgery approaches emerge at an\nunprecedented pace. Unfortunately, existing forgery detection methods suffer\nsignificant performance drops when applied to novel forgery approaches. In this\nwork, we address the few-shot forgery detection problem by designing a\ncomprehensive benchmark based on coverage analysis among various forgery\napproaches, and proposing Guided Adversarial Interpolation (GAI). Our key\ninsight is that there exist transferable distribution characteristics among\ndifferent forgery approaches with the majority and minority classes.\nSpecifically, we enhance the discriminative ability against novel forgery\napproaches via adversarially interpolating the artifacts of the minority\nsamples to the majority samples under the guidance of a teacher network. Unlike\nthe standard re-balancing method which usually results in over-fitting to\nminority classes, our method simultaneously takes account of the diversity of\nmajority information as well as the significance of minority information.\nExtensive experiments demonstrate that our GAI achieves state-of-the-art\nperformances on the established few-shot forgery detection benchmark. Notably,\nour method is also validated to be robust to choices of majority and minority\nforgery approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haonan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1\">Bei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huafeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search. (arXiv:2204.05941v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05941","description":"<p>Neural Architecture Search (NAS) aims to find efficient models for multiple\ntasks. Beyond seeking solutions for a single task, there are surging interests\nin transferring network design knowledge across multiple tasks. In this line of\nresearch, effectively modeling task correlations is vital yet highly neglected.\nTherefore, we propose \\textbf{Arch-Graph}, a transferable NAS method that\npredicts task-specific optimal architectures with respect to given task\nembeddings. It leverages correlations across multiple tasks by using their\nembeddings as a part of the predictor's input for fast adaptation. We also\nformulate NAS as an architecture relation graph prediction problem, with the\nrelational graph constructed by treating candidate architectures as nodes and\ntheir pairwise relations as edges. To enforce some basic properties such as\nacyclicity in the relational graph, we add additional constraints to the\noptimization process, converting NAS into the problem of finding a Maximal\nWeighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate\ncycles and only establish edges in the graph if the rank results can be\ntrusted. Through MWAS, Arch-Graph can effectively rank candidate models for\neach task with only a small budget to finetune the predictor. With extensive\nexperiments on TransNAS-Bench-101, we show Arch-Graph's transferability and\nhigh sample efficiency across numerous tasks, beating many NAS methods designed\nfor both single-task and multi-task search. It is able to find top 0.16\\% and\n0.29\\% architectures on average on two search spaces under the budget of only\n50 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minbin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhijian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RL-CoSeg : A Novel Image Co-Segmentation Algorithm with Deep Reinforcement Learning. (arXiv:2204.05951v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05951","description":"<p>This paper proposes an automatic image co-segmentation algorithm based on\ndeep reinforcement learning (RL). Existing co-segmentation tasks mainly rely on\ndeep learning methods, and the obtained foreground edges are often rough. In\norder to obtain more precise foreground edges, we use deep RL to solve this\nproblem and achieve the finer segmentation. To our best knowledge, this is the\nfirst work to apply RL methods to co-segmentation. We define the problem as a\nMarkov Decision Process (MDP) and optimize it by RL with asynchronous advantage\nactor-critic (A3C). The RL image co-segmentation network uses the correlation\nbetween images to segment common and salient objects from a set of related\nimages. In order to achieve automatic segmentation, our RL-CoSeg method\neliminates user's hints. For the image co-segmentation problem, we propose a\ncollaborative RL algorithm based on the A3C model. We propose a Siamese RL\nco-segmentation network structure to obtain the co-attention of images for\nco-segmentation. We improve the self-attention for automatic RL algorithm to\nobtain long-distance dependence and enlarge the receptive field. The image\nfeature information obtained by self-attention can be used to supplement the\ndeleted user's hints and help to obtain more accurate actions. Experimental\nresults have shown that our method can improve the performance effectively on\nboth coarse and fine initial segmentations, and it achieves the\nstate-of-the-art performance on Internet dataset, iCoseg dataset and MLMR-COS\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiabi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaopeng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mengqiao Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization Distillation for Object Detection. (arXiv:2204.05957v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05957","description":"<p>Previous knowledge distillation (KD) methods for object detection mostly\nfocus on feature imitation instead of mimicking the classification logits due\nto its inefficiency in distilling the localization information. In this paper,\nwe investigate whether logit mimicking always lags behind feature imitation.\nTowards this goal, we first present a novel localization distillation (LD)\nmethod which can efficiently transfer the localization knowledge from the\nteacher to the student. Second, we introduce the concept of valuable\nlocalization region that can aid to selectively distill the classification and\nlocalization knowledge for a certain region. Combining these two new\ncomponents, for the first time, we show that logit mimicking can outperform\nfeature imitation and the absence of localization distillation is a critical\nreason for why logit mimicking underperforms for years. The thorough studies\nexhibit the great potential of logit mimicking that can significantly alleviate\nthe localization ambiguity, learn robust feature representation, and ease the\ntraining difficulty in the early stage. We also provide the theoretical\nconnection between the proposed LD and the classification KD, that they share\nthe equivalent optimization effect. Our distillation scheme is simple as well\nas effective and can be easily applied to both dense horizontal object\ndetectors and rotated object detectors. Extensive experiments on the MS COCO,\nPASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve\nconsiderable AP improvement without any sacrifice on the inference speed. Our\nsource code and pretrained models are publicly available at\nhttps://github.com/HikariTJU/LD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rongguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Captioning: a comparative review of where we are and which could be the route. (arXiv:2204.05976v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05976","description":"<p>Video captioning is the process of describing the content of a sequence of\nimages capturing its semantic relationships and meanings. Dealing with this\ntask with a single image is arduous, not to mention how difficult it is for a\nvideo (or images sequence). The amount and relevance of the applications of\nvideo captioning are vast, mainly to deal with a significant amount of video\nrecordings in video surveillance, or assisting people visually impaired, to\nmention a few. To analyze where the efforts of our community to solve the video\ncaptioning task are, as well as what route could be better to follow, this\nmanuscript presents an extensive review of more than 105 papers for the period\nof 2016 to 2021. As a result, the most-used datasets and metrics are\nidentified. Also, the main approaches used and the best ones. We compute a set\nof rankings based on several performance metrics to obtain, according to its\nperformance, the best method with the best result on the video captioning task.\nFinally, some insights are concluded about which could be the next steps or\nopportunity areas to improve dealing with this complex task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_delReal_T/0/1/0/all/0/1\">Tania Ram&#xed;rez-delReal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Chavez_O/0/1/0/all/0/1\">Oth&#xf3;n Gonz&#xe1;lez-Ch&#xe1;vez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical flow GNSS for navigation in the Indian subcontinent (NavIC). (arXiv:2204.05980v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05980","description":"<p>This paper reveals about global navigation satellite system GNSS in the\nindian subcontinent known as the navigation in the indian subcontinent(NavIC)\nWe have tried to model a new technique in GNSS known as the optical flow\ntracking global navigation system (OF GNSS). This method using differential\nequations is very accurate for very small distances on the surface of the earth\nin the 1500km range of the Indian subcontinent satellite coverage. When we talk\nof accuracy of the GPS system it should be very accurate on the surface of the\nearth when used to show changes in coordinate of the moving body with respect\nto the ground by the satellite which is situated on the earths orbit. Optical\nflow is a method which uses movements with respect to x and y axis for\ninfinitesimal changes in its coordinates and then uses this algorithm to use it\nin global positioning system to find accurate position of the body with respect\nto the satellite coordinates with respect to ground positioning. The modern\nmethod of differential frames is also very accurate as it involves\ninfinitesimal frames which are modelled together from the satellite to find\nchanges in the coordinates on the earths surface, so we have designed a new\nalgorithm in this paper on the Optical flow GNSS system which is an alternative\nand can improve the study done in the design of these algorithms in this field\nof applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fulari_S/0/1/0/all/0/1\">Sunit Shantanu Digamber Fulari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harbinder Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison Analysis of Traditional Machine Learning and Deep Learning Techniques for Data and Image Classification. (arXiv:2204.05983v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05983","description":"<p>The purpose of the study is to analyse and compare the most common machine\nlearning and deep learning techniques used for computer vision 2D object\nclassification tasks. Firstly, we will present the theoretical background of\nthe Bag of Visual words model and Deep Convolutional Neural Networks (DCNN).\nSecondly, we will implement a Bag of Visual Words model, the VGG16 CNN\nArchitecture. Thirdly, we will present our custom and novice DCNN in which we\ntest the aforementioned implementations on a modified version of the Belgium\nTraffic Sign dataset. Our results showcase the effects of hyperparameters on\ntraditional machine learning and the advantage in terms of accuracy of DCNNs\ncompared to classical machine learning methods. As our tests indicate, our\nproposed solution can achieve similar - and in some cases better - results than\nexisting DCNNs architectures. Finally, the technical merit of this article lies\nin the presented computationally simpler DCNN architecture, which we believe\ncan pave the way towards using more efficient architectures for basic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karypidis_E/0/1/0/all/0/1\">Efstathios Karypidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouslech_S/0/1/0/all/0/1\">Stylianos G. Mouslech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoulariki_K/0/1/0/all/0/1\">Kassiani Skoulariki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazis_A/0/1/0/all/0/1\">Alexandros Gazis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Security against Data Poisoning: Are We There Yet?. (arXiv:2204.05986v1 [cs.CR])","link":"http://arxiv.org/abs/2204.05986","description":"<p>The recent success of machine learning has been fueled by the increasing\navailability of computing power and large amounts of data in many different\napplications. However, the trustworthiness of the resulting models can be\ncompromised when such data is maliciously manipulated to mislead the learning\nprocess. In this article, we first review poisoning attacks that compromise the\ntraining data used to learn machine-learning models, including attacks that aim\nto reduce the overall performance, manipulate the predictions on specific test\nsamples, and even implant backdoors in the model. We then discuss how to\nmitigate these attacks before, during, and after model training. We conclude\nour article by formulating some relevant open challenges which are hindering\nthe development of testing methods and benchmarks suitable for assessing and\nimproving the trustworthiness of machine-learning models against data poisoning\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cina_A/0/1/0/all/0/1\">Antonio Emanuele Cin&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosse_K/0/1/0/all/0/1\">Kathrin Grosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelillo_M/0/1/0/all/0/1\">Marcello Pelillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v1 [cs.CV])","link":"http://arxiv.org/abs/2204.05991","description":"<p>Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sanjay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">Will Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Malceiver: Perceiver with Hierarchical and Multi-modal Features for Android Malware Detection. (arXiv:2204.05994v1 [cs.CR])","link":"http://arxiv.org/abs/2204.05994","description":"<p>We propose the Malceiver, a hierarchical Perceiver model for Android malware\ndetection that makes use of multi-modal features. The primary inputs are the\nopcode sequence and the requested permissions of a given Android APK file. To\nreach a malware classification decision the model combines hierarchical\nfeatures extracted from the opcode sequence together with the requested\npermissions. The model's architecture is based on the Perceiver/PerceiverIO\nwhich allows for very long opcode sequences to be processed efficiently. Our\nproposed model can be easily extended to use multi-modal features. We show\nexperimentally that this model outperforms a conventional CNN architecture for\nopcode sequence based malware detection. We then show that using additional\nmodalities improves performance. Our proposed architecture opens new avenues\nfor the use of Transformer-style networks in malware research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"They are Not Completely Useless: Towards Recycling Transferable Unlabeled Data for Class-Mismatched Semi-Supervised Learning. (arXiv:2011.13529v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.13529","description":"<p>Semi-Supervised Learning (SSL) with mismatched classes deals with the problem\nthat the classes-of-interests in the limited labeled data is only a subset of\nthe classes in massive unlabeled data. As a result, the classes only possessed\nby the unlabeled data may mislead the classifier training and thus hindering\nthe realistic landing of various SSL methods. To solve this problem, existing\nmethods usually divide unlabeled data to in-distribution (ID) data and\nout-of-distribution (OOD) data, and directly discard or weaken the OOD data to\navoid their adverse impact. In other words, they treat OOD data as completely\nuseless and thus the potential valuable information for classification\ncontained by them is totally ignored. To remedy this defect, this paper\nproposes a \"Transferable OOD data Recycling\" (TOOR) method which properly\nutilizes ID data as well as the \"recyclable\" OOD data to enrich the information\nfor conducting class-mismatched SSL. Specifically, TOOR firstly attributes all\nunlabeled data to ID data or OOD data, among which the ID data are directly\nused for training. Then we treat the OOD data that have a close relationship\nwith ID data and labeled data as recyclable, and employ adversarial domain\nadaptation to project them to the space of ID data and labeled data. In other\nwords, the recyclability of an OOD datum is evaluated by its transferability,\nand the recyclable OOD data are transferred so that they are compatible with\nthe distribution of known classes-of-interests. Consequently, our TOOR method\nextracts more information from unlabeled data than existing approaches, so it\ncan achieve the improved performance which is demonstrated by the experiments\non typical benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning. (arXiv:2102.03586v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03586","description":"<p>Spatiotemporal predictive learning (ST-PL) is a hotspot with numerous\napplications, such as object movement and meteorological prediction. It aims at\npredicting the subsequent frames via observed sequences. However, inherent\nuncertainty among consecutive frames exacerbates the difficulty in long-term\nprediction. To tackle the increasing ambiguity during forecasting, we design\nCMS-LSTM to focus on context correlations and multi-scale spatiotemporal flow\nwith details on fine-grained locals, containing two elaborate designed blocks:\nContext Embedding (CE) and Spatiotemporal Expression (SE) blocks. CE is\ndesigned for abundant context interactions, while SE focuses on multi-scale\nspatiotemporal expression in hidden states. The newly introduced blocks also\nfacilitate other spatiotemporal models (e.g., PredRNN, SA-ConvLSTM) to produce\nrepresentative implicit features for ST-PL and improve prediction quality.\nQualitative and quantitative experiments demonstrate the effectiveness and\nflexibility of our proposed method. With fewer params, CMS-LSTM outperforms\nstate-of-the-art methods in numbers of metrics on two representative benchmarks\nand scenarios. Code is available at https://github.com/czh-98/CMS-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunpeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhihui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Federated Peer Learning for Skin Lesion Classification. (arXiv:2103.03703v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.03703","description":"<p>Globally, Skin carcinoma is among the most lethal diseases. Millions of\npeople are diagnosed with this cancer every year. Sill, early detection can\ndecrease the medication cost and mortality rate substantially. The recent\nimprovement in automated cancer classification using deep learning methods has\nreached a human-level performance requiring a large amount of annotated data\nassembled in one location, yet, finding such conditions usually is not\nfeasible. Recently, federated learning (FL) has been proposed to train\ndecentralized models in a privacy-preserved fashion depending on labeled data\nat the client-side, which is usually not available and costly. To address this,\nwe propose \\verb!FedPerl!, a semi-supervised federated learning method. Our\nmethod is inspired by peer learning from educational psychology and ensemble\naveraging from committee machines. FedPerl builds communities based on clients'\nsimilarities. Then it encourages communities members to learn from each other\nto generate more accurate pseudo labels for the unlabeled data. We also\nproposed the peer anonymization (PA) technique to anonymize clients. As a core\ncomponent of our method, PA is orthogonal to other methods without additional\ncomplexity and reduces the communication cost while enhancing performance.\nFinally, we propose a dynamic peer-learning policy that controls the learning\nstream to avoid any degradation in the performance, especially for individual\nclients. Our experimental setup consists of 71,000 skin lesion images collected\nfrom 5 publicly available datasets. We test our method in four different\nscenarios in SSFL. With few annotated data, FedPerl is on par with a\nstate-of-the-art method in skin lesion classification in the standard setup\nwhile outperforming SSFLs and the baselines by 1.8% and 15.8%, respectively.\nAlso, it generalizes better to unseen clients while being less sensitive to\nnoisy ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bdair_T/0/1/0/all/0/1\">Tariq Bdair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Staircase Sign Method for Boosting Adversarial Attacks. (arXiv:2104.09722v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09722","description":"<p>Crafting adversarial examples for the transfer-based attack is challenging\nand remains a research hot spot. Currently, such attack methods are based on\nthe hypothesis that the substitute model and the victim model learn similar\ndecision boundaries, and they conventionally apply Sign Method (SM) to\nmanipulate the gradient as the resultant perturbation. Although SM is\nefficient, it only extracts the sign of gradient units but ignores their value\ndifference, which inevitably leads to a deviation. Therefore, we propose a\nnovel Staircase Sign Method (S$^2$M) to alleviate this issue, thus boosting\nattacks. Technically, our method heuristically divides the gradient sign into\nseveral segments according to the values of the gradient units, and then\nassigns each segment with a staircase weight for better crafting adversarial\nperturbation. As a result, our adversarial examples perform better in both\nwhite-box and black-box manner without being more visible. Since S$^2$M just\nmanipulates the resultant gradient, our method can be generally integrated into\nthe family of FGSM algorithms, and the computational overhead is negligible.\nExtensive experiments on the ImageNet dataset demonstrate the effectiveness of\nour proposed methods, which significantly improve the transferability (i.e., on\naverage, \\textbf{5.1\\%} for normally trained models and \\textbf{12.8\\%} for\nadversarially trained defenses). Our code is available at\n\\url{https://github.com/qilong-zhang/Staircase-sign-method}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaosu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08543","description":"<p>In this work, we seek new insights into the underlying challenges of the\nScene Graph Generation (SGG) task. Quantitative and qualitative analysis of the\nVisual Genome dataset implies -- 1) Ambiguity: even if inter-object\nrelationship contains the same object (or predicate), they may not be visually\nor semantically similar, 2) Asymmetry: despite the nature of the relationship\nthat embodied the direction, it was not well addressed in previous studies, and\n3) Higher-order contexts: leveraging the identities of certain graph elements\ncan help to generate accurate scene graphs. Motivated by the analysis, we\ndesign a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).\nLocally, interactions extract the essence between three instances of subject,\nobject, and background, while baking direction awareness into the network by\nexplicitly constraining the input order of subject and object. Globally,\ninteractions encode the contexts between every graph component (i.e., nodes and\nedges). Finally, Attract &amp; Repel loss is utilized to fine-tune the distribution\nof predicate embeddings. By design, our framework enables predicting the scene\ngraph in a bottom-up manner, leveraging the possible complementariness. To\nquantify how much LOGIN is aware of relational direction, a new diagnostic task\ncalled Bidirectional Relationship Classification (BRC) is also proposed.\nExperimental results demonstrate that LOGIN can successfully distinguish\nrelational direction than existing methods (in BRC task), while showing\nstate-of-the-art results on the Visual Genome benchmark (in SGG task).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Optimal Source Task Performance Imply Optimal Pre-training for a Target Task?. (arXiv:2106.11174v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11174","description":"<p>Fine-tuning of pre-trained deep nets is commonly used to improve accuracies\nand training times for neural nets. It is generally assumed that pre-training a\nnet for optimal source task performance best prepares it for fine-tuning to\nlearn an arbitrary target task. This is generally not true. Stopping source\ntask training, prior to optimal performance, can create a pre-trained net\nbetter suited for fine-tuning to learn a new task. We perform several\nexperiments demonstrating this effect, as well as the influence of the amount\nof training and of learning rate. Additionally, our results indicate that this\nreflects a general loss of learning ability that even extends to relearning the\nsource task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutstein_S/0/1/0/all/0/1\">Steven Gutstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lance_B/0/1/0/all/0/1\">Brent Lance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unsupervised Domain Generalization. (arXiv:2107.06219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06219","description":"<p>Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which are usually costly or\nunavailable, however. Since unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalize across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization (UDG), which aims to learn generalizable\nmodels with unlabeled data and analyze the effects of pre-training on DG. In\nUDG, models are pretrained with unlabeled data from various source domains\nbefore being trained on labeled source data and eventually tested on unseen\ntarget domains. Then we propose a method named Domain-Aware Representation\nLearnING (DARLING) to cope with the significant and misleading heterogeneity\nwithin unlabeled pretraining data and severe distribution shifts between source\nand target data. Surprisingly we observe that DARLING can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are insufficient. As a\npretraining approach, DARLING shows superior or comparable performance compared\nwith ImageNet pretraining protocol even when the available data are unlabeled\nand of a vastly smaller amount compared to ImageNet, which may shed light on\nimproving generalization with large-scale unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic cerebral hemisphere segmentation in rat MRI with lesions via attention-based convolutional neural networks. (arXiv:2108.01941v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.01941","description":"<p>We present MedicDeepLabv3+, a convolutional neural network that is the first\ncompletely automatic method to segment cerebral hemispheres in magnetic\nresonance (MR) volumes of rats with lesions. MedicDeepLabv3+ improves the\nstate-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial\nattention layers and additional skip connections that, as we show in our\nexperiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR\nimage preprocessing, such as bias-field correction or registration to a\ntemplate, produces segmentations in less than a second, and its GPU memory\nrequirements can be adjusted based on the available resources. We optimized\nMedicDeepLabv3+ and six other state-of-the-art convolutional neural networks\n(DeepLabv3+, UNet, HighRes3DNet, V-Net, VoxResNet, Demon) on a heterogeneous\ntraining set comprised by MR volumes from 11 cohorts acquired at different\nlesion stages. Then, we evaluated the trained models and two approaches\nspecifically designed for rodent MRI skull stripping (RATS and RBET) on a large\ndataset of 655 MR rat brain volumes. In our experiments, MedicDeepLabv3+\noutperformed the other methods, yielding an average Dice coefficient of 0.952\nand 0.944 in the brain and contralateral hemisphere regions. Additionally, we\nshow that despite limiting the GPU memory and the training data, our\nMedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our\nmethod, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus,\nyielded excellent results in multiple scenarios, demonstrating its capability\nto reduce human workload in rat neuroimaging studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1\">Juan Miguel Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shatillo_A/0/1/0/all/0/1\">Artem Shatillo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feo_R/0/1/0/all/0/1\">Riccardo de Feo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Less to More: Spectral Splitting and Aggregation Network for Hyperspectral Face Super-Resolution. (arXiv:2108.13584v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13584","description":"<p>High-resolution (HR) hyperspectral face image plays an important role in face\nrelated computer vision tasks under uncontrolled conditions, such as low-light\nenvironment and spoofing attacks. However, the dense spectral bands of\nhyperspectral face images come at the cost of limited amount of photons reached\na narrow spectral window on average, which greatly reduces the spatial\nresolution of hyperspectral face images. In this paper, we investigate how to\nadapt the deep learning techniques to hyperspectral face image super-resolution\n(HFSR), especially when the training samples are very limited. Benefiting from\nthe amount of spectral bands, in which each band can be seen as an image, we\npresent a spectral splitting and aggregation network (SSANet) for HFSR with\nlimited training samples. In the shallow layers, we split the hyperspectral\nimage into different spectral groups. Then, we gradually aggregate the neighbor\nbands at deeper layers to exploit spectral correlations. By this spectral\nsplitting and aggregation strategy (SSAS), we can divide the original\nhyperspectral image into multiple samples (\\emph{from less to more}) to support\nthe efficient training of the network and effectively exploit the spectral\ncorrelations among spectrum. To cope with the challenge of small training\nsample size (S3) problem, we propose to expand the training samples by a\nself-representation model and symmetry-induced augmentation. Experiments show\nthat SSANet can well model the joint correlations of spatial and spectral\ninformation. By expanding the training samples, SSANet can effectively\nalleviate the S3 problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.13748","description":"<p>Neural networks, in particular autoencoders, are one of the most promising\nsolutions for unmixing hyperspectral data, i.e. reconstructing the spectra of\nobserved substances (endmembers) and their relative mixing fractions\n(abundances), which is needed for effective hyperspectral analysis and\nclassification. However, as we show in this paper, the training of autoencoders\nfor unmixing is highly dependent on weights initialisation; some sets of\nweights lead to degenerate or low-performance solutions, introducing negative\nbias in the expected performance. In this work, we experimentally investigate\nautoencoders stability as well as network reinitialisation methods based on\ncoefficients of neurons' dead activations. We demonstrate that the proposed\ntechniques have a positive effect on autoencoder training in terms of\nreconstruction, abundances and endmembers errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ksiazek_K/0/1/0/all/0/1\">Kamil Ksi&#x105;&#x17c;ek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glomb_P/0/1/0/all/0/1\">Przemys&#x142;aw G&#x142;omb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Romaszewski_M/0/1/0/all/0/1\">Micha&#x142; Romaszewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cholewa_M/0/1/0/all/0/1\">Micha&#x142; Cholewa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grabowski_B/0/1/0/all/0/1\">Bartosz Grabowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buza_K/0/1/0/all/0/1\">Kriszti&#xe1;n B&#xfa;za</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Colored Point Cloud to Image Alignment. (arXiv:2110.03249v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03249","description":"<p>Reconstruction of geometric structures from images using supervised learning\nsuffers from limited available amount of accurate data. One type of such data\nis accurate real-world RGB-D images. A major challenge in acquiring such ground\ntruth data is the accurate alignment between RGB images and the point cloud\nmeasured by a depth scanner. To overcome this difficulty, we consider a\ndifferential optimization method that aligns a colored point cloud with a given\ncolor image through iterative geometric and color matching. In the proposed\nframework, the optimization minimizes the photometric difference between the\ncolors of the point cloud and the corresponding colors of the image pixels.\nUnlike other methods that try to reduce this photometric error, we analyze the\ncomputation of the gradient on the image plane and propose a different direct\nscheme. We assume that the colors produced by the geometric scanner camera and\nthe color camera sensor are different and therefore characterized by different\nchromatic acquisition properties. Under these multimodal conditions, we find\nthe transformation between the camera image and the point cloud colors. We\nalternately optimize for aligning the position of the point cloud and matching\nthe different color spaces. The alignments produced by the proposed method are\ndemonstrated on both synthetic data with quantitative evaluation and real\nscenes with qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotstein_N/0/1/0/all/0/1\">Noam Rotstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracha_A/0/1/0/all/0/1\">Amit Bracha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field Extraction from Forms with Unlabeled Data. (arXiv:2110.04282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04282","description":"<p>We propose a novel framework to conduct field extraction from forms with\nunlabeled data. To bootstrap the training process, we develop a rule-based\nmethod for mining noisy pseudo-labels from unlabeled forms. Using the\nsupervisory signal from the pseudo-labels, we extract a discriminative token\nrepresentation from a transformer-based model by modeling the interaction\nbetween text in the form. To prevent the model from overfitting to label noise,\nwe introduce a refinement module based on a progressive pseudo-label ensemble.\nExperimental results demonstrate the effectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10502","description":"<p>In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline that splits the joint task into independent stages, or fuse\n2D and 3D information in an \"early-fusion\" or \"late-fusion\" manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, called CamLiFlow. It consists of 2D and 3D branches with multiple\nbidirectional connections between them in specific layers. Different from\nprevious work, we apply a point-based 3D branch to better extract the geometric\nfeatures and design a symmetric learnable operator to fuse dense image features\nand sparse point features. Experiments show that CamLiFlow achieves better\nperformance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow\nbenchmark, outperforming the previous art with 1/7 parameters. Code is\navailable at https://github.com/MCG-NJU/CamLiFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDAM: Top-Down Attention Module for Contextually Guided Feature Selection in CNNs. (arXiv:2111.13470v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13470","description":"<p>Attention modules for Convolutional Neural Networks (CNNs) are an effective\nmethod to enhance performance on multiple computer-vision tasks. While existing\nmethods appropriately model channel-, spatial- and self-attention, they\nprimarily operate in a feedforward bottom-up manner. Consequently, the\nattention mechanism strongly depends on the local information of a single input\nfeature map and does not incorporate relatively semantically-richer contextual\ninformation available at higher layers that can specify \"what and where to\nlook\" in lower-level feature maps through top-down information flow.\n</p>\n<p>Accordingly, in this work, we propose a lightweight top-down attention module\n(TDAM) that iteratively generates a \"visual searchlight\" to perform channel and\nspatial modulation of its inputs and outputs more contextually-relevant feature\nmaps at each computation step. Our experiments indicate that TDAM enhances the\nperformance of CNNs across multiple object-recognition benchmarks and\noutperforms prominent attention modules while being more parameter and memory\nefficient. Further, TDAM-based models learn to \"shift attention\" by localizing\nindividual objects or features at each computation step without any explicit\nsupervision resulting in a 5% improvement for ResNet50 on weakly-supervised\nobject localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1\">Shantanu Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Gating for Single-Photon 3D Imaging. (arXiv:2111.15047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15047","description":"<p>Single-photon avalanche diodes (SPADs) are growing in popularity for depth\nsensing tasks. However, SPADs still struggle in the presence of high ambient\nlight due to the effects of pile-up. Conventional techniques leverage fixed or\nasynchronous gating to minimize pile-up effects, but these gating schemes are\nall non-adaptive, as they are unable to incorporate factors such as scene\npriors and previous photon detections into their gating strategy. We propose an\nadaptive gating scheme built upon Thompson sampling. Adaptive gating\nperiodically updates the gate position based on prior photon observations in\norder to minimize depth errors. Our experiments show that our gating strategy\nresults in significantly reduced depth reconstruction error and acquisition\ntime, even when operating outdoors under strong sunlight conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Po_R/0/1/0/all/0/1\">Ryan Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pediredla_A/0/1/0/all/0/1\">Adithya Pediredla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkioulekas_I/0/1/0/all/0/1\">Ioannis Gkioulekas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Context Network for Person Search. (arXiv:2112.02500v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02500","description":"<p>Person search aims to jointly localize and identify a query person from\nnatural, uncropped images, which has been actively studied over the past few\nyears. In this paper, we delve into the rich context information globally and\nlocally surrounding the target person, which we refer to as scene and group\ncontext, respectively. Unlike previous works that treat the two types of\ncontext individually, we exploit them in a unified global-local context network\n(GLCNet) with the intuitive aim of feature enhancement. Specifically, re-ID\nembeddings and context features are simultaneously learned in a multi-stage\nfashion, ultimately leading to enhanced, discriminative features for person\nsearch. We conduct the experiments on two person search benchmarks (i.e.,\nCUHK-SYSU and PRW) as well as extend our approach to a more challenging setting\n(i.e., character search on MovieNet). Extensive experimental results\ndemonstrate the consistent improvement of the proposed GLCNet over the\nstate-of-the-art methods on all three datasets. Our source codes, pre-trained\nmodels, and the new setting w.r.t. character search are publicly available at:\nhttps://github.com/ZhengPeng7/GLCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaogang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation. (arXiv:2112.02582v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02582","description":"<p>The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging\nvision problem that aims to predict panoptic segmentation and depth in a video\nsimultaneously. The previous work solves this task by extending the existing\npanoptic segmentation method with extra dense depth prediction and instance\ntracking head. However, the relationship between the depth and panoptic\nsegmentation is not well explored -- simply combining existing methods leads to\ncompetetion and needs careful weight balancing. In this paper, we present\nPolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS\ntask and lead to more robust results. Our principal insight is that the depth\ncan be harmonized with the panoptic segmentation with our proposed new paradigm\nof predicting instance level depth maps with object queries. Then the\nrelationship between the two tasks via query-based learning is explored. In\nparticular, we propose to learn the correlations among these queries and\ncorresponding features via query learning including grouping, updating, and\nreasoning. From the experiments, we prove the benefits of our design from both\ndepth estimation and panoptic segmentation aspects. Since each thing query also\nencodes the instance-wise appearance information, it is natural to perform\ntracking directly with appearance learning. Our method achieves the\nstate-of-the-art results on two DVPS datasets (Semantic KITTI, Cityscapes), and\nranks 1st on the ICCV-2021 BMTT Challenge video + depth track. Code will be\navailable at https://github.com/HarborYuan/PolyphonicFormer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haobo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Glimpse-based Decoder for Detection with Transformer. (arXiv:2112.04632v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04632","description":"<p>Although detection with Transformer (DETR) is increasingly popular, its\nglobal attention modeling requires an extremely long training period to\noptimize and achieve promising detection performance. Alternative to existing\nstudies that mainly develop advanced feature or embedding designs to tackle the\ntraining issue, we point out that the Region-of-Interest (RoI) based detection\nrefinement can easily help mitigate the difficulty of training for DETR\nmethods. Based on this, we introduce a novel REcurrent Glimpse-based decOder\n(REGO) in this paper. In particular, the REGO employs a multi-stage recurrent\nprocessing structure to help the attention of DETR gradually focus on\nforeground objects more accurately. In each processing stage, visual features\nare extracted as glimpse features from RoIs with enlarged bounding box areas of\ndetection results from the previous stage. Then, a glimpse-based decoder is\nintroduced to provide refined detection results based on both the glimpse\nfeatures and the attention modeling outputs of the previous stage. In practice,\nREGO can be easily embedded in representative DETR variants while maintaining\ntheir fully end-to-end training and inference pipelines. In particular, REGO\nhelps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36\ntraining epochs, compared with the first DETR and the Deformable DETR that\nrequire 500 and 50 epochs to achieve comparable performance, respectively.\nExperiments also show that REGO consistently boosts the performance of\ndifferent DETR detectors by up to 7% relative gain at the same setting of 50\ntraining epochs. Code is available via\nhttps://github.com/zhechen/Deformable-DETR-REGO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tail Recognition via Compositional Knowledge Transfer. (arXiv:2112.06741v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06741","description":"<p>In this work, we introduce a novel strategy for long-tail recognition that\naddresses the tail classes' few-shot problem via training-free knowledge\ntransfer. Our objective is to transfer knowledge acquired from information-rich\ncommon classes to semantically similar, and yet data-hungry, rare classes in\norder to obtain stronger tail class representations. We leverage the fact that\nclass prototypes and learned cosine classifiers provide two different,\ncomplementary representations of class cluster centres in feature space, and\nuse an attention mechanism to select and recompose learned classifier features\nfrom common classes to obtain higher quality rare class representations. Our\nknowledge transfer process is training free, reducing overfitting risks, and\ncan afford continual extension of classifiers to new classes. Experiments show\nthat our approach can achieve significant performance boosts on rare classes\nwhile maintaining robust common class performance, outperforming directly\ncomparable state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1\">Sarah Parisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esperanca_P/0/1/0/all/0/1\">Pedro M. Esperanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madarasz_T/0/1/0/all/0/1\">Tamas J. Madarasz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Distillation Interaction Weighting Network for Lightweight Image Super-Resolution. (arXiv:2112.08655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08655","description":"<p>Convolutional neural networks based single-image super-resolution (SISR) has\nmade great progress in recent years. However, it is difficult to apply these\nmethods to real-world scenarios due to the computational and memory cost.\nMeanwhile, how to take full advantage of the intermediate features under the\nconstraints of limited parameters and calculations is also a huge challenge. To\nalleviate these issues, we propose a lightweight yet efficient Feature\nDistillation Interaction Weighted Network (FDIWN). Specifically, FDIWN utilizes\na series of specially designed Feature Shuffle Weighted Groups (FSWG) as the\nbackbone, and several novel mutual Wide-residual Distillation Interaction\nBlocks (WDIB) form an FSWG. In addition, Wide Identical Residual Weighting\n(WIRW) units and Wide Convolutional Residual Weighting (WCRW) units are\nintroduced into WDIB for better feature distillation. Moreover, a Wide-Residual\nDistillation Connection (WRDC) framework and a Self-Calibration Fusion (SCF)\nunit are proposed to interact features with different scales more flexibly and\nefficiently.Extensive experiments show that our FDIWN is superior to other\nmodels to strike a good balance between model performance and efficiency. The\ncode is available at https://github.com/IVIPLab/FDIWN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition. (arXiv:2112.14238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14238","description":"<p>Recent works have shown that the computational efficiency of video\nrecognition can be significantly improved by reducing the spatial redundancy.\nAs a representative work, the adaptive focus method (AdaFocus) has achieved a\nfavorable trade-off between accuracy and inference speed by dynamically\nidentifying and attending to the informative regions in each video frame.\nHowever, AdaFocus requires a complicated three-stage training pipeline\n(involving reinforcement learning), leading to slow convergence and is\nunfriendly to practitioners. This work reformulates the training of AdaFocus as\na simple one-stage algorithm by introducing a differentiable\ninterpolation-based patch selection operation, enabling efficient end-to-end\noptimization. We further present an improved training scheme to address the\nissues introduced by the one-stage formulation, including the lack of\nsupervision, input diversity and training stability. Moreover, a\nconditional-exit technique is proposed to perform temporal adaptive computation\non top of AdaFocus without additional training. Extensive experiments on six\nbenchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics,\nSomething-Something V1&amp;V2, and Jester) demonstrate that our model significantly\noutperforms the original AdaFocus and other competitive baselines, while being\nconsiderably more simple and efficient to train. Code is available at\nhttps://github.com/LeapLabTHU/AdaFocusV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zihang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_V/0/1/0/all/0/1\">Victor Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Mask Uncertainty in Hyperspectral Image Reconstruction. (arXiv:2112.15362v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.15362","description":"<p>Recently, hyperspectral imaging (HSI) has attracted increasing research\nattention, especially for the ones based on a coded aperture snapshot spectral\nimaging (CASSI) system. Existing deep HSI reconstruction models are generally\ntrained on paired data to retrieve original signals upon 2D compressed\nmeasurements given by a particular optical hardware mask in CASSI, during which\nthe mask largely impacts the reconstruction performance and could work as a\n\"model hyperparameter\" governing on data augmentations. This mask-specific\ntraining style will lead to a hardware miscalibration issue, which sets up\nbarriers to deploying deep HSI models among different hardware and noisy\nenvironments. To address this challenge, we introduce mask uncertainty for HSI\nwith a complete variational Bayesian learning treatment and explicitly model it\nthrough a mask decomposition inspired by real hardware. Specifically, we\npropose a novel Graph-based Self-Tuning (GST) network to reason uncertainties\nadapting to varying spatial structures of masks among different hardware.\nMoreover, we develop a bilevel optimization framework to balance HSI\nreconstruction and uncertainty estimation, accounting for the hyperparameter\nproperty of masks. Extensive experimental results and model discussions\nvalidate the effectiveness (over 33/30 dB) of the proposed GST method under two\nmiscalibration scenarios and demonstrate a highly competitive performance\ncompared with the state-of-the-art well-calibrated methods. Our code and\npre-trained model are available at\nhttps://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Faces with Faced Masks. (arXiv:2201.06427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06427","description":"<p>Modern face recognition systems (FRS) still fall short when the subjects are\nwearing facial masks, a common theme in the age of respiratory pandemics. An\nintuitive partial remedy is to add a mask detector to flag any masked faces so\nthat the FRS can act accordingly for those low-confidence masked faces. In this\nwork, we set out to investigate the potential vulnerability of such FRS\nequipped with a mask detector, on large-scale masked faces, which might trigger\na serious risk, e.g., letting a suspect evade the FRS where both facial\nidentity and mask are undetected. As existing face recognizers and mask\ndetectors have high performance in their respective tasks, it is significantly\nchallenging to simultaneously fool them and preserve the transferability of the\nattack. We formulate the new task as the generation of realistic &amp;\nadversarial-faced mask and make three main contributions: First, we study the\nnaive Delanunay-based masking method (DM) to simulate the process of wearing a\nfaced mask that is cropped from a template image, which reveals the main\nchallenges of this new task. Second, we further equip the DM with the\nadversarial noise attack and propose the adversarial noise Delaunay-based\nmasking method (AdvNoise-DM) that can fool the face recognition and mask\ndetection effectively but make the face less natural. Third, we propose the\nadversarial filtering Delaunay-based masking method denoted as MF2M by\nemploying the adversarial filtering for AdvNoise-DM and obtain more natural\nfaces. With the above efforts, the final version not only leads to significant\nperformance deterioration of the state-of-the-art (SOTA) deep learning-based\nFRS, but also remains undetected by the SOTA facial mask detector, thus\nsuccessfully fooling both systems at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiayi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images. (arXiv:2201.10324v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.10324","description":"<p>Biomedical image datasets can be imbalanced due to the rarity of targeted\ndiseases. Generative Adversarial Networks play a key role in addressing this\nimbalance by enabling the generation of synthetic images to augment datasets.\nIt is important to generate synthetic images that incorporate a diverse range\nof features to accurately represent the distribution of features present in the\ntraining imagery. Furthermore, the absence of diverse features in synthetic\nimages can degrade the performance of machine learning classifiers. The mode\ncollapse problem can impact a Generative Adversarial Network's capacity to\ngenerate diversified images. Mode collapse comes in two varieties: intra-class\nand inter-class. In this paper, the intra-class mode collapse problem is\ninvestigated, and its subsequent impact on the diversity of synthetic X-ray\nimages is evaluated. This work contributes an empirical demonstration of the\nbenefits of integrating the adaptive input-image normalization for the Deep\nConvolutional GAN to alleviate the intra-class mode collapse problem. Results\ndemonstrate that the DCGAN with adaptive input-image normalization outperforms\nDCGAN with un-normalized X-ray images as evident by the superior diversity\nscores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saad_M/0/1/0/all/0/1\">Muhammad Muneeb Saad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rehmani_M/0/1/0/all/0/1\">Mubashir Husain Rehmani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OReilly_R/0/1/0/all/0/1\">Ruairi O&#x27;Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10990","description":"<p>In this paper we consider the problem of classifying fine-grained, multi-step\nactivities (e.g., cooking different recipes, making disparate home\nimprovements, creating various forms of arts and crafts) from long videos\nspanning up to several minutes. Accurately categorizing these activities\nrequires not only recognizing the individual steps that compose the task but\nalso capturing their temporal dependencies. This problem is dramatically\ndifferent from traditional action classification, where models are typically\noptimized on videos that span only a few seconds and that are manually trimmed\nto contain simple atomic actions. While step annotations could enable the\ntraining of models to recognize the individual steps of procedural activities,\nexisting large-scale datasets in this area do not include such segment labels\ndue to the prohibitive cost of manually annotating temporal boundaries in long\nvideos. To address this issue, we propose to automatically identify steps in\ninstructional videos by leveraging the distant supervision of a textual\nknowledge base (wikiHow) that includes detailed descriptions of the steps\nneeded for the execution of a wide variety of complex activities. Our method\nuses a language model to match noisy, automatically-transcribed speech from the\nvideo to step descriptions in the knowledge base. We demonstrate that video\nmodels trained to recognize these automatically-labeled steps (without manual\nsupervision) yield a representation that achieves superior generalization\nperformance on four downstream tasks: recognition of procedural activities,\nstep classification, step forecasting and egocentric video classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks?. (arXiv:2202.03133v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2202.03133","description":"<p>Recent Spiking Neural Networks (SNNs) works focus on an image classification\ntask, therefore various coding techniques have been proposed to convert an\nimage into temporal binary spikes. Among them, rate coding and direct coding\nare regarded as prospective candidates for building a practical SNN system as\nthey show state-of-the-art performance on large-scale datasets. Despite their\nusage, there is little attention to comparing these two coding schemes in a\nfair manner. In this paper, we conduct a comprehensive analysis of the two\ncodings from three perspectives: accuracy, adversarial robustness, and\nenergy-efficiency. First, we compare the performance of two coding techniques\nwith various architectures and datasets. Then, we measure the robustness of the\ncoding techniques on two adversarial attack methods. Finally, we compare the\nenergy-efficiency of two coding schemes on a digital hardware platform. Our\nresults show that direct coding can achieve better accuracy especially for a\nsmall number of timesteps. In contrast, rate coding shows better robustness to\nadversarial attacks owing to the non-differentiable spike generation process.\nRate coding also yields higher energy-efficiency than direct coding which\nrequires multi-bit precision for the first layer. Our study explores the\ncharacteristics of two codings, which is an important design consideration for\nbuilding SNNs. The code is made available at\nhttps://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyoungseob Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Abhishek Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhiroop Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1\">Yeshwanth Venkatesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation. (arXiv:2202.08261v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.08261","description":"<p>Availability of large, diverse, and multi-national datasets is crucial for\nthe development of effective and clinically applicable AI systems in the\nmedical imaging domain. However, forming a global model by bringing these\ndatasets together at a central location, comes along with various data privacy\nand ownership problems. To alleviate these problems, several recent studies\nfocus on the federated learning paradigm, a distributed learning approach for\ndecentralized data. Federated learning leverages all the available data without\nany need for sharing collaborators' data with each other or collecting them on\na central server. Studies show that federated learning can provide competitive\nperformance with conventional central training, while having a good\ngeneralization capability. In this work, we have investigated several federated\nlearning approaches on the brain tumor segmentation problem. We explore\ndifferent strategies for faster convergence and better performance which can\nalso work on strong Non-IID cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isik_Polat_E/0/1/0/all/0/1\">Ece Isik-Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_A/0/1/0/all/0/1\">Altan Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Anomaly Detection from Time-of-Flight Depth Images. (arXiv:2203.01052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01052","description":"<p>Video anomaly detection (VAD) addresses the problem of automatically finding\nanomalous events in video data. The primary data modalities on which current\nVAD systems work on are monochrome or RGB images. Using depth data in this\ncontext instead is still hardly explored in spite of depth images being a\npopular choice in many other computer vision research areas and the increasing\navailability of inexpensive depth camera hardware. We evaluate the application\nof existing autoencoder-based methods on depth video and propose how the\nadvantages of using depth data can be leveraged by integration into the loss\nfunction. Training is done unsupervised using normal sequences without need for\nany additional annotations. We show that depth allows easy extraction of\nauxiliary information for scene analysis in the form of a foreground mask and\ndemonstrate its beneficial effect on the anomaly detection performance through\nevaluation on a large public dataset, for which we are also the first ones to\npresent results on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pascal Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirbach_B/0/1/0/all/0/1\">Bruno Mirbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02378","description":"<p>Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose DiT, a self-supervised pre-trained Document Image Transformer model\nusing large-scale unlabeled text images for Document AI tasks, which is\nessential since no supervised counterparts ever exist due to the lack of human\nlabeled document images. We leverage DiT as the backbone network in a variety\nof vision-based Document AI tasks, including document image classification,\ndocument layout analysis, table detection as well as text detection for OCR.\nExperiment results have illustrated that the self-supervised pre-trained DiT\nmodel achieves new state-of-the-art results on these downstream tasks, e.g.\ndocument image classification (91.11 $\\rightarrow$ 92.69), document layout\nanalysis (91.0 $\\rightarrow$ 94.9), table detection (94.23 $\\rightarrow$ 96.55)\nand text detection for OCR (93.07 $\\rightarrow$ 94.29). The code and\npre-trained models are publicly available at \\url{https://aka.ms/msdit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers. (arXiv:2203.04838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04838","description":"<p>Pixel-wise semantic segmentation of RGB images can be advanced by exploiting\ninformative features from supplementary modalities. In this work, we propose\nCMX, a vision-transformer-based cross-modal fusion framework for RGB-X semantic\nsegmentation. To generalize to different sensing modalities encompassing\nvarious supplements and uncertainties, we consider that comprehensive\ncross-modal interactions should be provided. CMX is built with two streams to\nextract features from RGB images and the complementary modality (X-modality).\nIn each feature extraction stage, we design a Cross-Modal Feature Rectification\nModule (CM-FRM) to calibrate the feature of the current modality by combining\nthe feature from the other modality, in spatial- and channel-wise dimensions.\nWith rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix\nthem for the final semantic prediction. FFM is constructed with a\ncross-attention mechanism, which enables exchange of long-range contexts,\nenhancing both modalities' features at a global level. Extensive experiments\nshow that CMX generalizes to diverse multi-modal combinations, achieving\nstate-of-the-art performances on five RGB-Depth benchmarks, as well as\nRGB-Thermal and RGB-Polarization datasets. Besides, to investigate the\ngeneralizability to dense-sparse data fusion, we establish an RGB-Event\nsemantic segmentation benchmark based on the EventScape dataset, on which CMX\nsets the new state-of-the-art. Code is available at\nhttps://github.com/huaaaliu/RGBX_Semantic_Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Synthesis: A Free lunch for Large-scale Palmprint Recognition Model Pretraining. (arXiv:2203.05703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05703","description":"<p>Palmprints are private and stable information for biometric recognition. In\nthe deep learning era, the development of palmprint recognition is limited by\nthe lack of sufficient training data. In this paper, by observing that palmar\ncreases are the key information to deep-learning-based palmprint recognition,\nwe propose to synthesize training data by manipulating palmar creases.\nConcretely, we introduce an intuitive geometric model which represents palmar\ncreases with parameterized B\\'ezier curves. By randomly sampling B\\'ezier\nparameters, we can synthesize massive training samples of diverse identities,\nwhich enables us to pretrain large-scale palmprint recognition models.\nExperimental results demonstrate that such synthetically pretrained models have\na very strong generalization ability: they can be efficiently transferred to\nreal datasets, leading to significant performance improvements on palmprint\nrecognition. For example, under the open-set protocol, our method improves the\nstrong ArcFace baseline by more than 10\\% in terms of TAR@1e-6. And under the\nclosed-set protocol, our method reduces the equal error rate (EER) by an order\nof magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralReshaper: Single-image Human-body Retouching with Deep Neural Networks. (arXiv:2203.10496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10496","description":"<p>In this paper, we present NeuralReshaper, a novel method for semantic\nreshaping of human bodies in single images using deep generative networks. To\nachieve globally coherent reshaping effects, our approach follows a\nfit-then-reshape pipeline, which first fits a parametric 3D human model to a\nsource human image and then reshapes the fitted 3D model with respect to\nuser-specified semantic attributes. Previous methods rely on image warping to\ntransfer 3D reshaping effects to the entire image domain and thus often cause\ndistortions in both foreground and background. In contrast, we resort to\ngenerative adversarial nets conditioned on the source image and a 2D warping\nfield induced by the reshaped 3D model, to achieve more realistic reshaping\nresults. Specifically, we separately encode the foreground and background\ninformation in the source image using a two-headed UNet-like generator, and\nguide the information flow from the foreground branch to the background branch\nvia feature space warping. Furthermore, to deal with the lack-of-data problem\nthat no paired data exist (i.e., the same human bodies in varying shapes), we\nintroduce a novel self-supervised strategy to train our network. Unlike\nprevious methods that often require manual efforts to correct undesirable\nartifacts caused by incorrect body-to-image fitting, our method is fully\nautomatic. Extensive experiments on both indoor and outdoor datasets\ndemonstrate the superiority of our method over previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark. (arXiv:2203.11089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11089","description":"<p>Methods for 3D lane detection have been recently proposed to address the\nissue of inaccurate lane layouts in many autonomous driving scenarios\n(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to\ntheir simple designs of the spatial transformation between front view and\nbird's eye view (BEV) and the lack of a realistic dataset. Towards these\nissues, we present PersFormer: an end-to-end monocular 3D lane detector with a\nnovel Transformer-based spatial feature transformation module. Our model\ngenerates BEV features by attending to related front-view local regions with\ncamera parameters as a reference. PersFormer adopts a unified 2D/3D anchor\ndesign and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing\nthe feature consistency and sharing the benefits of multi-task learning.\nMoreover, we release one of the first large-scale real-world 3D lane datasets,\nwhich is called OpenLane, with high-quality annotation and scenario diversity.\nOpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane\ncategories, along with scene tags and the closed-in-path object annotations to\nencourage the development of lane detection and more industrial-related\nautonomous driving methods. We show that PersFormer significantly outperforms\ncompetitive baselines in the 3D lane detection task on our new OpenLane dataset\nas well as Apollo 3D Lane Synthetic dataset, and is also on par with\nstate-of-the-art algorithms in the 2D task on OpenLane. The project page is\navailable at https://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane\ndataset is provided at https://github.com/OpenPerceptionX/OpenLane.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sima_C/0/1/0/all/0/1\">Chonghao Sima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zehan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiajie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiangwei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Long Short-term Memory Based Recurrent Neural Network for Interventional MRI Reconstruction. (arXiv:2203.14769v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14769","description":"<p>Interventional magnetic resonance imaging (i-MRI) for surgical guidance could\nhelp visualize the interventional process such as deep brain stimulation (DBS),\nimproving the surgery performance and patient outcome. Different from\nretrospective reconstruction in conventional dynamic imaging, i-MRI for DBS has\nto acquire and reconstruct the interventional images sequentially online. Here\nwe proposed a convolutional long short-term memory (Conv-LSTM) based recurrent\nneural network (RNN), or ConvLR, to reconstruct interventional images with\ngolden-angle radial sampling. By using an initializer and Conv-LSTM blocks, the\npriors from the pre-operative reference image and intra-operative frames were\nexploited for reconstructing the current frame. Data consistency for radial\nsampling was implemented by a soft-projection method. To improve the\nreconstruction accuracy, an adversarial learning strategy was adopted. A set of\ninterventional images based on the pre-operative and post-operative MR images\nwere simulated for algorithm validation. Results showed with only 10 radial\nspokes, ConvLR provided the best performance compared with state-of-the-art\nmethods, giving an acceleration up to 40 folds. The proposed algorithm has the\npotential to achieve real-time i-MRI for DBS and can be used for general\npurpose MR-guided intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Suhao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_P/0/1/0/all/0/1\">Pawel Herman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanle Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chencheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bomin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00923","description":"<p>Continuous Sign Language Recognition (CSLR) is a long challenging task in\nComputer Vision due to the difficulties in detecting the explicit boundaries\nbetween the words in a sign sentence. To deal with this challenge, we propose a\ntwo-stage model. In the first stage, the predictor model, which includes a\ncombination of CNN, SVD, and LSTM, is trained with the isolated signs. In the\nsecond stage, we apply a post-processing algorithm to the Softmax outputs\nobtained from the first part of the model in order to separate the isolated\nsigns in the continuous signs. Due to the lack of a large dataset, including\nboth the sign sequences and the corresponding isolated signs, two public\ndatasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and\nASLVID, are used for evaluation. Results of the continuous sign videos confirm\nthe efficiency of the proposed model to deal with isolated sign boundaries\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding. (arXiv:2204.01450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01450","description":"<p>Grounding temporal video segments described in natural language queries\neffectively and efficiently is a crucial capability needed in\nvision-and-language fields. In this paper, we deal with the fast video temporal\ngrounding (FVTG) task, aiming at localizing the target segment with high speed\nand favorable accuracy. Most existing approaches adopt elaborately designed\ncross-modal interaction modules to improve the grounding performance, which\nsuffer from the test-time bottleneck. Although several common space-based\nmethods enjoy the high-speed merit during inference, they can hardly capture\nthe comprehensive and explicit relations between visual and textual modalities.\nIn this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a\ncommonsense-aware cross-modal alignment (CCA) framework, which incorporates\ncommonsense-guided visual and text representations into a complementary common\nspace for fast video temporal grounding. Specifically, the commonsense concepts\nare explored and exploited by extracting the structural semantic information\nfrom a language corpus. Then, a commonsense-aware interaction module is\ndesigned to obtain bridged visual and text features by utilizing the learned\ncommonsense concepts. Finally, to maintain the original semantic information of\ntextual queries, a cross-modal complementary common space is optimized to\nobtain matching scores for performing FVTG. Extensive results on two\nchallenging benchmarks show that our CCA method performs favorably against\nstate-of-the-arts while running at high speed. Our code is available at\nhttps://github.com/ZiyueWu59/CCA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shucheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01737","description":"<p>Convolutional neural networks have enabled significant improvements in\nmedical image-based disease classification. It has, however, become\nincreasingly clear that these models are susceptible to performance degradation\ndue to spurious correlations and dataset shifts, which may lead to\nunderperformance on underrepresented patient groups, among other problems. In\nthis paper, we compare two classification schemes on the ADNI MRI dataset: a\nvery simple logistic regression model that uses manually selected volumetric\nfeatures as inputs, and a convolutional neural network trained on 3D MRI data.\nWe assess the robustness of the trained models in the face of varying dataset\nsplits, training set sex composition, and stage of disease. In contrast to\nearlier work on diagnosing lung diseases based on chest x-ray data, we do not\nfind a strong dependence of model performance for male and female test subjects\non the sex composition of the training dataset. Moreover, in our analysis, the\nlow-dimensional model with manually selected features outperforms the 3D CNN,\nthus emphasizing the need for automatic robust feature extraction methods and\nthe value of manual feature specification (based on prior knowledge) for\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1\">Eike Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zemsch_M/0/1/0/all/0/1\">Maria Luise da Costa Zemsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henriksen_A/0/1/0/all/0/1\">Anders Henriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christensen_O/0/1/0/all/0/1\">Oskar Eiler Wiese Christensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganz_M/0/1/0/all/0/1\">Melanie Ganz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.02480","description":"<p>The inherent slow imaging speed of Magnetic Resonance Image (MRI) has spurred\nthe development of various acceleration methods, typically through\nheuristically undersampling the MRI measurement domain known as k-space.\nRecently, deep neural networks have been applied to reconstruct undersampled\nk-space data and have shown improved reconstruction performance. While most of\nthese methods focus on designing novel reconstruction networks or new training\nstrategies for a given undersampling pattern, e.g., Cartesian undersampling or\nNon-Cartesian sampling, to date, there is limited research aiming to learn and\noptimize k-space sampling strategies using deep neural networks. This work\nproposes a novel optimization framework to learn k-space sampling trajectories\nby considering it as an Ordinary Differential Equation (ODE) problem that can\nbe solved using neural ODE. In particular, the sampling of k-space data is\nframed as a dynamic system, in which neural ODE is formulated to approximate\nthe system with additional constraints on MRI physics. In addition, we have\nalso demonstrated that trajectory optimization and image reconstruction can be\nlearned collaboratively for improved imaging efficiency and reconstruction\nperformance. Experiments were conducted on different in-vivo datasets (e.g.,\nbrain and knee images) acquired with different sequences. Initial results have\nshown that our proposed method can generate better image quality in accelerated\nMRI than conventional undersampling schemes in Cartesian and Non-Cartesian\nacquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1\">Li Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02557","description":"<p>While local-window self-attention performs notably in vision tasks, it\nsuffers from limited receptive field and weak modeling capability issues. This\nis mainly because it performs self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose MixFormer to find a\nsolution. First, we combine local-window self-attention with depth-wise\nconvolution in a parallel design, modeling cross-window connections to enlarge\nthe receptive fields. Second, we propose bi-directional interactions across\nbranches to provide complementary clues in the channel and spatial dimensions.\nThese two designs are integrated to achieve efficient feature mixing among\nwindows and dimensions. Our MixFormer provides competitive results on image\nclassification with EfficientNet and shows better results than RegNet and Swin\nTransformer. Performance in downstream tasks outperforms its alternatives by\nsignificant margins with less computational costs in 5 dense prediction tasks\non MS COCO, ADE20k, and LVIS. Code is available at\n\\url{https://github.com/PaddlePaddle/PaddleClas}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03382","description":"<p>Text-Video Retrieval plays an important role in multi-modal understanding and\nhas attracted increasing attention in recent years. Most existing methods focus\non constructing contrastive pairs between whole videos and complete caption\nsentences, while ignoring fine-grained cross-modal relationships, e.g., short\nclips and phrases or single frame and word. In this paper, we propose a novel\nmethod, named HunYuan\\_tvr, to explore hierarchical cross-modal interactions by\nsimultaneously exploring video-sentence, clip-phrase, and frame-word\nrelationships. Considering intrinsic semantic relations between frames,\nHunYuan\\_tvr first performs self-attention to explore frame-wise correlations\nand adaptively clusters correlated frames into clip-level representations.\nThen, the clip-wise correlation is explored to aggregate clip representations\ninto a compact one to describe the video globally. In this way, we can\nconstruct hierarchical video representations for frame-clip-video\ngranularities, and also explore word-wise correlations to form\nword-phrase-sentence embeddings for the text modality. Finally, hierarchical\ncontrastive learning is designed to explore cross-modal\nrelationships,~\\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which\nenables HunYuan\\_tvr to achieve a comprehensive multi-modal understanding.\nFurther boosted by adaptive label denosing and marginal sample enhancement,\nHunYuan\\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,\nRank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rong-Cheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.04218","description":"<p>Super-resolving medical images can help physicians in providing more accurate\ndiagnostics. In many situations, computed tomography (CT) or magnetic resonance\nimaging (MRI) techniques output several scans (modes) during a single\ninvestigation, which can jointly be used (in a multimodal fashion) to further\nboost the quality of super-resolution results. To this end, we propose a novel\nmultimodal multi-head convolutional attention module to super-resolve CT and\nMRI scans. Our attention module uses the convolution operation to perform joint\nspatial-channel attention on multiple concatenated input tensors, where the\nkernel (receptive field) size controls the reduction rate of the spatial\nattention and the number of convolutional filters controls the reduction rate\nof the channel attention, respectively. We introduce multiple attention heads,\neach head having a distinct receptive field size corresponding to a particular\nreduction rate for the spatial attention. We integrate our multimodal\nmulti-head convolutional attention (MMHCA) into two deep neural architectures\nfor super-resolution and conduct experiments on three data sets. Our empirical\nresults show the superiority of our attention module over the state-of-the-art\nattention mechanisms used in super-resolution. Moreover, we conduct an ablation\nstudy to assess the impact of the components involved in our attention module,\ne.g. the number of inputs or the number of heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miron_A/0/1/0/all/0/1\">Andreea-Iuliana Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savencu_O/0/1/0/all/0/1\">Olivian Savencu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verga_N/0/1/0/all/0/1\">Nicolae Verga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAN: Noise-Aware NeRFs for Burst-Denoising. (arXiv:2204.04668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04668","description":"<p>Burst denoising is now more relevant than ever, as computational photography\nhelps overcome sensitivity issues inherent in mobile phones and small cameras.\nA major challenge in burst-denoising is in coping with pixel misalignment,\nwhich was so far handled with rather simplistic assumptions of simple motion,\nor the ability to align in pre-processing. Such assumptions are not realistic\nin the presence of large motion and high levels of noise. We show that Neural\nRadiance Fields (NeRFs), originally suggested for physics-based novel-view\nrendering, can serve as a powerful framework for burst denoising. NeRFs have an\ninherent capability of handling noise as they integrate information from\nmultiple images, but they are limited in doing so, mainly since they build on\npixel-wise operations which are suitable to ideal imaging conditions. Our\napproach, termed NAN, leverages inter-view and spatial information in NeRFs to\nbetter deal with noise. It achieves state-of-the-art results in burst denoising\nand is especially successful in coping with large movement and occlusions,\nunder very high levels of noise. With the rapid advances in accelerating NeRFs,\nit could provide a powerful platform for denoising in challenging environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearl_N/0/1/0/all/0/1\">Naama Pearl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treibitz_T/0/1/0/all/0/1\">Tali Treibitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korman_S/0/1/0/all/0/1\">Simon Korman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreset of Hyperspectral Images on Small Quantum Computer. (arXiv:2204.04691v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2204.04691","description":"<p>Machine Learning (ML) techniques are employed to analyze and process big\nRemote Sensing (RS) data, and one well-known ML technique is a Support Vector\nMachine (SVM). An SVM is a quadratic programming (QP) problem, and a D-Wave\nquantum annealer (D-Wave QA) promises to solve this QP problem more efficiently\nthan a conventional computer. However, the D-Wave QA cannot solve directly the\nSVM due to its very few input qubits. Hence, we use a coreset (\"core of a\ndataset\") of given EO data for training an SVM on this small D-Wave QA. The\ncoreset is a small, representative weighted subset of an original dataset, and\nany training models generate competitive classes by using the coreset in\ncontrast to by using its original dataset. We measured the closeness between an\noriginal dataset and its coreset by employing a Kullback-Leibler (KL)\ndivergence measure. Moreover, we trained the SVM on the coreset data by using\nboth a D-Wave QA and a conventional method. We conclude that the coreset\ncharacterizes the original dataset with very small KL divergence measure. In\naddition, we present our KL divergence results for demonstrating the closeness\nbetween our original data and its coreset. As practical RS data, we use\nHyperspectral Image (HSI) of Indian Pine, USA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Otgonbaatar_S/0/1/0/all/0/1\">Soronzonbold Otgonbaatar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Grafting Network for One-Stage High Resolution Saliency Detection. (arXiv:2204.05041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05041","description":"<p>Recent salient object detection (SOD) methods based on deep neural network\nhave achieved remarkable performance. However, most of existing SOD models\ndesigned for low-resolution input perform poorly on high-resolution images due\nto the contradiction between the sampling depth and the receptive field size.\nAiming at resolving this contradiction, we propose a novel one-stage framework\ncalled Pyramid Grafting Network (PGNet), using transformer and CNN backbone to\nextract features from different resolution images independently and then graft\nthe features from transformer branch to CNN branch. An attention-based\nCross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine\nbroken detailed information more holistically, guided by different source\nfeature during decoding process. Moreover, we design an Attention Guided Loss\n(AGL) to explicitly supervise the attention matrix generated by CMGM to help\nthe network better interact with the attention from different models. We\ncontribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD,\ncontaining 5,920 images at 4K-8K resolutions. To our knowledge, it is the\nlargest dataset in both quantity and resolution for high-resolution SOD task,\nwhich can be used for training and testing in future research. Sufficient\nexperiments on UHRSD and widely-used SOD datasets demonstrate that our method\nachieves superior performance compared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chenxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Changqun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingcan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhirui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaowu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}