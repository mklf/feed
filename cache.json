{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-11-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Athena 2.0: Contextualized Dialogue Management for an Alexa Prize SocialBot. (arXiv:2111.02519v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02519","description":"<p>Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last\ntwo Alexa Prize Grand Challenges. One reason for Athena's success is its novel\ndialogue management strategy, which allows it to dynamically construct\ndialogues and responses from component modules, leading to novel conversations\nwith every interaction. Here we describe Athena's system design and performance\nin the Alexa Prize during the 20/21 competition. A live demo of Athena as well\nas video recordings will provoke discussion on the state of the art in\nconversational AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_K/0/1/0/all/0/1\">Kevin K. Bowden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_L/0/1/0/all/0/1\">Lena Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_V/0/1/0/all/0/1\">Vrindavan Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_O/0/1/0/all/0/1\">Omkar Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_R/0/1/0/all/0/1\">Rishi Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1\">Angela Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cecilia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_E/0/1/0/all/0/1\">Eduardo Zamora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Phillip Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bheemanpally_J/0/1/0/all/0/1\">Jeshwanth Bheemanpally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnaparkhi_A/0/1/0/all/0/1\">Adwait Ratnaparkhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLUES: Few-Shot Learning Evaluation in Natural Language Understanding. (arXiv:2111.02570v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02570","description":"<p>Most recent progress in natural language understanding (NLU) has been driven,\nin part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU\nmodels have now matched or exceeded \"human-level\" performance on many tasks in\nthese benchmarks. Most of these benchmarks, however, give models access to\nrelatively large amounts of labeled data for training. As such, the models are\nprovided far more data than required by humans to achieve strong performance.\nThat has motivated a line of work that focuses on improving few-shot learning\nperformance of NLU models. However, there is a lack of standardized evaluation\nbenchmarks for few-shot NLU resulting in different experimental settings in\ndifferent papers. To help accelerate this line of work, we introduce CLUES\n(Constrained Language Understanding Evaluation Standard), a benchmark for\nevaluating the few-shot learning capabilities of NLU models. We demonstrate\nthat while recent models reach human performance when they have access to large\namounts of labeled data, there is a huge gap in performance in the few-shot\nsetting for most tasks. We also demonstrate differences between alternative\nmodel families and adaptation techniques in the few shot setting. Finally, we\ndiscuss several principles and choices in designing the experimental settings\nfor evaluating the true few-shot learning performance and suggest a unified\nstandardized approach to few-shot learning evaluation. We aim to encourage\nresearch on NLU models that can generalize to new tasks with a small number of\nexamples. Code and data for CLUES are available at\nhttps://github.com/microsoft/CLUES.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saghar Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Greg Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meek_C/0/1/0/all/0/1\">Christopher Meek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Parsing for Multilingual Task-Oriented Dialogues. (arXiv:2111.02574v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02574","description":"<p>Robust state tracking for task-oriented dialogue systems currently remains\nrestricted to a few popular languages. This paper shows that given a\nlarge-scale dialogue data set in one language, we can automatically produce an\neffective semantic parser for other languages using machine translation. We\npropose automatic translation of dialogue datasets with alignment to ensure\nfaithful translation of slot values and eliminate costly human supervision used\nin previous benchmarks. We also propose a new contextual semantic parsing\nmodel, which encodes the formal slots and values, and only the last agent and\nuser utterances. We show that the succinct representation reduces the\ncompounding effect of translation errors, without harming the accuracy in\npractice.\n</p>\n<p>We evaluate our approach on several dialogue state tracking benchmarks. On\nRiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZ-ZH datasets we improve the state\nof the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a\ncomprehensive error analysis for all three datasets showing erroneous\nannotations can obscure judgments on the quality of the model.\n</p>\n<p>Finally, we present RiSAWOZ English and German datasets, created using our\ntranslation methodology. On these datasets, accuracy is within 11% of the\noriginal showing that high-accuracy multilingual dialogue datasets are possible\nwithout relying on expensive human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_V/0/1/0/all/0/1\">Victoria Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Semantic Cognition, Inductive Generalization, and Language Models. (arXiv:2111.02603v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02603","description":"<p>My doctoral research focuses on understanding semantic knowledge in neural\nnetwork models trained solely to predict natural language (referred to as\nlanguage models, or LMs), by drawing on insights from the study of concepts and\ncategories grounded in cognitive science. I propose a framework inspired by\n'inductive reasoning,' a phenomenon that sheds light on how humans utilize\nbackground knowledge to make inductive leaps and generalize from new pieces of\ninformation about concepts and their properties. Drawing from experiments that\nstudy inductive reasoning, I propose to analyze semantic inductive\ngeneralization in LMs using phenomena observed in human-induction literature,\ninvestigate inductive behavior on tasks such as implicit reasoning and emergent\nfeature recognition, and analyze and relate induction dynamics to the learned\nconceptual representation space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexically Aware Semi-Supervised Learning for OCR Post-Correction. (arXiv:2111.02622v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02622","description":"<p>Much of the existing linguistic data in many languages of the world is locked\naway in non-digitized books and documents. Optical character recognition (OCR)\ncan be used to produce digitized text, and previous work has demonstrated the\nutility of neural post-correction methods that improve the results of\ngeneral-purpose OCR systems on recognition of less-well-resourced languages.\nHowever, these methods rely on manually curated post-correction data, which are\nrelatively scarce compared to the non-annotated raw images that need to be\ndigitized.\n</p>\n<p>In this paper, we present a semi-supervised learning method that makes it\npossible to utilize these raw images to improve performance, specifically\nthrough the use of self-training, a technique where a model is iteratively\ntrained on its own outputs. In addition, to enforce consistency in the\nrecognized vocabulary, we introduce a lexically-aware decoding method that\naugments the neural post-correction model with a count-based language model\nconstructed from the recognized texts, implemented using weighted finite-state\nautomata (WFSA) for efficient and effective decoding.\n</p>\n<p>Results on four endangered languages demonstrate the utility of the proposed\nmethod, with relative error reductions of 15-29%, where we find the combination\nof self-training and lexically-aware decoding essential for achieving\nconsistent improvements. Data and code are available at\nhttps://shrutirij.github.io/ocr-el/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenblum_D/0/1/0/all/0/1\">Daisy Rosenblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Response Generation with Context-Aware Prompt Learning. (arXiv:2111.02643v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02643","description":"<p>Pre-trained language models (PLM) have marked a huge leap in neural dialogue\nmodeling. While PLMs are pre-trained on large-scale text corpora, they are\nusually fine-tuned on scarce dialogue data with specific domain knowledge and\ndialogue styles. However, tailoring the language models while fully utilizing\nprior knowledge in large pre-trained models remains a challenge. In this paper,\nwe present a novel approach for pre-trained dialogue modeling that casts the\ndialogue generation problem as a prompt-learning task. Instead of fine-tuning\non limited dialogue data, our approach, DialogPrompt, learns continuous prompt\nembeddings optimized for dialogue contexts, which appropriately elicit\nknowledge from the large pre-trained model. To encourage the model to better\nutilize the prompt embeddings, the prompt encoders are designed to be\nconditioned on the input dialogue context. Experiments on popular conversation\ndatasets show that our approach significantly outperforms the fine-tuning\nbaseline and the generic prompt-learning methods. Furthermore, human\nevaluations strongly support the superiority of DialogPrompt in regard to\nresponse generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech recognition for air traffic control via feature learning and end-to-end training. (arXiv:2111.02654v1 [cs.SD])","link":"http://arxiv.org/abs/2111.02654","description":"<p>In this work, we propose a new automatic speech recognition (ASR) system\nbased on feature learning and an end-to-end training procedure for air traffic\ncontrol (ATC) systems. The proposed model integrates the feature learning\nblock, recurrent neural network (RNN), and connectionist temporal\nclassification loss to build an end-to-end ASR model. Facing the complex\nenvironments of ATC speech, instead of the handcrafted features, a learning\nblock is designed to extract informative features from raw waveforms for\nacoustic modeling. Both the SincNet and 1D convolution blocks are applied to\nprocess the raw waveforms, whose outputs are concatenated to the RNN layers for\nthe temporal modeling. Thanks to the ability to learn representations from raw\nwaveforms, the proposed model can be optimized in a complete end-to-end manner,\ni.e., from waveform to text. Finally, the multilingual issue in the ATC domain\nis also considered to achieve the ASR task by constructing a combined\nvocabulary of Chinese characters and English letters. The proposed approach is\nvalidated on a multilingual real-world corpus (ATCSpeech), and the experimental\nresults demonstrate that the proposed approach outperforms other baselines,\nachieving a 6.9\\% character error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_P/0/1/0/all/0/1\">Peng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongyue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Conversion Can Improve ASR in Very Low-Resource Settings. (arXiv:2111.02674v1 [eess.AS])","link":"http://arxiv.org/abs/2111.02674","description":"<p>Voice conversion (VC) has been proposed to improve speech recognition systems\nin low-resource languages by using it to augment limited training data. But\nuntil recently, practical issues such as compute speed have limited the use of\nVC for this purpose. Moreover, it is still unclear whether a VC model trained\non one well-resourced language can be applied to speech from another\nlow-resource language for the purpose of data augmentation. In this work we\nassess whether a VC system can be used cross-lingually to improve low-resource\nspeech recognition. Concretely, we combine several recent techniques to design\nand train a practical VC system in English, and then use this system to augment\ndata for training a speech recognition model in several low-resource languages.\nWe find that when using a sensible amount of augmented data, speech recognition\nperformance is improved in all four low-resource languages considered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1\">Matthew Baas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoreLM: Coreference-aware Language Model Fine-Tuning. (arXiv:2111.02687v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02687","description":"<p>Language Models are the underpin of all modern Natural Language Processing\n(NLP) tasks. The introduction of the Transformers architecture has contributed\nsignificantly into making Language Modeling very effective across many NLP\ntask, leading to significant advancements in the field. However, Transformers\ncome with a big computational cost, which grows quadratically with respect to\nthe input length. This presents a challenge as to understand long texts\nrequires a lot of context. In this paper, we propose a Fine-Tuning framework,\nnamed CoreLM, that extends the architecture of current Pretrained Language\nModels so that they incorporate explicit entity information. By introducing\nentity representations, we make available information outside the contextual\nspace of the model, which results in a better Language Model for a fraction of\nthe computational cost. We implement our approach using GPT2 and compare the\nfine-tuned model to the original. Our proposed model achieves a lower\nPerplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a\nfine-tuned version of GPT2 without any changes. We also compare the models'\nperformance in terms of Accuracy in LAMBADA and Children's Book Test, with and\nwithout the use of model-created coreference annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_N/0/1/0/all/0/1\">Nikolaos Stylianou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlahavas_I/0/1/0/all/0/1\">Ioannis Vlahavas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Multimodal AutoML for Tabular Data with Text Fields. (arXiv:2111.02705v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02705","description":"<p>We consider the use of automated supervised learning systems for data tables\nthat not only contain numeric/categorical columns, but one or more text fields\nas well. Here we assemble 18 multimodal data tables that each contain some text\nfields and stem from a real business application. Our publicly-available\nbenchmark enables researchers to comprehensively evaluate their own methods for\nsupervised learning with numeric, categorical, and text features. To ensure\nthat any single modeling strategy which performs well over all 18 datasets will\nserve as a practical foundation for multimodal text/tabular AutoML, the diverse\ndatasets in our benchmark vary greatly in: sample size, problem types (a mix of\nclassification and regression tasks), number of features (with the number of\ntext columns ranging from 1 to 28 between datasets), as well as how the\npredictive signal is decomposed between text vs. numeric/categorical features\n(and predictive interactions thereof). Over this benchmark, we evaluate various\nstraightforward pipelines to model such data, including standard two-stage\napproaches where NLP is used to featurize the text such that AutoML for tabular\ndata can then be applied. Compared with human data science teams, the fully\nautomated methodology that performed best on our benchmark (stack ensembling a\nmultimodal Transformer with various tree models) also manages to rank 1st place\nwhen fit to the raw text/tabular data in two MachineHack prediction\ncompetitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price\nSuggestion Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erickson_N/0/1/0/all/0/1\">Nick Erickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. (arXiv:2111.02735v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02735","description":"<p>Self-supervised speech representations such as wav2vec 2.0 and HuBERT are\nmaking revolutionary progress in Automatic Speech Recognition (ASR). However,\nself-supervised models have not been totally proved to produce better\nperformance on tasks other than ASR. In this work, we explore partial\nfine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models\nfor three non-ASR speech tasks : Speech Emotion Recognition, Speaker\nVerification and Spoken Language Understanding. We also compare pre-trained\nmodels with/without ASR fine-tuning. With simple down-stream frameworks, the\nbest scores reach 79.58% weighted accuracy for Speech Emotion Recognition on\nIEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 87.51%\naccuracy for Intent Classification and 75.32% F1 for Slot Filling on SLURP,\nthus setting a new state-of-the-art for these three benchmarks, proving that\nfine-tuned wav2vec 2.0 and HuBERT models can better learn prosodic, voice-print\nand semantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boumadane_A/0/1/0/all/0/1\">Abdelmoumene Boumadane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heba_A/0/1/0/all/0/1\">Abdelwahab Heba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medicines Question Answering System, MeQA. (arXiv:2111.02760v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02760","description":"<p>In this paper we present the first system in Spanish capable of answering\nquestions about medicines for human use, called MeQA (Medicines Question\nAnswering), a project created by the Spanish Agency for Medicines and Health\nProducts (AEMPS, for its acronym in Spanish). Online services that offer\nmedical help have proliferated considerably, mainly due to the current pandemic\nsituation due to COVID-19. For example, websites such as Doctoralia, Savia, or\nSaludOnNet, offer Doctor Answers type consultations, in which patients or users\ncan send questions to doctors and specialists, and receive an answer in less\nthan 24 hours. Many of the questions received are related to medicines for\nhuman use, and most can be answered through the leaflets. Therefore, a system\nsuch as MeQA capable of answering these types of questions automatically could\nalleviate the burden on these websites, and it would be of great use to such\npatients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_J/0/1/0/all/0/1\">Jes&#xfa;s Santamar&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel. (arXiv:2111.02827v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02827","description":"<p>While multi-agent reinforcement learning has been used as an effective means\nto study emergent communication between agents, existing work has focused\nalmost exclusively on communication with discrete symbols. Human communication\noften takes place (and emerged) over a continuous acoustic channel; human\ninfants acquire language in large part through continuous signalling with their\ncaregivers. We therefore ask: Are we able to observe emergent language between\nagents with a continuous communication channel trained through reinforcement\nlearning? And if so, what is the impact of channel characteristics on the\nemerging language? We propose an environment and training methodology to serve\nas a means to carry out an initial exploration of these questions. We use a\nsimple messaging environment where a \"speaker\" agent needs to convey a concept\nto a \"listener\". The Speaker is equipped with a vocoder that maps symbols to a\ncontinuous waveform, this is passed over a lossy continuous channel, and the\nListener needs to map the continuous signal to the concept. Using deep\nQ-learning, we show that basic compositionality emerges in the learned language\nrepresentations. We find that noise is essential in the communication channel\nwhen conveying unseen concept combinations. And we show that we can ground the\nemergent communication by introducing a caregiver predisposed to \"hearing\" or\n\"speaking\" English. Finally, we describe how our platform serves as a starting\npoint for future work that uses a combination of deep reinforcement learning\nand multi-agent systems to study our questions of continuous signalling in\nlanguage learning and emergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1\">Arnu Pretorius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman A. Engelbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. (arXiv:2111.02840v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02840","description":"<p>Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A text autoencoder from transformer for fast encoding language representation. (arXiv:2111.02844v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02844","description":"<p>In recent years BERT shows apparent advantages and great potential in natural\nlanguage processing tasks. However, both training and applying BERT requires\nintensive time and resources for computing contextual language representations,\nwhich hinders its universality and applicability. To overcome this bottleneck,\nwe propose a deep bidirectional language model by using window masking\nmechanism at attention layer. This work computes contextual language\nrepresentations without random masking as does in BERT and maintains the deep\nbidirectional architecture like BERT. To compute the same sentence\nrepresentation, our method shows O(n) complexity less compared to other\ntransformer-based models with O($n^2$). To further demonstrate its superiority,\ncomputing context language representations on CPU environments is conducted, by\nusing the embeddings from the proposed method, logistic regression shows much\nhigher accuracy in terms of SMS classification. Moverover, the proposed method\nalso achieves significant higher performance in semantic similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised and Distributional Detection of Machine-Generated Text. (arXiv:2111.02878v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02878","description":"<p>The power of natural language generation models has provoked a flurry of\ninterest in automatic methods to detect if a piece of text is human or\nmachine-authored. The problem so far has been framed in a standard supervised\nway and consists in training a classifier on annotated data to predict the\norigin of one given new document. In this paper, we frame the problem in an\nunsupervised and distributional way: we assume that we have access to a large\ncollection of unannotated documents, a big fraction of which is\nmachine-generated. We propose a method to detect those machine-generated\ndocuments leveraging repeated higher-order n-grams, which we show over-appear\nin machine-generated text as compared to human ones. That weak signal is the\nstarting point of a self-training setting where pseudo-labelled documents are\nused to train an ensemble of classifiers. Our experiments show that leveraging\nthat signal allows us to rank suspicious documents accurately. Precision at\n5000 is over 90% for top-k sampling strategies, and over 80% for nucleus\nsampling for the largest model we used (GPT2-large). The drop with increased\nsize of model is small, which could indicate that the results hold for other\ncurrent and future large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the impact of out of vocabulary words in the translation of natural language questions into SPARQL queries. (arXiv:2111.03000v1 [cs.CL])","link":"http://arxiv.org/abs/2111.03000","description":"<p>Accessing the large volumes of information available in public knowledge\nbases might be complicated for those users unfamiliar with the SPARQL query\nlanguage. Automatic translation of questions posed in natural language in\nSPARQL has the potential of overcoming this problem. Existing systems based on\nneural-machine translation are very effective but easily fail in recognizing\nwords that are Out Of the Vocabulary (OOV) of the training set. This is a\nserious issue while querying large ontologies. In this paper, we combine Named\nEntity Linking, Named Entity Recognition, and Neural Machine Translation to\nperform automatic translation of natural language questions into SPARQL\nqueries. We demonstrate empirically that our approach is more effective and\nresilient to OOV words than existing approaches by running the experiments on\nMonument, QALD-9, and LC-QuAD v1, which are well-known datasets for Question\nAnswering over DBpedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santana_M/0/1/0/all/0/1\">Manuel A. Borroto Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricca_F/0/1/0/all/0/1\">Francesco Ricca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuteri_B/0/1/0/all/0/1\">Bernardo Cuteri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting a Knowledge Base of COVID-19 Events from Social Media. (arXiv:2006.02567v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.02567","description":"<p>In this paper, we present a manually annotated corpus of 10,000 tweets\ncontaining public reports of five COVID-19 events, including positive and\nnegative tests, deaths, denied access to testing, claimed cures and\npreventions. We designed slot-filling questions for each event type and\nannotated a total of 31 fine-grained slots, such as the location of events,\nrecent travel, and close contacts. We show that our corpus can support\nfine-tuning BERT-based classifiers to automatically extract publicly reported\nevents and help track the spread of a new disease. We also demonstrate that, by\naggregating events extracted from millions of tweets, we achieve surprisingly\nhigh precision when answering complex queries, such as \"Which organizations\nhave employees that tested positive in Philadelphia?\" We will release our\ncorpus (with user-information removed), automatic extraction models, and the\ncorresponding knowledge base to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. (arXiv:2011.10369v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.10369","description":"<p>Backdoor attacks are a kind of emergent training-time threat to deep neural\nnetworks (DNNs). They can manipulate the output of DNNs and possess high\ninsidiousness. In the field of natural language processing, some attack methods\nhave been proposed and achieve very high attack success rates on multiple\npopular models. Nevertheless, there are few studies on defending against\ntextual backdoor attacks. In this paper, we propose a simple and effective\ntextual backdoor defense named ONION, which is based on outlier word detection\nand, to the best of our knowledge, is the first method that can handle all the\ntextual backdoor attack situations. Experiments demonstrate the effectiveness\nof our model in defending BiLSTM and BERT against five different backdoor\nattacks. All the code and data of this paper can be obtained at\nhttps://github.com/thunlp/ONION.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teach Me to Explain: A Review of Datasets for Explainable NLP. (arXiv:2102.12060v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12060","description":"<p>Explainable NLP (ExNLP) has increasingly focused on collecting\nhuman-annotated textual explanations. These explanations are used downstream in\nthree ways: as data augmentation to improve performance on a predictive task,\nas supervision to train models to produce explanations for their predictions,\nand as a ground-truth to evaluate model-generated explanations. In this review,\nwe identify 65 datasets with three predominant classes of textual explanations\n(highlights, free-text, and structured), organize the literature on annotating\neach type, identify strengths and shortcomings of existing collection\nmethodologies, and give recommendations for collecting ExNLP datasets in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Hate Speech with GPT-3. (arXiv:2103.12407v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12407","description":"<p>Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist? We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an\naccuracy between 48 per cent and 69 per cent. With few-shot learning and an\ninstruction included in the prompt, the model's accuracy can be as high as 78\nper cent. We conclude that large language models have a role to play in hate\nspeech detection, and that with further development language models could be\nused to counter hate speech and even self-police.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">Ke-Li Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_R/0/1/0/all/0/1\">Rohan Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04927","description":"<p>Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04847","description":"<p>Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art Natural\nLanguage Processing (NLP) models. Here, we present an empirical study that\ncompares different uncertainty-based algorithms with BERT$_{base}$ as the used\nclassifier. We evaluate the algorithms on two NLP classification datasets:\nStanford Sentiment Treebank and KvK-Frontpages. Additionally, we explore\nheuristics that aim to solve presupposed problems of uncertainty-based AL;\nnamely, that it is unscalable and that it is prone to selecting outliers.\nFurthermore, we explore the influence of the query-pool size on the performance\nof AL. Whereas it was found that the proposed heuristics for AL did not improve\nperformance of AL; our results show that using uncertainty-based AL with\nBERT$_{base}$ outperforms random sampling of data. This difference in\nperformance can decrease as the query-pool size gets larger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1\">Pieter Floris Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenniger_G/0/1/0/all/0/1\">Gideon Maillette de Buy Wenniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling. (arXiv:2109.11541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11541","description":"<p>Conversational semantic role labeling (CSRL) is believed to be a crucial step\ntowards dialogue understanding. However, it remains a major challenge for\nexisting CSRL parser to handle conversational structural information. In this\npaper, we present a simple and effective architecture for CSRL which aims to\naddress this problem. Our model is based on a conversational structure-aware\ngraph network which explicitly encodes the speaker dependent information. We\nalso propose a multi-task learning method to further improve the model.\nExperimental results on benchmark datasets show that our model with our\nproposed training objectives significantly outperforms previous baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who speaks like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning. (arXiv:2109.14199v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14199","description":"<p>Abstractive dialogue summarization is a challenging task for several reasons.\nFirst, most of the important pieces of information in a conversation are\nscattered across utterances through multi-party interactions with different\ntextual styles. Second, dialogues are often informal structures, wherein\ndifferent individuals express personal perspectives, unlike text summarization,\ntasks that usually target formal documents such as news articles. To address\nthese issues, we focused on the association between utterances from individual\nspeakers and unique syntactic structures. Speakers have unique textual styles\nthat can contain linguistic information, such as voiceprint. Therefore, we\nconstructed a syntax-aware model by leveraging linguistic information (i.e.,\nPOS tagging), which alleviates the above issues by inherently distinguishing\nsentences uttered from individual speakers. We employed multi-task learning of\nboth syntax-aware information and dialogue summarization. To the best of our\nknowledge, our approach is the first method to apply multi-task learning to the\ndialogue summarization task. Experiments on a SAMSum corpus (a large-scale\ndialogue summarization corpus) demonstrated that our method improved upon the\nvanilla model. We further analyze the costs and benefits of our approach\nrelative to baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kisu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum: Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04400","description":"<p>Existing abstractive summarization models lack explicit control mechanisms\nthat would allow users to influence the stylistic features of the model\noutputs. This results in generating generic summaries that do not cater to the\nusers needs or preferences. To address this issue we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels, e.g. BART, to a mixture-of-experts version consisting of multiple\ndecoders. Our proposed model encourages each expert, i.e. decoder, to learn and\ngenerate stylistically-distinct summaries along dimensions such as\nabstractiveness, length, specificity, and others. At each time step, HydraSum\nemploys a gating mechanism that decides the contribution of each individual\ndecoder to the next token's output probability distribution. Through\nexperiments on three summarization datasets (CNN, Newsroom, XSum), we\ndemonstrate that this gating mechanism automatically learns to assign\ncontrasting summary styles to different HydraSum decoders under the standard\ntraining objective without the need for additional supervision. We further show\nthat a guided version of the training process can explicitly govern which\nsummary style is partitioned between decoders, e.g. high abstractiveness vs.\nlow abstractiveness or high specificity vs. low specificity, and also increase\nthe stylistic-difference between individual decoders. Finally, our experiments\ndemonstrate that our decoder framework is highly flexible: during inference, we\ncan sample from individual decoders or mixtures of different subsets of the\ndecoders to yield a diverse set of summaries and enforce single- and\nmulti-style control over summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures. (arXiv:2110.15225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15225","description":"<p>Recent years have seen a growing adoption of Transformer models such as BERT\nin Natural Language Processing and even in Computer Vision. However, due to the\nsize, there has been limited adoption of such models within\nresource-constrained computing environments This paper proposes novel pruning\nalgorithms to compress transformer models by eliminating redundant Attention\nHeads. We apply the A* search algorithm to obtain a pruned model with minimal\naccuracy guarantees. Our results indicate that the method could eliminate as\nmuch as 40% of the attention heads in the BERT transformer model with almost no\nloss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnami_A/0/1/0/all/0/1\">Archit Parnami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying causal associations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01225","description":"<p>Objective: Leveraging machine learning methods, we aim to extract both\nexplicit and implicit cause-effect associations in patient-reported,\ndiabetes-related tweets and provide a tool to better understand opinion,\nfeelings and observations shared within the diabetes online community from a\ncausality perspective. Materials and Methods: More than 30 million\ndiabetes-related tweets in English were collected between April 2017 and\nJanuary 2021. Deep learning and natural language processing methods were\napplied to focus on tweets with personal and emotional content. A\ncause-effect-tweet dataset was manually labeled and used to train 1) a\nfine-tuned Bertweet model to detect causal sentences containing a causal\nassociation 2) a CRF model with BERT based features to extract possible\ncause-effect associations. Causes and effects were clustered in a\nsemi-supervised approach and visualised in an interactive cause-effect-network.\nResults: Causal sentences were detected with a recall of 68% in an imbalanced\ndataset. A CRF model with BERT based features outperformed a fine-tuned BERT\nmodel for cause-effect detection with a macro recall of 68%. This led to 96,676\nsentences with cause-effect associations. \"Diabetes\" was identified as the\ncentral cluster followed by \"Death\" and \"Insulin\". Insulin pricing related\ncauses were frequently associated with \"Death\". Conclusions: A novel\nmethodology was developed to detect causal sentences and identify both explicit\nand implicit, single and multi-word cause and corresponding effect as expressed\nin diabetes-related tweets leveraging BERT-based architectures and visualised\nas cause-effect-network. Extracting causal associations on real-life, patient\nreported outcomes in social media data provides a useful complementary source\nof information in diabetes research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahne_A/0/1/0/all/0/1\">Adrian Ahne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbessat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czernichow_T/0/1/0/all/0/1\">Thomas Czernichow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_F/0/1/0/all/0/1\">Francisco Orchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bour_C/0/1/0/all/0/1\">Charline Bour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagherazzi_G/0/1/0/all/0/1\">Guy Fagherazzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching. (arXiv:2111.02188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02188","description":"<p>This paper presents a deep neural architecture, for Natural Language Sentence\nMatching (NLSM) by adding a deep recursive encoder to BERT so called BERT with\nDeep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that\nBERT still does not capture the full complexity of text, so a deep recursive\nencoder is applied on top of BERT. Three Bi-LSTM layers with residual\nconnection are used to design a recursive encoder and an attention module is\nused on top of this encoder. To obtain the final vector, a pooling layer\nconsisting of average and maximum pooling is used. We experiment our model on\nfour benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian\nreligious questions dataset. This paper focuses on improving the BERT results\nin the NLSM task. In this regard, comparisons between BERT-DRE and BERT are\nconducted, and it is shown that in all cases, BERT-DRE outperforms BERT. The\nBERT algorithm on the religious dataset achieved an accuracy of 89.70%, and\nBERT-DRE architectures improved to 90.29% using the same dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavan_E/0/1/0/all/0/1\">Ehsan Tavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafi_M/0/1/0/all/0/1\">Maryam Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibak_S/0/1/0/all/0/1\">Saeed Bibak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_Z/0/1/0/all/0/1\">Zahed Rahmati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion Mining. (arXiv:2111.02259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02259","description":"<p>User-generated content from social media is produced in many languages,\nmaking it technically challenging to compare the discussed themes from one\ndomain across different cultures and regions. It is relevant for domains in a\nglobalized world, such as market research, where people from two nations and\nmarkets might have different requirements for a product. We propose a simple,\nmodern, and effective method for building a single topic model with sentiment\nanalysis capable of covering multiple languages simultanteously, based on a\npre-trained state-of-the-art deep neural network for natural language\nunderstanding. To demonstrate its feasibility, we apply the model to newspaper\narticles and user comments of a specific domain, i.e., organic food products\nand related consumption behavior. The themes match across languages.\nAdditionally, we obtain an high proportion of stable and domain-relevant\ntopics, a meaningful relation between topics and their respective textual\ncontents, and an interpretable representation for social media documents.\nMarketing can potentially benefit from our method, since it provides an\neasy-to-use means of addressing specific customer interests from different\nmarket regions around the globe. For reproducibility, we provide the code,\ndata, and results of our study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_W/0/1/0/all/0/1\">Wing Sheung Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiaoxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Transparency of Deep Neural Networks for Medical Image Analysis: A Review of Interpretability Methods. (arXiv:2111.02398v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02398","description":"<p>Artificial Intelligence has emerged as a useful aid in numerous clinical\napplications for diagnosis and treatment decisions. Deep neural networks have\nshown same or better performance than clinicians in many tasks owing to the\nrapid increase in the available data and computational power. In order to\nconform to the principles of trustworthy AI, it is essential that the AI system\nbe transparent, robust, fair and ensure accountability. Current deep neural\nsolutions are referred to as black-boxes due to a lack of understanding of the\nspecifics concerning the decision making process. Therefore, there is a need to\nensure interpretability of deep neural networks before they can be incorporated\nin the routine clinical workflow. In this narrative review, we utilized\nsystematic keyword searches and domain expertise to identify nine different\ntypes of interpretability methods that have been used for understanding deep\nlearning models for medical image analysis applications based on the type of\ngenerated explanations and technical similarities. Furthermore, we report the\nprogress made towards evaluating the explanations produced by various\ninterpretability methods. Finally we discuss limitations, provide guidelines\nfor using interpretability methods and future directions concerning the\ninterpretability of deep neural networks for medical imaging analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Salahuddin_Z/0/1/0/all/0/1\">Zohaib Salahuddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodruff_H/0/1/0/all/0/1\">Henry C Woodruff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambin_P/0/1/0/all/0/1\">Philippe Lambin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach. (arXiv:2111.02399v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02399","description":"<p>As a deep learning model typically contains millions of trainable weights,\nthere has been a growing demand for a more efficient network structure with\nreduced storage space and improved run-time efficiency. Pruning is one of the\nmost popular network compression techniques. In this paper, we propose a novel\nunstructured pruning pipeline, Attention-based Simultaneous sparse structure\nand Weight Learning (ASWL). Unlike traditional channel-wise or weight-wise\nattention mechanism, ASWL proposed an efficient algorithm to calculate the\npruning ratio through layer-wise attention for each layer, and both weights for\nthe dense network and the sparse network are tracked so that the pruned\nstructure is simultaneously learned from randomly initialized weights. Our\nexperiments on MNIST, Cifar10, and ImageNet show that ASWL achieves superior\npruning results in terms of accuracy, pruning ratio and operating efficiency\nwhen compared with state-of-the-art network pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qisheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_L/0/1/0/all/0/1\">Loren Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weisong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep AUC Maximization for Medical Image Classification: Challenges and Opportunities. (arXiv:2111.02400v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02400","description":"<p>In this extended abstract, we will present and discuss opportunities and\nchallenges brought about by a new deep learning method by AUC maximization (aka\n\\underline{\\bf D}eep \\underline{\\bf A}UC \\underline{\\bf M}aximization or {\\bf\nDAM}) for medical image classification. Since AUC (aka area under ROC curve) is\na standard performance measure for medical image classification, hence directly\noptimizing AUC could achieve a better performance for learning a deep neural\nnetwork than minimizing a traditional loss function (e.g., cross-entropy loss).\nRecently, there emerges a trend of using deep AUC maximization for large-scale\nmedical image classification. In this paper, we will discuss these recent\nresults by highlighting (i) the advancements brought by stochastic non-convex\noptimization algorithms for DAM; (ii) the promising results on various medical\nimage classification problems. Then, we will discuss challenges and\nopportunities of DAM for medical image classification from three perspectives,\nfeature learning, large-scale optimization, and learning trustworthy AI models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Cancer Classification using Inception Network and Transfer Learning. (arXiv:2111.02402v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02402","description":"<p>Medical data classification is typically a challenging task due to imbalance\nbetween classes. In this paper, we propose an approach to classify\ndermatoscopic images from HAM10000 (Human Against Machine with 10000 training\nimages) dataset, consisting of seven imbalanced types of skin lesions, with\ngood precision and low resources requirements. Classification is done by using\na pretrained convolutional neural network. We evaluate the accuracy and\nperformance of the proposal and illustrate possible extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Benedetti_P/0/1/0/all/0/1\">Priscilla Benedetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perri_D/0/1/0/all/0/1\">Damiano Perri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simonetti_M/0/1/0/all/0/1\">Marco Simonetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gervasi_O/0/1/0/all/0/1\">Osvaldo Gervasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reali_G/0/1/0/all/0/1\">Gianluca Reali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Femminella_M/0/1/0/all/0/1\">Mauro Femminella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WORD: Revisiting Organs Segmentation in the Whole Abdominal Region. (arXiv:2111.02403v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02403","description":"<p>Whole abdominal organs segmentation plays an important role in abdomen lesion\ndiagnosis, radiotherapy planning, and follow-up. However, delineating all\nabdominal organs by oncologists manually is time-consuming and very expensive.\nRecently, deep learning-based medical image segmentation has shown the\npotential to reduce manual delineation efforts, but it still requires a\nlarge-scale fine annotated dataset for training. Although many efforts in this\ntask, there are still few large image datasets covering the whole abdomen\nregion with accurate and detailed annotations for the whole abdominal organ\nsegmentation. In this work, we establish a large-scale \\textit{W}hole abdominal\n\\textit{OR}gans \\textit{D}ataset (\\textit{WORD}) for algorithms research and\nclinical applications development. This dataset contains 150 abdominal CT\nvolumes (30495 slices) and each volume has 16 organs with fine pixel-level\nannotations and scribble-based sparse annotation, which may be the largest\ndataset with whole abdominal organs annotation. Several state-of-the-art\nsegmentation methods are evaluated on this dataset. And, we also invited\nclinical oncologists to revise the model predictions to measure the gap between\nthe deep learning method and real oncologists. We further introduce and\nevaluate a new scribble-based weakly supervised segmentation on this dataset.\nThe work provided a new benchmark for the abdominal multi-organ segmentation\ntask and these experiments can serve as the baseline for future research and\nclinical application development. The codebase and dataset will be released at:\nhttps://github.com/HiLab-git/WORD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1\">Wenjun Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianghong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial supervision for the FeTA challenge 2021. (arXiv:2111.02408v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02408","description":"<p>This paper describes our method for our participation in the FeTA\nchallenge2021 (team name: TRABIT). The performance of convolutional neural\nnetworks for medical image segmentation is thought to correlate positively with\nthe number of training data. The FeTA challenge does not restrict participants\nto using only the provided training data but also allows for using other\npublicly available sources. Yet, open access fetal brain data remains limited.\nAn advantageous strategy could thus be to expand the training data to cover\nbroader perinatal brain imaging sources. Perinatal brain MRIs, other than the\nFeTA challenge data, that are currently publicly available, span normal and\npathological fetal atlases as well as neonatal scans. However, perinatal brain\nMRIs segmented in different datasets typically come with different annotation\nprotocols. This makes it challenging to combine those datasets to train a deep\nneural network. We recently proposed a family of loss functions, the label-set\nloss functions, for partially supervised learning. Label-set loss functions\nallow to train deep neural networks with partially segmented images, i.e.\nsegmentations in which some classes may be grouped into super-classes. We\npropose to use label-set loss functions to improve the segmentation performance\nof a state-of-the-art deep learning pipeline for multi-class fetal brain\nsegmentation by merging several publicly available datasets. To promote\ngeneralisability, our approach does not introduce any additional\nhyper-parameters tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1\">Philippe Demaerel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Classification Using: Pixel Interpolation. (arXiv:2111.02409v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02409","description":"<p>Image Processing represents the backbone research area within engineering and\ncomputer science specialization. It is promptly growing technologies today, and\nits applications founded in various aspects of biomedical fields especially in\ncancer disease. Breast cancer is considered the fatal one of all cancer types\naccording to recent statistics all over the world. It is the most commonly\ncancer in women and the second reason of cancer death between females. About\n23% of the total cancer cases in both developing and developed countries. In\nthis work, an interpolation process was used to classify the breast cancer into\nmain types, benign and malignant. This scheme dependent on the morphologic\nspectrum of mammographic masses. Malignant tumors had irregular shape percent\nhigher than the benign tumors. By this way the boundary of the tumor will be\ninterpolated by additional pixels to make the boundary smoothen as possible,\nthese needed pixels is proportional with irregularity shape of the tumor, so\nthat the increasing in interpolated pixels meaning the tumor goes toward the\nmalignant case. The proposed system is implemented using MATLAB programming and\ntested over several images taken from the Mammogram Image Analysis Society\n(MIAS) image database. The MIAS offers a regular classification for\nmammographic studies. The system works faster so that any radiologist can take\na clear decision about the appearance of calcifications by visual inspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shahin_O/0/1/0/all/0/1\">Osama Rezq Shahin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelash_H/0/1/0/all/0/1\">Hamdy Mohammed Kelash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Attiya_G/0/1/0/all/0/1\">Gamal Mahrous Attiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allah_O/0/1/0/all/0/1\">Osama Slah Farg Allah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic 3D Scene Reconstruction From a Single RGB Image. (arXiv:2111.02444v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02444","description":"<p>Understanding 3D scenes from a single image is fundamental to a wide variety\nof tasks, such as for robotics, motion planning, or augmented reality. Existing\nworks in 3D perception from a single RGB image tend to focus on geometric\nreconstruction only, or geometric reconstruction with semantic segmentation or\ninstance segmentation. Inspired by 2D panoptic segmentation, we propose to\nunify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D\ninstance segmentation into the task of panoptic 3D scene reconstruction - from\na single RGB image, predicting the complete geometric reconstruction of the\nscene in the camera frustum of the image, along with semantic and instance\nsegmentations. We thus propose a new approach for holistic 3D scene\nunderstanding from a single RGB image which learns to lift and propagate 2D\nfeatures from an input image to a 3D volumetric scene representation. We\ndemonstrate that this holistic view of joint scene reconstruction, semantic,\nand instance segmentation is beneficial over treating the tasks independently,\nthus outperforming alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahnert_M/0/1/0/all/0/1\">Manuel Dahnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Frequency Bias of Generative Models. (arXiv:2111.02447v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02447","description":"<p>The key objective of Generative Adversarial Networks (GANs) is to generate\nnew data with the same statistics as the provided training data. However,\nmultiple recent works show that state-of-the-art architectures yet struggle to\nachieve this goal. In particular, they report an elevated amount of high\nfrequencies in the spectral statistics which makes it straightforward to\ndistinguish real and generated images. Explanations for this phenomenon are\ncontroversial: While most works attribute the artifacts to the generator, other\nworks point to the discriminator. We take a sober look at those explanations\nand provide insights on what makes proposed measures against high-frequency\nartifacts effective. To achieve this, we first independently assess the\narchitectures of both the generator and discriminator and investigate if they\nexhibit a frequency bias that makes learning the distribution of high-frequency\ncontent particularly problematic. Based on these experiments, we make the\nfollowing four observations: 1) Different upsampling operations bias the\ngenerator towards different spectral properties. 2) Checkerboard artifacts\nintroduced by upsampling cannot explain the spectral discrepancies alone as the\ngenerator is able to compensate for these artifacts. 3) The discriminator does\nnot struggle with detecting high frequencies per se but rather struggles with\nfrequencies of low magnitude. 4) The downsampling operations in the\ndiscriminator can impair the quality of the training signal it provides. In\nlight of these findings, we analyze proposed measures against high-frequency\nartifacts in state-of-the-art GAN training but find that none of the existing\napproaches can fully resolve spectral artifacts yet. Our results suggest that\nthere is great potential in improving the discriminator and that this could be\nkey to match the distribution of the training data more closely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1\">Katja Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise. (arXiv:2111.02450v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02450","description":"<p>We propose an end-to-end unified 3D mesh recovery of humans and quadruped\nanimals trained in a weakly-supervised way. Unlike recent work focusing on a\nsingle target class only, we aim to recover 3D mesh of broader classes with a\nsingle multi-task model. However, there exists no dataset that can directly\nenable multi-task learning due to the absence of both human and animal\nannotations for a single object, e.g., a human image does not have animal pose\nannotations; thus, we have to devise a new way to exploit heterogeneous\ndatasets. To make the unstable disjoint multi-task learning jointly trainable,\nwe propose to exploit the morphological similarity between humans and animals,\nmotivated by animal exercise where humans imitate animal poses. We realize the\nmorphological similarity by semantic correspondences, called sub-keypoint,\nwhich enables joint training of human and animal mesh regression branches.\nBesides, we propose class-sensitive regularization methods to avoid a\nmean-shape bias and to improve the distinctiveness across multi-classes. Our\nmethod performs favorably against recent uni-modal models on various human and\nanimal datasets while being far more compact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1\">Kim Youwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Yeon_K/0/1/0/all/0/1\">Kim Ji-Yeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_K/0/1/0/all/0/1\">Kyungdon Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slapping Cats, Bopping Heads, and Oreo Shakes: Understanding Indicators of Virality in TikTok Short Videos. (arXiv:2111.02452v1 [cs.CY])","link":"http://arxiv.org/abs/2111.02452","description":"<p>Short videos have become one of the leading media used by younger generations\nto express themselves online and thus a driving force in shaping online\nculture. In this context, TikTok has emerged as a platform where viral videos\nare often posted first. In this paper, we study what elements of short videos\nposted on TikTok contribute to their virality. We apply a mixed-method approach\nto develop a codebook and identify important virality features. We do so\nvis-\\`a-vis three research hypotheses; namely, that: 1) the video content, 2)\nTikTok's recommendation algorithm, and 3) the popularity of the video creator\ncontribute to virality.\n</p>\n<p>We collect and label a dataset of 400 TikTok videos and train classifiers to\nhelp us identify the features that influence virality the most. While the\nnumber of followers is the most powerful predictor, close-up and medium-shot\nscales also play an essential role. So does the lifespan of the video, the\npresence of text, and the point of view. Our research highlights the\ncharacteristics that distinguish viral from non-viral TikTok videos, laying the\ngroundwork for developing additional approaches to create more engaging online\ncontent and proactively identify possibly risky content that is likely to reach\na large audience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blackburn_J/0/1/0/all/0/1\">Jeremy Blackburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristofaro_E/0/1/0/all/0/1\">Emiliano De Cristofaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stringhini_G/0/1/0/all/0/1\">Gianluca Stringhini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic ultrasound vessel segmentation with deep spatiotemporal context learning. (arXiv:2111.02461v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02461","description":"<p>Accurate, real-time segmentation of vessel structures in ultrasound image\nsequences can aid in the measurement of lumen diameters and assessment of\nvascular diseases. This, however, remains a challenging task, particularly for\nextremely small vessels that are difficult to visualize. We propose to leverage\nthe rich spatiotemporal context available in ultrasound to improve segmentation\nof small-scale lower-extremity arterial vasculature. We describe efficient deep\nlearning methods that incorporate temporal, spatial, and feature-aware\ncontextual embeddings at multiple resolution scales while jointly utilizing\ninformation from B-mode and Color Doppler signals. Evaluating on femoral and\ntibial artery scans performed on healthy subjects by an expert\nultrasonographer, and comparing to consensus expert ground-truth annotations of\ninner lumen boundaries, we demonstrate real-time segmentation using the\ncontext-aware models and show that they significantly outperform comparable\nbaseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_B/0/1/0/all/0/1\">Baichuan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alvin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bharat_S/0/1/0/all/0/1\">Shyam Bharat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_M/0/1/0/all/0/1\">Mingxin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadmap on Signal Processing for Next Generation Measurement Systems. (arXiv:2111.02493v1 [eess.SP])","link":"http://arxiv.org/abs/2111.02493","description":"<p>Signal processing is a fundamental component of almost any sensor-enabled\nsystem, with a wide range of applications across different scientific\ndisciplines. Time series data, images, and video sequences comprise\nrepresentative forms of signals that can be enhanced and analysed for\ninformation extraction and quantification. The recent advances in artificial\nintelligence and machine learning are shifting the research attention towards\nintelligent, data-driven, signal processing. This roadmap presents a critical\noverview of the state-of-the-art methods and applications aiming to highlight\nfuture challenges and research opportunities towards next generation\nmeasurement systems. It covers a broad spectrum of topics ranging from basic to\nindustrial research, organized in concise thematic sections that reflect the\ntrends and the impacts of current and future developments per research field.\nFurthermore, it offers guidance to researchers and funding agencies in\nidentifying new prospects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iakovidis_D/0/1/0/all/0/1\">D.K. Iakovidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ooi_M/0/1/0/all/0/1\">M. Ooi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_Y/0/1/0/all/0/1\">Y.C. Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Damidenko_S/0/1/0/all/0/1\">S. Damidenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shestakov_A/0/1/0/all/0/1\">A. Shestakov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinistin_V/0/1/0/all/0/1\">V. Sinistin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henry_M/0/1/0/all/0/1\">M. Henry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sciacchitano_A/0/1/0/all/0/1\">A. Sciacchitano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Discetti_A/0/1/0/all/0/1\">A. Discetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Donati_S/0/1/0/all/0/1\">S. Donati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norgia_M/0/1/0/all/0/1\">M. Norgia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menychtas_A/0/1/0/all/0/1\">A. Menychtas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maglogiannis_I/0/1/0/all/0/1\">I. Maglogiannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wriessnegger_S/0/1/0/all/0/1\">S.C. Wriessnegger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chacon_L/0/1/0/all/0/1\">L.A. Barradas Chacon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimas_G/0/1/0/all/0/1\">G. Dimas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filos_D/0/1/0/all/0/1\">D. Filos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aletras_A/0/1/0/all/0/1\">A.H. Aletras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toger_J/0/1/0/all/0/1\">J. T&#xf6;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1\">F. Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uhl_A/0/1/0/all/0/1\">A. Uhl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paziewski_J/0/1/0/all/0/1\">J. Paziewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_J/0/1/0/all/0/1\">J. Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fioranelli_F/0/1/0/all/0/1\">F. Fioranelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_R/0/1/0/all/0/1\">R.M. Narayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_C/0/1/0/all/0/1\">C. Fernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stiller_C/0/1/0/all/0/1\">C. Stiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malamousi_K/0/1/0/all/0/1\">K. Malamousi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamnis_S/0/1/0/all/0/1\">S. Kamnis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delibasis_K/0/1/0/all/0/1\">K. Delibasis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">D. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">J. Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">R.X. Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pose Estimation through Contextual Activity Fusion. (arXiv:2111.02500v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02500","description":"<p>This research presents the idea of activity fusion into existing Pose\nEstimation architectures to enhance their predictive ability. This is motivated\nby the rise in higher level concepts found in modern machine learning\narchitectures, and the belief that activity context is a useful piece of\ninformation for the problem of pose estimation. To analyse this concept we take\nan existing deep learning architecture and augment it with an additional 1x1\nconvolution to fuse activity information into the model. We perform evaluation\nand comparison on a common pose estimation dataset, and show a performance\nimprovement over our baseline model, especially in uncommon poses and on\ntypically difficult joints. Additionally, we perform an ablative analysis to\nindicate that the performance improvement does in fact draw from the activity\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poulton_D/0/1/0/all/0/1\">David Poulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1\">Richard Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resampling and super-resolution of hexagonally sampled images using deep learning. (arXiv:2111.02520v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02520","description":"<p>Super-resolution (SR) aims to increase the resolution of imagery.\nApplications include security, medical imaging, and object recognition. We\npropose a deep learning-based SR system that takes a hexagonally sampled\nlow-resolution image as an input and generates a rectangularly sampled SR image\nas an output. For training and testing, we use a realistic observation model\nthat includes optical degradation from diffraction and sensor degradation from\ndetector integration. Our SR approach first uses non-uniform interpolation to\npartially upsample the observed hexagonal imagery and convert it to a\nrectangular grid. We then leverage a state-of-the-art convolutional neural\nnetwork (CNN) architecture designed for SR known as Residual Channel Attention\nNetwork (RCAN). In particular, we use RCAN to further upsample and restore the\nimagery to produce the final SR image estimate. We demonstrate that this system\nis superior to applying RCAN directly to rectangularly sampled LR imagery with\nequivalent sample density. The theoretical advantages of hexagonal sampling are\nwell known. However, to the best of our knowledge, the practical benefit of\nhexagonal sampling in light of modern processing techniques such as RCAN SR is\nheretofore untested. Our SR system demonstrates a notable advantage of\nhexagonally sampled imagery when employing a modified RCAN for hexagonal SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Flaute_D/0/1/0/all/0/1\">Dylan Flaute</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hardie_R/0/1/0/all/0/1\">Russell C. Hardie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elwarfalli_H/0/1/0/all/0/1\">Hamed Elwarfalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Modeling for Action Identification at High Temporal Resolution. (arXiv:2111.02521v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02521","description":"<p>Automatic action identification from video and kinematic data is an important\nmachine learning problem with applications ranging from robotics to smart\nhealth. Most existing works focus on identifying coarse actions such as\nrunning, climbing, or cutting a vegetable, which have relatively long\ndurations. This is an important limitation for applications that require the\nidentification of subtle motions at high temporal resolution. For example, in\nstroke recovery, quantifying rehabilitation dose requires differentiating\nmotions with sub-second durations. Our goal is to bridge this gap. To this end,\nwe introduce a large-scale, multimodal dataset, StrokeRehab, as a new\naction-recognition benchmark that includes subtle short-duration actions\nlabeled at a high temporal resolution. These short-duration actions are called\nfunctional primitives, and consist of reaches, transports, repositions,\nstabilizations, and idles. The dataset consists of high-quality Inertial\nMeasurement Unit sensors and video data of 41 stroke-impaired patients\nperforming activities of daily living like feeding, brushing teeth, etc. We\nshow that current state-of-the-art models based on segmentation produce noisy\npredictions when applied to these data, which often leads to overcounting of\nactions. To address this, we propose a novel approach for high-resolution\naction identification, inspired by speech-recognition techniques, which is\nbased on a sequence-to-sequence model that directly predicts the sequence of\nactions. This approach outperforms current state-of-the-art methods on the\nStrokeRehab dataset, as well as on the standard benchmark datasets 50Salads,\nBreakfast, and Jigsaws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnandi_A/0/1/0/all/0/1\">Avinash Parnandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajamohan_H/0/1/0/all/0/1\">Haresh Rengaraj Rajamohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_K/0/1/0/all/0/1\">Kannan Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1\">Anita Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirtanen_A/0/1/0/all/0/1\">Audre Wirtanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandit_N/0/1/0/all/0/1\">Natasha Pandit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schambra_H/0/1/0/all/0/1\">Heidi Schambra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Cross Domain Presentation Attack Detection for Visible Face Recognition. (arXiv:2111.02548v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02548","description":"<p>Face signatures, including size, shape, texture, skin tone, eye color,\nappearance, and scars/marks, are widely used as discriminative, biometric\ninformation for access control. Despite recent advancements in facial\nrecognition systems, presentation attacks on facial recognition systems have\nbecome increasingly sophisticated. The ability to detect presentation attacks\nor spoofing attempts is a pressing concern for the integrity, security, and\ntrust of facial recognition systems. Multi-spectral imaging has been previously\nintroduced as a way to improve presentation attack detection by utilizing\nsensors that are sensitive to different regions of the electromagnetic spectrum\n(e.g., visible, near infrared, long-wave infrared). Although multi-spectral\npresentation attack detection systems may be discriminative, the need for\nadditional sensors and computational resources substantially increases\ncomplexity and costs. Instead, we propose a method that exploits information\nfrom infrared imagery during training to increase the discriminability of\nvisible-based presentation attack detection systems. We introduce (1) a new\ncross-domain presentation attack detection framework that increases the\nseparability of bonafide and presentation attacks using only visible spectrum\nimagery, (2) an inverse domain regularization technique for added training\nstability when optimizing our cross-domain presentation attack detection\nframework, and (3) a dense domain adaptation subnetwork to transform\nrepresentations between visible and non-visible domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamblin_J/0/1/0/all/0/1\">Jennifer Hamblin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikhal_K/0/1/0/all/0/1\">Kshitij Nikhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riggan_B/0/1/0/all/0/1\">Benjamin S. Riggan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Damage Mapping with Self-PositiveUnlabeled Learning. (arXiv:2111.02586v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02586","description":"<p>Humanitarian organizations must have fast and reliable data to respond to\ndisasters. Deep learning approaches are difficult to implement in real-world\ndisasters because it might be challenging to collect ground truth data of the\ndamage situation (training data) soon after the event. The implementation of\nrecent self-paced positive-unlabeled learning (PU) is demonstrated in this work\nby successfully applying to building damage assessment with very limited\nlabeled data and a large amount of unlabeled data. Self-PU learning is compared\nwith the supervised baselines and traditional PU learning using different\ndatasets collected from the 2011 Tohoku earthquake, the 2018 Palu tsunami, and\nthe 2018 Hurricane Michael. By utilizing only a portion of labeled damaged\nsamples, we show how models trained with self-PU techniques may achieve\ncomparable performance as supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Junshi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adriano_B/0/1/0/all/0/1\">Bruno Adriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples. (arXiv:2111.02625v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02625","description":"<p>Model quantization is known as a promising method to compress deep neural\nnetworks, especially for inferences on lightweight mobile or edge devices.\nHowever, model quantization usually requires access to the original training\ndata to maintain the accuracy of the full-precision models, which is often\ninfeasible in real-world scenarios for security and privacy issues. A popular\napproach to perform quantization without access to the original data is to use\nsynthetically generated samples, based on batch-normalization statistics or\nadversarial learning. However, the drawback of such approaches is that they\nprimarily rely on random noise input to the generator to attain diversity of\nthe synthetic samples. We find that this is often insufficient to capture the\ndistribution of the original data, especially around the decision boundaries.\nTo this end, we propose Qimera, a method that uses superposed latent embeddings\nto generate synthetic boundary supporting samples. For the superposed\nembeddings to better reflect the original distribution, we also propose using\nan additional disentanglement mapping layer and extracting information from the\nfull-precision model. The experimental results show that Qimera achieves\nstate-of-the-art performances for various settings on data-free quantization.\nCode is available at https://github.com/iamkanghyunchoi/qimera.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kanghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Deokki Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Fusion Based Mutli-scale Semantic Segmentation for Detecting Concealed Baggage Threats. (arXiv:2111.02651v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02651","description":"<p>Detection of illegal and threatening items in baggage is one of the utmost\nsecurity concern nowadays. Even for experienced security personnel, manual\ndetection is a time-consuming and stressful task. Many academics have created\nautomated frameworks for detecting suspicious and contraband data from X-ray\nscans of luggage. However, to our knowledge, no framework exists that utilizes\ntemporal baggage X-ray imagery to effectively screen highly concealed and\noccluded objects which are barely visible even to the naked eye. To address\nthis, we present a novel temporal fusion driven multi-scale residual fashioned\nencoder-decoder that takes series of consecutive scans as input and fuses them\nto generate distinct feature representations of the suspicious and\nnon-suspicious baggage content, leading towards a more accurate extraction of\nthe contraband data. The proposed methodology has been thoroughly tested using\nthe publicly accessible GDXray dataset, which is the only dataset containing\ntemporally linked grayscale X-ray scans showcasing extremely concealed\ncontraband data. The proposed framework outperforms its competitors on the\nGDXray dataset on various metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafay_M/0/1/0/all/0/1\">Muhammed Shafay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damiani_E/0/1/0/all/0/1\">Ernesto Damiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LVIS Challenge Track Technical Report 1st Place Solution: Distribution Balanced and Boundary Refinement for Large Vocabulary Instance Segmentation. (arXiv:2111.02668v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02668","description":"<p>This report introduces the technical details of the team FuXi-Fresher for\nLVIS Challenge 2021. Our method focuses on the problem in following two\naspects: the long-tail distribution and the segmentation quality of mask and\nboundary. Based on the advanced HTC instance segmentation algorithm, we connect\ntransformer backbone(Swin-L) through composite connections inspired by CBNetv2\nto enhance the baseline results. To alleviate the problem of long-tail\ndistribution, we design a Distribution Balanced method which includes dataset\nbalanced and loss function balaced modules. Further, we use a Mask and Boundary\nRefinement method composed with mask scoring and refine-mask algorithms to\nimprove the segmentation quality. In addition, we are pleasantly surprised to\nfind that early stopping combined with EMA method can achieve a great\nimprovement. Finally, by using multi-scale testing and increasing the upper\nlimit of the number of objects detected per image, we achieved more than 45.4%\nboundary AP on the val set of LVIS Challenge 2021. On the test data of LVIS\nChallenge 2021, we rank 1st and achieve 48.1% AP. Notably, our APr 47.5% is\nvery closed to the APf 48.0%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">WeiFu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">CongChong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Ting Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">TianLiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A semi-automatic ultrasound image analysis system for the grading diagnosis of COVID-19 pneumonia. (arXiv:2111.02676v1 [physics.med-ph])","link":"http://arxiv.org/abs/2111.02676","description":"<p>This paper proposes a semi-automatic system based on quantitative\ncharacterization of the specific image patterns in lung ultrasound (LUS)\nimages, in order to assess the lung conditions of patients with COVID-19\npneumonia, as well as to differentiate between the severe / and no-severe\ncases. Specifically, four parameters are extracted from each LUS image, namely\nthe thickness (TPL) and roughness (RPL) of the pleural line, and the\naccumulated with (AWBL) and acoustic coefficient (ACBL) of B lines. 27 patients\nare enrolled in this study, which are grouped into 13 moderate patients, 7\nsevere patients and 7 critical patients. Furthermore, the severe and critical\npatients are regarded as the severe cases, and the moderate patients are\nregarded as the non-severe cases. Biomarkers among different groups are\ncompared. Each single biomarker and a classifier with all the biomarkers as\ninput are utilized for the binary diagnosis of severe case and non-severe case,\nrespectively. The classifier achieves the best classification performance among\nall the compared methods (area under the receiver operating characteristics\ncurve = 0.93, sensitivity = 0.93, specificity = 0.85). The proposed image\nanalysis system could be potentially applied to the grading and prognosis\nevaluation of patients with COVID-19 pneumonia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanyuan Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+He_Q/0/1/0/all/0/1\">Qiong He</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liao_H/0/1/0/all/0/1\">Hongen Liao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_J/0/1/0/all/0/1\">Jianwen Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSiam: A Mixture-based Approach to Self-supervised Representation Learning. (arXiv:2111.02679v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02679","description":"<p>Recently contrastive learning has shown significant progress in learning\nvisual representations from unlabeled data. The core idea is training the\nbackbone to be invariant to different augmentations of an instance. While most\nmethods only maximize the feature similarity between two augmented data, we\nfurther generate more challenging training samples and force the model to keep\npredicting discriminative representation on these hard samples. In this paper,\nwe propose MixSiam, a mixture-based approach upon the traditional siamese\nnetwork. On the one hand, we input two augmented images of an instance to the\nbackbone and obtain the discriminative representation by performing an\nelement-wise maximum of two features. On the other hand, we take the mixture of\nthese augmented images as input, and expect the model prediction to be close to\nthe discriminative representation. In this way, the model could access more\nvariant data samples of an instance and keep predicting invariant\ndiscriminative representations for them. Thus the learned model is more robust\ncompared to previous contrastive learning methods. Extensive experiments on\nlarge-scale datasets show that MixSiam steadily improves the baseline and\nachieves competitive results with state-of-the-art methods. Our code will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianhao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation. (arXiv:2111.02682v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02682","description":"<p>The recent developments of deep learning models that capture the complex\ntemporal patterns of crop phenology have greatly advanced crop classification\nof Satellite Image Time Series (SITS). However, when applied to target regions\nspatially different from the training region, these models perform poorly\nwithout any target labels due to the temporal shift of crop phenology between\nregions. To address this unsupervised cross-region adaptation setting, existing\nmethods learn domain-invariant features without any target supervision, but not\nthe temporal shift itself. As a consequence, these techniques provide only\nlimited benefits for SITS. In this paper, we propose TimeMatch, a new\nunsupervised domain adaptation method for SITS that directly accounts for the\ntemporal shift. TimeMatch consists of two components: 1) temporal shift\nestimation, which estimates the temporal shift of the unlabeled target region\nwith a source-trained model, and 2) TimeMatch learning, which combines temporal\nshift estimation with semi-supervised learning to adapt a classifier to an\nunlabeled target region. We also introduce an open-access dataset for\ncross-region adaptation with SITS from four different regions in Europe. On\nthis dataset, we demonstrate that TimeMatch outperforms all competing methods\nby 11% in F1-score across five different adaptation scenarios, setting a new\nstate-of-the-art for cross-region adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1\">Joachim Nyborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1\">Charlotte Pelletier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1\">Ira Assent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Smart Monitored AM: Open Source in-Situ Layer-wise 3D Printing Image Anomaly Detection Using Histograms of Oriented Gradients and a Physics-Based Rendering Engine. (arXiv:2111.02703v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02703","description":"<p>This study presents an open source method for detecting 3D printing anomalies\nby comparing images of printed layers from a stationary monocular camera with\nG-code-based reference images of an ideal process generated with Blender, a\nphysics rendering engine. Recognition of visual deviations was accomplished by\nanalyzing the similarity of histograms of oriented gradients (HOG) of local\nimage areas. The developed technique requires preliminary modeling of the\nworking environment to achieve the best match for orientation, color rendering,\nlighting, and other parameters of the printed part. The output of the algorithm\nis a level of mismatch between printed and synthetic reference layers. Twelve\nsimilarity and distance measures were implemented and compared for their\neffectiveness at detecting 3D printing errors on six different representative\nfailure types and their control error-free print images. The results show that\nalthough Kendall tau, Jaccard, and Sorensen similarities are the most\nsensitive, Pearson r, Spearman rho, cosine, and Dice similarities produce the\nmore reliable results. This open source method allows the program to notice\ncritical errors in the early stages of their occurrence and either pause\nmanufacturing processes for further investigation by an operator or in the\nfuture AI-controlled automatic error correction. The implementation of this\nnovel method does not require preliminary data for training, and the greatest\nefficiency can be achieved with the mass production of parts by either additive\nor subtractive manufacturing of the same geometric shape. It can be concluded\nthis open source method is a promising means of enabling smart distributed\nrecycling for additive manufacturing using complex feedstocks as well as other\nchallenging manufacturing environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petsiuk_A/0/1/0/all/0/1\">Aliaksei Petsiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_J/0/1/0/all/0/1\">Joshua M. Pearce</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards dynamic multi-modal phenotyping using chest radiographs and physiological data. (arXiv:2111.02710v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02710","description":"<p>The healthcare domain is characterized by heterogeneous data modalities, such\nas imaging and physiological data. In practice, the variety of medical data\nassists clinicians in decision-making. However, most of the current\nstate-of-the-art deep learning models solely rely upon carefully curated data\nof a single modality. In this paper, we propose a dynamic training approach to\nlearn modality-specific data representations and to integrate auxiliary\nfeatures, instead of solely relying on a single modality. Our preliminary\nexperiments results for a patient phenotyping task using physiological data in\nMIMIC-IV &amp; chest radiographs in the MIMIC- CXR dataset show that our proposed\napproach achieves the highest area under the receiver operating characteristic\ncurve (AUROC) (0.764 AUROC) compared to the performance of the benchmark method\nin previous work, which only used physiological data (0.740 AUROC). For a set\nof five recurring or chronic diseases with periodic acute episodes, including\ncardiac dysrhythmia, conduction disorders, and congestive heart failure, the\nAUROC improves from 0.747 to 0.798. This illustrates the benefit of leveraging\nthe chest imaging modality in the phenotyping task and highlights the potential\nof multi-modal learning in medical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Emotion Recognition using Deep Residual Networks in Real-World Environments. (arXiv:2111.02717v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02717","description":"<p>Automatic affect recognition using visual cues is an important task towards a\ncomplete interaction between humans and machines. Applications can be found in\ntutoring systems and human computer interaction. A critical step towards that\ndirection is facial feature extraction. In this paper, we propose a facial\nfeature extractor model trained on an in-the-wild and massively collected video\ndataset provided by the RealEyes company. The dataset consists of a million\nlabelled frames and 2,616 thousand subjects. As temporal information is\nimportant to the emotion recognition domain, we utilise LSTM cells to capture\nthe temporal dynamics in the data. To show the favourable properties of our\npre-trained model on modelling facial affect, we use the RECOLA database, and\ncompare with the current state-of-the-art approach. Our model provides the best\nresults in terms of concordance correlation coefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boros_D/0/1/0/all/0/1\">D&#xe9;nes Boros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiyev_E/0/1/0/all/0/1\">Elnar Hajiyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tea Chrysanthemum Detection under Unstructured Environments Using the TC-YOLO Model. (arXiv:2111.02724v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02724","description":"<p>Tea chrysanthemum detection at its flowering stage is one of the key\ncomponents for selective chrysanthemum harvesting robot development. However,\nit is a challenge to detect flowering chrysanthemums under unstructured field\nenvironments given the variations on illumination, occlusion and object scale.\nIn this context, we propose a highly fused and lightweight deep learning\narchitecture based on YOLO for tea chrysanthemum detection (TC-YOLO). First, in\nthe backbone component and neck component, the method uses the Cross-Stage\nPartially Dense Network (CSPDenseNet) as the main network, and embeds custom\nfeature fusion modules to guide the gradient flow. In the final head component,\nthe method combines the recursive feature pyramid (RFP) multiscale fusion\nreflow structure and the Atrous Spatial Pyramid Pool (ASPP) module with cavity\nconvolution to achieve the detection task. The resulting model was tested on\n300 field images, showing that under the NVIDIA Tesla P100 GPU environment, if\nthe inference speed is 47.23 FPS for each image (416 * 416), TC-YOLO can\nachieve the average precision (AP) of 92.49% on our own tea chrysanthemum\ndataset. In addition, this method (13.6M) can be deployed on a single mobile\nGPU, and it could be further developed as a perception system for a selective\nchrysanthemum harvesting robot in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chao Qi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junfeng Gao</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_S/0/1/0/all/0/1\">Simon Pearson</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Harman_H/0/1/0/all/0/1\">Helen Harman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunjie Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a> (1) ((1) Nanjing Agricultural University, (2) University of Lincoln)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Neural Networks Using Different Sensors Create Similar Features. (arXiv:2111.02732v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02732","description":"<p>Multimodal problems are omnipresent in the real world: autonomous driving,\nrobotic grasping, scene understanding, etc... We draw from the well-developed\nanalysis of similarity to provide an example of a problem where neural networks\nare trained from different sensors, and where the features extracted from these\nsensors still carry similar information. More precisely, we demonstrate that\nfor each sensor, the linear combination of the features from the last layer\nthat correlates the most with other sensors corresponds to the classification\ncomponents of the classification layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreau_H/0/1/0/all/0/1\">Hugues Moreau</a> (CEA-LETI, LIRIS), <a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1\">Andr&#xe9;a Vassilev</a> (CEA-LETI), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a> (LIRIS, ECL)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods for Daily Wildfire Danger Forecasting. (arXiv:2111.02736v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02736","description":"<p>Wildfire forecasting is of paramount importance for disaster risk reduction\nand environmental sustainability. We approach daily fire danger prediction as a\nmachine learning task, using historical Earth observation data from the last\ndecade to predict next-day's fire danger. To that end, we collect, pre-process\nand harmonize an open-access datacube, featuring a set of covariates that\njointly affect the fire occurrence and spread, such as weather conditions,\nsatellite-derived products, topography features and variables related to human\nactivity. We implement a variety of Deep Learning (DL) models to capture the\nspatial, temporal or spatio-temporal context and compare them against a Random\nForest (RF) baseline. We find that either spatial or temporal context is enough\nto surpass the RF, while a ConvLSTM that exploits the spatio-temporal context\nperforms best with a test Area Under the Receiver Operating Characteristic of\n0.926. Our DL-based proof-of-concept provides national-scale daily fire danger\nmaps at a much higher spatial resolution than existing operational solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1\">Ioannis Prapas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondylatos_S/0/1/0/all/0/1\">Spyros Kondylatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1\">Gustau Camps-Valls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronco_M/0/1/0/all/0/1\">Michele Ronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Torres_M/0/1/0/all/0/1\">Miguel-&#xc1;ngel Fern&#xe1;ndez-Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillem_M/0/1/0/all/0/1\">Maria Piles Guillem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalhais_N/0/1/0/all/0/1\">Nuno Carvalhais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale 2D Representation Learning for weakly-supervised moment retrieval. (arXiv:2111.02741v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02741","description":"<p>Video moment retrieval aims to search the moment most relevant to a given\nlanguage query. However, most existing methods in this community often require\ntemporal boundary annotations which are expensive and time-consuming to label.\nHence weakly supervised methods have been put forward recently by only using\ncoarse video-level label. Despite effectiveness, these methods usually process\nmoment candidates independently, while ignoring a critical issue that the\nnatural temporal dependencies between candidates in different temporal scales.\nTo cope with this issue, we propose a Multi-scale 2D Representation Learning\nmethod for weakly supervised video moment retrieval. Specifically, we first\nconstruct a two-dimensional map for each temporal scale to capture the temporal\ndependencies between candidates. Two dimensions in this map indicate the start\nand end time points of these candidates. Then, we select top-K candidates from\neach scale-varied map with a learnable convolutional neural network. With a\nnewly designed Moments Evaluation Module, we obtain the alignment scores of the\nselected candidates. At last, the similarity between captions and language\nquery is served as supervision for further training the candidates' selector.\nExperiments on two benchmark datasets Charades-STA and ActivityNet Captions\ndemonstrate that our approach achieves superior performance to state-of-the-art\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Ding Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yongqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEAFA+: An Extended Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation. (arXiv:2111.02751v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02751","description":"<p>Nearly all existing Facial Action Coding System-based datasets that include\nfacial action unit (AU) intensity information annotate the intensity values\nhierarchically using A--E levels. However, facial expressions change\ncontinuously and shift smoothly from one state to another. Therefore, it is\nmore effective to regress the intensity value of local facial AUs to represent\nwhole facial expression changes, particularly in the fields of expression\ntransfer and facial animation. We introduce an extension of FEAFA in\ncombination with the relabeled DISFA database, which is available at\nhttps://www.iiplab.net/feafa+/ now. Extended FEAFA (FEAFA+) includes 150 video\nsequences from FEAFA and DISFA, with a total of 230,184 frames being manually\nannotated on floating-point intensity value of 24 redefined AUs using the\nExpression Quantitative Tool. We also list crude numerical results for posed\nand spontaneous subsets and provide a baseline comparison for the AU intensity\nregression task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Wei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yanfu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiayi Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning via Multiple Deep Metric Learning and Uncertainty-guided Episodic Memory Replay -- 3rd Place Solution for ICCV 2021 Workshop SSLAD Track 3A Continual Object Classification. (arXiv:2111.02757v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02757","description":"<p>Online continual learning in the wild is a very difficult task in machine\nlearning. Non-stationarity in online continual learning potentially brings\nabout catastrophic forgetting in neural networks. Specifically, online\ncontinual learning for autonomous driving with SODA10M dataset exhibits extra\nproblems on extremely long-tailed distribution with continuous distribution\nshift. To address these problems, we propose multiple deep metric\nrepresentation learning via both contrastive and supervised contrastive\nlearning alongside soft labels distillation to improve model generalization.\nMoreover, we exploit modified class-balanced focal loss for sensitive\npenalization in class imbalanced and hard-easy samples. We also store some\nsamples under guidance of uncertainty metric for rehearsal and perform online\nand periodical memory updates. Our proposed method achieves considerable\ngeneralization with average mean class accuracy (AMCA) 64.01% on validation and\n64.53% AMCA on test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_M/0/1/0/all/0/1\">Muhammad Rifki Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The role of MRI physics in brain segmentation CNNs: achieving acquisition invariance and instructive uncertainties. (arXiv:2111.02771v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02771","description":"<p>Being able to adequately process and combine data arising from different\nsites is crucial in neuroimaging, but is difficult, owing to site, sequence and\nacquisition-parameter dependent biases. It is important therefore to design\nalgorithms that are not only robust to images of differing contrasts, but also\nbe able to generalise well to unseen ones, with a quantifiable measure of\nuncertainty. In this paper we demonstrate the efficacy of a physics-informed,\nuncertainty-aware, segmentation network that employs augmentation-time MR\nsimulations and homogeneous batch feature stratification to achieve acquisition\ninvariance. We show that the proposed approach also accurately extrapolates to\nout-of-distribution sequence samples, providing well calibrated volumetric\nbounds on these. We demonstrate a significant improvement in terms of\ncoefficients of variation, backed by uncertainty based volumetric validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Borges_P/0/1/0/all/0/1\">Pedro Borges</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaw_R/0/1/0/all/0/1\">Richard Shaw</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varsavsky_T/0/1/0/all/0/1\">Thomas Varsavsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klaser_K/0/1/0/all/0/1\">Kerstin Klaser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_D/0/1/0/all/0/1\">David Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drobnjak_I/0/1/0/all/0/1\">Ivana Drobnjak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable and Compact Face Recognition via Unlabeled Data Driven Sparse Representation-Based Classification. (arXiv:2111.02847v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02847","description":"<p>Sparse representation-based classification (SRC) has attracted much attention\nby casting the recognition problem as simple linear regression problem. SRC\nmethods, however, still is limited to enough labeled samples per category,\ninsufficient use of unlabeled samples, and instability of representation. For\ntackling these problems, an unlabeled data driven inverse projection\npseudo-full-space representation-based classification model is proposed with\nlow-rank sparse constraints. The proposed model aims to mine the hidden\nsemantic information and intrinsic structure information of all available data,\nwhich is suitable for few labeled samples and proportion imbalance between\nlabeled samples and unlabeled samples problems in frontal face recognition. The\nmixed Gauss-Seidel and Jacobian ADMM algorithm is introduced to solve the\nmodel. The convergence, representation capability and stability of the model\nare analyzed. Experiments on three public datasets show that the proposed\nLR-S-PFSRC model achieves stable results, especially for proportion imbalance\nof samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haolin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing using Privileged Information by Adapting Features with Statistical Dependence. (arXiv:2111.02865v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02865","description":"<p>Given an imperfect predictor, we exploit additional features at test time to\nimprove the predictions made, without retraining and without knowledge of the\nprediction function. This scenario arises if training labels or data are\nproprietary, restricted, or no longer available, or if training itself is\nprohibitively expensive. We assume that the additional features are useful if\nthey exhibit strong statistical dependence to the underlying perfect predictor.\nThen, we empirically estimate and strengthen the statistical dependence between\nthe initial noisy predictor and the additional features via manifold denoising.\nAs an example, we show that this approach leads to improvement in real-world\nvisual attribute ranking. Project webpage: <a href=\"http://www.jamestompkin.com/tupi\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extended Abstract Version: CNN-based Human Detection System for UAVs in Search and Rescue. (arXiv:2111.02870v1 [cs.RO])","link":"http://arxiv.org/abs/2111.02870","description":"<p>This paper proposes an approach for the task of searching and detecting human\nusing a convolutional neural network and a Quadcopter hardware platform. A\npre-trained CNN model is applied to a Raspberry Pi B and a single camera is\nequipped at the bottom of the Quadcopter. The Quadcopter uses\naccelerometer-gyroscope sensor and ultrasonic sensor for balancing control.\nHowever, these sensors are susceptible to noise caused by the driving forces\nsuch as the vibration of the motors, thus, noise processing is implemented.\nExperiments proved that the system works well on the Raspberry Pi B with a\nprocessing speed of 3 fps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mesvan_N/0/1/0/all/0/1\">Nikite Mesvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certainty Volume Prediction for Unsupervised Domain Adaptation. (arXiv:2111.02901v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02901","description":"<p>Unsupervised domain adaptation (UDA) deals with the problem of classifying\nunlabeled target domain data while labeled data is only available for a\ndifferent source domain. Unfortunately, commonly used classification methods\ncannot fulfill this task adequately due to the domain gap between the source\nand target data. In this paper, we propose a novel uncertainty-aware domain\nadaptation setup that models uncertainty as a multivariate Gaussian\ndistribution in feature space. We show that our proposed uncertainty measure\ncorrelates with other common uncertainty quantifications and relates to\nsmoothing the classifier's decision boundary, therefore improving the\ngeneralization capabilities. We evaluate our proposed pipeline on challenging\nUDA datasets and achieve state-of-the-art results. Code for our method is\navailable at https://gitlab.com/tringwald/cvp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ringwald_T/0/1/0/all/0/1\">Tobias Ringwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Change Detection of Extreme Events Using ML On-Board. (arXiv:2111.02995v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02995","description":"<p>In this paper, we introduce RaVAEn, a lightweight, unsupervised approach for\nchange detection in satellite data based on Variational Auto-Encoders (VAEs)\nwith the specific purpose of on-board deployment. Applications such as disaster\nmanagement enormously benefit from the rapid availability of satellite\nobservations. Traditionally, data analysis is performed on the ground after all\ndata is transferred - downlinked - to a ground station. Constraint on the\ndownlink capabilities therefore affects any downstream application. In\ncontrast, RaVAEn pre-processes the sampled data directly on the satellite and\nflags changed areas to prioritise for downlink, shortening the response time.\nWe verified the efficacy of our system on a dataset composed of time series of\ncatastrophic events - which we plan to release alongside this publication -\ndemonstrating that RaVAEn outperforms pixel-wise baselines. Finally we tested\nour approach on resource-limited hardware for assessing computational and\nmemory limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R%5Cr%7Bu%7Dzicka_V/0/1/0/all/0/1\">V&#xed;t R&#x16f;&#x17e;i&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaughan_A/0/1/0/all/0/1\">Anna Vaughan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1\">Daniele De Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_J/0/1/0/all/0/1\">James Fulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1\">Valentina Salvatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bridges_C/0/1/0/all/0/1\">Chris Bridges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateo_Garcia_G/0/1/0/all/0/1\">Gonzalo Mateo-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1\">Valentina Zantedeschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Panoptic 3D Parsing for Single Image in the Wild. (arXiv:2111.03039v1 [cs.CV])","link":"http://arxiv.org/abs/2111.03039","description":"<p>Performing single image holistic understanding and 3D reconstruction is a\ncentral task in computer vision. This paper presents an integrated system that\nperforms holistic image segmentation, object detection, instance segmentation,\ndepth estimation, and object instance 3D reconstruction for indoor and outdoor\nscenes from a single RGB image. We name our system panoptic 3D parsing in which\npanoptic segmentation (\"stuff\" segmentation and \"things\"\ndetection/segmentation) with 3D reconstruction is performed. We design a\nstage-wise system where a complete set of annotations is absent. Additionally,\nwe present an end-to-end pipeline trained on a synthetic dataset with a full\nset of annotations. We show results on both indoor (3D-FRONT) and outdoor (COCO\nand Cityscapes) scenes. Our proposed panoptic 3D parsing framework points to a\npromising direction in computer vision. It can be applied to various\napplications, including autonomous driving, mapping, robotics, design, computer\ngraphics, robotics, human-computer interaction, and augmented reality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sainan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Compositional Energy Concepts. (arXiv:2111.03042v1 [cs.CV])","link":"http://arxiv.org/abs/2111.03042","description":"<p>Humans are able to rapidly understand scenes by utilizing concepts extracted\nfrom prior experience. Such concepts are diverse, and include global scene\ndescriptors, such as the weather or lighting, as well as local scene\ndescriptors, such as the color or size of a particular object. So far,\nunsupervised discovery of concepts has focused on either modeling the global\nscene-level or the local object-level factors of variation, but not both. In\nthis work, we propose COMET, which discovers and represents concepts as\nseparate energy functions, enabling us to represent both global concepts as\nwell as objects under a unified framework. COMET discovers energy functions\nthrough recomposing the input image, which we find captures independent factors\nwithout additional supervision. Sample generation in COMET is formulated as an\noptimization process on underlying energy functions, enabling us to generate\nimages with permuted and composed concepts. Finally, discovered visual concepts\nin COMET generalize well, enabling us to compose concepts between separate\nmodalities of images as well as with other concepts discovered by a separate\ninstance of COMET trained on a different dataset. Code and data available at\nhttps://energy-based-model.github.io/comet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep ensemble approach to X-ray polarimetry. (arXiv:2111.03047v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2111.03047","description":"<p>X-ray polarimetry will soon open a new window on the high energy universe\nwith the launch of NASA's Imaging X-ray Polarimetry Explorer (IXPE).\nPolarimeters are currently limited by their track reconstruction algorithms,\nwhich typically use linear estimators and do not consider individual event\nquality. We present a modern deep learning method for maximizing the\nsensitivity of X-ray telescopic observations with imaging polarimeters, with a\nfocus on the gas pixel detectors (GPDs) to be flown on IXPE. We use a weighted\nmaximum likelihood combination of predictions from a deep ensemble of ResNets,\ntrained on Monte Carlo event simulations. We derive and apply the optimal event\nweighting for maximizing the polarization signal-to-noise ratio (SNR) in track\nreconstruction algorithms. For typical power-law source spectra, our method\nimproves on the current state of the art, providing a ~40% decrease in required\nexposure times for a given SNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Peirson_A/0/1/0/all/0/1\">A.L.Peirson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Romani_R/0/1/0/all/0/1\">R.W.Romani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrap Your Object Detector via Mixed Training. (arXiv:2111.03056v1 [cs.CV])","link":"http://arxiv.org/abs/2111.03056","description":"<p>We introduce MixTraining, a new training paradigm for object detection that\ncan improve the performance of existing detectors for free. MixTraining\nenhances data augmentation by utilizing augmentations of different strengths\nwhile excluding the strong augmentations of certain training samples that may\nbe detrimental to training. In addition, it addresses localization noise and\nmissing labels in human annotations by incorporating pseudo boxes that can\ncompensate for these errors. Both of these MixTraining capabilities are made\npossible through bootstrapping on the detector, which can be used to predict\nthe difficulty of training on a strong augmentation, as well as to generate\nreliable pseudo boxes thanks to the robustness of neural networks to labeling\nerror. MixTraining is found to bring consistent improvements across various\ndetectors on the COCO dataset. In particular, the performance of Faster R-CNN\n\\cite{ren2015faster} with a ResNet-50 \\cite{he2016deep} backbone is improved\nfrom 41.7 mAP to 44.0 mAP, and the accuracy of Cascade-RCNN\n\\cite{cai2018cascade} with a Swin-Small \\cite{liu2021swin} backbone is raised\nfrom 50.9 mAP to 52.8 mAP. The code and models will be made publicly available\nat \\url{https://github.com/MendelXu/MixTraining}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning. (arXiv:2111.03062v1 [cs.RO])","link":"http://arxiv.org/abs/2111.03062","description":"<p>Dexterous manipulation of arbitrary objects, a fundamental daily task for\nhumans, has been a grand challenge for autonomous robotic systems. Although\ndata-driven approaches using reinforcement learning can develop specialist\npolicies that discover behaviors to control a single object, they often exhibit\npoor generalization to unseen ones. In this work, we show that policies learned\nby existing reinforcement learning algorithms can in fact be generalist when\ncombined with multi-task learning and a well-chosen object representation. We\nshow that a single generalist policy can perform in-hand manipulation of over\n100 geometrically-diverse real-world objects and generalize to new objects with\nunseen shape or size. Interestingly, we find that multi-task learning with\nobject point cloud representations not only generalizes better but even\noutperforms the single-object specialist policies on both training as well as\nheld-out test objects. Video results at\nhttps://huangwl18.github.io/geometry-dex\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Variational Semi-Supervised Novelty Detection. (arXiv:1911.04971v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1911.04971","description":"<p>In anomaly detection (AD), one seeks to identify whether a test sample is\nabnormal, given a data set of normal samples. A recent and promising approach\nto AD relies on deep generative models, such as variational autoencoders\n(VAEs), for unsupervised learning of the normal data distribution. In\nsemi-supervised AD (SSAD), the data also includes a small sample of labeled\nanomalies. In this work, we propose two variational methods for training VAEs\nfor SSAD. The intuitive idea in both methods is to train the encoder to\n`separate' between latent vectors for normal and outlier data. We show that\nthis idea can be derived from principled probabilistic formulations of the\nproblem, and propose simple and effective algorithms. Our methods can be\napplied to various data types, as we demonstrate on SSAD datasets ranging from\nnatural images to astronomy and medicine, can be combined with any VAE model\narchitecture, and are naturally compatible with ensembling. When comparing to\nstate-of-the-art SSAD methods that are not specific to particular data types,\nwe obtain marked improvement in outlier detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1\">Tal Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1\">Thanard Kurutach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross modal video representations for weakly supervised active speaker localization. (arXiv:2003.04358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.04358","description":"<p>An objective understanding of media depictions, such as inclusive portrayals\nof how much someone is heard and seen on screen such as in film and television,\nrequires the machines to discern automatically who, when, how, and where\nsomeone is talking, and not. Speaker activity can be automatically discerned\nfrom the rich multimodal information present in the media content. This is\nhowever a challenging problem due to the vast variety and contextual\nvariability in the media content, and the lack of labeled data. In this work,\nwe present a cross-modal neural network for learning visual representations,\nwhich have implicit information pertaining to the spatial location of a speaker\nin the visual frames. Avoiding the need for manual annotations for active\nspeakers in visual frames, acquiring of which is very expensive, we present a\nweakly supervised system for the task of localizing active speakers in movie\ncontent. We use the learned cross-modal visual representations, and provide\nweak supervision from movie subtitles acting as a proxy for voice activity,\nthus requiring no manual annotations. We evaluate the performance of the\nproposed system on the AVA active speaker dataset and demonstrate the\neffectiveness of the cross-modal embeddings for localizing active speakers in\ncomparison to fully supervised systems. We also demonstrate state-of-the-art\nperformance for the task of voice activity detection in an audio-visual\nframework, especially when speech is accompanied by noise and music.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texture Memory-Augmented Deep Patch-Based Image Inpainting. (arXiv:2009.13240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13240","description":"<p>Patch-based methods and deep networks have been employed to tackle image\ninpainting problem, with their own strengths and weaknesses. Patch-based\nmethods are capable of restoring a missing region with high-quality texture\nthrough searching nearest neighbor patches from the unmasked regions. However,\nthese methods bring problematic contents when recovering large missing regions.\nDeep networks, on the other hand, show promising results in completing large\nregions. Nonetheless, the results often lack faithful and sharp details that\nresemble the surrounding area. By bringing together the best of both paradigms,\nwe propose a new deep inpainting framework where texture generation is guided\nby a texture memory of patch samples extracted from unmasked regions. The\nframework has a novel design that allows texture memory retrieval to be trained\nend-to-end with the deep inpainting network. In addition, we introduce a patch\ndistribution loss to encourage high-quality patch synthesis. The proposed\nmethod shows superior performance both qualitatively and quantitatively on\nthree challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris\nStreet-View datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minghao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Diffusion Implicit Models. (arXiv:2010.02502v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.02502","description":"<p>Denoising diffusion probabilistic models (DDPMs) have achieved high quality\nimage generation without adversarial training, yet they require simulating a\nMarkov chain for many steps to produce a sample. To accelerate sampling, we\npresent denoising diffusion implicit models (DDIMs), a more efficient class of\niterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a\nMarkovian diffusion process. We construct a class of non-Markovian diffusion\nprocesses that lead to the same training objective, but whose reverse process\ncan be much faster to sample from. We empirically demonstrate that DDIMs can\nproduce high quality samples $10 \\times$ to $50 \\times$ faster in terms of\nwall-clock time compared to DDPMs, allow us to trade off computation for sample\nquality, and can perform semantically meaningful image interpolation directly\nin the latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Learning Approach for Training Variational Autoencoder Priors. (arXiv:2010.02917v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.02917","description":"<p>Variational autoencoders (VAEs) are one of the powerful likelihood-based\ngenerative models with applications in many domains. However, they struggle to\ngenerate high-quality images, especially when samples are obtained from the\nprior without any tempering. One explanation for VAEs' poor generative quality\nis the prior hole problem: the prior distribution fails to match the aggregate\napproximate posterior. Due to this mismatch, there exist areas in the latent\nspace with high density under the prior that do not correspond to any encoded\nimage. Samples from those areas are decoded to corrupted images. To tackle this\nissue, we propose an energy-based prior defined by the product of a base prior\ndistribution and a reweighting factor, designed to bring the base closer to the\naggregate posterior. We train the reweighting factor by noise contrastive\nestimation, and we generalize it to hierarchical VAEs with many latent variable\ngroups. Our experiments confirm that the proposed noise contrastive priors\nimprove the generative performance of state-of-the-art VAEs by a large margin\non the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is\nsimple and can be applied to a wide variety of VAEs to improve the expressivity\nof their prior distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Do Better Loss Functions Lead to Less Transferable Features?. (arXiv:2010.16402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.16402","description":"<p>Previous work has proposed many new loss functions and regularizers that\nimprove test accuracy on image classification tasks. However, it is not clear\nwhether these loss functions learn better representations for downstream tasks.\nThis paper studies how the choice of training objective affects the\ntransferability of the hidden representations of convolutional neural networks\ntrained on ImageNet. We show that many objectives lead to statistically\nsignificant improvements in ImageNet accuracy over vanilla softmax\ncross-entropy, but the resulting fixed feature extractors transfer\nsubstantially worse to downstream tasks, and the choice of loss has little\neffect when networks are fully fine-tuned on the new tasks. Using centered\nkernel alignment to measure similarity between hidden representations of\nnetworks, we find that differences among loss functions are apparent only in\nthe last few layers of the network. We delve deeper into representations of the\npenultimate layer, finding that different objectives and hyperparameter\ncombinations lead to dramatically different levels of class separation.\nRepresentations with higher class separation obtain higher accuracy on the\noriginal task, but their features are less useful for downstream tasks. Our\nresults suggest there exists a trade-off between learning invariant features\nfor the original task and features relevant for transfer tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFTrack++: A Fast Learnable Spectral Segmentation Approach for Space-Time Consistent Tracking. (arXiv:2011.13843v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13843","description":"<p>We propose an object tracking method, SFTrack++, that smoothly learns to\npreserve the tracked object consistency over space and time dimensions by\ntaking a spectral clustering approach over the graph of pixels from the video,\nusing a fast 3D filtering formulation for finding the principal eigenvector of\nthis graph's adjacency matrix. To better capture complex aspects of the tracked\nobject, we enrich our formulation to multi-channel inputs, which permit\ndifferent points of view for the same input. The channel inputs are in our\nexperiments, the output of multiple tracking methods. After combining them,\ninstead of relying only on hidden layers representations to predict a good\ntracking bounding box, we explicitly learn an intermediate, more refined one,\nnamely the segmentation map of the tracked object. This prevents the rough\ncommon bounding box approach to introduce noise and distractors in the learning\nprocess. We test our method, SFTrack++, on five tracking benchmarks: OTB, UAV,\nNFS, GOT-10k, and TrackingNet, using five top trackers as input. Our\nexperimental results validate the pre-registered hypothesis. We obtain\nconsistent and robust results, competitive on the three traditional benchmarks\n(OTB, UAV, NFS) and significantly on top of others (by over $1.1\\%$ on\naccuracy) on GOT-10k and TrackingNet, which are newer, larger, and more varied\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1\">Elena Burceanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientLPS: Efficient LiDAR Panoptic Segmentation. (arXiv:2102.08009v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08009","description":"<p>Panoptic segmentation of point clouds is a crucial task that enables\nautonomous vehicles to comprehend their vicinity using their highly accurate\nand reliable LiDAR sensors. Existing top-down approaches tackle this problem by\neither combining independent task-specific networks or translating methods from\nthe image domain ignoring the intricacies of LiDAR data and thus often\nresulting in sub-optimal performance. In this paper, we present the novel\ntop-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that\naddresses multiple challenges in segmenting LiDAR point clouds including\ndistance-dependent sparsity, severe occlusions, large scale-variations, and\nre-projection errors. EfficientLPS comprises of a novel shared backbone that\nencodes with strengthened geometric transformation modeling capacity and\naggregates semantically rich range-aware multi-scale features. It incorporates\nnew scale-invariant semantic and instance segmentation heads along with the\npanoptic fusion module which is supervised by our proposed panoptic periphery\nloss function. Additionally, we formulate a regularized pseudo labeling\nframework to further improve the performance of EfficientLPS by training on\nunlabelled data. We benchmark our proposed model on two large-scale LiDAR\ndatasets: nuScenes, for which we also provide ground truth annotations, and\nSemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both\nthese datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1\">Kshitij Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1\">Daniel B&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Video Prediction for Time Series Forecasting. (arXiv:2102.12061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12061","description":"<p>Time series forecasting is essential for decision making in many domains. In\nthis work, we address the challenge of predicting prices evolution among\nmultiple potentially interacting financial assets. A solution to this problem\nhas obvious importance for governments, banks, and investors. Statistical\nmethods such as Auto Regressive Integrated Moving Average (ARIMA) are widely\napplied to these problems. In this paper, we propose to approach economic time\nseries forecasting of multiple financial assets in a novel way via video\nprediction. Given past prices of multiple potentially interacting financial\nassets, we aim to predict the prices evolution in the future. Instead of\ntreating the snapshot of prices at each time point as a vector, we spatially\nlayout these prices in 2D as an image, such that we can harness the power of\nCNNs in learning a latent representation for these financial assets. Thus, the\nhistory of these prices becomes a sequence of images, and our goal becomes\npredicting future images. We build on a state-of-the-art video prediction\nmethod for forecasting future images. Our experiments involve the prediction\ntask of the price evolution of nine financial assets traded in U.S. stock\nmarkets. The proposed method outperforms baselines including ARIMA, Prophet,\nand variations of the proposed method, demonstrating the benefits of harnessing\nthe power of CNNs in the problem of economic time series forecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised deep convolutional neural network for chest X-ray classification. (arXiv:2103.03055v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.03055","description":"<p>Chest radiography is a relatively cheap, widely available medical procedure\nthat conveys key information for making diagnostic decisions. Chest X-rays are\nalmost always used in the diagnosis of respiratory diseases such as pneumonia\nor the recent COVID-19. In this paper, we propose a self-supervised deep neural\nnetwork that is pretrained on an unlabeled chest X-ray dataset. The learned\nrepresentations are transferred to downstream task - the classification of\nrespiratory diseases. The results obtained on four public datasets show that\nour approach yields competitive results without requiring large amounts of\nlabeled training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gazda_M/0/1/0/all/0/1\">Matej Gazda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gazda_J/0/1/0/all/0/1\">Jakub Gazda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plavka_J/0/1/0/all/0/1\">Jan Plavka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drotar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Mean Teacher for Semi-supervised Chest X-ray Classification. (arXiv:2103.03629v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.03629","description":"<p>The training of deep learning models generally requires a large amount of\nannotated data for effective convergence and generalisation. However, obtaining\nhigh-quality annotations is a laboursome and expensive process due to the need\nof expert radiologists for the labelling task. The study of semi-supervised\nlearning in medical image analysis is then of crucial importance given that it\nis much less expensive to obtain unlabelled images than to acquire images\nlabelled by expert radiologists. Essentially, semi-supervised methods leverage\nlarge sets of unlabelled data to enable better training convergence and\ngeneralisation than using only the small set of labelled images. In this paper,\nwe propose Self-supervised Mean Teacher for Semi-supervised (S$^2$MTS$^2$)\nlearning that combines self-supervised mean-teacher pre-training with\nsemi-supervised fine-tuning. The main innovation of S$^2$MTS$^2$ is the\nself-supervised mean-teacher pre-training based on the joint contrastive\nlearning, which uses an infinite number of pairs of positive query and key\nfeatures to improve the mean-teacher representation. The model is then\nfine-tuned using the exponential moving average teacher framework trained with\nsemi-supervised learning. We validate S$^2$MTS$^2$ on the multi-label\nclassification problems from Chest X-ray14 and CheXpert, and the multi-class\nclassification from ISIC2018, where we show that it outperforms the previous\nSOTA semi-supervised learning methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1\">Filipe R. Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles. (arXiv:2103.03975v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03975","description":"<p>NASA's Global Ecosystem Dynamics Investigation (GEDI) is a key climate\nmission whose goal is to advance our understanding of the role of forests in\nthe global carbon cycle. While GEDI is the first space-based LIDAR explicitly\noptimized to measure vertical forest structure predictive of aboveground\nbiomass, the accurate interpretation of this vast amount of waveform data\nacross the broad range of observational and environmental conditions is\nchallenging. Here, we present a novel supervised machine learning approach to\ninterpret GEDI waveforms and regress canopy top height globally. We propose a\nprobabilistic deep learning approach based on an ensemble of deep convolutional\nneural networks(CNN) to avoid the explicit modelling of unknown effects, such\nas atmospheric noise. The model learns to extract robust features that\ngeneralize to unseen geographical regions and, in addition, yields reliable\nestimates of predictive uncertainty. Ultimately, the global canopy top height\nestimates produced by our model have an expected RMSE of 2.7 m with low bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalischek_N/0/1/0/all/0/1\">Nikolai Kalischek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armston_J/0/1/0/all/0/1\">John Armston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubayah_R/0/1/0/all/0/1\">Ralph Dubayah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-aware Neural Network for Semantic Segmentation of Multi-resolution Remote Sensing Images. (arXiv:2103.07935v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07935","description":"<p>Assigning geospatial objects with specific categories at the pixel level is a\nfundamental task in remote sensing image analysis. Along with rapid development\nin sensor technologies, remotely sensed images can be captured at multiple\nspatial resolutions (MSR) with information content manifested at different\nscales. Extracting information from these MSR images represents huge\nopportunities for enhanced feature representation and characterisation.\nHowever, MSR images suffer from two critical issues: 1) increased scale\nvariation of geo-objects and 2) loss of detailed information at coarse spatial\nresolutions. To bridge these gaps, in this paper, we propose a novel\nscale-aware neural network (SaNet) for semantic segmentation of MSR remotely\nsensed imagery. SaNet deploys a densely connected feature network (DCFFM)\nmodule to capture high-quality multi-scale context, such that the scale\nvariation is handled properly and the quality of segmentation is increased for\nboth large and small objects. A spatial feature recalibration (SFRM) module is\nfurther incorporated into the network to learn intact semantic content with\nenhanced spatial relationships, where the negative effects of information loss\nare removed. The combination of DCFFM and SFRM allows SaNet to learn\nscale-aware feature representation, which outperforms the existing multi-scale\nfeature representation. Extensive experiments on three semantic segmentation\ndatasets demonstrated the effectiveness of the proposed SaNet in\ncross-resolution segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Whitney extension problem for near isometries and beyond. (arXiv:2103.09748v3 [math.CA] UPDATED)","link":"http://arxiv.org/abs/2103.09748","description":"<p>In this memoir, we develop a general framework which allows for a\nsimultaneous study of labeled and unlabeled near alignment data problems in\n$\\mathbb R^D$ and the Whitney near isometry extension problem for discrete and\nnon-discrete subsets of $\\mathbb R^D$ with certain geometries. In addition, we\nsurvey related work of ours on clustering, dimension reduction, manifold\nlearning, vision as well as minimal energy partitions, discrepancy and min-max\noptimization. Numerous open problems in harmonic analysis, computer vision,\nmanifold learning and signal processing connected to our work are given.\n</p>\n<p>A significant portion of the work in this memoir is based on joint research\nwith Charles Fefferman in the papers [48], [49], [50], [51].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Damelin_S/0/1/0/all/0/1\">Steven B. Damelin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift. (arXiv:2103.14417v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.14417","description":"<p>The human ability to synchronize the feedback from all their senses inspired\nrecent works in multi-task and multi-modal learning. While these works rely on\nexpensive supervision, our multi-task graph requires only pseudo-labels from\nexpert models. Every graph node represents a task, and each edge learns between\ntasks transformations. Once initialized, the graph learns self-supervised,\nbased on a novel consensus shift algorithm that intelligently exploits the\nagreement between graph pathways to generate new pseudo-labels for the next\nlearning cycle. We demonstrate significant improvement from one unsupervised\nlearning iteration to the next, outperforming related recent methods in\nextensive multi-task learning experiments on two challenging datasets. Our code\nis available at https://github.com/bit-ml/cshift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_E/0/1/0/all/0/1\">Emanuela Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1\">Elena Burceanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leordeanu_M/0/1/0/all/0/1\">Marius Leordeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Neural Operations for Diverse Tasks. (arXiv:2103.15798v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.15798","description":"<p>An important goal of AutoML is to automate-away the design of neural networks\non new tasks in under-explored domains. Motivated by this goal, we study the\nproblem of enabling users to discover the right neural operations given data\nfrom their specific domain. We introduce a search space of operations called\nXD-Operations that mimic the inductive bias of standard multi-channel\nconvolutions while being much more expressive: we prove that it includes many\nnamed operations across multiple application areas. Starting with any standard\nbackbone such as ResNet, we show how to transform it into a search space over\nXD-operations and how to traverse the space using a simple weight-sharing\nscheme. On a diverse set of tasks -- solving PDEs, distance prediction for\nprotein folding, and music modeling -- our approach consistently yields models\nwith lower error than baseline networks and often even lower error than\nexpert-designed domain-specific approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liam Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Measuring Fairness in AI: the Casual Conversations Dataset. (arXiv:2104.02821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02821","description":"<p>This paper introduces a novel dataset to help researchers evaluate their\ncomputer vision and audio models for accuracy across a diverse set of age,\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\n15 videos per person. The videos were recorded in multiple U.S. states with a\ndiverse set of adults in various age, gender and apparent skin tone groups. A\nkey feature is that each subject agreed to participate for their likenesses to\nbe used. Additionally, our age and gender annotations are provided by the\nsubjects themselves. A group of trained annotators labeled the subjects'\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\nfor videos recorded in low ambient lighting are also provided. As an\napplication to measure robustness of predictions across certain attributes, we\nprovide a comprehensive study on the top five winners of the DeepFake Detection\nChallenge (DFDC). Experimental evaluation shows that the winning models are\nless performant on some specific groups of people, such as subjects with darker\nskin tones and thus may not generalize to all people. In addition, we also\nevaluate the state-of-the-art apparent age and gender classification methods.\nOur experiments provides a thorough analysis on these models in terms of fair\ntreatment of people from various backgrounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_J/0/1/0/all/0/1\">Joanna Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolhansky_B/0/1/0/all/0/1\">Brian Dolhansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jacqueline Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1\">Albert Gordo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landmark-Aware and Part-based Ensemble Transfer Learning Network for Facial Expression Recognition from Static images. (arXiv:2104.11274v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11274","description":"<p>Facial Expression Recognition from static images is a challenging problem in\ncomputer vision applications. Convolutional Neural Network (CNN), the\nstate-of-the-art method for various computer vision tasks, has had limited\nsuccess in predicting expressions from faces having extreme poses,\nillumination, and occlusion conditions. To mitigate this issue, CNNs are often\naccompanied by techniques like transfer, multi-task, or ensemble learning that\noften provide high accuracy at the cost of increased computational complexity.\nIn this work, we propose a Part-based Ensemble Transfer Learning network that\nmodels how humans recognize facial expressions by correlating the spatial\norientation pattern of the facial features with a specific expression. It\nconsists of 5 sub-networks, and each sub-network performs transfer learning\nfrom one of the five subsets of facial landmarks: eyebrows, eyes, nose, mouth,\nor jaw to expression classification. We show that our proposed ensemble network\nuses visual patterns emanating from facial muscles' motor movements to predict\nexpressions and demonstrate the usefulness of transfer learning from Facial\nLandmark Localization to Facial Expression Recognition. We test the proposed\nnetwork on the CK+, JAFFE, and SFEW datasets, and it outperforms the benchmark\nfor CK+ and JAFFE datasets by 0.51% and 5.34%, respectively. Additionally, the\nproposed ensemble network consists of only 1.65M model parameters, ensuring\ncomputational efficiency during training and real-time deployment. Grad-CAM\nvisualizations of our proposed ensemble highlight the complementary nature of\nits sub-networks, a key design parameter of an effective ensemble network.\nLastly, cross-dataset evaluation results reveal that our proposed ensemble has\na high generalization capacity, making it suitable for real-world usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.02468","description":"<p>Understanding why deep nets can classify data in large dimensions remains a\nchallenge. It has been proposed that they do so by becoming stable to\ndiffeomorphisms, yet existing empirical measurements support that it is often\nnot the case. We revisit this question by defining a maximum-entropy\ndistribution on diffeomorphisms, that allows to study typical diffeomorphisms\nof a given norm. We confirm that stability toward diffeomorphisms does not\nstrongly correlate to performance on benchmark data sets of images. By\ncontrast, we find that the stability toward diffeomorphisms relative to that of\ngeneric transformations $R_f$ correlates remarkably with the test error\n$\\epsilon_t$. It is of order unity at initialization but decreases by several\ndecades during training for state-of-the-art architectures. For CIFAR10 and 15\nknown architectures, we find $\\epsilon_t\\approx 0.2\\sqrt{R_f}$, suggesting that\nobtaining a small $R_f$ is important to achieve good performance. We study how\n$R_f$ depends on the size of the training set and compare it to a simple model\nof invariant learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1\">Leonardo Petrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1\">Alessandro Favero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1\">Mario Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1\">Matthieu Wyart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics. (arXiv:2106.02749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02749","description":"<p>Deep neural networks excel at image classification, but their performance is\nfar less robust to input perturbations than human perception. In this work we\nexplore whether this shortcoming may be partly addressed by incorporating\nbrain-inspired recurrent dynamics in deep convolutional networks. We take\ninspiration from a popular framework in neuroscience: 'predictive coding'. At\neach layer of the hierarchical model, generative feedback 'predicts' (i.e.,\nreconstructs) the pattern of activity in the previous layer. The reconstruction\nerrors are used to iteratively update the network's representations across\ntimesteps, and to optimize the network's feedback weights over the natural\nimage dataset-a form of unsupervised training. We show that implementing this\nstrategy into two popular networks, VGG16 and EfficientNetB0, improves their\nrobustness against various corruptions and adversarial attacks. We hypothesize\nthat other feedforward networks could similarly benefit from the proposed\nframework. To promote research in this direction, we provide an open-sourced\nPyTorch-based package called Predify, which can be used to implement and\ninvestigate the impacts of the predictive coding dynamics in any convolutional\nneural network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1\">Milad Mozafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1\">Callum Biggs O&#x27;May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ador_B/0/1/0/all/0/1\">Benjamin Ador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Don't Need $\\mathbf{u}$: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05238","description":"<p>Recently there has been a renaissance in identifiability results in deep\ngenerative models, not least for non-linear ICA. For i.i.d. data, prior works\nhave assumed access to a sufficiently-informative auxiliary set of\nobservations, denoted $\\mathbf{u}$. We show here how identifiability can be\nobtained in the absence of this side-information. Previous methods have had to\nmake strong assumptions in order to obtain identifiable models. Here we obtain\nempirically identifiable models under a much looser set of constraints. In\nparticular, we focus on generative models which perform clustering in their\nlatent space -- a model structure which matches previous identifiable models,\nbut with the learnt clustering providing a synthetic form of auxiliary\ninformation. We evaluate our proposals, including via statistical tests, and\nfind that the learned clusterings function effectively: deep generative models\nwith latent clusterings are empirically identifiable, to the same degree as\nmodels which rely on side information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1\">Matthew Willetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1\">Brooks Paige</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Contextual Design of Convolutional Neural Network for Steganalysis. (arXiv:2106.10430v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2106.10430","description":"<p>In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Brijesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pinaki Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are conditional GANs explicitly conditional?. (arXiv:2106.15011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15011","description":"<p>This paper proposes two important contributions for conditional Generative\nAdversarial Networks (cGANs) to improve the wide variety of applications that\nexploit this architecture. The first main contribution is an analysis of cGANs\nto show that they are not explicitly conditional. In particular, it will be\nshown that the discriminator and subsequently the cGAN does not automatically\nlearn the conditionality between inputs. The second contribution is a new\nmethod, called a contrario cGAN, that explicitly models conditionality for both\nparts of the adversarial architecture via a novel a contrario loss that\ninvolves training the discriminator to learn unconditional (adverse) examples.\nThis leads to a novel type of data augmentation approach for GANs (a contrario\nlearning) which allows to restrict the search space of the generator to\nconditional outputs using adverse examples. Extensive experimentation is\ncarried out to evaluate the conditionality of the discriminator by proposing a\nprobability distribution analysis. Comparisons with the cGAN architecture for\ndifferent applications show significant improvements in performance on well\nknown datasets including, semantic image synthesis, image segmentation,\nmonocular depth prediction and \"single label\"-to-image using different metrics\nincluding Fr\\'echet Inception Distance (FID), mean Intersection over Union\n(mIoU), Root Mean Square Error log (RMSE log) and Number of\nstatistically-Different Bins (NDB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem-eddine Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.15753","description":"<p>Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00717","description":"<p>Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet. SIMILAR is available as a part of the DISTIL\ntoolkit: \"https://github.com/decile-team/distil\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset are publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04927","description":"<p>Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Conditioned GAN. (arXiv:2109.05070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05070","description":"<p>Generative Adversarial Networks (GANs) can generate near photo realistic\nimages in narrow domains such as human faces. Yet, modeling complex\ndistributions of datasets such as ImageNet and COCO-Stuff remains challenging\nin unconditional settings. In this paper, we take inspiration from kernel\ndensity estimation techniques and introduce a non-parametric approach to\nmodeling distributions of complex datasets. We partition the data manifold into\na mixture of overlapping neighborhoods described by a datapoint and its nearest\nneighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),\nwhich learns the distribution around each datapoint. Experimental results on\nImageNet and COCO-Stuff show that IC-GAN significantly improves over\nunconditional models and unsupervised data partitioning baselines. Moreover, we\nshow that IC-GAN can effortlessly transfer to datasets not seen during training\nby simply changing the conditioning instances, and still generate realistic\nimages. Finally, we extend IC-GAN to the class-conditional case and show\nsemantically controllable generation and competitive quantitative results on\nImageNet; while improving over BigGAN on ImageNet-LT. Code and trained models\nto reproduce the reported results are available at\nhttps://github.com/facebookresearch/ic_gan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1\">Arantxa Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Careil_M/0/1/0/all/0/1\">Marl&#xe8;ne Careil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UFO-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2109.14382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14382","description":"<p>Vision transformers have become one of the most important models for computer\nvision tasks. Although they outperform prior works, they require heavy\ncomputational resources on a scale that is quadratic to $N$. This is a major\ndrawback of the traditional self-attention (SA) algorithm. Here, we propose the\nUnit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has\nlinear complexity. The main approach of this work is to eliminate nonlinearity\nfrom the original SA. We factorize the matrix multiplication of the SA\nmechanism without complicated linear approximation. By modifying only a few\nlines of code from the original SA, the proposed models outperform most\ntransformer-based models on image classification and dense prediction tasks on\nmost capacity regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jeong-geun Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Unsupervised Domain Adaptive Re-Identification via Source-Guided Selection of Pseudo-Labeling Hyperparameters. (arXiv:2110.07897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07897","description":"<p>Unsupervised Domain Adaptation (UDA) for re-identification (re-ID) is a\nchallenging task: to avoid a costly annotation of additional data, it aims at\ntransferring knowledge from a domain with annotated data to a domain of\ninterest with only unlabeled data. Pseudo-labeling approaches have proven to be\neffective for UDA re-ID. However, the effectiveness of these approaches heavily\ndepends on the choice of some hyperparameters (HP) that affect the generation\nof pseudo-labels by clustering. The lack of annotation in the domain of\ninterest makes this choice non-trivial. Current approaches simply reuse the\nsame empirical value for all adaptation tasks and regardless of the target data\nrepresentation that changes through pseudo-labeling training phases. As this\nsimplistic choice may limit their performance, we aim at addressing this issue.\nWe propose new theoretical grounds on HP selection for clustering UDA re-ID as\nwell as method of automatic and cyclic HP tuning for pseudo-labeling UDA\nclustering: HyPASS. HyPASS consists in incorporating two modules in\npseudo-labeling methods: (i) HP selection based on a labeled source validation\nset and (ii) conditional domain alignment of feature discriminativeness to\nimprove HP selection based on source samples. Experiments on commonly used\nperson re-ID and vehicle re-ID datasets show that our proposed HyPASS\nconsistently improves the best state-of-the-art methods in re-ID compared to\nthe commonly used empirical HP setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubourvieux_F/0/1/0/all/0/1\">Fabian Dubourvieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Cross-Domain Adaptation for Robust Retinopathy Screening via Bayesian Deep Learning. (arXiv:2110.09319v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09319","description":"<p>Retinopathy represents a group of retinal diseases that, if not treated\ntimely, can cause severe visual impairments or even blindness. Many researchers\nhave developed autonomous systems to recognize retinopathy via fundus and\noptical coherence tomography (OCT) imagery. However, most of these frameworks\nemploy conventional transfer learning and fine-tuning approaches, requiring a\ndecent amount of well-annotated training data to produce accurate diagnostic\nperformance. This paper presents a novel incremental cross-domain adaptation\ninstrument that allows any deep classification model to progressively learn\nabnormal retinal pathologies in OCT and fundus imagery via few-shot training.\nFurthermore, unlike its competitors, the proposed instrument is driven via a\nBayesian multi-objective function that not only enforces the candidate\nclassification network to retain its prior learned knowledge during incremental\ntraining but also ensures that the network understands the structural and\nsemantic relationships between previously learned pathologies and newly added\ndisease categories to effectively recognize them at the inference stage. The\nproposed framework, evaluated on six public datasets acquired with three\ndifferent scanners to screen thirteen retinal pathologies, outperforms the\nstate-of-the-art competitors by achieving an overall accuracy and F1 score of\n0.9826 and 0.9846, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_M/0/1/0/all/0/1\">Muhammad Usman Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashmi_S/0/1/0/all/0/1\">Shahrukh Hashmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taguri_A/0/1/0/all/0/1\">Abdel Hakim Taguri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning. (arXiv:2110.11128v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11128","description":"<p>Few-shot learning (FSL) aims to learn models that generalize to novel classes\nwith limited training samples. Recent works advance FSL towards a scenario\nwhere unlabeled examples are also available and propose semi-supervised FSL\nmethods. Another line of methods also cares about the performance of base\nclasses in addition to the novel ones and thus establishes the incremental FSL\nscenario. In this paper, we generalize the above two under a more realistic yet\ncomplex setting, named by Semi-Supervised Incremental Few-Shot Learning (S2\nI-FSL). To tackle the task, we propose a novel paradigm containing two parts:\n(1) a well-designed meta-training algorithm for mitigating ambiguity between\nbase and novel classes caused by unreliable pseudo labels and (2) a model\nadaptation mechanism to learn discriminative features for novel classes while\npreserving base knowledge using few labeled and all the unlabeled data.\nExtensive experiments on standard FSL, semi-supervised FSL, incremental FSL,\nand the firstly built S2 I-FSL benchmarks demonstrate the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linlan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dashan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiangzhong Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers. (arXiv:2110.14120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14120","description":"<p>Adversarial patch attacks that craft the pixels in a confined region of the\ninput images show their powerful attack effectiveness in physical environments\neven with noises or deformations. Existing certified defenses towards\nadversarial patch attacks work well on small images like MNIST and CIFAR-10\ndatasets, but achieve very poor certified accuracy on higher-resolution images\nlike ImageNet. It is urgent to design both robust and effective defenses\nagainst such a practical and harmful attack in industry-level larger images. In\nthis work, we propose the certified defense methodology that achieves high\nprovable robustness for high-resolution images and largely improves the\npracticality for real adoption of the certified defense. The basic insight of\nour work is that the adversarial patch intends to leverage localized\nsuperficial important neurons (SIN) to manipulate the prediction results.\nHence, we leverage the SIN-based DNN compression techniques to significantly\nimprove the certified accuracy, by reducing the adversarial region searching\noverhead and filtering the prediction noises. Our experimental results show\nthat the certified accuracy is increased from 36.3% (the state-of-the-art\ncertified detection) to 60.4% on the ImageNet dataset, largely pushing the\ncertified defenses for practical use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Husheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaobing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Ling Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-realistic Perspective of Computer Vision. (arXiv:2111.00791v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00791","description":"<p>We present an optimization-based theory describing spiking cortical ensembles\nequipped with Spike-Timing-Dependent Plasticity (STDP) learning, as empirically\nobserved in the visual cortex. Using our methods, we build a class of\nfully-connected, convolutional and action-based feature descriptors for\nevent-based camera that we respectively assess on N-MNIST, challenging\nCIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant\naccuracy improvements compared to conventional state-of-the-art event-based\nfeature descriptors (+8% on CIFAR10-DVS). We report large improvements in\naccuracy compared to state-of-the-art STDP-based systems (+10% on N-MNIST,\n+7.74% on IBM DVS128 Gesture). In addition to ultra-low-power learning in\nneuromorphic edge devices, our work helps paving the way towards a\nbiologically-realistic, optimization-based theory of cortical vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion. (arXiv:2111.00993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00993","description":"<p>In this paper, we address the problem of forecasting the trajectory of an\negocentric camera wearer (ego-person) in crowded spaces. The trajectory\nforecasting ability learned from the data of different camera wearers walking\naround in the real world can be transferred to assist visually impaired people\nin navigation, as well as to instill human navigation behaviours in mobile\nrobots, enabling better human-robot interactions. To this end, a novel\negocentric human trajectory forecasting dataset was constructed, containing\nreal trajectories of people navigating in crowded spaces wearing a camera, as\nwell as extracted rich contextual data. We extract and utilize three different\nmodalities to forecast the trajectory of the camera wearer, i.e., his/her past\ntrajectory, the past trajectories of nearby people, and the environment such as\nthe scene semantics or the depth of the scene. A Transformer-based\nencoder-decoder neural network model, integrated with a novel cascaded\ncross-attention mechanism that fuses multiple modalities, has been designed to\npredict the future trajectory of the camera wearer. Extensive experiments have\nbeen conducted, and the results have shown that our model outperforms the\nstate-of-the-art methods in egocentric human trajectory forecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Ya-Yen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Eye-in-Hand Camera Calibration from a Single Image. (arXiv:2111.01245v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.01245","description":"<p>Eye-in-hand camera calibration is a fundamental and long-studied problem in\nrobotics. We present a study on using learning-based methods for solving this\nproblem online from a single RGB image, whilst training our models with\nentirely synthetic data. We study three main approaches: one direct regression\nmodel that directly predicts the extrinsic matrix from an image, one sparse\ncorrespondence model that regresses 2D keypoints and then uses PnP, and one\ndense correspondence model that uses regressed depth and segmentation maps to\nenable ICP pose estimation. In our experiments, we benchmark these methods\nagainst each other and against well-established classical methods, to find the\nsurprising result that direct regression outperforms other approaches, and we\nperform noise-sensitivity analysis to gain further insights into these results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreczkowski_K/0/1/0/all/0/1\">Kamil Dreczkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima. (arXiv:2111.01549v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.01549","description":"<p>This paper considers incremental few-shot learning, which requires a model to\ncontinually recognize new categories with only a few examples provided. Our\nstudy shows that existing methods severely suffer from catastrophic forgetting,\na well-known problem in incremental learning, which is aggravated due to data\nscarcity and imbalance in the few-shot setting. Our analysis further suggests\nthat to prevent catastrophic forgetting, actions need to be taken in the\nprimitive stage -- the training of base classes instead of later few-shot\nlearning sessions. Therefore, we propose to search for flat local minima of the\nbase training objective function and then fine-tune the model parameters within\nthe flat region on new tasks. In this way, the model can efficiently learn new\nclasses while preserving the old ones. Comprehensive experimental results\ndemonstrate that our approach outperforms all prior state-of-the-art methods\nand is very close to the approximate upper bound. The source code is available\nat https://github.com/moukamisama/F2M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Li-Ming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}