{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. (arXiv:2205.05718v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05718","description":"<p>Human language offers a powerful window into our thoughts -- we tell stories,\ngive explanations, and express our beliefs and goals through words. Abundant\nevidence also suggests that language plays a developmental role in structuring\nour learning. Here, we ask: how much of human-like thinking can be captured by\nlearning statistical patterns in language alone? We first contribute a new\nchallenge benchmark for comparing humans and distributional large language\nmodels (LLMs). Our benchmark contains two problem-solving domains (planning and\nexplanation generation) and is designed to require generalization to new,\nout-of-distribution problems expressed in language. We find that humans are far\nmore robust than LLMs on this benchmark. Next, we propose a hybrid\nParse-and-Solve model, which augments distributional LLMs with a structured\nsymbolic reasoning module. We find that this model shows more robust adaptation\nto out-of-distribution planning problems, demonstrating the promise of hybrid\nAI models for more human-like reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1\">Katherine M. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Catherine Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiahai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Megan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Grammatical Errors are Frequent, Others are Important. (arXiv:2205.05730v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05730","description":"<p>In Grammatical Error Correction, systems are evaluated by the number of\nerrors they correct. However, no one has assessed whether all error types are\nequally important. We provide and apply a method to quantify the importance of\ndifferent grammatical error types to humans. We show that some rare errors are\nconsidered disturbing while other common ones are not. This affects possible\ndirections to improve both systems and their evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shifman_O/0/1/0/all/0/1\">Ofir Shifman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISARM: Detecting the Victims Targeted by Harmful Memes. (arXiv:2205.05738v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05738","description":"<p>Internet memes have emerged as an increasingly popular means of communication\non the Web. Although typically intended to elicit humour, they have been\nincreasingly used to spread hatred, trolling, and cyberbullying, as well as to\ntarget specific individuals, communities, or society on political,\nsocio-cultural, and psychological grounds. While previous work has focused on\ndetecting harmful, hateful, and offensive memes, identifying whom they attack\nremains a challenging and underexplored area. Here we aim to bridge this gap.\nIn particular, we create a dataset where we annotate each meme with its\nvictim(s) such as the name of the targeted person(s), organization(s), and\ncommunity(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful\nMemes), a framework that uses named entity recognition and person\nidentification to detect all entities a meme is referring to, and then,\nincorporates a novel contextualized multimodal deep neural network to classify\nwhether the meme intends to harm these entities. We perform several systematic\nexperiments on three test setups, corresponding to entities that are (a) all\nseen while training, (b) not seen as a harmful target on training, and (c) not\nseen at all on training. The evaluation results show that DISARM significantly\noutperforms ten unimodal and multimodal systems. Finally, we show that DISARM\nis interpretable and comparatively more generalizable and that it can reduce\nthe relative error rate for harmful target identification by up to 9 points\nabsolute over several strong multimodal rivals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be suboptimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog. The key contribution of our\nframework is a novel multimodal question generator that learns to ask questions\nthat maximize the subsequent video retrieval performance. Our multimodal\nquestion generator uses (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. Furthermore, we also demonstrate that\nour proposed approach also generalizes to the real-world settings that involve\ninteractions with real humans, thus, demonstrating the robustness and\ngenerality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SubER: A Metric for Automatic Evaluation of Subtitle Quality. (arXiv:2205.05805v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05805","description":"<p>This paper addresses the problem of evaluating the quality of automatically\ngenerated subtitles, which includes not only the quality of the\nmachine-transcribed or translated speech, but also the quality of line\nsegmentation and subtitle timing. We propose SubER - a single novel metric\nbased on edit distance with shifts that takes all of these subtitle properties\ninto account. We compare it to existing metrics for evaluating transcription,\ntranslation, and subtitle quality. A careful human evaluation in a post-editing\nscenario shows that the new metric has a high correlation with the post-editing\neffort and direct human assessment scores, outperforming baseline metrics\nconsidering only the subtitle text, such as WER and BLEU, and existing methods\nto integrate segmentation and timing features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilken_P/0/1/0/all/0/1\">Patrick Wilken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgakopoulou_P/0/1/0/all/0/1\">Panayota Georgakopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusov_E/0/1/0/all/0/1\">Evgeny Matusov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AppTek's Submission to the IWSLT 2022 Isometric Spoken Language Translation Task. (arXiv:2205.05807v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05807","description":"<p>To participate in the Isometric Spoken Language Translation Task of the IWSLT\n2022 evaluation, constrained condition, AppTek developed neural\nTransformer-based systems for English-to-German with various mechanisms of\nlength control, ranging from source-side and target-side pseudo-tokens to\nencoding of remaining length in characters that replaces positional encoding.\nWe further increased translation length compliance by sentence-level selection\nof length-compliant hypotheses from different system variants, as well as\nrescoring of N-best candidates from a single system. Length-compliant\nback-translated and forward-translated synthetic data, as well as other\nparallel data variants derived from the original MuST-C training corpus were\nimportant for a good quality/desired length trade-off. Our experimental results\nshow that length compliance levels above 90% can be reached while minimizing\nlosses in MT quality as measured in BERT and BLEU scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilken_P/0/1/0/all/0/1\">Patrick Wilken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusov_E/0/1/0/all/0/1\">Evgeny Matusov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Vocabulary Extreme Classification Using Generative Models. (arXiv:2205.05812v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05812","description":"<p>The extreme multi-label classification (XMC) task aims at tagging content\nwith a subset of labels from an extremely large label set. The label vocabulary\nis typically defined in advance by domain experts and assumed to capture all\nnecessary tags. However in real world scenarios this label set, although large,\nis often incomplete and experts frequently need to refine it. To develop\nsystems that simplify this process, we introduce the task of open vocabulary\nXMC (OXMC): given a piece of content, predict a set of labels, some of which\nmay be outside of the known tag set. Hence, in addition to not having training\ndata for some labels - as is the case in zero-shot classification - models need\nto invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model\nfor OXMC that generates the set of labels as a flat sequence and is trained\nusing a novel loss independent of predicted label order. We show the efficacy\nof the approach, experimenting with popular XMC datasets for which GROOV is\nable to predict meaningful labels outside the given vocabulary while performing\non par with state-of-the-art solutions for known labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanki_P/0/1/0/all/0/1\">Pouya Yanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_K/0/1/0/all/0/1\">Kashyap Popat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Christina Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition. (arXiv:2205.05832v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05832","description":"<p>Recently, FLAT has achieved great success in Chinese Named Entity Recognition\n(NER). This method achieves lexical enhancement by constructing a flat lattice,\nwhich mitigates the difficulties posed by blurred word boundaries and the lack\nof word semantics. To this end, FLAT uses the position information of the\nstarting and ending characters to connect the matching words. However, this\nmethod is likely to match more words when dealing with long texts, resulting in\nvery long input sequences. Therefore, it increases the memory used by\nself-attention and computational costs. To deal with this issue, we advocate a\nnovel lexical enhancement method, InterFormer, that effectively reduces the\namount of computational and memory costs by constructing the non-flat-lattice.\nFurthermore, we implement a complete model, namely NFLAT, for the Chinese NER\ntask. NFLAT decouples lexicon fusion and context feature encoding. Compared\nwith FLAT, it reduces unnecessary attention calculations in \"word-character\"\nand \"word-word\". This reduces the memory usage by about 50\\% and can use more\nextensive lexicons or higher batches for network training. The experimental\nresults obtained on several well-known benchmarks demonstrate the superiority\nof the proposed method over the state-of-the-art character-word hybrid models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoning Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhenhua Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaojun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supplementary Material: Implementation and Experiments for GAU-based Model. (arXiv:2205.05842v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05842","description":"<p>In February this year Google proposed a new Transformer variant called FLASH,\nwhich has a faster speed, lower VRAM footprint and better performance. This is\nachieved by designing a performant layer named GAU (Gated Attention Unit),\nwhich combines the Attention layer and FFN. In this paper, some implementation\ndetails are re-analyzed both theoretically and practically. We then propose a\nnovel GAU-based model and pre-train it model on a Chinese corpus. Results of\nthe CLUE benchmark show that our model achieves a dev average score of 75.02,\n1% higher than RoFormerV1 and being 45% faster, which is also competitive with\nRoFormerV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"e-CARE: a New Dataset for Exploring Explainable Causal Reasoning. (arXiv:2205.05849v1 [cs.AI])","link":"http://arxiv.org/abs/2205.05849","description":"<p>Understanding causality has vital importance for various Natural Language\nProcessing (NLP) applications. Beyond the labeled instances, conceptual\nexplanations of the causality can provide deep understanding of the causal\nfacts to facilitate the causal reasoning process. However, such explanation\ninformation still remains absent in existing causal reasoning resources. In\nthis paper, we fill this gap by presenting a human-annotated explainable CAusal\nREasoning dataset (e-CARE), which contains over 21K causal reasoning questions,\ntogether with natural language formed explanations of the causal questions.\nExperimental results show that generating valid explanations for causal facts\nstill remains especially challenging for the state-of-the-art models, and the\nexplanation information can be helpful for promoting the accuracy and stability\nof causal reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1\">Kai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Open Arabic Named Entity Recognition Tools. (arXiv:2205.05857v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05857","description":"<p>The main objective of this paper is to compare and evaluate the performances\nof three open Arabic NER tools: CAMeL, Hatmi, and Stanza. We collected a corpus\nconsisting of 30 articles written in MSA and manually annotated all the\nentities of the person, organization, and location types at the article\n(document) level. Our results suggest a similarity between Stanza and Hatmi\nwith the latter receiving the highest F1 score for the three entity types.\nHowever, CAMeL achieved the highest precision values for names of people and\norganizations. Following this, we implemented a \"merge\" method that combined\nthe results from the three tools and a \"vote\" method that tagged named entities\nonly when two of the three identified them as entities. Our results showed that\nmerging achieved the highest overall F1 scores. Moreover, merging had the\nhighest recall values while voting had the highest precision values for the\nthree entity types. This indicates that merging is more suitable when recall is\ndesired, while voting is optimal when precision is required. Finally, we\ncollected a corpus of 21,635 articles related to COVID-19 and applied the merge\nand vote methods. Our analysis demonstrates the tradeoff between precision and\nrecall for the two methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldumaykhi_A/0/1/0/all/0/1\">Abdullah Aldumaykhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otai_S/0/1/0/all/0/1\">Saad Otai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsudais_A/0/1/0/all/0/1\">Abdulkareem Alsudais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05862","description":"<p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in\nachieving both representation learning and generation for natural language.\nHowever, existing VAE-based language models either employ elementary RNNs,\nwhich is not powerful to handle multi-tasks, or fine-tunes two pre-trained\nlanguage models (PLMs) for any downstream task, which requires huge energy\nconsumption. In this paper, we introduce the first VAE framework empowered with\nadaptive GPT-2s (AdaVAE). Different from mentioned systems, we unify both the\nencoder and decoder of VAE model using GPT-2s with adaptive parameter-efficient\ncomponents. Experiments from multiple dimensions validate that AdaVAE is\ncompetent to better organize language in generation and representation\nmodeling, even with less than $15\\%$ additionally activated parameters during\ntraining. Our code is available at \\url{https://github.com/ImKeTT/adavae}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinshuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Chit-Chats Enhanced Task-Oriented Dialogue Corpora for Fuse-Motive Conversation Systems. (arXiv:2205.05886v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05886","description":"<p>The goal of building intelligent dialogue systems has largely been separately\npursued under two motives: task-oriented dialogue (TOD) systems, and\nopen-domain systems for chit-chat (CC). Although previous TOD dialogue systems\nwork well in the testing sets of benchmarks, they would lead to undesirable\nfailure when being exposed to natural scenarios in practice, where user\nutterances can be of high motive-diversity that fusing both TOD and CC in\nmulti-turn interaction. Since an industrial TOD system should be able to\nconverse with the user between TOD and CC motives, constructing a fuse-motive\ndialogue dataset that contains both TOD or CC is important. Most prior work\nrelies on crowd workers to collect and annotate large scale dataset and is\nrestricted to English language setting. Our work, on the contrary, addresses\nthis problem in a more effective way and releases a multi-turn dialogues\ndataset called CCET (Chinese Chat-Enhanced-Task). Meanwhile, we also propose a\nline of fuse-motive dialogues formalization approach, along with several\nevaluation metrics for TOD sessions that are integrated by CC utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changhong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap between Reality and Ideality of Entity Matching: A Revisiting and Benchmark Re-Construction. (arXiv:2205.05889v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05889","description":"<p>Entity matching (EM) is the most critical step for entity resolution (ER).\nWhile current deep learningbased methods achieve very impressive performance on\nstandard EM benchmarks, their realworld application performance is much\nfrustrating. In this paper, we highlight that such the gap between reality and\nideality stems from the unreasonable benchmark construction process, which is\ninconsistent with the nature of entity matching and therefore leads to biased\nevaluations of current EM approaches. To this end, we build a new EM corpus and\nre-construct EM benchmarks to challenge critical assumptions implicit in the\nprevious benchmark construction process by step-wisely changing the restricted\nentities, balanced labels, and single-modal records in previous benchmarks into\nopen entities, imbalanced labels, and multimodal records in an open\nenvironment. Experimental results demonstrate that the assumptions made in the\nprevious benchmark construction process are not coincidental with the open\nenvironment, which conceal the main challenges of the task and therefore\nsignificantly overestimate the current progress of entity matching. The\nconstructed benchmarks and code are publicly released\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianshu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiuwen Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Gender Stereotypes in Hindi and Marathi. (arXiv:2205.05901v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05901","description":"<p>As the use of natural language processing increases in our day-to-day life,\nthe need to address gender bias inherent in these systems also amplifies. This\nis because the inherent bias interferes with the semantic structure of the\noutput of these systems while performing tasks like machine translation. While\nresearch is being done in English to quantify and mitigate bias, debiasing\nmethods in Indic Languages are either relatively nascent or absent for some\nIndic languages altogether. Most Indic languages are gendered, i.e., each noun\nis assigned a gender according to each language's grammar rules. As a\nconsequence, evaluation differs from what is done in English. This paper\nevaluates the gender stereotypes in Hindi and Marathi languages. The\nmethodologies will differ from the ones in the English language because there\nare masculine and feminine counterparts in the case of some words. We create a\ndataset of neutral and gendered occupation words, emotion words and measure\nbias with the help of Embedding Coherence Test (ECT) and Relative Norm Distance\n(RND). We also attempt to mitigate this bias from the embeddings. Experiments\nshow that our proposed debiasing techniques reduce gender bias in these\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirtane_N/0/1/0/all/0/1\">Neeraja Kirtane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_T/0/1/0/all/0/1\">Tanvi Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs. (arXiv:2205.05943v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05943","description":"<p>We propose a generative model for text generation, which exhibits\ndisentangled latent representations of syntax and semantics. Contrary to\nprevious work, this model does not need syntactic information such as\nconstituency parses, or semantic information such as paraphrase pairs. Our\nmodel relies solely on the inductive bias found in attention-based\narchitectures such as Transformers.\n</p>\n<p>In the attention of Transformers, keys handle information selection while\nvalues specify what information is conveyed. Our model, dubbed QKVAE, uses\nAttention in its decoder to read latent variables where one latent variable\ninfers keys while another infers values. We run experiments on latent\nrepresentations and experiments on syntax/semantics transfer which show that\nQKVAE displays clear signs of disentangled syntax and semantics. We also show\nthat our model displays competitive syntax transfer capabilities when compared\nto supervised models and that comparable supervised models need a fairly large\namount of data (more than 50K samples) to outperform it on both syntactic and\nsemantic transfer. The code for our experiments is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felhi_G/0/1/0/all/0/1\">Ghazi Felhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Joseph Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Acquisition Model for Multimodal Word Categorization. (arXiv:2205.05974v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05974","description":"<p>Recent advances in self-supervised modeling of text and images open new\nopportunities for computational models of child language acquisition, which is\nbelieved to rely heavily on cross-modal signals. However, prior studies have\nbeen limited by their reliance on vision models trained on large image datasets\nannotated with a pre-defined set of depicted object categories. This is (a) not\nfaithful to the information children receive and (b) prohibits the evaluation\nof such models with respect to category learning tasks, due to the pre-imposed\ncategory structure. We address this gap, and present a cognitively-inspired,\nmultimodal acquisition model, trained from image-caption pairs on naturalistic\ndata using cross-modal self-supervision. We show that the model learns word\ncategories and object recognition abilities, and presents trends reminiscent of\nthose reported in the developmental literature. We make our code and trained\nmodels public for future reference and use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_U/0/1/0/all/0/1\">Uri Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiSocrates: Towards Answering Ethical Quandary Questions. (arXiv:2205.05989v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05989","description":"<p>Considerable advancements have been made in various NLP tasks based on the\nimpressive power of large pre-trained language models (LLMs). These results\nhave inspired efforts to understand the limits of LLMs so as to evaluate how\nfar we are from achieving human level general natural language understanding.\nIn this work, we challenge the capability of LLMs with the new task of Ethical\nQuandary Generative Question Answering. Ethical quandary questions are more\nchallenging to address because multiple conflicting answers may exist to a\nsingle quandary. We propose a system, AiSocrates, that provides an answer with\na deliberative exchange of different perspectives to an ethical quandary, in\nthe approach of Socratic philosophy, instead of providing a closed answer like\nan oracle. AiSocrates searches for different ethical principles applicable to\nthe ethical quandary and generates an answer conditioned on the chosen\nprinciples through prompt-based few-shot learning. We also address safety\nconcerns by providing a human controllability option in choosing ethical\nprinciples. We show that AiSocrates generates promising answers to ethical\nquandary questions with multiple perspectives, 6.92% more often than answers\nwritten by human philosophers by one measure, but the system still needs\nimprovement to match the coherence of human philosophers fully. We argue that\nAiSocrates is a promising step toward developing an NLP system that\nincorporates human values explicitly by prompt instructions. We are releasing\nthe code for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1\">Leila Khalatbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kee_H/0/1/0/all/0/1\">Hayden Kee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Formality in Low-Resource NMT with Domain Adaptation and Re-Ranking: SLT-CDT-UoS at IWSLT2022. (arXiv:2205.05990v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05990","description":"<p>This paper describes the SLT-CDT-UoS group's submission to the first Special\nTask on Formality Control for Spoken Language Translation, part of the IWSLT\n2022 Evaluation Campaign. Our efforts were split between two fronts: data\nengineering and altering the objective function for best hypothesis selection.\nWe used language-independent methods to extract formal and informal sentence\npairs from the provided corpora; using English as a pivot language, we\npropagated formality annotations to languages treated as zero-shot in the task;\nwe also further improved formality controlling with a hypothesis re-ranking\napproach. On the test sets for English-to-German and English-to-Spanish, we\nachieved an average accuracy of .935 within the constrained setting and .995\nwithin unconstrained setting. In a zero-shot setting for English-to-Russian and\nEnglish-to-Italian, we scored average accuracy of .590 for constrained setting\nand .659 for unconstrained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1\">Sebastian T. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization. (arXiv:2205.06009v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06009","description":"<p>Neural abstractive summarization models are prone to generate summaries which\nare factually inconsistent with their source documents. Previous work has\nintroduced the task of recognizing such factual inconsistency as a downstream\napplication of natural language inference (NLI). However, state-of-the-art NLI\nmodels perform poorly in this context due to their inability to generalize to\nthe target task. In this work, we show that NLI models can be effective for\nthis task when the training data is augmented with high-quality task-oriented\nexamples. We introduce Falsesum, a data generation pipeline leveraging a\ncontrollable text generation model to perturb human-annotated summaries,\nintroducing varying types of factual inconsistencies. Unlike previously\nintroduced document-level NLI datasets, our generated dataset contains examples\nthat are diverse and inconsistent yet plausible. We show that models trained on\na Falsesum-augmented NLI dataset improve the state-of-the-art performance\nacross four benchmarks for detecting factual inconsistency in summarization.\n</p>\n<p>The code to obtain the dataset is available online at\nhttps://github.com/joshbambrick/Falsesum\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Utama_P/0/1/0/all/0/1\">Prasetya Ajie Utama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bambrick_J/0/1/0/all/0/1\">Joshua Bambrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DTW at Qur'an QA 2022: Utilising Transfer Learning with Transformers for Question Answering in a Low-resource Domain. (arXiv:2205.06025v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06025","description":"<p>The task of machine reading comprehension (MRC) is a useful benchmark to\nevaluate the natural language understanding of machines. It has gained\npopularity in the natural language processing (NLP) field mainly due to the\nlarge number of datasets released for many languages. However, the research in\nMRC has been understudied in several domains, including religious texts. The\ngoal of the Qur'an QA 2022 shared task is to fill this gap by producing\nstate-of-the-art question answering and reading comprehension research on\nQur'an. This paper describes the DTW entry to the Quran QA 2022 shared task.\nOur methodology uses transfer learning to take advantage of available Arabic\nMRC data. We further improve the results using various ensemble learning\nstrategies. Our approach provided a partial Reciprocal Rank (pRR) score of 0.49\non the test set, proving its strong performance on the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Premasiri_D/0/1/0/all/0/1\">Damith Premasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitkov_R/0/1/0/all/0/1\">Ruslan Mitkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling with Attribute-Related Information for Controlling Language Models. (arXiv:2205.06036v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06036","description":"<p>The dominant approaches for controlling language models are based on\nfine-tuning large language models or prompt engineering. However, these methods\noften require condition-specific data or considerable hand-crafting. We propose\na new simple guided decoding method, Gamma Sampling, which does not require\ncomplex engineering and any extra data. Gamma Sampling introduces\nattribute-related information (provided by humans or language models\nthemselves) into the sampling process to guide language models to generate\ntexts with desired attributes. Experiments on controlling topics and sentiments\nof generated text show Gamma Sampling to be superior in diversity, attribute\nrelevance and overall quality of generated samples while maintaining a fast\ngeneration speed. In addition, we successfully applied Gamma Sampling to\ncontrol other attributes of language such as relatedness and repetition, which\nfurther demonstrates the versatility and effectiveness of this method. Gamma\nSampling is now available in the python package samplings via import gamma\nsampling from samplings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimRelUz: Similarity and Relatedness scores as a Semantic Evaluation dataset for Uzbek language. (arXiv:2205.06072v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06072","description":"<p>Semantic relatedness between words is one of the core concepts in natural\nlanguage processing, thus making semantic evaluation an important task. In this\npaper, we present a semantic model evaluation dataset: SimRelUz - a collection\nof similarity and relatedness scores of word pairs for the low-resource Uzbek\nlanguage. The dataset consists of more than a thousand pairs of words carefully\nselected based on their morphological features, occurrence frequency, semantic\nrelation, as well as annotated by eleven native Uzbek speakers from different\nage groups and gender. We also paid attention to the problem of dealing with\nrare words and out-of-vocabulary words to thoroughly evaluate the robustness of\nsemantic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaev_U/0/1/0/all/0/1\">Ulugbek Salaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuriyozov_E/0/1/0/all/0/1\">Elmurod Kuriyozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language. (arXiv:2205.06111v1 [cs.AI])","link":"http://arxiv.org/abs/2205.06111","description":"<p>To solve difficult tasks, humans ask questions to acquire knowledge from\nexternal sources. In contrast, classical reinforcement learning agents lack\nsuch an ability and often resort to exploratory behavior. This is exacerbated\nas few present-day environments support querying for knowledge. In order to\nstudy how agents can be taught to query external knowledge via language, we\nfirst introduce two new environments: the grid-world-based Q-BabyAI and the\ntext-based Q-TextWorld. In addition to physical interactions, an agent can\nquery an external knowledge source specialized for these environments to gather\ninformation. Second, we propose the \"Asking for Knowledge\" (AFK) agent, which\nlearns to generate language commands to query for meaningful knowledge that\nhelps solve the tasks. AFK leverages a non-parametric memory, a pointer\nmechanism and an episodic exploration bonus to tackle (1) a large query\nlanguage space, (2) irrelevant information, (3) delayed reward for making\nmeaningful queries. Extensive experiments demonstrate that the AFK agent\noutperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1\">Iou-Jen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Findings of the Shared Task on Offensive Span Identification from Code-Mixed Tamil-English Comments. (arXiv:2205.06118v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06118","description":"<p>Offensive content moderation is vital in social media platforms to support\nhealthy online discussions. However, their prevalence in codemixed Dravidian\nlanguages is limited to classifying whole comments without identifying part of\nit contributing to offensiveness. Such limitation is primarily due to the lack\nof annotated data for offensive spans. Accordingly, in this shared task, we\nprovide Tamil-English code-mixed social comments with offensive spans. This\npaper outlines the dataset so released, methods, and results of the submitted\nsystems\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravikiran_M/0/1/0/all/0/1\">Manikandan Ravikiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madasamy_A/0/1/0/all/0/1\">Anand Kumar Madasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanesan_S/0/1/0/all/0/1\">Sangeetha Sivanesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajalakshmi_R/0/1/0/all/0/1\">Ratnavel Rajalakshmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1\">Sajeetha Thavareesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnusamy_R/0/1/0/all/0/1\">Rahul Ponnusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+/_S/0/1/0/all/0/1\">Shankar Mahadevan./</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction. (arXiv:2205.06119v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06119","description":"<p>This paper investigates the effectiveness of sentence-level transformers for\nzero-shot offensive span identification on a code-mixed Tamil dataset. More\nspecifically, we evaluate rationale extraction methods of Local Interpretable\nModel Agnostic Explanations (LIME) \\cite{DBLP:conf/kdd/Ribeiro0G16} and\nIntegrated Gradients (IG) \\cite{DBLP:conf/icml/SundararajanTY17} for adapting\ntransformer based offensive language classification models for zero-shot\noffensive span identification. To this end, we find that LIME and IG show\nbaseline $F_{1}$ of 26.35\\% and 44.83\\%, respectively. Besides, we study the\neffect of data set size and training process on the overall accuracy of span\nidentification. As a result, we find both LIME and IG to show significant\nimprovement with Masked Data Augmentation and Multilabel Training, with $F_{1}$\nof 50.23\\% and 47.38\\% respectively. \\textit{Disclaimer : This paper contains\nexamples that may be considered profane, vulgar, or offensive. The examples do\nnot represent the views of the authors or their employers/graduate schools\ntowards any person(s), group(s), practice(s), or entity/entities. Instead they\nare used to emphasize only the linguistic research challenges.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravikiran_M/0/1/0/all/0/1\">Manikandan Ravikiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code. (arXiv:2205.06126v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06126","description":"<p>People perceive the world with multiple senses (e.g., through hearing sounds,\nreading words and seeing objects). However, most existing AI systems only\nprocess an individual modality. This paper presents an approach that excels at\nhandling multiple modalities of information with a single model. In our\n\"{SkillNet}\" model, different parts of the parameters are specialized for\nprocessing different modalities. Unlike traditional dense models that always\nactivate all the model parameters, our model sparsely activates parts of the\nparameters whose skills are relevant to the task. Such model design enables\nSkillNet to learn skills in a more interpretable way. We develop our model for\nfive modalities including text, image, sound, video and code. Results show\nthat, SkillNet performs comparably to five modality-specific fine-tuned models.\nMoreover, our model supports self-supervised pretraining with the same sparsely\nactivated way, resulting in better initialized parameters for different\nmodalities. We find that pretraining significantly improves the performance of\nSkillNet on five modalities, on par with or even better than baselines with\nmodality-specific pretraining. On the task of Chinese text-to-image retrieval,\nour final system achieves higher accuracy than existing leading systems\nincluding Wukong{ViT-B} and Wenlan 2.0 while using less number of activated\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liangxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models. (arXiv:2205.06130v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06130","description":"<p>Massively Multilingual Transformer based Language Models have been observed\nto be surprisingly effective on zero-shot transfer across languages, though the\nperformance varies from language to language depending on the pivot language(s)\nused for fine-tuning. In this work, we build upon some of the existing\ntechniques for predicting the zero-shot performance on a task, by modeling it\nas a multi-task learning problem. We jointly train predictive models for\ndifferent tasks which helps us build more accurate predictors for tasks where\nwe have test data in very few languages to measure the actual performance of\nthe model. Our approach also lends us the ability to perform a much more robust\nfeature selection and identify a common set of features that influence\nzero-shot performance across a variety of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair NLP Models with Differentially Private Text Encoders. (arXiv:2205.06135v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06135","description":"<p>Encoded text representations often capture sensitive attributes about\nindividuals (e.g., race or gender), which raise privacy concerns and can make\ndownstream models unfair to certain groups. In this work, we propose FEDERATE,\nan approach that combines ideas from differential privacy and adversarial\ntraining to learn private text representations which also induces fairer\nmodels. We empirically evaluate the trade-off between the privacy of the\nrepresentations and the fairness and accuracy of the downstream model on four\nNLP datasets. Our results show that FEDERATE consistently improves upon\nprevious methods, and thus suggest that privacy and fairness can positively\nreinforce each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_G/0/1/0/all/0/1\">Gaurav Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_P/0/1/0/all/0/1\">Pascal Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Mikaela Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1\">Aur&#xe9;lien Bellet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?. (arXiv:2205.06149v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06149","description":"<p>In recent years, deep neural language models have made strong progress in\nvarious NLP tasks. This work explores one facet of the question whether\nstate-of-the-art NLP models exhibit elementary mechanisms known from human\ncognition. The exploration is focused on a relatively primitive mechanism for\nwhich there is a lot of evidence from various psycholinguistic experiments with\ninfants. The computation of \"abstract sameness relations\" is assumed to play an\nimportant role in human language acquisition and processing, especially in\nlearning more complex grammar rules. In order to investigate this mechanism in\nBERT and other pre-trained language models (PLMs), the experiment designs from\nstudies with infants were taken as the starting point. On this basis, we\ndesigned experimental settings in which each element from the original studies\nwas mapped to a component of language models. Even though the task in our\nexperiments was relatively simple, the results suggest that the cognitive\nfaculty of computing abstract sameness relations is stronger in infants than in\nall investigated PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_L/0/1/0/all/0/1\">Lukas Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding. (arXiv:2205.06153v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06153","description":"<p>Data augmentation is an effective approach to tackle over-fitting. Many\nprevious works have proposed different data augmentations strategies for NLP,\nsuch as noise injection, word replacement, back-translation etc. Though\neffective, they missed one important characteristic of\nlanguage--compositionality, meaning of a complex expression is built from its\nsub-parts. Motivated by this, we propose a compositional data augmentation\napproach for natural language understanding called TreeMix. Specifically,\nTreeMix leverages constituency parsing tree to decompose sentences into\nconstituent sub-structures and the Mixup data augmentation technique to\nrecombine them to generate new sentences. Compared with previous approaches,\nTreeMix introduces greater diversity to the samples generated and encourages\nmodels to learn compositionality of NLP data. Extensive experiments on text\nclassification and SCAN demonstrate that TreeMix outperforms current\nstate-of-the-art data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Prefix-Tuning for Generative Template-based Event Extraction. (arXiv:2205.06166v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06166","description":"<p>We consider event extraction in a generative manner with template-based\nconditional generation. Although there is a rising trend of casting the task of\nevent extraction as a sequence generation problem with prompts, these\ngeneration-based methods have two significant challenges, including using\nsuboptimal prompts and static event type information. In this paper, we propose\na generative template-based event extraction method with dynamic prefix\n(GTEE-DynPref) by integrating context information with type-specific prefixes\nto learn a context-specific prefix for each context. Experimental results show\nthat our model achieves competitive results with the state-of-the-art\nclassification-based model OneIE on ACE 2005 and achieves the best performances\non ERE. Additionally, our model is proven to be portable to new types of events\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Ge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using dependency parsing for few-shot learning in distributional semantics. (arXiv:2205.06168v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06168","description":"<p>In this work, we explore the novel idea of employing dependency parsing\ninformation in the context of few-shot learning, the task of learning the\nmeaning of a rare word based on a limited amount of context sentences. Firstly,\nwe use dependency-based word embedding models as background spaces for few-shot\nlearning. Secondly, we introduce two few-shot learning methods which enhance\nthe additive baseline model by using dependencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preda_S/0/1/0/all/0/1\">Stefania Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalist Agent. (arXiv:2205.06175v1 [cs.AI])","link":"http://arxiv.org/abs/2205.06175","description":"<p>Inspired by progress in large-scale language modeling, we apply a similar\napproach towards building a single generalist agent beyond the realm of text\noutputs. The agent, which we refer to as Gato, works as a multi-modal,\nmulti-task, multi-embodiment generalist policy. The same network with the same\nweights can play Atari, caption images, chat, stack blocks with a real robot\narm and much more, deciding based on its context whether to output text, joint\ntorques, button presses, or other tokens. In this report we describe the model\nand the data, and document the current capabilities of Gato.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reed_S/0/1/0/all/0/1\">Scott Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolna_K/0/1/0/all/0/1\">Konrad Zolna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1\">Emilio Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colmenarejo_S/0/1/0/all/0/1\">Sergio Gomez Colmenarejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikov_A/0/1/0/all/0/1\">Alexander Novikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1\">Gabriel Barth-Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_M/0/1/0/all/0/1\">Mai Gimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulsky_Y/0/1/0/all/0/1\">Yury Sulsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Jackie Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1\">Jost Tobias Springenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eccles_T/0/1/0/all/0/1\">Tom Eccles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1\">Jake Bruce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1\">Ali Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1\">Ashley Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordbar_M/0/1/0/all/0/1\">Mahyar Bordbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Meta Learning for Low Resource Speech Recognition. (arXiv:2205.06182v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06182","description":"<p>We propose a new meta learning based framework for low resource speech\nrecognition that improves the previous model agnostic meta learning (MAML)\napproach. The MAML is a simple yet powerful meta learning approach. However,\nthe MAML presents some core deficiencies such as training instabilities and\nslower convergence speed. To address these issues, we adopt multi-step loss\n(MSL). The MSL aims to calculate losses at every step of the inner loop of MAML\nand then combines them with a weighted importance vector. The importance vector\nensures that the loss at the last step has more importance than the previous\nsteps. Our empirical evaluation shows that MSL significantly improves the\nstability of the training procedure and it thus also improves the accuracy of\nthe overall system. Our proposed system outperforms MAML based low resource ASR\nsystem on various languages in terms of character error rates and stable\ntraining behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satwinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Human Psychometric Properties Using Computational Language Models. (arXiv:2205.06203v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06203","description":"<p>Transformer-based language models (LMs) continue to achieve state-of-the-art\nperformance on natural language processing (NLP) benchmarks, including tasks\ndesigned to mimic human-inspired \"commonsense\" competencies. To better\nunderstand the degree to which LMs can be said to have certain linguistic\nreasoning skills, researchers are beginning to adapt the tools and concepts\nfrom psychometrics. But to what extent can benefits flow in the other\ndirection? In other words, can LMs be of use in predicting the psychometric\nproperties of test items, when those items are given to human participants? If\nso, the benefit for psychometric practitioners is enormous, as it can reduce\nthe need for multiple rounds of empirical testing. We gather responses from\nnumerous human participants and LMs (transformer- and non-transformer-based) on\na broad diagnostic test of linguistic competencies. We then use the human\nresponses to calculate standard psychometric properties of the items in the\ndiagnostic test, using the human responses and the LM responses separately. We\nthen determine how well these two sets of predictions correlate. We find that\ntransformer-based LMs predict the human psychometric data consistently well\nacross most categories, suggesting that they can be used to gather human-like\npsychometric data without the need for extensive human trials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1\">Animesh Nighojkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation. (arXiv:2205.06207v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06207","description":"<p>Scientific extreme summarization (TLDR) aims to form ultra-short summaries of\nscientific papers. Previous efforts on curating scientific TLDR datasets failed\nto scale up due to the heavy human annotation and domain expertise required. In\nthis paper, we propose a simple yet effective approach to automatically\nextracting TLDR summaries for scientific papers from their citation texts.\nBased on the proposed approach, we create a new benchmark CiteSum without human\nannotation, which is around 30 times larger than the previous human-curated\ndataset SciTLDR. We conduct a comprehensive analysis of CiteSum, examining its\ndata characteristics and establishing strong baselines. We further demonstrate\nthe usefulness of CiteSum by adapting models pre-trained on CiteSum (named\nCITES) to new tasks and domains with limited supervision. For scientific\nextreme summarization, CITES outperforms most fully-supervised methods on\nSciTLDR without any fine-tuning and obtains state-of-the-art results with only\n128 examples. For news extreme summarization, CITES achieves significant gains\non XSum over its base model (not pre-trained on CiteSum), e.g., +7.2 ROUGE-1\nzero-shot performance and state-of-the-art few-shot performance. For news\nheadline generation, CITES performs the best among unsupervised and zero-shot\nmethods on Gigaword.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics. (arXiv:2205.06253v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06253","description":"<p>While there have been significant gains in the field of automated video\ndescription, the generalization performance of automated description models to\nnovel domains remains a major barrier to using these systems in the real world.\nMost visual description methods are known to capture and exploit patterns in\nthe training data leading to evaluation metric increases, but what are those\npatterns? In this work, we examine several popular visual description datasets,\nand capture, analyze, and understand the dataset-specific linguistic patterns\nthat models exploit but do not generalize to new domains. At the token level,\nsample level, and dataset level, we find that caption diversity is a major\ndriving factor behind the generation of generic and uninformative captions. We\nfurther show that state-of-the-art models even outperform held-out ground truth\ncaptions on modern metrics, and that this effect is an artifact of linguistic\ndiversity in datasets. Understanding this linguistic diversity is key to\nbuilding strong captioning models, we recommend several methods and approaches\nfor maintaining diversity in the collection of new data, and dealing with the\nconsequences of limited diversity when using current models and metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1\">Austin Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1\">Sudheendra Vijayanarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1\">Bryan Seybold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John F. Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue. (arXiv:2205.06262v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06262","description":"<p>Task transfer, transferring knowledge contained in related tasks, holds the\npromise of reducing the quantity of labeled data required to fine-tune language\nmodels. Dialogue understanding encompasses many diverse tasks, yet task\ntransfer has not been thoroughly studied in conversational AI. This work\nexplores conversational task transfer by introducing FETA: a benchmark for\nfew-sample task transfer in open-domain dialogue. FETA contains two underlying\nsets of conversations upon which there are 10 and 7 tasks annotated, enabling\nthe study of intra-dataset task transfer; task transfer without domain\nadaptation. We utilize three popular language models and three learning\nalgorithms to analyze the transferability between 132 source-target task pairs\nand create a baseline for future work. We run experiments in the single- and\nmulti-source settings and report valuable findings, e.g., most performance\ntrends are model-specific, and span extraction and multiple-choice tasks\nbenefit the most from task transfer. In addition to task transfer, FETA can be\na valuable resource for future research into the efficiency and\ngeneralizability of pre-training datasets and model architectures, as well as\nfor learning settings such as continual and multitask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1\">Connor Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1\">Luke Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1\">Deepak Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers. (arXiv:2205.06266v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06266","description":"<p>Multilingual pre-trained models are known to suffer from the curse of\nmultilinguality, which causes per-language performance to drop as they cover\nmore languages. We address this issue by introducing language-specific modules,\nwhich allows us to grow the total capacity of the model, while keeping the\ntotal number of trainable parameters per language constant. In contrast with\nprior work that learns language-specific components post-hoc, we pre-train the\nmodules of our Cross-lingual Modular (X-Mod) models from the start. Our\nexperiments on natural language inference, named entity recognition and\nquestion answering show that our approach not only mitigates the negative\ninterference between languages, but also enables positive transfer, resulting\nin improved monolingual and cross-lingual performance. Furthermore, our\napproach enables adding languages post-hoc with no measurable drop in\nperformance, no longer limiting the model usage to the set of pre-trained\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The jsRealB Text Realizer: Organization and Use Cases -- Revised version. (arXiv:2012.15425v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15425","description":"<p>This paper describes the design principles behind jsRealB (Version 4.0), a\nsurface realizer written JavaScript for English or French sentences from a\nspecification inspired by the constituent syntax formalism but for which a\ndependency-based input notation is also available. jsRealB can be used either\nwithin a web page or as a node.js module. We show that the seemingly simple\nprocess of text realization involves many interesting implementation challenges\nin order to take into account the specifics of each language. jsRealB has a\nlarge coverage of English and French and has been used to develop realistic\ndata-to-text applications and to reproduce existing literary texts and\nsentences from Universal Dependency annotations. Its source code and that of\nits applications are available on GitHub. The port of this approach to Python\n(pyrealb) is also presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lapalme_G/0/1/0/all/0/1\">Guy Lapalme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01226","description":"<p>More predictable words are easier to process - they are read faster and\nelicit smaller neural signals associated with processing difficulty, most\nnotably, the N400 component of the event-related brain potential. Thus, it has\nbeen argued that prediction of upcoming words is a key component of language\ncomprehension, and that studying the amplitude of the N400 is a valuable way to\ninvestigate the predictions we make. In this study, we investigate whether the\nlinguistic predictions of computational language models or humans better\nreflect the way in which natural language stimuli modulate the amplitude of the\nN400. One important difference in the linguistic predictions of humans versus\ncomputational language models is that while language models base their\npredictions exclusively on the preceding linguistic context, humans may rely on\nother factors. We find that the predictions of three top-of-the-line\ncontemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more\nclosely than human predictions. This suggests that the predictive processes\nunderlying the N400 may be more sensitive to the surface-level statistics of\nlanguage than previously thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A Semantic Evidence View. (arXiv:2109.11800v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11800","description":"<p>Knowledge Graph Embedding (KGE) aims to learn representations for entities\nand relations. Most KGE models have gained great success, especially on\nextrapolation scenarios. Specifically, given an unseen triple (h, r, t), a\ntrained model can still correctly predict t from (h, r, ?), or h from (?, r,\nt), such extrapolation ability is impressive. However, most existing KGE works\nfocus on the design of delicate triple modeling function, which mainly tells us\nhow to measure the plausibility of observed triples, but offers limited\nexplanation of why the methods can extrapolate to unseen data, and what are the\nimportant factors to help KGE extrapolate. Therefore in this work, we attempt\nto study the KGE extrapolation of two problems: 1. How does KGE extrapolate to\nunseen data? 2. How to design the KGE model with better extrapolation ability?\nFor the problem 1, we first discuss the impact factors for extrapolation and\nfrom relation, entity and triple level respectively, propose three Semantic\nEvidences (SEs), which can be observed from train set and provide important\nsemantic information for extrapolation. Then we verify the effectiveness of SEs\nthrough extensive experiments on several typical KGE methods. For the problem\n2, to make better use of the three levels of SE, we propose a novel GNN-based\nKGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In\nSE-GNN, each level of SE is modeled explicitly by the corresponding neighbor\npattern, and merged sufficiently by the multi-layer aggregation, which\ncontributes to obtaining more extrapolative knowledge representation. Finally,\nthrough extensive experiments on FB15k-237 and WN18RR datasets, we show that\nSE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task\nand performs a better extrapolation ability. Our code is available at\nhttps://github.com/renli1024/SE-GNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning. (arXiv:2110.07867v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07867","description":"<p>Why can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidence indicating that the adaptations of PLMs to\nvarious few-shot tasks can be reparameterized as optimizing only a few free\nparameters in a unified low-dimensional intrinsic task subspace, which may help\nus understand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. To find such a subspace and examine its universality, we propose an\nanalysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort\nto the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen data or tasks by only tuning parameters in\nthis subspace. In the experiments, we study diverse few-shot NLP tasks and\nsurprisingly find that in a 250-dimensional subspace found with 100 tasks, by\nonly tuning 250 free parameters, we can recover 97% and 83% of the full prompt\ntuning performance for 100 seen tasks (using different training data) and 20\nunseen tasks, respectively, showing great generalization ability of the found\nintrinsic task subspace. Besides being an analysis tool, IPT could further\nbring practical benefits, such as improving the prompt tuning stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COPA-SSE: Semi-structured Explanations for Commonsense Reasoning. (arXiv:2201.06777v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06777","description":"<p>We present Semi-Structured Explanations for COPA (COPA-SSE), a new\ncrowdsourced dataset of 9,747 semi-structured, English common sense\nexplanations for Choice of Plausible Alternatives (COPA) questions. The\nexplanations are formatted as a set of triple-like common sense statements with\nConceptNet relations but freely written concepts. This semi-structured format\nstrikes a balance between the high quality but low coverage of structured data\nand the lower quality but high coverage of free-form crowdsourcing. Each\nexplanation also includes a set of human-given quality ratings. With their\nfamiliar format, the explanations are geared towards commonsense reasoners\noperating on knowledge graphs and serve as a starting point for ongoing work on\nimproving such systems. The dataset is available at\nhttps://github.com/a-brassard/copa-sse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brassard_A/0/1/0/all/0/1\">Ana Brassard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinzerling_B/0/1/0/all/0/1\">Benjamin Heinzerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavumba_P/0/1/0/all/0/1\">Pride Kavumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering. (arXiv:2204.04581v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04581","description":"<p>Retrieval augmented language models have recently become the standard for\nknowledge intensive tasks. Rather than relying purely on latent semantics\nwithin the parameters of large neural models, these methods enlist a\nsemi-parametric memory to encode an index of knowledge for the model to\nretrieve over. Most prior work has employed text passages as the unit of\nknowledge, which has high coverage at the cost of interpretability,\ncontrollability, and efficiency. The opposite properties arise in other methods\nwhich have instead relied on knowledge base (KB) facts. At the same time, more\nrecent work has demonstrated the effectiveness of storing and retrieving from\nan index of Q-A pairs derived from text \\citep{lewis2021paq}. This approach\nyields a high coverage knowledge representation that maintains KB-like\nproperties due to its representations being more atomic units of information.\nIn this work we push this line of research further by proposing a\nquestion-answer augmented encoder-decoder model and accompanying pretraining\nstrategy. This yields an end-to-end system that not only outperforms prior QA\nretrieval methods on single-hop QA tasks but also enables compositional\nreasoning, as demonstrated by strong performance on two multi-hop QA datasets.\nTogether, these methods improve the ability to interpret and control the model\nwhile narrowing the performance gap with passage retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. (arXiv:2205.04733v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.04733","description":"<p>Neural retrievers based on dense representations combined with Approximate\nNearest Neighbors search have recently received a lot of attention, owing their\nsuccess to distillation and/or better sampling of examples for training --\nwhile still relying on the same backbone architecture. In the meantime, sparse\nrepresentation learning fueled by traditional inverted indexing techniques has\nseen a growing interest, inheriting from desirable IR priors such as explicit\nlexical matching. While some architectural variants have been proposed, a\nlesser effort has been put in the training of such models. In this work, we\nbuild on SPLADE -- a sparse expansion-based retriever -- and show to which\nextent it is able to benefit from the same training improvements as dense\nmodels, by studying the effect of distillation, hard-negative mining as well as\nthe Pre-trained Language Model initialization. We furthermore study the link\nbetween effectiveness and efficiency, on in-domain and zero-shot settings,\nleading to state-of-the-art results in both scenarios for sufficiently\nexpressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Climate Awareness in NLP Research. (arXiv:2205.05071v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05071","description":"<p>The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers. (arXiv:2205.05055v2 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based language models are able to perform few-shot learning\n(also known as in-context learning), without having been explicitly trained for\nit. We hypothesized that specific distributional properties of natural language\nmight drive this emergent phenomenon, as these characteristics might lead to a\nkind of interpolation between few-shot meta-training (designed to elicit rapid\nfew-shot learning) and standard supervised training (designed to elicit gradual\nin-weights learning). We also hypothesized that these distributional properties\ncould lead to emergent few-shot learning in domains outside of language.\nInspired by this idea, we ran a series of experiments on a standard image-based\nfew-shot dataset. We discovered that a number of data properties did indeed\npromote the emergence of few-shot learning in transformer models. All of these\nproperties are present in natural language -- burstiness, long-tailedness, and\nmany-to-one or one-to-many label mappings. The data influenced whether models\nwere biased towards either few-shot learning vs. memorizing information in\ntheir weights; models could generally perform well at only one or the other.\nHowever, we discovered that an additional distributional property could allow\nthe two capabilities to co-exist in the same model -- a skewed, Zipfian\ndistribution over classes -- which occurs in language as well. Notably,\ntraining data that could elicit few-shot learning in transformers were unable\nto elicit few-shot learning in recurrent models. In sum, we find that few-shot\nlearning emerges only from applying the right architecture to the right data\ndistribution; neither component is sufficient on its own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Closer Look at Audio-Visual Multi-Person Speech Recognition and Active Speaker Selection. (arXiv:2205.05684v1 [eess.AS])","link":"http://arxiv.org/abs/2205.05684","description":"<p>Audio-visual automatic speech recognition is a promising approach to robust\nASR under noisy conditions. However, up until recently it had been\ntraditionally studied in isolation assuming the video of a single speaking face\nmatches the audio, and selecting the active speaker at inference time when\nmultiple people are on screen was put aside as a separate problem. As an\nalternative, recent work has proposed to address the two problems\nsimultaneously with an attention mechanism, baking the speaker selection\nproblem directly into a fully differentiable model. One interesting finding was\nthat the attention indirectly learns the association between the audio and the\nspeaking face even though this correspondence is never explicitly provided at\ntraining time. In the present work we further investigate this connection and\nexamine the interplay between the two problems. With experiments involving over\n50 thousand hours of public YouTube videos as training data, we first evaluate\nthe accuracy of the attention layer on an active speaker selection task.\nSecondly, we show under closer scrutiny that an end-to-end model performs at\nleast as well as a considerably larger two-step system that utilizes a hard\ndecision boundary under various noise conditions and number of parallel face\ntracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1\">Otavio Braga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Class 3D Object Detection with Single-Class Supervision. (arXiv:2205.05703v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05703","description":"<p>While multi-class 3D detectors are needed in many robotics applications,\ntraining them with fully labeled datasets can be expensive in labeling cost. An\nalternative approach is to have targeted single-class labels on disjoint data\nsamples. In this paper, we are interested in training a multi-class 3D object\ndetection model, while using these single-class labeled data. We begin by\ndetailing the unique stance of our \"Single-Class Supervision\" (SCS) setting\nwith respect to related concepts such as partial supervision and semi\nsupervision. Then, based on the case study of training the multi-class version\nof Range Sparse Net (RSN), we adapt a spectrum of algorithms -- from supervised\nlearning to pseudo-labeling -- to fully exploit the properties of our SCS\nsetting, and perform extensive ablation studies to identify the most effective\nalgorithm and practice. Empirical experiments on the Waymo Open Dataset show\nthat proper training under SCS can approach or match full supervision training\nwhile saving labeling costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Maoqing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Z/0/1/0/all/0/1\">Zhaoqi Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Video Generation from a Single Video. (arXiv:2205.05725v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05725","description":"<p>GANs are able to perform generation and manipulation tasks, trained on a\nsingle video. However, these single video GANs require unreasonable amount of\ntime to train on a single video, rendering them almost impractical. In this\npaper we question the necessity of a GAN for generation from a single video,\nand introduce a non-parametric baseline for a variety of generation and\nmanipulation tasks. We revive classical space-time patches-nearest-neighbors\napproaches and adapt them to a scalable unconditional generative model, without\nany learning. This simple baseline surprisingly outperforms single-video GANs\nin visual quality and realism (confirmed by quantitative and qualitative\nevaluations), and is disproportionately faster (runtime reduced from several\ndays to seconds). Our approach is easily scaled to Full-HD videos. We also use\nthe same framework to demonstrate video analogies and spatio-temporal\nretargeting. These observations show that classical approaches significantly\noutperform heavy deep learning machinery for these tasks. This sets a new\nbaseline for single-video generation and manipulation tasks, and no less\nimportant -- makes diverse generation from a single video practically possible\nfor the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haim_N/0/1/0/all/0/1\">Niv Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feinstein_B/0/1/0/all/0/1\">Ben Feinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granot_N/0/1/0/all/0/1\">Niv Granot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shocher_A/0/1/0/all/0/1\">Assaf Shocher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1\">Shai Bagon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational behavior recognition in child and adolescent psychiatry: A statistical and machine learning analysis plan. (arXiv:2205.05737v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05737","description":"<p>Motivation: Behavioral observations are an important resource in the study\nand evaluation of psychological phenomena, but it is costly, time-consuming,\nand susceptible to bias. Thus, we aim to automate coding of human behavior for\nuse in psychotherapy and research with the help of artificial intelligence (AI)\ntools. Here, we present an analysis plan. Methods: Videos of a gold-standard\nsemi-structured diagnostic interview of 25 youth with obsessive-compulsive\ndisorder (OCD) and 12 youth without a psychiatric diagnosis (no-OCD) will be\nanalyzed. Youth were between 8 and 17 years old. Features from the videos will\nbe extracted and used to compute ratings of behavior, which will be compared to\nratings of behavior produced by mental health professionals trained to use a\nspecific behavioral coding manual. We will test the effect of OCD diagnosis on\nthe computationally-derived behavior ratings using multivariate analysis of\nvariance (MANOVA). Using the generated features, a binary classification model\nwill be built and used to classify OCD/no-OCD classes. Discussion: Here, we\npresent a pre-defined plan for how data will be pre-processed, analyzed and\npresented in the publication of results and their interpretation. A challenge\nfor the proposed study is that the AI approach will attempt to derive\nbehavioral ratings based solely on vision, whereas humans use visual,\nparalinguistic and linguistic cues to rate behavior. Another challenge will be\nusing machine learning models for body and facial movement detection trained\nprimarily on adults and not on children. If the AI tools show promising\nresults, this pre-registered analysis plan may help reduce interpretation bias.\nTrial registration: ClinicalTrials.gov - H-18010607\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lonfeldt_N/0/1/0/all/0/1\">Nicole N. L&#xf8;nfeldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frumosu_F/0/1/0/all/0/1\">Flavia D. Frumosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mora_Jensen_A/0/1/0/all/0/1\">A.-R. Cecilie Mora-Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lund_N/0/1/0/all/0/1\">Nicklas Leander Lund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sneha Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagsberg_A/0/1/0/all/0/1\">A. Katrine Pagsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemmensen_L/0/1/0/all/0/1\">Line K. H. Clemmensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISARM: Detecting the Victims Targeted by Harmful Memes. (arXiv:2205.05738v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05738","description":"<p>Internet memes have emerged as an increasingly popular means of communication\non the Web. Although typically intended to elicit humour, they have been\nincreasingly used to spread hatred, trolling, and cyberbullying, as well as to\ntarget specific individuals, communities, or society on political,\nsocio-cultural, and psychological grounds. While previous work has focused on\ndetecting harmful, hateful, and offensive memes, identifying whom they attack\nremains a challenging and underexplored area. Here we aim to bridge this gap.\nIn particular, we create a dataset where we annotate each meme with its\nvictim(s) such as the name of the targeted person(s), organization(s), and\ncommunity(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful\nMemes), a framework that uses named entity recognition and person\nidentification to detect all entities a meme is referring to, and then,\nincorporates a novel contextualized multimodal deep neural network to classify\nwhether the meme intends to harm these entities. We perform several systematic\nexperiments on three test setups, corresponding to entities that are (a) all\nseen while training, (b) not seen as a harmful target on training, and (c) not\nseen at all on training. The evaluation results show that DISARM significantly\noutperforms ten unimodal and multimodal systems. Finally, we show that DISARM\nis interpretable and comparatively more generalizable and that it can reduce\nthe relative error rate for harmful target identification by up to 9 points\nabsolute over several strong multimodal rivals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05739","description":"<p>The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be suboptimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog. The key contribution of our\nframework is a novel multimodal question generator that learns to ask questions\nthat maximize the subsequent video retrieval performance. Our multimodal\nquestion generator uses (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. Furthermore, we also demonstrate that\nour proposed approach also generalizes to the real-world settings that involve\ninteractions with real humans, thus, demonstrating the robustness and\ngenerality of our framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Representation for Point Clouds. (arXiv:2205.05740v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05740","description":"<p>Most prior work represents the shapes of point clouds by coordinates.\nHowever, it is insufficient to describe the local geometry directly. In this\npaper, we present \\textbf{RepSurf} (representative surfaces), a novel\nrepresentation of point clouds to \\textbf{explicitly} depict the very local\nstructure. We explore two variants of RepSurf, Triangular RepSurf and Umbrella\nRepSurf inspired by triangle meshes and umbrella curvature in computer\ngraphics. We compute the representations of RepSurf by predefined geometric\npriors after surface reconstruction. RepSurf can be a plug-and-play module for\nmost point cloud models thanks to its free collaboration with irregular points.\nBased on a simple baseline of PointNet++ (SSG version), Umbrella RepSurf\nsurpasses the previous state-of-the-art by a large margin for classification,\nsegmentation and detection on various benchmarks in terms of performance and\nefficiency. With an increase of around \\textbf{0.008M} number of parameters,\n\\textbf{0.04G} FLOPs, and \\textbf{1.12ms} inference time, our method achieves\n\\textbf{94.7\\%} (+0.5\\%) on ModelNet40, and \\textbf{84.6\\%} (+1.8\\%) on\nScanObjectNN for classification, while \\textbf{74.3\\%} (+0.8\\%) mIoU on S3DIS\n6-fold, and \\textbf{70.0\\%} (+1.6\\%) mIoU on ScanNet for segmentation. For\ndetection, previous state-of-the-art detector with our RepSurf obtains\n\\textbf{71.2\\%} (+2.1\\%) mAP$\\mathit{_{25}}$, \\textbf{54.8\\%} (+2.0\\%)\nmAP$\\mathit{_{50}}$ on ScanNetV2, and \\textbf{64.9\\%} (+1.9\\%)\nmAP$\\mathit{_{25}}$, \\textbf{47.7\\%} (+2.5\\%) mAP$\\mathit{_{50}}$ on SUN RGB-D.\nOur lightweight Triangular RepSurf performs its excellence on these benchmarks\nas well. The code is publicly available at\n\\url{https://github.com/hancyran/RepSurf}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEWS: Real-time Social Media Manipulation Detection and Analysis. (arXiv:2205.05783v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05783","description":"<p>This article presents a beta-version of MEWS (Misinformation Early Warning\nSystem). It describes the various aspects of the ingestion, manipulation\ndetection, and graphing algorithms employed to determine--in near\nreal-time--the relationships between social media images as they emerge and\nspread on social media platforms. By combining these various technologies into\na single processing pipeline, MEWS can identify manipulated media items as they\narise and identify when these particular items begin trending on individual\nsocial media platforms or even across multiple platforms. The emergence of a\nnovel manipulation followed by rapid diffusion of the manipulated content\nsuggests a disinformation campaign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ford_T/0/1/0/all/0/1\">Trenton W. Ford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yankoski_M/0/1/0/all/0/1\">Michael Yankoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yankoski_M/0/1/0/all/0/1\">Michael Yankoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Tom Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashman_F/0/1/0/all/0/1\">Farah Khashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dearstyne_K/0/1/0/all/0/1\">Katherine R. Dearstyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1\">Tim Weninger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous wavelet transform of multiview images using wavelets based on voxel patterns. (arXiv:2205.05823v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05823","description":"<p>We propose the multiview wavelets based on voxel patterns of autostereoscopic\nmultiview displays. Direct and inverse continuous wavelet transforms of binary\nand gray-scale images were performed. The input to the inverse wavelet\ntransform was the array of wavelet coefficients of the direct transform. A\nrestored image reproduces the structure of the multiview image correctly. Also,\nwe modified the dimension of the parallax and the depth of 3D images. The\nrestored and modified images were displayed in 3D using lenticular plates. In\neach case, the visual 3D picture corresponds to the applied modifications. The\nresults can be applied to the autostereoscopic 3D displays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saveljev_V/0/1/0/all/0/1\">Vladimir Saveljev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling. (arXiv:2205.05826v1 [cs.AR])","link":"http://arxiv.org/abs/2205.05826","description":"<p>In recent years, many accelerators have been proposed to efficiently process\nsparse tensor algebra applications (e.g., sparse neural networks). However,\nthese proposals are single points in a large and diverse design space. The lack\nof systematic description and modeling support for these sparse tensor\naccelerators impedes hardware designers from efficient and effective design\nspace exploration. This paper first presents a unified taxonomy to\nsystematically describe the diverse sparse tensor accelerator design space.\nBased on the proposed taxonomy, it then introduces Sparseloop, the first fast,\naccurate, and flexible analytical modeling framework to enable early-stage\nevaluation and exploration of sparse tensor accelerators. Sparseloop\ncomprehends a large set of architecture specifications, including various\ndataflows and sparse acceleration features (e.g., elimination of zero-based\ncompute). Using these specifications, Sparseloop evaluates a design's\nprocessing speed and energy efficiency while accounting for data movement and\ncompute incurred by the employed dataflow as well as the savings and overhead\nintroduced by the sparse acceleration features using stochastic tensor density\nmodels. Across representative accelerators and workloads, Sparseloop achieves\nover 2000 times faster modeling speed than cycle-level simulations, maintains\nrelative performance trends, and achieves 0.1% to 8% average error. With a case\nstudy, we demonstrate Sparseloop's ability to help reveal important insights\nfor designing sparse tensor accelerators (e.g., it is important to co-design\northogonal design aspects).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yannan Nellie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_P/0/1/0/all/0/1\">Po-An Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parashar_A/0/1/0/all/0/1\">Angshuman Parashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sze_V/0/1/0/all/0/1\">Vivienne Sze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emer_J/0/1/0/all/0/1\">Joel S. Emer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Few-shot Meta-learning Using Stacking. (arXiv:2205.05831v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05831","description":"<p>Cross-domain few-shot meta-learning (CDFSML) addresses learning problems\nwhere knowledge needs to be transferred from several source domains into an\ninstance-scarce target domain with an explicitly different input distribution.\nRecently published CDFSML methods generally construct a \"universal model\" that\ncombines knowledge of multiple source domains into one backbone feature\nextractor. This enables efficient inference but necessitates re-computation of\nthe backbone whenever a new source domain is added. Moreover, state-of-the-art\nmethods derive their universal model from a collection of backbones -- normally\none for each source domain -- and the backbones may be constrained to have the\nsame architecture as the universal model. We propose a CDFSML method that is\ninspired by the classic stacking approach to meta learning. It imposes no\nconstraints on the backbones' architecture or feature shape and does not incur\nthe computational overhead of (re-)computing a universal model. Given a\ntarget-domain task, it fine-tunes each backbone independently, uses\ncross-validation to extract meta training data from the task's instance-scarce\nsupport set, and learns a simple linear meta classifier from this data. We\nevaluate our stacking approach on the well-known Meta-Dataset benchmark,\ntargeting image classification with convolutional neural networks, and show\nthat it often yields substantially higher accuracy than competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayo_M/0/1/0/all/0/1\">Michael Mayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_G/0/1/0/all/0/1\">Geoffrey Holmes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Uncertainty for Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images. (arXiv:2205.05841v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05841","description":"<p>Trained using only image class label, deep weakly supervised methods allow\nimage classification and ROI segmentation for interpretability. Despite their\nsuccess on natural images, they face several challenges over histology data\nwhere ROI are visually similar to background making models vulnerable to high\npixel-wise false positives. These methods lack mechanisms for modeling\nexplicitly non-discriminative regions which raises false-positive rates. We\npropose novel regularization terms, which enable the model to seek both\nnon-discriminative and discriminative regions, while discouraging unbalanced\nsegmentations and using only image class label. Our method is composed of two\nnetworks: a localizer that yields segmentation mask, followed by a classifier.\nThe training loss pushes the localizer to build a segmentation mask that holds\nmost discrimiantive regions while simultaneously modeling background regions.\nComprehensive experiments over two histology datasets showed the merits of our\nmethod in reducing false positives and accurately segmenting ROI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-level Alignment for Cross-Domain Crowd Counting. (arXiv:2205.05844v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05844","description":"<p>Recently, crowd density estimation has received increasing attention. The\nmain challenge for this task is to achieve high-quality manual annotations on a\nlarge amount of training data. To avoid reliance on such annotations, previous\nworks apply unsupervised domain adaptation (UDA) techniques by transferring\nknowledge learned from easily accessible synthetic data to real-world datasets.\nHowever, current state-of-the-art methods either rely on external data for\ntraining an auxiliary task or apply an expensive coarse-to-fine estimation. In\nthis work, we aim to develop a new adversarial learning based method, which is\nsimple and efficient to apply. To reduce the domain gap between the synthetic\nand real data, we design a bi-level alignment framework (BLA) consisting of (1)\ntask-driven data alignment and (2) fine-grained feature alignment. In contrast\nto previous domain augmentation methods, we introduce AutoML to search for an\noptimal transform on source, which well serves for the downstream task. On the\nother hand, we do fine-grained alignment for foreground and background\nseparately to alleviate the alignment difficulty. We evaluate our approach on\nfive real-world crowd counting benchmarks, where we outperform existing\napproaches by a large margin. Also, our approach is simple, easy to implement\nand efficient to apply. The code is publicly available at\nhttps://github.com/Yankeegsj/BLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shenjian Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFFIRM: Affinity Fusion-based Framework for Iteratively Random Motion correction of multi-slice fetal brain MRI. (arXiv:2205.05851v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05851","description":"<p>Multi-slice magnetic resonance images of the fetal brain are usually\ncontaminated by severe and arbitrary fetal and maternal motion. Hence, stable\nand robust motion correction is necessary to reconstruct high-resolution 3D\nfetal brain volume for clinical diagnosis and quantitative analysis. However,\nthe conventional registration-based correction has a limited capture range and\nis insufficient for detecting relatively large motions. Here, we present a\nnovel Affinity Fusion-based Framework for Iteratively Random Motion (AFFIRM)\ncorrection of the multi-slice fetal brain MRI. It learns the sequential motion\nfrom multiple stacks of slices and integrates the features between 2D slices\nand reconstructed 3D volume using affinity fusion, which resembles the\niterations between slice-to-volume registration and volumetric reconstruction\nin the regular pipeline. The method accurately estimates the motion regardless\nof brain orientations and outperforms other state-of-the-art learning-based\nmethods on the simulated motion-corrupted data, with a 48.4% reduction of mean\nabsolute error for rotation and 61.3% for displacement. We then incorporated\nAFFIRM into the multi-resolution slice-to-volume registration and tested it on\nthe real-world fetal MRI scans at different gestation stages. The results\nindicated that adding AFFIRM to the conventional pipeline improved the success\nrate of fetal brain super-resolution reconstruction from 77.2% to 91.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haoan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Cong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jiwei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yamin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshu Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guangbin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1\">Dan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-aware and Motion-aware Transformers for Language-driven Action Localization in Videos. (arXiv:2205.05854v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05854","description":"<p>Language-driven action localization in videos is a challenging task that\ninvolves not only visual-linguistic matching but also action boundary\nprediction. Recent progress has been achieved through aligning language query\nto video segments, but estimating precise boundaries is still under-explored.\nIn this paper, we propose entity-aware and motion-aware Transformers that\nprogressively localizes actions in videos by first coarsely locating clips with\nentity queries and then finely predicting exact boundaries in a shrunken\ntemporal region with motion queries. The entity-aware Transformer incorporates\nthe textual entities into visual representation learning via cross-modal and\ncross-frame attentions to facilitate attending action-related video clips. The\nmotion-aware Transformer captures fine-grained motion changes at multiple\ntemporal scales via integrating long short-term memory into the self-attention\nmodule to further improve the precision of action boundary prediction.\nExtensive experiments on the Charades-STA and TACoS datasets demonstrate that\nour method achieves better performance than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxiao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3E-GNN: Sparse Spatial Scene Embedding with Graph Neural Networks for Camera Relocalization. (arXiv:2205.05861v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05861","description":"<p>Camera relocalization is the key component of simultaneous localization and\nmapping (SLAM) systems. This paper proposes a learning-based approach, named\nSparse Spatial Scene Embedding with Graph Neural Networks (S3E-GNN), as an\nend-to-end framework for efficient and robust camera relocalization. S3E-GNN\nconsists of two modules. In the encoding module, a trained S3E network encodes\nRGB images into embedding codes to implicitly represent spatial and semantic\nembedding code. With embedding codes and the associated poses obtained from a\nSLAM system, each image is represented as a graph node in a pose graph. In the\nGNN query module, the pose graph is transformed to form a embedding-aggregated\nreference graph for camera relocalization. We collect various scene datasets in\nthe challenging environments to perform experiments. Our results demonstrate\nthat S3E-GNN method outperforms the traditional Bag-of-words (BoW) for camera\nrelocalization due to learning-based embedding and GNN powered scene matching\nmechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lige Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View Synthesis with Sculpted Neural Points. (arXiv:2205.05869v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05869","description":"<p>We address the task of view synthesis, which can be posed as recovering a\nrendering function that renders new views from a set of existing images. In\nmany recent works such as NeRF, this rendering function is parameterized using\nimplicit neural representations of scene geometry. Implicit neural\nrepresentations have achieved impressive visual quality but have drawbacks in\ncomputational efficiency. In this work, we propose a new approach that performs\nview synthesis using point clouds. It is the first point-based method to\nachieve better visual quality than NeRF while being more than 100x faster in\nrendering speed. Our approach builds on existing works on differentiable\npoint-based rendering but introduces a novel technique we call \"Sculpted Neural\nPoints (SNP)\", which significantly improves the robustness to errors and holes\nin the reconstructed point cloud. Experiments show that on the task of view\nsynthesis, our sculpting technique closes the gap between point-based and\nimplicit representation-based methods. Code is available at\nhttps://github.com/princeton-vl/SNP and supplementary video at\nhttps://youtu.be/dBwCQP9uNws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yiming Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinction Maximization Loss: Efficiently Improving Classification Accuracy, Uncertainty Estimation, and Out-of-Distribution Detection Simply Replacing the Loss and Calibrating. (arXiv:2205.05874v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05874","description":"<p>Building robust deterministic deep neural networks is still a challenge. On\nthe one hand, some approaches improve out-of-distribution detection at the cost\nof reducing classification accuracy in some situations. On the other hand, some\nmethods simultaneously increase classification accuracy, out-of-distribution\ndetection, and uncertainty estimation, but reduce inference efficiency, in\naddition to training the same model many times to tune hyperparameters. In this\npaper, we propose training deterministic deep neural networks using our DisMax\nloss, which works as a drop-in replacement for the commonly used SoftMax loss\n(i.e., the combination of the linear output layer, the SoftMax activation, and\nthe cross-entropy loss). Starting from the IsoMax+ loss, we created novel\nlogits that are based on the distance to all prototypes rather than just the\none associated with the correct class. We also propose a novel way to augment\nimages to construct what we call fractional probability regularization.\nMoreover, we propose a new score to perform out-of-distribution detection and a\nfast way to calibrate the network after training. Our experiments show that\nDisMax usually outperforms all current approaches simultaneously in\nclassification accuracy, uncertainty estimation, inference efficiency, and\nout-of-distribution detection, avoiding hyperparameter tuning and repetitive\nmodel training. The code to replace the SoftMax loss with the DisMax loss and\nreproduce the results in this paper is available at\nhttps://github.com/dlmacedo/distinction-maximization-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Decomposition and Bilinear Pooling Network for Blind Night-Time Image Quality Evaluation. (arXiv:2205.05880v1 [cs.MM])","link":"http://arxiv.org/abs/2205.05880","description":"<p>Blind image quality assessment (BIQA), which aims to accurately predict the\nimage quality without any pristine reference information, has been highly\nconcerned in the past decades. Especially, with the help of deep neural\nnetworks, great progress has been achieved so far. However, it remains less\ninvestigated on BIQA for night-time images (NTIs) which usually suffer from\ncomplicated authentic distortions such as reduced visibility, low contrast,\nadditive noises, and color distortions. These diverse authentic degradations\nparticularly challenges the design of effective deep neural network for blind\nNTI quality evaluation (NTIQE). In this paper, we propose a novel deep\ndecomposition and bilinear pooling network (DDB-Net) to better address this\nissue. The DDB-Net contains three modules, i.e., an image decomposition module,\na feature encoding module, and a bilinear pooling module. The image\ndecomposition module is inspired by the Retinex theory and involves decoupling\nthe input NTI into an illumination layer component responsible for illumination\ninformation and a reflectance layer component responsible for content\ninformation. Then, the feature encoding module involves learning multi-scale\nfeature representations of degradations that are rooted in the two decoupled\ncomponents separately. Finally, by modeling illumination-related and\ncontent-related degradations as two-factor variations, the two multi-scale\nfeature sets are bilinearly pooled and concatenated together to form a unified\nrepresentation for quality prediction. The superiority of the proposed DDB-Net\nis well validated by extensive experiments on two publicly available night-time\nimage databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiawu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Action Detection Guided by Audio Narration. (arXiv:2205.05895v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05895","description":"<p>Videos are more well-organized curated data sources for visual concept\nlearning than images. Unlike the 2-dimensional images which only involve the\nspatial information, the additional temporal dimension bridges and synchronizes\nmultiple modalities. However, in most video detection benchmarks, these\nadditional modalities are not fully utilized. For example, EPIC Kitchens is the\nlargest dataset in first-person (egocentric) vision, yet it still relies on\ncrowdsourced information to refine the action boundaries to provide\ninstance-level action annotations.\n</p>\n<p>We explored how to eliminate the expensive annotations in video detection\ndata which provide refined boundaries. We propose a model to learn from the\nnarration supervision and utilize multimodal features, including RGB, motion\nflow, and ambient sound. Our model learns to attend to the frames related to\nthe narration label while suppressing the irrelevant frames from being used.\nOur experiments show that noisy audio narration suffices to learn a good action\ndetection model, thus reducing annotation expenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Keren Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Label Guided Multi-Contrast Generalization for Non-Contrast Organ-Aware Segmentation. (arXiv:2205.05898v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05898","description":"<p>Non-contrast computed tomography (NCCT) is commonly acquired for lung cancer\nscreening, assessment of general abdominal pain or suspected renal stones,\ntrauma evaluation, and many other indications. However, the absence of contrast\nlimits distinguishing organ in-between boundaries. In this paper, we propose a\nnovel unsupervised approach that leverages pairwise contrast-enhanced CT (CECT)\ncontext to compute non-contrast segmentation without ground-truth label. Unlike\ngenerative adversarial approaches, we compute the pairwise morphological\ncontext with CECT to provide teacher guidance instead of generating fake\nanatomical context. Additionally, we further augment the intensity correlations\nin 'organ-specific' settings and increase the sensitivity to organ-aware\nboundary. We validate our approach on multi-organ segmentation with paired\nnon-contrast &amp; contrast-enhanced CT scans using five-fold cross-validation.\nFull external validations are performed on an independent non-contrast cohort\nfor aorta segmentation. Compared with current abdominal organs segmentation\nstate-of-the-art in fully supervised setting, our proposed pipeline achieves a\nsignificantly higher Dice by 3.98% (internal multi-organ annotated), and 8.00%\n(external aorta annotated) for abdominal organs segmentation. The code and\npretrained models are publicly available at\nhttps://github.com/MASILab/ContrastMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terry_J/0/1/0/all/0/1\">James G. Terry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carr_J/0/1/0/all/0/1\">J. Jeffrey Carr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared Invisible Clothing:Hiding from Infrared Detectors at Multiple Angles in Real World. (arXiv:2205.05909v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05909","description":"<p>Thermal infrared imaging is widely used in body temperature measurement,\nsecurity monitoring, and so on, but its safety research attracted attention\nonly in recent years. We proposed the infrared adversarial clothing, which\ncould fool infrared pedestrian detectors at different angles. We simulated the\nprocess from cloth to clothing in the digital world and then designed the\nadversarial \"QR code\" pattern. The core of our method is to design a basic\npattern that can be expanded periodically, and make the pattern after random\ncropping and deformation still have an adversarial effect, then we can process\nthe flat cloth with an adversarial pattern into any 3D clothes. The results\nshowed that the optimized \"QR code\" pattern lowered the Average Precision (AP)\nof YOLOv3 by 87.7%, while the random \"QR code\" pattern and blank pattern\nlowered the AP of YOLOv3 by 57.9% and 30.1%, respectively, in the digital\nworld. We then manufactured an adversarial shirt with a new material: aerogel.\nPhysical-world experiments showed that the adversarial \"QR code\" pattern\nclothing lowered the AP of YOLOv3 by 64.6%, while the random \"QR code\" pattern\nclothing and fully heat-insulated clothing lowered the AP of YOLOv3 by 28.3%\nand 22.8%, respectively. We used the model ensemble technique to improve the\nattack transferability to unseen models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaopei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Facade Parsing R-CNN. (arXiv:2205.05912v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05912","description":"<p>Building facade parsing, which predicts pixel-level labels for building\nfacades, has applications in computer vision perception for autonomous vehicle\n(AV) driving. However, instead of a frontal view, an on-board camera of an AV\ncaptures a deformed view of the facade of the buildings on both sides of the\nroad the AV is travelling on, due to the camera perspective. We propose Facade\nR-CNN, which includes a transconv module, generalized bounding box detection,\nand convex regularization, to perform parsing of deformed facade views.\nExperiments demonstrate that Facade R-CNN achieves better performance than the\ncurrent state-of-the-art facade parsing models, which are primarily developed\nfor frontal views. We also publish a new building facade parsing dataset\nderived from the Oxford RobotCar dataset, which we call the Oxford RobotCar\nFacade dataset. This dataset contains 500 street-view images from the Oxford\nRobotCar dataset augmented with accurate annotations of building facade\nobjects. The published dataset is available at\nhttps://github.com/sijieaaa/Oxford-RobotCar-Facade\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1\">Qiyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1\">Rui She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1\">Wee Peng Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_D/0/1/0/all/0/1\">Diego Navarro Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmannsgruber_A/0/1/0/all/0/1\">Andreas Hartmannsgruber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry. (arXiv:2205.05916v1 [cs.RO])","link":"http://arxiv.org/abs/2205.05916","description":"<p>We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based\nvisual odometry, TartanVO. TartanVO, like other direct methods rather than\nfeature-based, estimates camera pose through dense optical flow, which only\napplies to static scenes and disregards dynamic objects. Due to the color\nconstancy assumption, optical flow is not able to differentiate between dynamic\nand static pixels. Therefore, to reconstruct a static map through such direct\nmethods, our pipeline resolves dynamic/static segmentation by leveraging the\noptical flow output, and only fuse static points into the map. Moreover, we\nrerender the input frames such that the dynamic pixels are removed and\niteratively pass them back into the visual odometry to refine the pose\nestimate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yilin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiayi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangzhao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fall detection using multimodal data. (arXiv:2205.05918v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05918","description":"<p>In recent years, the occurrence of falls has increased and has had\ndetrimental effects on older adults. Therefore, various machine learning\napproaches and datasets have been introduced to construct an efficient fall\ndetection algorithm for the social community. This paper studies the fall\ndetection problem based on a large public dataset, namely the UP-Fall Detection\nDataset. This dataset was collected from a dozen of volunteers using different\nsensors and two cameras. We propose several techniques to obtain valuable\nfeatures from these sensors and cameras and then construct suitable models for\nthe main problem. The experimental results show that our proposed methods can\nbypass the state-of-the-art methods on this dataset in terms of accuracy,\nprecision, recall, and F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Thao V. Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Son T. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Trung T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group R-CNN for Weakly Semi-supervised Object Detection with Points. (arXiv:2205.05920v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05920","description":"<p>We study the problem of weakly semi-supervised object detection with points\n(WSSOD-P), where the training data is combined by a small set of fully\nannotated images with bounding boxes and a large set of weakly-labeled images\nwith only a single point annotated for each instance. The core of this task is\nto train a point-to-box regressor on well-labeled images that can be used to\npredict credible bounding boxes for each point annotation. We challenge the\nprior belief that existing CNN-based detectors are not compatible with this\ntask. Based on the classic R-CNN architecture, we propose an effective\npoint-to-box regressor: Group R-CNN. Group R-CNN first uses instance-level\nproposal grouping to generate a group of proposals for each point annotation\nand thus can obtain a high recall rate. To better distinguish different\ninstances and improve precision, we propose instance-level proposal assignment\nto replace the vanilla assignment strategy adopted in the original R-CNN\nmethods. As naive instance-level assignment brings converging difficulty, we\npropose instance-aware representation learning which consists of instance-aware\nfeature enhancement and instance-aware parameter generation to overcome this\nissue. Comprehensive experiments on the MS-COCO benchmark demonstrate the\neffectiveness of our method. Specifically, Group R-CNN significantly\noutperforms the prior method Point DETR by 3.9 mAP with 5% well-labeled images,\nwhich is the most challenging scenario. The source code can be found at\nhttps://github.com/jshilong/GroupRCNN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhuoran Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation. (arXiv:2205.05922v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05922","description":"<p>Neural Radiance Fields (NeRF) have emerged as a potent paradigm for\nrepresenting scenes and synthesizing photo-realistic images. A main limitation\nof conventional NeRFs is that they often fail to produce high-quality\nrenderings under novel viewpoints that are significantly different from the\ntraining viewpoints. In this paper, instead of exploiting few-shot image\nsynthesis, we study the novel view extrapolation setting that (1) the training\nimages can well describe an object, and (2) there is a notable discrepancy\nbetween the training and test viewpoints' distributions. We present RapNeRF\n(RAy Priors) as a solution. Our insight is that the inherent appearances of a\n3D surface's arbitrary visible projections should be consistent. We thus\npropose a random ray casting policy that allows training unseen views using\nseen views. Furthermore, we show that a ray atlas pre-computed from the\nobserved rays' viewing directions could further enhance the rendering quality\nfor extrapolated views. A main limitation is that RapNeRF would remove the\nstrong view-dependent effects because it leverages the multi-view consistency\nproperty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Bowen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinchi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Rongfei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Binqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Single-shot Detector for Small Object Detection in Remote Sensing Images. (arXiv:2205.05927v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05927","description":"<p>Small-object detection is a challenging problem. In the last few years, the\nconvolution neural networks methods have been achieved considerable progress.\nHowever, the current detectors struggle with effective features extraction for\nsmall-scale objects. To address this challenge, we propose image pyramid\nsingle-shot detector (IPSSD). In IPSSD, single-shot detector is adopted\ncombined with an image pyramid network to extract semantically strong features\nfor generating candidate regions. The proposed network can enhance the\nsmall-scale features from a feature pyramid network. We evaluated the\nperformance of the proposed model on two public datasets and the results show\nthe superior performance of our model compared to the other state-of-the-art\nobject detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCPSR: Simple Contrastive Learning for Paper Submission Recommendation System. (arXiv:2205.05940v1 [cs.IR])","link":"http://arxiv.org/abs/2205.05940","description":"<p>The recommendation system plays a vital role in many areas, especially\nacademic fields, to support researchers in submitting and increasing the\nacceptance of their work through the conference or journal selection process.\nThis study proposes a transformer-based model using transfer learning as an\nefficient approach for the paper submission recommendation system. By combining\nessential information (such as the title, the abstract, and the list of\nkeywords) with the aims and scopes of journals, the model can recommend the Top\nK journals that maximize the acceptance of the paper. Our model had developed\nthrough two states: (i) Fine-tuning the pre-trained language model (LM) with a\nsimple contrastive learning framework. We utilized a simple supervised\ncontrastive objective to fine-tune all parameters, encouraging the LM to learn\nthe document representation effectively. (ii) The fine-tuned LM was then\ntrained on different combinations of the features for the downstream task. This\nstudy suggests a more advanced method for enhancing the efficiency of the paper\nsubmission recommendation system compared to previous approaches when we\nrespectively achieve 0.5173, 0.8097, 0.8862, 0.9496 for Top 1, 3, 5, and 10\naccuracies on the test set for combining the title, abstract, and keywords as\ninput features. Incorporating the journals' aims and scopes, our model shows an\nexciting result by getting 0.5194, 0.8112, 0.8866, and 0.9496 respective to Top\n1, 3, 5, and 10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc H. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tram T. Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Son T. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Economical Precise Manipulation and Auto Eye-Hand Coordination with Binocular Visual Reinforcement Learning. (arXiv:2205.05963v1 [cs.RO])","link":"http://arxiv.org/abs/2205.05963","description":"<p>Precision robotic manipulation tasks (insertion, screwing, precisely pick,\nprecisely place) are required in many scenarios. Previous methods achieved good\nperformance on such manipulation tasks. However, such methods typically require\ntedious calibration or expensive sensors. 3D/RGB-D cameras and torque/force\nsensors add to the cost of the robotic application and may not always be\neconomical. In this work, we aim to solve these but using only weak-calibrated\nand low-cost webcams. We propose Binocular Alignment Learning (BAL), which\ncould automatically learn the eye-hand coordination and points alignment\ncapabilities to solve the four tasks. Our work focuses on working with unknown\neye-hand coordination and proposes different ways of performing eye-in-hand\ncamera calibration automatically. The algorithm was trained in simulation and\nused a practical pipeline to achieve sim2real and test it on the real robot.\nOur method achieves a competitively good result with minimal cost on the four\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_X/0/1/0/all/0/1\">Xian Yao Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPSRS: A Fusion Approach for Paper Submission Recommendation System. (arXiv:2205.05965v1 [cs.IR])","link":"http://arxiv.org/abs/2205.05965","description":"<p>Recommender systems have been increasingly popular in entertainment and\nconsumption and are evident in academics, especially for applications that\nsuggest submitting scientific articles to scientists. However, because of the\nvarious acceptance rates, impact factors, and rankings in different publishers,\nsearching for a proper venue or journal to submit a scientific work usually\ntakes a lot of time and effort. In this paper, we aim to present two newer\napproaches extended from our paper [13] presented at the conference IAE/AIE\n2021 by employing RNN structures besides using Conv1D. In addition, we also\nintroduce a new method, namely DistilBertAims, using DistillBert for two cases\nof uppercase and lower-case words to vectorize features such as Title,\nAbstract, and Keywords, and then use Conv1d to perform feature extraction.\nFurthermore, we propose a new calculation method for similarity score for Aim &amp;\nScope with other features; this helps keep the weights of similarity score\ncalculation continuously updated and then continue to fit more data. The\nexperimental results show that the second approach could obtain a better\nperformance, which is 62.46% and 12.44% higher than the best of the previous\nstudy [13] in terms of the Top 1 accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Son T. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_N/0/1/0/all/0/1\">Nhi Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dac H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_P/0/1/0/all/0/1\">Phong T. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target Aware Network Architecture Search and Compression for Efficient Knowledge Transfer. (arXiv:2205.05967v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05967","description":"<p>Transfer Learning enables Convolutional Neural Networks (CNN) to acquire\nknowledge from a source domain and transfer it to a target domain, where\ncollecting large-scale annotated examples is both time-consuming and expensive.\nConventionally, while transferring the knowledge learned from one task to\nanother task, the deeper layers of a pre-trained CNN are finetuned over the\ntarget dataset. However, these layers that are originally designed for the\nsource task are over-parameterized for the target task. Thus, finetuning these\nlayers over the target dataset reduces the generalization ability of the CNN\ndue to high network complexity. To tackle this problem, we propose a two-stage\nframework called TASCNet which enables efficient knowledge transfer. In the\nfirst stage, the configuration of the deeper layers is learned automatically\nand finetuned over the target dataset. Later, in the second stage, the\nredundant filters are pruned from the fine-tuned CNN to decrease the network's\ncomplexity for the target task while preserving the performance. This two-stage\nmechanism finds a compact version of the pre-trained CNN with optimal structure\n(number of filters in a convolutional layer, number of neurons in a dense\nlayer, and so on) from the hypothesis space. The efficacy of the proposed\nmethod is evaluated using VGG-16, ResNet-50, and DenseNet-121 on CalTech-101,\nCalTech-256, and Stanford Dogs datasets. The proposed TASCNet reduces the\ncomputational complexity of pre-trained CNNs over the target task by reducing\nboth trainable parameters and FLOPs which enables resource-efficient knowledge\ntransfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basha_S/0/1/0/all/0/1\">S.H.Shabbeer Basha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tula_D/0/1/0/all/0/1\">Debapriya Tula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinakota_S/0/1/0/all/0/1\">Sravan Kumar Vinakota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaDeR: A New Task Dependency Recommendation for Project Management Platform. (arXiv:2205.05976v1 [cs.IR])","link":"http://arxiv.org/abs/2205.05976","description":"<p>Many startups and companies worldwide have been using project management\nsoftware and tools to monitor, track and manage their projects. For software\nprojects, the number of tasks from the beginning to the end is quite a large\nnumber that sometimes takes a lot of time and effort to search and link the\ncurrent task to a group of previous ones for further references. This paper\nproposes an efficient task dependency recommendation algorithm to suggest tasks\ndependent on a given task that the user has just created. We present an\nefficient feature engineering step and construct a deep neural network to this\naim. We performed extensive experiments on two different large projects\n(MDLSITE from moodle.org and FLUME from apache.org) to find the best features\nin 28 combinations of features and the best performance model using two\nembedding methods (GloVe and FastText). We consider three types of models (GRU,\nCNN, LSTM) using Accuracy@K, MRR@K, and Recall@K (where K = 1, 2, 3, and 5) and\nbaseline models using traditional methods: TF-IDF with various matching score\ncalculating such as cosine similarity, Euclidean distance, Manhattan distance,\nand Chebyshev distance. After many experiments, the GloVe Embedding and CNN\nmodel reached the best result in our dataset, so we chose this model as our\nproposed method. In addition, adding the time filter in the post-processing\nstep can significantly improve the recommendation system's performance. The\nexperimental results show that our proposed method can reach 0.2335 in\nAccuracy@1 and MRR@1 and 0.2011 in Recall@1 of dataset FLUME. With the MDLSITE\ndataset, we obtained 0.1258 in Accuracy@1 and MRR@1 and 0.1141 in Recall@1. In\nthe top 5, our model reached 0.3040 in Accuracy@5, 0.2563 MRR@5, and 0.2651\nRecall@5 in FLUME. In the MDLSITE dataset, our model got 0.5270 Accuracy@5,\n0.2689 MRR@5, and 0.2651 Recall@5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quynh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dac H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Son T. Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_H/0/1/0/all/0/1\">Hoa K. Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPPNet: Multi-Frame Feature Intertwining with Proxy Points for 3D Temporal Object Detection. (arXiv:2205.05979v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05979","description":"<p>Accurate and reliable 3D detection is vital for many applications including\nautonomous driving vehicles and service robots. In this paper, we present a\nflexible and high-performance 3D detection framework, named MPPNet, for 3D\ntemporal object detection with point cloud sequences. We propose a novel\nthree-hierarchy framework with proxy points for multi-frame feature encoding\nand interactions to achieve better detection. The three hierarchies conduct\nper-frame feature encoding, short-clip feature fusion, and whole-sequence\nfeature aggregation, respectively. To enable processing long-sequence point\nclouds with reasonable computational resources, intra-group feature mixing and\ninter-group feature attention are proposed to form the second and third feature\nencoding hierarchies, which are recurrently applied for aggregating multi-frame\ntrajectory features. The proxy points not only act as consistent object\nrepresentations for each frame, but also serve as the courier to facilitate\nfeature interaction between frames. The experiments on largeWaymo Open dataset\nshow that our approach outperforms state-of-the-art methods with large margins\nwhen applied to both short (e.g., 4-frame) and long (e.g., 16-frame) point\ncloud sequences. Specifically, MPPNet achieves 74.21%, 74.62% and 73.31% for\nvehicle, pedestrian and cyclist classes on the LEVEL 2 mAPH metric with\n16-frame input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuesong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Benjin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Teaching Independent Parts Separately\"(TIPS-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation. (arXiv:2205.05980v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05980","description":"<p>We present TIPS-GAN, a new approach to improve the accuracy and stability in\nunsupervised adversarial 2D to 3D human pose estimation. In our work we\ndemonstrate that the human kinematic skeleton should not be assumed as one\nspatially dependent structure. In fact, we believe when a full 2D pose is\nprovided during training, there is an inherent bias learned where the 3D\ncoordinate of a keypoint is spatially codependent on the 2D locations of all\nother keypoints. To investigate our theory we follow previous adversarial\napproaches but trained two generators on spatially independent parts of the\nkinematic skeleton, the torso and the legs. During our study we find that\nimproving self-consistency is key to lowering the evaluation error and\ntherefore introduce new consistency constraints within the standard adversarial\ncycle. We then produced a final TIPS model via knowledge distillation which can\npredict the 3D coordinates for the entire 2D pose with improved results.\nFurthermore we help address the question left unanswered in prior adversarial\nlearning papers of how long to train for a truly unsupervised scenario. We show\nthat two independent generators training adversarially can hold a minimum error\nagainst a discriminator for a longer period of time than that of a solo\ngenerator which will diverge due to the adversarial network becoming unstable.\nTIPS decreases the average error by 18\\% when compared to that of a baseline\nsolo generator. TIPS improves upon other unsupervised approaches while also\nperforming strongly against supervised and weakly-supervised approaches during\nevaluation on both the Human3.6M and MPI-INF-3DHP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_P/0/1/0/all/0/1\">Peter Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasmahapatra_S/0/1/0/all/0/1\">Srinandan Dasmahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hansung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blueprint Separable Residual Network for Efficient Image Super-Resolution. (arXiv:2205.05996v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05996","description":"<p>Recent advances in single image super-resolution (SISR) have achieved\nextraordinary performance, but the computational cost is too heavy to apply in\nedge devices. To alleviate this problem, many novel and effective solutions\nhave been proposed. Convolutional neural network (CNN) with the attention\nmechanism has attracted increasing attention due to its efficiency and\neffectiveness. However, there is still redundancy in the convolution operation.\nIn this paper, we propose Blueprint Separable Residual Network (BSRN)\ncontaining two efficient designs. One is the usage of blueprint separable\nconvolution (BSConv), which takes place of the redundant convolution operation.\nThe other is to enhance the model ability by introducing more effective\nattention modules. The experimental results show that BSRN achieves\nstate-of-the-art performance among existing efficient SR methods. Moreover, a\nsmaller variant of our model BSRN-S won the first place in model complexity\ntrack of NTIRE 2022 Efficient SR Challenge. The code is available at\nhttps://github.com/xiaom233/BSRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accounting for the Sequential Nature of States to Learn Features for Reinforcement Learning. (arXiv:2205.06000v1 [cs.LG])","link":"http://arxiv.org/abs/2205.06000","description":"<p>In this work, we investigate the properties of data that cause popular\nrepresentation learning approaches to fail. In particular, we find that in\nenvironments where states do not significantly overlap, variational\nautoencoders (VAEs) fail to learn useful features. We demonstrate this failure\nin a simple gridworld domain, and then provide a solution in the form of metric\nlearning. However, metric learning requires supervision in the form of a\ndistance function, which is absent in reinforcement learning. To overcome this,\nwe leverage the sequential nature of states in a replay buffer to approximate a\ndistance metric and provide a weak supervision signal, under the assumption\nthat temporally close states are also semantically similar. We modify a VAE\nwith triplet loss and demonstrate that this approach is able to learn useful\nfeatures for downstream tasks, without additional supervision, in environments\nwhere standard VAEs fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1\">Nathan Michlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarvis_D/0/1/0/all/0/1\">Devon Jarvis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1\">Richard Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3T-GAN: Data-Dependent Domain Transfer GANs for Few-shot Image Generation. (arXiv:2205.06032v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06032","description":"<p>As an important and challenging problem, few-shot image generation aims at\ngenerating realistic images through training a GAN model given few samples. A\ntypical solution for few-shot generation is to transfer a well-trained GAN\nmodel from a data-rich source domain to the data-deficient target domain. In\nthis paper, we propose a novel self-supervised transfer scheme termed D3T-GAN,\naddressing the cross-domain GANs transfer in few-shot image generation.\nSpecifically, we design two individual strategies to transfer knowledge between\ngenerators and discriminators, respectively. To transfer knowledge between\ngenerators, we conduct a data-dependent transformation, which projects and\nreconstructs the target samples into the source generator space. Then, we\nperform knowledge transfer from transformed samples to generated samples. To\ntransfer knowledge between discriminators, we design a multi-level discriminant\nknowledge distillation from the source discriminator to the target\ndiscriminator on both the real and fake samples. Extensive experiments show\nthat our method improve the quality of generated images and achieves the\nstate-of-the-art FID scores on commonly used datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep morphological recognition of kidney stones using intra-operative endoscopic digital videos. (arXiv:2205.06093v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06093","description":"<p>The collection and the analysis of kidney stone morphological criteria are\nessential for an aetiological diagnosis of stone disease. However, in-situ\nLASER-based fragmentation of urinary stones, which is now the most established\nchirurgical intervention, may destroy the morphology of the targeted stone. In\nthe current study, we assess the performance and added value of processing\ncomplete digital endoscopic video sequences for the automatic recognition of\nstone morphological features during a standard-of-care intra-operative session.\nTo this end, a computer-aided video classifier was developed to predict in-situ\nthe morphology of stone using an intra-operative digital endoscopic video\nacquired in a clinical setting.\n</p>\n<p>The proposed technique was evaluated on pure (i.e. include one morphology)\nand mixed (i.e. include at least two morphologies) stones involving \"Ia/Calcium\nOxalate Monohydrate (COM)\", \"IIb/ Calcium Oxalate Dihydrate (COD)\" and\n\"IIIb/Uric Acid (UA)\" morphologies. 71 digital endoscopic videos (50 exhibited\nonly one morphological type and 21 displayed two) were analyzed using the\nproposed video classifier (56840 frames processed in total). Using the proposed\napproach, diagnostic performances (averaged over both pure and mixed stone\ntypes) were as follows: balanced accuracy=88%, sensitivity=80%,\nspecificity=95%, precision=78% and F1-score=78%.\n</p>\n<p>The obtained results demonstrate that AI applied on digital endoscopic video\nsequences is a promising tool for collecting morphological information during\nthe time-course of the stone fragmentation process without resorting to any\nhuman intervention for stone delineation or selection of good quality steady\nframes. To this end, irrelevant image information must be removed from the\nprediction process at both frame and pixel levels, which is now feasible thanks\nto the use of AI-dedicated networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estrade_V/0/1/0/all/0/1\">Vincent Estrade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudon_M/0/1/0/all/0/1\">Michel Daudon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_E/0/1/0/all/0/1\">Emmanuel Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernhard_J/0/1/0/all/0/1\">Jean-Christophe Bernhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bladou_F/0/1/0/all/0/1\">Franck Bladou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert_G/0/1/0/all/0/1\">Gregoire Robert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facq_L/0/1/0/all/0/1\">Laurent Facq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senneville_B/0/1/0/all/0/1\">Baudouin Denis de Senneville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor-based Emotion Editing in the StyleGAN Latent Space. (arXiv:2205.06102v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06102","description":"<p>In this paper, we use a tensor model based on the Higher-Order Singular Value\nDecomposition (HOSVD) to discover semantic directions in Generative Adversarial\nNetworks. This is achieved by first embedding a structured facial expression\ndatabase into the latent space using the e4e encoder. Specifically, we discover\ndirections in latent space corresponding to the six prototypical emotions:\nanger, disgust, fear, happiness, sadness, and surprise, as well as a direction\nfor yaw rotation. These latent space directions are employed to change the\nexpression or yaw rotation of real face images. We compare our found directions\nto similar directions found by two other methods. The results show that the\nvisual quality of the resultant edits are on par with State-of-the-Art. It can\nalso be concluded that the tensor-based model is well suited for emotion and\nyaw editing, i.e., that the emotion or yaw rotation of a novel face image can\nbe robustly changed without a significant effect on identity or other\nattributes in the images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haas_R/0/1/0/all/0/1\">Ren&#xe9; Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasshof_S/0/1/0/all/0/1\">Stella Gra&#xdf;hof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1\">Sami S. Brandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code. (arXiv:2205.06126v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06126","description":"<p>People perceive the world with multiple senses (e.g., through hearing sounds,\nreading words and seeing objects). However, most existing AI systems only\nprocess an individual modality. This paper presents an approach that excels at\nhandling multiple modalities of information with a single model. In our\n\"{SkillNet}\" model, different parts of the parameters are specialized for\nprocessing different modalities. Unlike traditional dense models that always\nactivate all the model parameters, our model sparsely activates parts of the\nparameters whose skills are relevant to the task. Such model design enables\nSkillNet to learn skills in a more interpretable way. We develop our model for\nfive modalities including text, image, sound, video and code. Results show\nthat, SkillNet performs comparably to five modality-specific fine-tuned models.\nMoreover, our model supports self-supervised pretraining with the same sparsely\nactivated way, resulting in better initialized parameters for different\nmodalities. We find that pretraining significantly improves the performance of\nSkillNet on five modalities, on par with or even better than baselines with\nmodality-specific pretraining. On the task of Chinese text-to-image retrieval,\nour final system achieves higher accuracy than existing leading systems\nincluding Wukong{ViT-B} and Wenlan 2.0 while using less number of activated\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liangxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth-Reduce: Leveraging Patches for Improved Certified Robustness. (arXiv:2205.06154v1 [cs.LG])","link":"http://arxiv.org/abs/2205.06154","description":"<p>Randomized smoothing (RS) has been shown to be a fast, scalable technique for\ncertifying the robustness of deep neural network classifiers. However, methods\nbased on RS require augmenting data with large amounts of noise, which leads to\nsignificant drops in accuracy. We propose a training-free, modified smoothing\napproach, Smooth-Reduce, that leverages patching and aggregation to provide\nimproved classifier certificates. Our algorithm classifies overlapping patches\nextracted from an input image, and aggregates the predicted logits to certify a\nlarger radius around the input. We study two aggregation schemes -- max and\nmean -- and show that both approaches provide better certificates in terms of\ncertified accuracy, average certified radii and abstention rates as compared to\nconcurrent approaches. We also provide theoretical guarantees for such\ncertificates, and empirically show significant improvements over other\nrandomized smoothing methods that require expensive retraining. Further, we\nextend our approach to videos and provide meaningful certificates for video\nclassifiers. A project page can be found at\nhttps://nyu-dice-lab.github.io/SmoothReduce/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ameya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condessa_F/0/1/0/all/0/1\">Filipe Condessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Vision-Language Matching for Open-vocabulary Object Detection. (arXiv:2205.06160v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06160","description":"<p>In this work, we propose an open-world object detection method that, based on\nimage-caption pairs, learns to detect novel object classes along with a given\nset of known classes. It is a two-stage training approach that first uses a\nlocation-guided image-caption matching technique to learn class labels for both\nnovel and known classes in a weakly-supervised manner and second specializes\nthe model for the object detection task using known class annotations. We show\nthat a simple language model fits better than a large contextualized language\nmodel for detecting novel objects. Moreover, we introduce a\nconsistency-regularization technique to better exploit image-caption pair\ninformation. Our method compares favorably to existing open-world detection\napproaches while being data-efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bravo_M/0/1/0/all/0/1\">Maria A. Bravo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sudhanshu Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Visual and Inertial Odometry with Adaptive Visual Modality Selection. (arXiv:2205.06187v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06187","description":"<p>In recent years, deep learning-based approaches for visual-inertial odometry\n(VIO) have shown remarkable performance outperforming traditional geometric\nmethods. Yet, all existing methods use both the visual and inertial\nmeasurements for every pose estimation incurring potential computational\nredundancy. While visual data processing is much more expensive than that for\nthe inertial measurement unit (IMU), it may not always contribute to improving\nthe pose estimation accuracy. In this paper, we propose an adaptive\ndeep-learning based VIO method that reduces computational redundancy by\nopportunistically disabling the visual modality. Specifically, we train a\npolicy network that learns to deactivate the visual feature extractor on the\nfly based on the current motion state and IMU readings. A Gumbel-Softmax trick\nis adopted to train the policy network to make the decision process\ndifferentiable for end-to-end system training. The learned strategy is\ninterpretable, and it shows scenario-dependent decision patterns for adaptive\ncomplexity reduction. Experiment results show that our method achieves a\nsimilar or even better performance than the full-modality baseline with up to\n78.8% computational complexity reduction for KITTI dataset evaluation. Our code\nwill be shared in https://github.com/mingyuyng/Visual-Selective-VIO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hun-Seok Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Segmentation with Topological Priors. (arXiv:2205.06197v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06197","description":"<p>Solving segmentation tasks with topological priors proved to make fewer\nerrors in fine-scale structures. In this work, we use topological priors both\nbefore and during the deep neural network training procedure. We compared the\nresults of the two approaches with simple segmentation on various accuracy\nmetrics and the Betti number error, which is directly related to topological\ncorrectness, and discovered that incorporating topological information into the\nclassical UNet model performed significantly better. We conducted experiments\non the ISBI EM segmentation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sofi_S/0/1/0/all/0/1\">Shakir Showkat Sofi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsahanova_N/0/1/0/all/0/1\">Nadezhda Alsahanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied vision for learning object representations. (arXiv:2205.06198v1 [cs.LG])","link":"http://arxiv.org/abs/2205.06198","description":"<p>Recent time-contrastive learning approaches manage to learn invariant object\nrepresentations without supervision. This is achieved by mapping successive\nviews of an object onto close-by internal representations. When considering\nthis learning approach as a model of the development of human object\nrecognition, it is important to consider what visual input a toddler would\ntypically observe while interacting with objects. First, human vision is highly\nfoveated, with high resolution only available in the central region of the\nfield of view. Second, objects may be seen against a blurry background due to\ninfants' limited depth of field. Third, during object manipulation a toddler\nmostly observes close objects filling a large part of the field of view due to\ntheir rather short arms. Here, we study how these effects impact the quality of\nvisual representations learnt through time-contrastive learning. To this end,\nwe let a visually embodied agent \"play\" with objects in different locations of\na near photo-realistic flat. During each play session the agent views an object\nin multiple orientations before turning its body to view another object. The\nresulting sequence of views feeds a time-contrastive learning algorithm. Our\nresults show that visual statistics mimicking those of a toddler improve object\nrecognition accuracy in both familiar and novel environments. We argue that\nthis effect is caused by the reduction of features extracted in the background,\na neural network bias for large features in the image and a greater similarity\nbetween novel and familiar background regions. We conclude that the embodied\nnature of visual learning may be crucial for understanding the development of\nhuman object perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1\">Arthur Aubret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teuliere_C/0/1/0/all/0/1\">C&#xe9;line Teuli&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1\">Jochen Triesch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F3A-GAN: Facial Flow for Face Animation with Generative Adversarial Networks. (arXiv:2205.06204v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06204","description":"<p>Formulated as a conditional generation problem, face animation aims at\nsynthesizing continuous face images from a single source image driven by a set\nof conditional face motion. Previous works mainly model the face motion as\nconditions with 1D or 2D representation (e.g., action units, emotion codes,\nlandmark), which often leads to low-quality results in some complicated\nscenarios such as continuous generation and largepose transformation. To tackle\nthis problem, the conditions are supposed to meet two requirements, i.e.,\nmotion information preserving and geometric continuity. To this end, we propose\na novel representation based on a 3D geometric flow, termed facial flow, to\nrepresent the natural motion of the human face at any pose. Compared with other\nprevious conditions, the proposed facial flow well controls the continuous\nchanges to the face. After that, in order to utilize the facial flow for face\nediting, we build a synthesis framework generating continuous images with\nconditional facial flows. To fully take advantage of the motion information of\nfacial flows, a hierarchical conditional framework is designed to combine the\nextracted multi-scale appearance features from images and motion features from\nflows in a hierarchical manner. The framework then decodes multiple fused\nfeatures back to images progressively. Experimental results demonstrate the\neffectiveness of our method compared to other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lingyun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets. (arXiv:2205.06218v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06218","description":"<p>This paper performs comprehensive analysis on datasets for occlusion-aware\nface segmentation, a task that is crucial for many downstream applications. The\ncollection and annotation of such datasets are time-consuming and\nlabor-intensive. Although some efforts have been made in synthetic data\ngeneration, the naturalistic aspect of data remains less explored. In our\nstudy, we propose two occlusion generation techniques, Naturalistic Occlusion\nGeneration (NatOcc), for producing high-quality naturalistic synthetic occluded\nfaces; and Random Occlusion Generation (RandOcc), a more general synthetic\noccluded data generation method. We empirically show the effectiveness and\nrobustness of both methods, even for unseen occlusions. To facilitate model\nevaluation, we present two high-resolution real-world occluded face datasets\nwith fine-grained annotations, RealOcc and RealOcc-Wild, featuring both careful\nalignment preprocessing and an in-the-wild setting for robustness test. We\nfurther conduct a comprehensive analysis on a newly introduced segmentation\nbenchmark, offering insights for future exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voo_K/0/1/0/all/0/1\">Kenny T. R. Voo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2205.06230v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06230","description":"<p>Combining simple architectures with large-scale pre-training has led to\nmassive improvements in image classification. For object detection,\npre-training and scaling approaches are less well established, especially in\nthe long-tailed and open-vocabulary setting, where training data is relatively\nscarce. In this paper, we propose a strong recipe for transferring image-text\nmodels to open-vocabulary object detection. We use a standard Vision\nTransformer architecture with minimal modifications, contrastive image-text\npre-training, and end-to-end detection fine-tuning. Our analysis of the scaling\nproperties of this setup shows that increasing image-level pre-training and\nmodel size yield consistent improvements on the downstream detection task. We\nprovide the adaptation strategies and regularizations needed to attain very\nstrong performance on zero-shot text-conditioned and one-shot image-conditioned\nobject detection. Code and models are available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1\">Maxim Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1\">Dirk Weissenborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1\">Aravindh Mahendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhuoran Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation for Multi-Target Domain Adaptation in Real-Time Person Re-Identification. (arXiv:2205.06237v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06237","description":"<p>Despite the recent success of deep learning architectures, person\nre-identification (ReID) remains a challenging problem in real-word\napplications. Several unsupervised single-target domain adaptation (STDA)\nmethods have recently been proposed to limit the decline in ReID accuracy\ncaused by the domain shift that typically occurs between source and target\nvideo data. Given the multimodal nature of person ReID data (due to variations\nacross camera viewpoints and capture conditions), training a common CNN\nbackbone to address domain shifts across multiple target domains, can provide\nan efficient solution for real-time ReID applications. Although multi-target\ndomain adaptation (MTDA) has not been widely addressed in the ReID literature,\na straightforward approach consists in blending different target datasets, and\nperforming STDA on the mixture to train a common CNN. However, this approach\nmay lead to poor generalization, especially when blending a growing number of\ndistinct target domains to train a smaller CNN.\n</p>\n<p>To alleviate this problem, we introduce a new MTDA method based on knowledge\ndistillation (KD-ReID) that is suitable for real-time person ReID applications.\nOur method adapts a common lightweight student backbone CNN over the target\ndomains by alternatively distilling from multiple specialized teacher CNNs,\neach one adapted on data from a specific target domain. Extensive experiments\nconducted on several challenging person ReID datasets indicate that our\napproach outperforms state-of-art methods for MTDA, including blending methods,\nparticularly when training a compact CNN backbone like OSNet. Results suggest\nthat our flexible MTDA approach can be employed to design cost-effective ReID\nsystems for real-time video surveillance applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remigereau_F/0/1/0/all/0/1\">F&#xe9;lix Remigereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekhazni_D/0/1/0/all/0/1\">Djebril Mekhazni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_S/0/1/0/all/0/1\">Sajjad Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Meidine_L/0/1/0/all/0/1\">Le Thanh Nguyen-Meidine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1\">Rafael M. O. Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics. (arXiv:2205.06253v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06253","description":"<p>While there have been significant gains in the field of automated video\ndescription, the generalization performance of automated description models to\nnovel domains remains a major barrier to using these systems in the real world.\nMost visual description methods are known to capture and exploit patterns in\nthe training data leading to evaluation metric increases, but what are those\npatterns? In this work, we examine several popular visual description datasets,\nand capture, analyze, and understand the dataset-specific linguistic patterns\nthat models exploit but do not generalize to new domains. At the token level,\nsample level, and dataset level, we find that caption diversity is a major\ndriving factor behind the generation of generic and uninformative captions. We\nfurther show that state-of-the-art models even outperform held-out ground truth\ncaptions on modern metrics, and that this effect is an artifact of linguistic\ndiversity in datasets. Understanding this linguistic diversity is key to\nbuilding strong captioning models, we recommend several methods and approaches\nfor maintaining diversity in the collection of new data, and dealing with the\nconsequences of limited diversity when using current models and metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1\">Austin Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1\">Sudheendra Vijayanarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1\">Bryan Seybold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John F. Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Vertex Descent: A New Direction for 3D Human Model Fitting. (arXiv:2205.06254v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06254","description":"<p>We propose a novel optimization-based paradigm for 3D human model fitting on\nimages and scans. In contrast to existing approaches that directly regress the\nparameters of a low-dimensional statistical body model (e.g. SMPL) from input\nimages, we train an ensemble of per-vertex neural fields network. The network\npredicts, in a distributed manner, the vertex descent direction towards the\nground truth, based on neural features extracted at the current vertex\nprojection. At inference, we employ this network, dubbed LVD, within a\ngradient-descent optimization pipeline until its convergence, which typically\noccurs in a fraction of a second even when initializing all vertices into a\nsingle point. An exhaustive evaluation demonstrates that our approach is able\nto capture the underlying body of clothed people with very different body\nshapes, achieving a significant improvement compared to state-of-the-art. LVD\nis also applicable to 3D model fitting of humans and hands, for which we show a\nsignificant improvement to the SOTA with a much simpler and faster method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enric Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alenya_G/0/1/0/all/0/1\">Guillem Aleny&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Moments from Near-Duplicate Photos. (arXiv:2205.06255v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06255","description":"<p>We introduce 3D Moments, a new computational photography effect. As input we\ntake a pair of near-duplicate photos, i.e., photos of moving subjects from\nsimilar viewpoints, common in people's photo collections. As output, we produce\na video that smoothly interpolates the scene motion from the first photo to the\nsecond, while also producing camera motion with parallax that gives a\nheightened sense of 3D. To achieve this effect, we represent the scene as a\npair of feature-based layered depth images augmented with scene flow. This\nrepresentation enables motion interpolation along with independent control of\nthe camera viewpoint. Our system produces photorealistic space-time videos with\nmotion parallax and scene dynamics, while plausibly recovering regions occluded\nin the original views. We conduct extensive experiments demonstrating superior\nperformance over baselines on public datasets and in-the-wild photos. Project\npage: https://3d-moments.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesin_D/0/1/0/all/0/1\">David Salesin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontkanen_J/0/1/0/all/0/1\">Janne Kontkanen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELODI: Ensemble Logit Difference Inhibition for Positive-Congruent Training. (arXiv:2205.06265v1 [cs.LG])","link":"http://arxiv.org/abs/2205.06265","description":"<p>Negative flips are errors introduced in a classification system when a legacy\nmodel is replaced with a new one. Existing methods to reduce the negative flip\nrate (NFR) either do so at the expense of overall accuracy using model\ndistillation, or use ensembles, which multiply inference cost prohibitively. We\npresent a method to train a classification system that achieves paragon\nperformance in both error rate and NFR, at the inference cost of a single\nmodel. Our method introduces a generalized distillation objective, Logit\nDifference Inhibition (LDI), that penalizes changes in the logits between the\nnew and old model, without forcing them to coincide as in ordinary\ndistillation. LDI affords the model flexibility to reduce error rate along with\nNFR. The method uses a homogeneous ensemble as the reference model for LDI,\nhence the name Ensemble LDI, or ELODI. The reference model can then be\nsubstituted with a single model at inference time. The method leverages the\nobservation that negative flips are typically not close to the decision\nboundary, but often exhibit large deviations in the distance among their\nlogits, which are reduced by ELODI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yantao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiele_B/0/1/0/all/0/1\">Bernt Shiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topologically-Aware Deformation Fields for Single-View 3D Reconstruction. (arXiv:2205.06267v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06267","description":"<p>We present a new framework for learning 3D object shapes and dense\ncross-object 3D correspondences from just an unaligned category-specific image\ncollection. The 3D shapes are generated implicitly as deformations to a\ncategory-specific signed distance field and are learned in an unsupervised\nmanner solely from unaligned image collections without any 3D supervision.\nGenerally, image collections on the internet contain several intra-category\ngeometric and topological variations, for example, different chairs can have\ndifferent topologies, which makes the task of joint shape and correspondence\nestimation much more challenging. Because of this, prior works either focus on\nlearning each 3D object shape individually without modeling cross-instance\ncorrespondences or perform joint shape and correspondence estimation on\ncategories with minimal intra-category topological variations. We overcome\nthese restrictions by learning a topologically-aware implicit deformation field\nthat maps a 3D point in the object space to a higher dimensional point in the\ncategory-specific canonical space. At inference time, given a single image, we\nreconstruct the underlying 3D shape by first implicitly deforming each 3D point\nin the object space to the learned category-specific canonical space using the\ntopologically-aware deformation field and then reconstructing the 3D shape as a\ncanonical signed distance field. Both canonical shape and deformation field are\nlearned end-to-end in an inverse-graphics fashion using a learned recurrent ray\nmarcher (SRN) as a differentiable rendering module. Our approach, dubbed TARS,\nachieves state-of-the-art reconstruction fidelity on several datasets:\nShapeNet, Pascal3D+, CUB, and Pix3D chairs. Result videos and code at\nhttps://shivamduggal4.github.io/tars-3D/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1\">Shivam Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-negative Sparse and Collaborative Representation for Pattern Classification. (arXiv:1908.07956v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.07956","description":"<p>Sparse representation (SR) and collaborative representation (CR) have been\nsuccessfully applied in many pattern classification tasks such as face\nrecognition. In this paper, we propose a novel Non-negative Sparse and\nCollaborative Representation (NSCR) for pattern classification. The NSCR\nrepresentation of each test sample is obtained by seeking a non-negative sparse\nand collaborative representation vector that represents the test sample as a\nlinear combination of training samples. We observe that the non-negativity can\nmake the SR and CR more discriminative and effective for pattern\nclassification. Based on the proposed NSCR, we propose a NSCR based classifier\nfor pattern classification. Extensive experiments on benchmark datasets\ndemonstrate that the proposed NSCR based classifier outperforms the previous SR\nor CR based approach, as well as state-of-the-art deep approaches, on diverse\nchallenging pattern classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhou Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wangpeng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResLT: Residual Learning for Long-tailed Recognition. (arXiv:2101.10633v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10633","description":"<p>Deep learning algorithms face great challenges with long-tailed data\ndistribution which, however, is quite a common case in real-world scenarios.\nPrevious methods tackle the problem from either the aspect of input space\n(re-sampling classes with different frequencies) or loss space (re-weighting\nclasses with different weights), suffering from heavy over-fitting to tail\nclasses or hard optimization during training. To alleviate these issues, we\npropose a more fundamental perspective for long-tailed recognition, i.e., from\nthe aspect of parameter space, and aims to preserve specific capacity for\nclasses with low frequencies. From this perspective, the trivial solution\nutilizes different branches for the head, medium, and tail classes\nrespectively, and then sums their outputs as the final results is not feasible.\nInstead, we design the effective residual fusion mechanism -- with one main\nbranch optimized to recognize images from all classes, another two residual\nbranches are gradually fused and optimized to enhance images from medium+tail\nclasses and tail classes respectively. Then the branches are aggregated into\nfinal results by additive shortcuts. We test our method on several benchmarks,\ni.e., long-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and\niNaturalist 2018. Experimental results manifest the effectiveness of our\nmethod. Our code is available at https://github.com/jiequancui/ResLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Model Compression based on the Training History. (arXiv:2102.00160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00160","description":"<p>Deep Convolutional Neural Networks (DCNNs) have shown promising performances\nin several visual recognition problems which motivated the researchers to\npropose popular architectures such as LeNet, AlexNet, VGGNet, ResNet, and many\nmore. These architectures come at a cost of high computational complexity and\nparameter storage. To get rid of storage and computational complexity, deep\nmodel compression methods have been evolved. We propose a \"History Based Filter\nPruning (HBFP)\" method that utilizes network training history for filter\npruning. Specifically, we prune the redundant filters by observing similar\npatterns in the filter's L1-norms (absolute sum of weights) over the training\nepochs. We iteratively prune the redundant filters of a CNN in three steps.\nFirst, we train the model and select the filter pairs with redundant filters in\neach pair. Next, we optimize the network to ensure an increased measure of\nsimilarity between the filters in a pair. This optimization of the network\nfacilitates us to prune one filter from each pair based on its importance\nwithout much information loss. Finally, we retrain the network to regain the\nperformance, which is dropped due to filter pruning. We test our approach on\npopular architectures such as LeNet-5 on MNIST dataset; VGG-16, ResNet-56, and\nResNet-110 on CIFAR-10 dataset, and ResNet-50 on ImageNet. The proposed pruning\nmethod outperforms the state-of-the-art in terms of FLOPs reduction\n(floating-point operations) by 97.98%, 83.42%, 78.43%, 74.95%, and 75.45% for\nLeNet-5, VGG-16, ResNet-56, ResNet-110, and ResNet-50, respectively, while\nmaintaining the less error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basha_S/0/1/0/all/0/1\">S.H.Shabbeer Basha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farazuddin_M/0/1/0/all/0/1\">Mohammad Farazuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulabaigari_V/0/1/0/all/0/1\">Viswanath Pulabaigari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Snehasis Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How I failed machine learning in medical imaging -- shortcomings and recommendations. (arXiv:2103.10292v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.10292","description":"<p>Medical imaging is an important research field with many opportunities for\nimproving patients' health. However, there are a number of challenges that are\nslowing down the progress of the field as a whole, such optimizing for\npublication. In this paper we reviewed several problems related to choosing\ndatasets, methods, evaluation metrics, and publication strategies. With a\nreview of literature and our own analysis, we show that at every step,\npotential biases can creep in. On a positive note, we also see that initiatives\nto counteract these problems are already being started. Finally we provide a\nbroad range of recommendations on how to further these address problems in the\nfuture. For reproducibility, data and code for our analyses are available on\n\\url{https://github.com/GaelVaroquaux/ml_med_imaging_failures}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Multi-Target Domain Adaptation for Object Detection with Efficient Domain Transfer. (arXiv:2104.06476v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06476","description":"<p>Recent advances in unsupervised domain adaptation have significantly improved\nthe recognition accuracy of CNNs by alleviating the domain shift between\n(labeled) source and (unlabeled) target data distributions. While the problem\nof single-target domain adaptation (STDA) for object detection has recently\nreceived much attention, multi-target domain adaptation (MTDA) remains largely\nunexplored, despite its practical relevance in several real-world applications,\nsuch as multi-camera video surveillance. Compared to the STDA problem that may\ninvolve large domain shifts between complex source and target distributions,\nMTDA faces additional challenges, most notably the computational requirements\nand catastrophic forgetting of previously-learned targets, which can depend on\nthe order of target adaptations. STDA for detection can be applied to MTDA by\nadapting one model per target, or one common model with a mixture of data from\ntarget domains. However, these approaches are either costly or inaccurate. The\nonly state-of-art MTDA method specialized for detection learns targets\nincrementally, one target at a time, and mitigates the loss of knowledge by\nusing a duplicated detection model for knowledge distillation, which is\ncomputationally expensive and does not scale well to many domains. In this\npaper, we introduce an efficient approach for incremental learning that\ngeneralizes well to multiple target domains. Our MTDA approach is more suitable\nfor real-world applications since it allows updating the detection model\nincrementally, without storing data from previous-learned target domains, nor\nretraining when a new target domain becomes available. Our proposed method,\nMTDA-DTM, achieved the highest level of detection accuracy compared against\nstate-of-the-art approaches on several MTDA detection benchmarks and Wildtrack,\na benchmark for multi-camera pedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Meidine_L/0/1/0/all/0/1\">Le Thanh Nguyen-Meidine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_M/0/1/0/all/0/1\">Madhu Kiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_Morin_L/0/1/0/all/0/1\">Louis-Antoine Blais-Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation. (arXiv:2104.13921v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13921","description":"<p>We aim at advancing open-vocabulary object detection, which detects objects\ndescribed by arbitrary text inputs. The fundamental challenge is the\navailability of training data. It is costly to further scale up the number of\nclasses contained in existing object detection datasets. To overcome this\nchallenge, we propose ViLD, a training method via Vision and Language knowledge\nDistillation. Our method distills the knowledge from a pretrained\nopen-vocabulary image classification model (teacher) into a two-stage detector\n(student). Specifically, we use the teacher model to encode category texts and\nimage regions of object proposals. Then we train a student detector, whose\nregion embeddings of detected boxes are aligned with the text and image\nembeddings inferred by the teacher. We benchmark on LVIS by holding out all\nrare categories as novel categories that are not seen during training. ViLD\nobtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the\nsupervised counterpart by 3.8. When trained with a stronger teacher model\nALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other\ndatasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on\nCOCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous\nstate-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are\nopen-sourced at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiuye Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v10 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive gradient methods were\nrecently studied, they mainly focus on either empirical or theoretical aspects\nand also only work for specific problems by using some specific adaptive\nlearning rates. Thus, it is desired to design a universal framework for\npractical algorithms of adaptive gradients with theoretical guarantee to solve\ngeneral problems. To fill this gap, we propose a faster and universal framework\nof adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive\nmatrix that includes most existing adaptive gradient forms. Moreover, our\nframework can flexibly integrate the momentum and variance reduced techniques.\nIn particular, our novel framework provides the convergence analysis support\nfor adaptive gradient methods under the nonconvex setting. In theoretical\nanalysis, we prove that our SUPER-ADAM algorithm can achieve the best known\ngradient (i.e., stochastic first-order oracle (SFO)) complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing the Budget: Feature Selection and Tracking for Multi-Camera Visual-Inertial Odometry. (arXiv:2109.05975v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.05975","description":"<p>We present a multi-camera visual-inertial odometry system based on factor\ngraph optimization which estimates motion by using all cameras simultaneously\nwhile retaining a fixed overall feature budget. We focus on motion tracking in\nchallenging environments, such as narrow corridors, dark spaces with aggressive\nmotions, and abrupt lighting changes. These scenarios cause traditional\nmonocular or stereo odometry to fail. While tracking motion with extra cameras\nshould theoretically prevent failures, it leads to additional complexity and\ncomputational burden. To overcome these challenges, we introduce two novel\nmethods to improve multi-camera feature tracking. First, instead of tracking\nfeatures separately in each camera, we track features continuously as they move\nfrom one camera to another. This increases accuracy and achieves a more compact\nfactor graph representation. Second, we select a fixed budget of tracked\nfeatures across the cameras to reduce back-end optimization time. We have found\nthat using a smaller set of informative features can maintain the same tracking\naccuracy. Our proposed method was extensively tested using a\nhardware-synchronized device consisting of an IMU and four cameras (a front\nstereo pair and two lateral) in scenarios including: an underground mine, large\nopen spaces, and building interiors with narrow stairs and corridors. Compared\nto stereo-only state-of-the-art visual-inertial odometry methods, our approach\nreduces the drift rate, relative pose error, by up to 80% in translation and\n39% in rotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lintong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Stage Keypoint-Based Category-Level Object Pose Estimation from an RGB Image. (arXiv:2109.06161v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06161","description":"<p>Prior work on 6-DoF object pose estimation has largely focused on\ninstance-level processing, in which a textured CAD model is available for each\nobject being detected. Category-level 6-DoF pose estimation represents an\nimportant step toward developing robotic vision systems that operate in\nunstructured, real-world scenarios. In this work, we propose a single-stage,\nkeypoint-based approach for category-level object pose estimation that operates\non unknown object instances within a known category using a single RGB image as\ninput. The proposed network performs 2D object detection, detects 2D keypoints,\nestimates 6-DoF pose, and regresses relative bounding cuboid dimensions. These\nquantities are estimated in a sequential fashion, leveraging the recent idea of\nconvGRU for propagating information from easier tasks to those that are more\ndifficult. We favor simplicity in our design choices: generic cuboid vertex\ncoordinates, single-stage network, and monocular RGB input. We conduct\nextensive experiments on the challenging Objectron benchmark, outperforming\nstate-of-the-art methods on the 3D IoU metric (27.6% higher than the MobilePose\nsingle-stage approach and 7.1% higher than the related two-stage approach).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yunzhi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyree_S/0/1/0/all/0/1\">Stephen Tyree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vela_P/0/1/0/all/0/1\">Patricio A. Vela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1\">Stan Birchfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instant Response Few-shot Object Detection with Meta Strategy and Explicit Localization Inference. (arXiv:2110.13377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13377","description":"<p>Aiming at recognizing and localizing the object of novel categories by a few\nreference samples, few-shot object detection (FSOD) is a quite challenging\ntask. Previous works often depend on the fine-tuning process to transfer their\nmodel to the novel category and rarely consider the defect of fine-tuning,\nresulting in many application drawbacks. For example, these methods are far\nfrom satisfying in the episode-changeable scenarios due to excessive\nfine-tuning times, and their performance on low-quality (e.g., low-shot and\nclass-incomplete) support sets degrades severely. To this end, this paper\nproposes an instant response few-shot object detector (IR-FSOD) that can\naccurately and directly detect the objects of novel categories without the\nfine-tuning process. To accomplish the objective, we carefully analyze the\ndefects of individual modules in the Faster R-CNN framework under the FSOD\nsetting and then extend it to IR-FSOD by improving these defects. Specifically,\nwe first propose two simple but effective meta-strategies for the box\nclassifier and RPN module to enable the object detection of novel categories\nwith instant response. Then, we introduce two explicit inferences into the\nlocalization module to alleviate its over-fitting to the base categories,\nincluding explicit localization score and semi-explicit box regression.\nExtensive experiments show that the IR-FSOD framework not only achieves\nfew-shot object detection with the instant response but also reaches\nstate-of-the-art performance in precision and recall under various FSOD\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Adaptive Teacher for Object Detection. (arXiv:2111.13216v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13216","description":"<p>We address the task of domain adaptation in object detection, where there is\na domain gap between a domain with annotations (source) and a domain of\ninterest without annotations (target). As an effective semi-supervised learning\nmethod, the teacher-student framework (a student model is supervised by the\npseudo labels from a teacher model) has also yielded a large accuracy gain in\ncross-domain object detection. However, it suffers from the domain shift and\ngenerates many low-quality pseudo labels (\\textit{e.g.,} false positives),\nwhich leads to sub-optimal performance. To mitigate this problem, we propose a\nteacher-student framework named Adaptive Teacher (AT) which leverages domain\nadversarial learning and weak-strong data augmentation to address the domain\ngap. Specifically, we employ feature-level adversarial training in the student\nmodel, allowing features derived from the source and target domains to share\nsimilar distributions. This process ensures the student model produces\ndomain-invariant features. Furthermore, we apply weak-strong augmentation and\nmutual learning between the teacher model (taking data from the target domain)\nand the student model (taking data from both domains). This enables the teacher\nmodel to learn the knowledge from the student model without being biased to the\nsource domain. We show that AT demonstrates superiority over existing\napproaches and even Oracle (fully-supervised) models by a large margin. For\nexample, we achieve 50.9% (49.3%) mAP on Foggy Cityscape (Clipart1K), which is\n9.2% (5.2%) and 8.2% (11.0%) higher than previous state-of-the-art and Oracle,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaoliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chih-Yao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zijian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Camera LiDAR Inertial Extension to the Newer College Dataset. (arXiv:2112.08854v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.08854","description":"<p>We present a multi-camera LiDAR inertial dataset of 4.5 km walking distance\nas an expansion of the Newer College Dataset. The global shutter multi-camera\ndevice is hardware synchronized with both the IMU and LiDAR, which is more\naccurate than the original dataset with software synchronization. This dataset\nalso provides six Degrees of Freedom (DoF) ground truth poses at LiDAR\nfrequency (10 Hz). Three data collections are described and an example use case\nof multi-camera visual-inertial odometry is demonstrated. This expansion\ndataset contains small and narrow passages, large scale open spaces, as well as\nvegetated areas, to test localization and mapping systems. Furthermore, some\nsequences present challenging situations such as abrupt lighting change,\ntextureless surfaces, and aggressive motion. The dataset is available at:\nhttps://ori-drs.github. io/newer-college-dataset/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lintong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are?. (arXiv:2201.06251v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.06251","description":"<p>Cancer is one of the leading causes of death worldwide, and head and neck\n(H&amp;N) cancer is amongst the most prevalent types. Positron emission tomography\nand computed tomography are used to detect, segment and quantify the tumor\nregion. Clinically, tumor segmentation is extensively time-consuming and prone\nto error. Machine learning, and deep learning in particular, can assist to\nautomate this process, yielding results as accurate as the results of a\nclinician. In this paper, we investigate a vision transformer-based method to\nautomatically delineate H&amp;N tumor, and compare its results to leading\nconvolutional neural network (CNN)-based models. We use multi-modal data from\nCT and PET scans to perform the segmentation task. We show that a solution with\na transformer-based model has the potential to achieve comparable results to\nCNN-based ones. With cross validation, the model achieves a mean dice\nsimilarity coefficient (DSC) of 0.736, mean precision of 0.766 and mean recall\nof 0.766. This is only 0.021 less than the 2020 competition winning model\n(cross validated in-house) in terms of the DSC score. On the testing set, the\nmodel performs similarly, with DSC of 0.736, precision of 0.773, and recall of\n0.760, which is only 0.023 lower in DSC than the 2020 competition winning\nmodel. This work shows that cancer segmentation via transformer-based models is\na promising research area to further explore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sobirov_I/0/1/0/all/0/1\">Ikboljon Sobirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nazarov_O/0/1/0/all/0/1\">Otabek Nazarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alasmawi_H/0/1/0/all/0/1\">Hussain Alasmawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.13392","description":"<p>The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v4 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2202.06299","description":"<p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles\n(APMV) and cars can perform non-driving tasks, such as reading books and\nsmartphones, while driving. It has been pointed out that such activities may\nincrease motion sickness. Many studies have been conducted to build\ncountermeasures, of which various computational motion sickness models have\nbeen developed. Many of these are based on subjective vertical conflict (SVC)\ntheory, which describes vertical changes in direction sensed by human sensory\norgans vs. those expected by the central nervous system. Such models are\nexpected to be applied to autonomous driving scenarios. However, no current\ncomputational model can integrate visual vertical information with vestibular\nsensations.\n</p>\n<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical\nblock into a conventional six-degrees-of-freedom SVC model to predict VV\ndirections from image data simulating the visual input of a human. Hence, a\nsimple image-based VV estimation method is proposed.\n</p>\n<p>As the validation of the proposed model, this paper focuses on describing the\nfact that the motion sickness increases as a passenger reads a book while using\nan AMPV, assuming that visual vertical (VV) plays an important role. In the\nstatic experiment, it is demonstrated that the estimated VV by the proposed\nmethod accurately described the gravitational acceleration direction with a low\nmean absolute deviation. In addition, the results of the driving experiment\nusing an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe\nthat the increased motion sickness experienced when the VV and gravitational\nacceleration directions were different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hailong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shota Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takahiro Wada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Overconfidence Predictions for Autonomous Driving Perception. (arXiv:2202.07825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07825","description":"<p>In state-of-the-art deep learning for object recognition, SoftMax and Sigmoid\nfunctions are most commonly employed as the predictor outputs. Such layers\noften produce overconfident predictions rather than proper probabilistic\nscores, which can thus harm the decision-making of `critical' perception\nsystems applied in autonomous driving and robotics. Given this, the experiments\nin this work propose a probabilistic approach based on distributions calculated\nout of the Logit layer scores of pre-trained networks. We demonstrate that\nMaximum Likelihood (ML) and Maximum a-Posteriori (MAP) functions are more\nsuitable for probabilistic interpretations than SoftMax and Sigmoid-based\npredictions for object recognition. We explore distinct sensor modalities via\nRGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5\ndatasets, where our approach shows promising performance compared to the usual\nSoftMax and Sigmoid layers, with the benefit of enabling interpretable\nprobabilistic predictions. Another advantage of the approach introduced in this\npaper is that the ML and MAP functions can be implemented in existing trained\nnetworks, that is, the approach benefits from the output of the Logit layer of\npre-trained networks. Thus, there is no need to carry out a new training phase\nsince the ML and MAP functions are used in the test/prediction phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melotti_G/0/1/0/all/0/1\">Gledson Melotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_D/0/1/0/all/0/1\">Diego R. Faria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_N/0/1/0/all/0/1\">Nuno Gon&#xe7;alves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connections between Deep Equilibrium and Sparse Representation models with Application to Hyperspectral Imaging. (arXiv:2203.15901v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15901","description":"<p>In this study, the problem of computing a sparse representation of\nmulti-dimensional visual data is considered. In general, such data e.g.,\nhyperspectral images, color images or video data consists of signals that\nexhibit strong local dependencies. A new computationally efficient sparse\ncoding optimization problem is derived by employing regularization terms that\nare adapted to the properties of the signals of interest. Exploiting the merits\nof the learnable regularization techniques, a neural network is employed to act\nas structure prior and reveal the underlying signal dependencies. To solve the\noptimization problem Deep unrolling and Deep equilibrium based algorithms are\ndeveloped, forming highly interpretable and concise deep-learning-based\narchitectures, that process the input dataset in a block-by-block fashion.\nExtensive simulation results, in the context of hyperspectral image denoising,\nare provided, which demonstrate that the proposed algorithms outperform\nsignificantly other sparse coding approaches and exhibit superior performance\nagainst recent state-of-the-art deep-learning-based denoising models. In a\nwider perspective, our work provides a unique bridge between a classic\napproach, that is the sparse representation theory, and modern representation\ntools that are based on deep learning modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkillas_A/0/1/0/all/0/1\">Alexandros Gkillas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ampeliotis_D/0/1/0/all/0/1\">Dimitris Ampeliotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberidis_K/0/1/0/all/0/1\">Kostas Berberidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.16996","description":"<p>Background: Egocentric video has recently emerged as a potential solution for\nmonitoring hand function in individuals living with tetraplegia in the\ncommunity, especially for its ability to detect functional use in the home\nenvironment. Objective: To develop and validate a wearable vision-based system\nfor measuring hand use in the home among individuals living with tetraplegia.\nMethods: Several deep learning algorithms for detecting functional hand-object\ninteractions were developed and compared. The most accurate algorithm was used\nto extract measures of hand function from 65 hours of unscripted video recorded\nat home by 20 participants with tetraplegia. These measures were: the\npercentage of interaction time over total recording time (Perc); the average\nduration of individual interactions (Dur); the number of interactions per hour\n(Num). To demonstrate the clinical validity of the technology, egocentric\nmeasures were correlated with validated clinical assessments of hand function\nand independence (Graded Redefined Assessment of Strength, Sensibility and\nPrehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord\nIndependent Measure - SCIM). Results: Hand-object interactions were\nautomatically detected with a median F1-score of 0.80 (0.67-0.87). Our results\ndemonstrated that higher UEMS and better prehension were related to greater\ntime spent interacting, whereas higher SCIM and better hand sensation resulted\nin a higher number of interactions performed during the egocentric video\nrecordings. Conclusions: For the first time, measures of hand function\nautomatically estimated in an unconstrained environment in individuals with\ntetraplegia have been validated against internationally accepted measures of\nhand function. Future work will necessitate a formal evaluation of the\nreliability and responsiveness of the egocentric-based performance measures for\nhand use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandini_A/0/1/0/all/0/1\">Andrea Bandini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dousty_M/0/1/0/all/0/1\">Mehdy Dousty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hitzig_S/0/1/0/all/0/1\">Sander L. Hitzig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Craven_B/0/1/0/all/0/1\">B. Catharine Craven</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalsi_Ryan_S/0/1/0/all/0/1\">Sukhvinder Kalsi-Ryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zariffa_J/0/1/0/all/0/1\">Jos&#xe9; Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01297","description":"<p>Human motion prediction is a challenging task due to the dynamic\nspatiotemporal correlations in different motion sequences. How to efficiently\nrepresent spatiotemporal correlations and model dynamic correlation variances\nbetween different motion sequences is a challenge for spatiotemporal\nrepresentation in motion prediction. In this work, we propose Dynamic\nSpatioTemporal Decompose Graph Convolution (DSTD-GC), which decomposes dynamic\nspatiotemporal graph modeling with a combination of Dynamic Spatial Graph\nConvolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). The dynamic\nspatial/temporal correlations in DS-GC/DT-GC are efficiently represented by\nConstrained Dynamic Correlation Modeling, which is inspired by the common\nconstraints in human motion like body connections and dynamic patterns from\ndifferent samples. The Constrained Dynamic Correlation Modeling represents the\nspatial/temporal graph as a combination of a shared spatial/temporal\ncorrelation and an unshared correlation extraction function. This\nspatiotemporal representation is of square space complexity and only requires\n28.6% parameters of the state-of-the-art sample-shared decomposition\nrepresentation. It also explicitly models sample-specific spatiotemporal\ncorrelation variances. Moreover, we also mathematically reformulate graph\nconvolutions on spatiotemporal graphs into a unified form and find that DSTD-GC\nrelaxes certain constraints of other graph convolutions, which leads to a\nstronger representation capability. Combining DSTD-GC with prior knowledge like\nbody connection and temporal context, we propose a powerful spatiotemporal\ngraph convolution network called DSTD-GCN. On the Human3.6M and CMU Mocap\ndatasets, DSTD-GCN outperforms state-of-the-art methods by 3.9% - 5.7% in\nprediction accuracy with 55.0% - 96.9% parameter reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiajun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03475","description":"<p>ImageNet serves as the primary dataset for evaluating the quality of\ncomputer-vision models. The common practice today is training each architecture\nwith a tailor-made scheme, designed and tuned by an expert. In this paper, we\npresent a unified scheme for training any backbone on ImageNet. The scheme,\nnamed USI (Unified Scheme for ImageNet), is based on knowledge distillation and\nmodern tricks. It requires no adjustments or hyper-parameters tuning between\ndifferent models, and is efficient in terms of training times. We test USI on a\nwide variety of architectures, including CNNs, Transformers, Mobile-oriented\nand MLP-only. On all models tested, USI outperforms previous state-of-the-art\nresults. Hence, we are able to transform training on ImageNet from an\nexpert-oriented task to an automatic seamless routine. Since USI accepts any\nbackbone and trains it to top results, it also enables to perform methodical\ncomparisons, and identify the most efficient backbones along the speed-accuracy\nPareto curve. Implementation is available\nat:https://github.com/Alibaba-MIIL/Solving_ImageNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawen_H/0/1/0/all/0/1\">Hussam Lawen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation-Consistent Probabilistic Lesion Counting. (arXiv:2204.05276v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.05276","description":"<p>Lesion counts are important indicators of disease severity, patient\nprognosis, and treatment efficacy, yet counting as a task in medical imaging is\noften overlooked in favor of segmentation. This work introduces a novel\ncontinuously differentiable function that maps lesion segmentation predictions\nto lesion count probability distributions in a consistent manner. The proposed\nend-to-end approach--which consists of voxel clustering, lesion-level voxel\nprobability aggregation, and Poisson-binomial counting--is non-parametric and\nthus offers a robust and consistent way to augment lesion segmentation models\nwith post hoc counting capabilities. Experiments on Gadolinium-enhancing lesion\ncounting demonstrate that our method outputs accurate and well-calibrated count\ndistributions that capture meaningful uncertainty information. They also reveal\nthat our model is suitable for multi-task learning of lesion segmentation, is\nefficient in low data regimes, and is robust to adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schroeter_J/0/1/0/all/0/1\">Julien Schroeter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myers_Colet_C/0/1/0/all/0/1\">Chelsea Myers-Colet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems. (arXiv:2204.10384v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10384","description":"<p>Monocular depth estimation (MDE) aims to transform an RGB image of a scene\ninto a pixelwise depth map from the same camera view. It is fundamentally\nill-posed due to missing information: any single image can have been taken from\nmany possible 3D scenes. Part of the MDE task is, therefore, to learn which\nvisual cues in the image can be used for depth estimation, and how. With\ntraining data limited by cost of annotation or network capacity limited by\ncomputational power, this is challenging. In this work we demonstrate that\nexplicitly injecting visual cue information into the model is beneficial for\ndepth estimation. Following research into biological vision systems, we focus\non semantic information and prior knowledge of object sizes and their\nrelations, to emulate the biological cues of relative size, familiar size, and\nabsolute size. We use state-of-the-art semantic and instance segmentation\nmodels to provide external information, and exploit language embeddings to\nencode relational information between classes. We also provide a prior on the\naverage real-world size of objects. This external information overcomes the\nlimitation in data availability, and ensures that the limited capacity of a\ngiven network is focused on known-helpful cues, therefore improving\nperformance. We experimentally validate our hypothesis and evaluate the\nproposed model on the widely used NYUD2 indoor depth estimation benchmark. The\nresults show improvements in depth prediction when the semantic information,\nsize prior and instance size are explicitly provided along with the RGB images,\nand our method can be easily adapted to any depth estimation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Auty_D/0/1/0/all/0/1\">Dylan Auty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra-fast image categorization in vivo and in silico. (arXiv:2205.03635v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2205.03635","description":"<p>Humans are able to robustly categorize images and can, for instance, detect\nthe presence of an animal in a briefly flashed image in as little as 120 ms.\nInitially inspired by neuroscience, deep-learning algorithms literally bloomed\nup in the last decade such that the accuracy of machines is at present superior\nto humans for visual recognition tasks. However, these artificial networks are\nusually trained and evaluated on very specific tasks, for instance on the 1000\nseparate categories of ImageNet. In that regard, biological visual systems are\nmore flexible and efficient compared to artificial systems on generic\necological tasks. In order to deepen this comparison, we re-trained the\nstandard VGG Convolutional Neural Network (CNN) on two independent tasks which\nare ecologically relevant for humans: one task defined as detecting the\npresence of an animal and the other as detecting the presence of an artifact.\nWe show that retraining the network achieves human-like performance level which\nis reported in psychophysical tasks. We also compare the accuracy of the\ndetection on an image-by-image basis. This showed in particular that the two\nmodels perform better when combining their outputs. Indeed, animals (e.g.\nlions) tend to be less present in photographs containing artifacts (e.g.\nbuildings). These re-trained models could reproduce some unexpected behavioral\nobservations from humans psychophysics such as the robustness to rotations\n(e.g. upside-down or slanted image) or to a grayscale transformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Jeremie_J/0/1/0/all/0/1\">Jean-Nicolas J&#xe9;r&#xe9;mie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Perrinet_L/0/1/0/all/0/1\">Laurent U Perrinet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised segmentation of referring expressions. (arXiv:2205.04725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04725","description":"<p>Visual grounding localizes regions (boxes or segments) in the image\ncorresponding to given referring expressions. In this work we address image\nsegmentation from referring expressions, a problem that has so far only been\naddressed in a fully-supervised setting. A fully-supervised setup, however,\nrequires pixel-wise supervision and is hard to scale given the expense of\nmanual annotation. We therefore introduce a new task of weakly-supervised image\nsegmentation from referring expressions and propose Text grounded semantic\nSEGgmentation (TSEG) that learns segmentation masks directly from image-level\nreferring expressions without pixel-level annotations. Our transformer-based\nmethod computes patch-text similarities and guides the classification objective\nduring training with a new multi-label patch assignment mechanism. The\nresulting visual grounding model segments image regions corresponding to given\nnatural language expressions. Our approach TSEG demonstrates promising results\nfor weakly-supervised referring expression segmentation on the challenging\nPhraseCut and RefCOCO datasets. TSEG also shows competitive performance when\nevaluated in a zero-shot setting for semantic segmentation on Pascal VOC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04812","description":"<p>Robust detection of vulnerable road users is a safety critical requirement\nfor the deployment of autonomous vehicles in heterogeneous traffic. One of the\nmost complex outstanding challenges is that of partial occlusion where a target\nobject is only partially available to the sensor due to obstruction by another\nforeground object. A number of leading pedestrian detection benchmarks provide\nannotation for partial occlusion, however each benchmark varies greatly in\ntheir definition of the occurrence and severity of occlusion. Recent research\ndemonstrates that a high degree of subjectivity is used to classify occlusion\nlevel in these cases and occlusion is typically categorized into 2 to 3 broad\ncategories such as partially and heavily occluded. This can lead to inaccurate\nor inconsistent reporting of pedestrian detection model performance depending\non which benchmark is used. This research introduces a novel, objective\nbenchmark for partially occluded pedestrian detection to facilitate the\nobjective characterization of pedestrian detection models. Characterization is\ncarried out on seven popular pedestrian detection models for a range of\nocclusion levels from 0-99%. Results demonstrate that pedestrian detection\nperformance degrades, and the number of false negative detections increase as\npedestrian occlusion level increases. Of the seven popular pedestrian detection\nroutines characterized, CenterNet has the greatest overall performance,\nfollowed by SSDlite. RetinaNet has the lowest overall detection performance\nacross the range of occlusion levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1\">Ashkan Parsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Detection on Mobile: Five Implementations and Analysis. (arXiv:2205.05572v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05572","description":"<p>In many practical cases face detection on smartphones or other highly\nportable devices is a necessity. Applications include mobile face access\ncontrol systems, driver status tracking, emotion recognition, etc. Mobile\ndevices have limited processing power and should have long-enough battery life\neven with face detection application running. Thus, striking the right balance\nbetween algorithm quality and complexity is crucial. In this work we adapt 5\nalgorithms to mobile. These algorithms are based on handcrafted or\nneural-network-based features and include: Viola-Jones (Haar cascade), LBP,\nHOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on\ndifferent devices with different input image resolutions. We provide guidance,\nwhich algorithms are the best fit for mobile face access control systems and\npotentially other mobile applications. Interestingly, we note that cascaded\nalgorithms perform faster on scenes without faces, while BlazeFace is slower on\nempty scenes. Exploiting this behavior might be useful in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khabarlak_K/0/1/0/all/0/1\">Kostiantyn Khabarlak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}