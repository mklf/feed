{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Dataset for N-ary Relation Extraction of Drug Combinations. (arXiv:2205.02289v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02289","description":"<p>Combination therapies have become the standard of care for diseases such as\ncancer, tuberculosis, malaria and HIV. However, the combinatorial set of\navailable multi-drug treatments creates a challenge in identifying effective\ncombination therapies available in a situation. To assist medical professionals\nin identifying beneficial drug-combinations, we construct an expert-annotated\ndataset for extracting information about the efficacy of drug combinations from\nthe scientific literature. Beyond its practical utility, the dataset also\npresents a unique NLP challenge, as the first relation extraction dataset\nconsisting of variable-length relations. Furthermore, the relations in this\ndataset predominantly require language understanding beyond the sentence level,\nadding to the challenge of this task. We provide a promising baseline model and\nidentify clear areas for further improvement. We release our dataset, code, and\nbaseline models publicly to encourage the NLP community to participate in this\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiktinsky_A/0/1/0/all/0/1\">Aryeh Tiktinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niezni_D/0/1/0/all/0/1\">Danna Niezni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azagury_D/0/1/0/all/0/1\">Dana Meron Azagury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamay_Y/0/1/0/all/0/1\">Yosi Shamay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1\">Hillel Taub-Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02293","description":"<p>Human-translated text displays distinct features from naturally written text\nin the same language. This phenomena, known as translationese, has been argued\nto confound the machine translation (MT) evaluation. Yet, we find that existing\nwork on translationese neglects some important factors and the conclusions are\nmostly correlational but not causal. In this work, we collect CausalMT, a\ndataset where the MT training data are also labeled with the human translation\ndirections. We inspect two critical factors, the train-test direction match\n(whether the human translation directions in the training and test sets are\naligned), and data-model direction match (whether the model learns in the same\ndirection as the human translation direction in the dataset). We show that\nthese two factors have a large causal effect on the MT performance, in addition\nto the test-model direction mismatch highlighted by existing work on the impact\nof translationese. In light of our findings, we provide a set of suggestions\nfor MT training and evaluation. Our code and data are at\nhttps://github.com/EdisonNi-hku/CausalMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer. (arXiv:2205.02309v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02309","description":"<p>Recent studies show that auto-encoder based approaches successfully perform\nlanguage generation, smooth sentence interpolation, and style transfer over\nunseen attributes using unlabelled datasets in a zero-shot manner. The latent\nspace geometry of such models is organised well enough to perform on datasets\nwhere the style is \"coarse-grained\" i.e. a small fraction of words alone in a\nsentence are enough to determine the overall style label. A recent study uses a\ndiscrete token-based perturbation approach to map \"similar\" sentences\n(\"similar\" defined by low Levenshtein distance/ high word overlap) close by in\nlatent space. This definition of \"similarity\" does not look into the underlying\nnuances of the constituent words while mapping latent space neighbourhoods and\ntherefore fails to recognise sentences with different style-based semantics\nwhile mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed\nAdversarial AutoEncoders) which completes this perturbation model, by adding a\nfinely adjustable noise component on the continuous embeddings space. We\nempirically show that this (a) produces a better organised latent space that\nclusters stylistically similar sentences together, (b) performs best on a\ndiverse set of text style transfer tasks than similar denoising-inspired\nbaselines, and (c) is capable of fine-grained control of Style Transfer\nstrength. We also extend the text style transfer tasks to NLI datasets and show\nthat these more complex definitions of style are learned best by EPAAE. To the\nbest of our knowledge, extending style transfer to NLI tasks has not been\nexplored before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1\">Sharan Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Suvodip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models in the Loop: Incorporating Prompting into Weak Supervision. (arXiv:2205.02318v1 [cs.LG])","link":"http://arxiv.org/abs/2205.02318","description":"<p>We propose a new strategy for applying large pre-trained language models to\nnovel tasks when labeled training data is limited. Rather than apply the model\nin a typical zero-shot or few-shot fashion, we treat the model as the basis for\nlabeling functions in a weak supervision framework. To create a classifier, we\nfirst prompt the model to answer multiple distinct queries about an example and\ndefine how the possible responses should be mapped to votes for labels and\nabstentions. We then denoise these noisy label sources using the Snorkel system\nand train an end classifier with the resulting training data. Our experimental\nevaluation shows that prompting large language models within a weak supervision\nframework can provide significant gains in accuracy. On the WRENCH weak\nsupervision benchmark, this approach can significantly improve over zero-shot\nperformance, an average 19.5% reduction in errors. We also find that this\napproach produces classifiers with comparable or superior accuracy to those\ntrained from hand-engineered rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_R/0/1/0/all/0/1\">Ryan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason A. Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hancock_B/0/1/0/all/0/1\">Braden Hancock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation of Russian Language Models with Reduction of Vocabulary. (arXiv:2205.02340v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02340","description":"<p>Today, transformer language models serve as a core component for majority of\nnatural language processing tasks. Industrial application of such models\nrequires minimization of computation time and memory footprint. Knowledge\ndistillation is one of approaches to address this goal. Existing methods in\nthis field are mainly focused on reducing the number of layers or dimension of\nembeddings/hidden representations. Alternative option is to reduce the number\nof tokens in vocabulary and therefore the embeddings matrix of the student\nmodel. The main problem with vocabulary minimization is mismatch between input\nsequences and output class distributions of a teacher and a student models. As\na result, it is impossible to directly apply KL-based knowledge distillation.\nWe propose two simple yet effective alignment techniques to make knowledge\ndistillation to the students with reduced vocabulary. Evaluation of distilled\nmodels on a number of common benchmarks for Russian such as Russian SuperGLUE,\nSberQuAD, RuSentiment, ParaPhaser, Collection-3 demonstrated that our\ntechniques allow to achieve compression from $17\\times$ to $49\\times$, while\nmaintaining quality of $1.7\\times$ compressed student with the full-sized\nvocabulary, but reduced number of Transformer layers only. We make our code and\ndistilled models available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_A/0/1/0/all/0/1\">Alina Kolesnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1\">Yuri Kuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konovalov_V/0/1/0/all/0/1\">Vasily Konovalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02355","description":"<p>Pre-trained language models have contributed significantly to relation\nextraction by demonstrating remarkable few-shot learning abilities. However,\nprompt tuning methods for relation extraction may still fail to generalize to\nthose rare or hard patterns. Note that the previous parametric learning\nparadigm can be viewed as memorization regarding training data as a book and\ninference as the close-book test. Those long-tailed or hard patterns can hardly\nbe memorized in parameters given few-shot instances. To this end, we regard RE\nas an open-book examination and propose a new semiparametric paradigm of\nretrieval-enhanced prompt tuning for relation extraction. We construct an\nopen-book datastore for retrieval regarding prompt-based instance\nrepresentations and corresponding relation labels as memorized key-value pairs.\nDuring inference, the model can infer relations by linearly interpolating the\nbase output of PLM with the non-parametric nearest neighbor distribution over\nthe datastore. In this way, our model not only infers relation through\nknowledge stored in the weights during training but also assists\ndecision-making by unwinding and querying examples in the open-book datastore.\nExtensive experiments on benchmark datasets show that our method can achieve\nstate-of-the-art in both standard supervised and few-shot settings. Code are\navailable in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02357","description":"<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02364","description":"<p>This research developed a Kencorpus Swahili Question Answering Dataset\nKenSwQuAD from raw data of Swahili language, which is a low resource language\npredominantly spoken in Eastern African and also has speakers in other parts of\nthe world. Question Answering datasets are important for machine comprehension\nof natural language processing tasks such as internet search and dialog\nsystems. However, before such machine learning systems can perform these tasks,\nthey need training data such as the gold standard Question Answering (QA) set\nthat is developed in this research. The research engaged annotators to\nformulate question answer pairs from Swahili texts that had been collected by\nthe Kencorpus project, a Kenyan languages corpus that collected data from three\nKenyan languages. The total Swahili data collection had 2,585 texts, out of\nwhich we annotated 1,445 story texts with at least 5 QA pairs each, resulting\ninto a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the\nannotated texts was subjected to re-evaluation by different annotators who\nconfirmed that the QA pairs were all correctly annotated. A proof of concept on\napplying the set to machine learning on the question answering task confirmed\nthat the dataset can be used for such practical tasks. The research therefore\ndeveloped KenSwQuAD, a question-answer dataset for Swahili that is useful to\nthe natural language processing community who need training and gold standard\nsets for their machine learning applications. The research also contributed to\nthe resourcing of the Swahili language which is important for communication\naround the globe. Updating this set and providing similar sets for other low\nresource languages is an important research area that is worthy of further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1\">Barack Wanjawa</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1\">Lilian Wanzare</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1\">Florence Indede</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1\">Owen McOnyango</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1\">Lawrence Muchemi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1\">Edward Ombui</a> (3) ((1) University of Nairobi Kenya, (2) Maseno University Kenya (3) Africa Nazarene University Kenya)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02370","description":"<p>The recent increase in the volume of online meetings necessitates automated\ntools for managing and organizing the material, especially when an attendee has\nmissed the discussion and needs assistance in quickly exploring it. In this\nwork, we propose a novel end-to-end framework for generating interactive\nquestionnaires for preference-based meeting exploration. As a result, users are\nsupplied with a list of suggested questions reflecting their preferences. Since\nthe task is new, we introduce an automatic evaluation strategy. Namely, it\nmeasures how much the generated questions via questionnaire are answerable to\nensure factual correctness and covers the source meeting for the depth of\npossible exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadvand_A/0/1/0/all/0/1\">Ali Ahmadvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021. (arXiv:2205.02388v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02388","description":"<p>Human intelligence has the remarkable ability to quickly adapt to new tasks\nand environments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research in\nthis direction, we propose \\emph{IGLU: Interactive Grounded Language\nUnderstanding in a Collaborative Environment}.\n</p>\n<p>The primary goal of the competition is to approach the problem of how to\nbuild interactive agents that learn to solve a task while provided with\ngrounded natural language instructions in a collaborative environment.\nUnderstanding the complexity of the challenge, we split it into sub-tasks to\nmake it feasible for participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9; Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdrazakov_L/0/1/0/all/0/1\">Linar Abdrazakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churin_I/0/1/0/all/0/1\">Igor Churin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manggala_P/0/1/0/all/0/1\">Putra Manggala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naszadi_K/0/1/0/all/0/1\">Kata Naszadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meer_M/0/1/0/all/0/1\">Michiel van der Meer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Conversational Agents against Imperceptible Toxicity Triggers. (arXiv:2205.02392v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02392","description":"<p>Warning: this paper contains content that maybe offensive or upsetting.\nRecent research in Natural Language Processing (NLP) has advanced the\ndevelopment of various toxicity detection models with the intention of\nidentifying and mitigating toxic language from existing systems. Despite the\nabundance of research in this area, less attention has been given to\nadversarial attacks that force the system to generate toxic language and the\ndefense against them. Existing work to generate such attacks is either based on\nhuman-generated attacks which is costly and not scalable or, in case of\nautomatic attacks, the attack vector does not conform to human-like language,\nwhich can be detected using a language model loss. In this work, we propose\nattacks against conversational agents that are imperceptible, i.e., they fit\nthe conversation in terms of coherency, relevancy, and fluency, while they are\neffective and scalable, i.e., they can automatically trigger the system into\ngenerating toxic language. We then propose a defense mechanism against such\nattacks which not only mitigates the attack but also attempts to maintain the\nconversational flow. Through automatic and human evaluations, we show that our\ndefense is effective at avoiding toxic language generation even against\nimperceptible toxicity triggers while the generated language fits the\nconversation in terms of coherency and relevancy. Lastly, we establish the\ngeneralizability of such a defense mechanism on language generation models\nbeyond conversational agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimising Equal Opportunity Fairness in Model Training. (arXiv:2205.02393v1 [cs.LG])","link":"http://arxiv.org/abs/2205.02393","description":"<p>Real-world datasets often encode stereotypes and societal biases. Such biases\ncan be implicitly captured by trained models, leading to biased predictions and\nexacerbating existing societal preconceptions. Existing debiasing methods, such\nas adversarial training and removing protected information from\nrepresentations, have been shown to reduce bias. However, a disconnect between\nfairness criteria and training objectives makes it difficult to reason\ntheoretically about the effectiveness of different techniques. In this work, we\npropose two novel training objectives which directly optimise for the\nwidely-used criterion of {\\it equal opportunity}, and show that they are\neffective in reducing bias while maintaining high performance over two\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_A/0/1/0/all/0/1\">Aili Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dangling-Aware Entity Alignment with Mixed High-Order Proximities. (arXiv:2205.02406v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02406","description":"<p>We study dangling-aware entity alignment in knowledge graphs (KGs), which is\nan underexplored but important problem. As different KGs are naturally\nconstructed by different sets of entities, a KG commonly contains some dangling\nentities that cannot find counterparts in other KGs. Therefore, dangling-aware\nentity alignment is more realistic than the conventional entity alignment where\nprior studies simply ignore dangling entities. We propose a framework using\nmixed high-order proximities on dangling-aware entity alignment. Our framework\nutilizes both the local high-order proximity in a nearest neighbor subgraph and\nthe global high-order proximity in an embedding space for both dangling\ndetection and entity alignment. Extensive experiments with two evaluation\nsettings shows that our framework more precisely detects dangling entities, and\nbetter aligns matchable entities. Further investigations demonstrate that our\nframework can mitigate the hubness problem on dangling-aware entity alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juncheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiaokui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Representation Learning in Visually-Rich Documents. (arXiv:2205.02411v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02411","description":"<p>Relational understanding is critical for a number of visually-rich documents\n(VRDs) understanding tasks. Through multi-modal pre-training, recent studies\nprovide comprehensive contextual representations and exploit them as prior\nknowledge for downstream tasks. In spite of their impressive results, we\nobserve that the widespread relational hints (e.g., relation of key/value\nfields on receipts) built upon contextual knowledge are not excavated yet. To\nmitigate this gap, we propose DocReL, a Document Relational Representation\nLearning framework. The major challenge of DocReL roots in the variety of\nrelations. From the simplest pairwise relation to the complex global structure,\nit is infeasible to conduct supervised training due to the definition of\nrelation varies and even conflicts in different tasks. To deal with the\nunpredictable definition of relations, we propose a novel contrastive learning\ntask named Relational Consistency Modeling (RCM), which harnesses the fact that\nexisting relations should be consistent in differently augmented positive\nviews. RCM provides relational representations which are more compatible to the\nurgent need of downstream tasks, even without any knowledge about the exact\ndefinition of relation. DocReL achieves better performance on a wide variety of\nVRD relational understanding tasks, including table structure recognition, key\ninformation extraction and reading order detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Deqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Contrastive Learning for Speech Translation. (arXiv:2205.02444v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02444","description":"<p>How can we learn unified representations for spoken utterances and their\nwritten text? Learning similar representations for semantically similar speech\nand text is important for speech translation. To this end, we propose ConST, a\ncross-modal contrastive learning method for end-to-end speech-to-text\ntranslation. We evaluate ConST and a variety of previous baselines on a popular\nbenchmark MuST-C. Experiments show that the proposed ConST consistently\noutperforms the previous methods on, and achieves an average BLEU of 29.4. The\nanalysis further verifies that ConST indeed closes the representation gap of\ndifferent modalities -- its learned representation improves the accuracy of\ncross-modal speech-text retrieval from 4% to 88%. Code and models are available\nat https://github.com/ReneeYe/ConST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assistive Recipe Editing through Critiquing. (arXiv:2205.02454v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02454","description":"<p>There has recently been growing interest in the automatic generation of\ncooking recipes that satisfy some form of dietary restrictions, thanks in part\nto the availability of online recipe data. Prior studies have used pre-trained\nlanguage models, or relied on small paired recipe data (e.g., a recipe paired\nwith a similar one that satisfies a dietary constraint). However, pre-trained\nlanguage models generate inconsistent or incoherent recipes, and paired\ndatasets are not available at scale. We address these deficiencies with\nRecipeCrit, a hierarchical denoising auto-encoder that edits recipes given\ningredient-level critiques. The model is trained for recipe completion to learn\nsemantic relationships within recipes. Our work's main innovation is our\nunsupervised critiquing module that allows users to edit recipes by interacting\nwith the predicted ingredients; the system iteratively rewrites recipes to\nsatisfy users' feedback. Experiments on the Recipe1M recipe dataset show that\nour model can more effectively edit recipes compared to strong\nlanguage-modeling baselines, creating recipes that satisfy user constraints and\nare more correct, serendipitous, coherent, and relevant as measured by human\njudges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COGMEN: COntextualized GNN based Multimodal Emotion recognitioN. (arXiv:2205.02455v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02455","description":"<p>Emotions are an inherent part of human interactions, and consequently, it is\nimperative to develop AI systems that understand and recognize human emotions.\nDuring a conversation involving various people, a person's emotions are\ninfluenced by the other speaker's utterances and their own emotional state over\nthe utterances. In this paper, we propose COntextualized Graph Neural Network\nbased Multimodal Emotion recognitioN (COGMEN) system that leverages local\ninformation (i.e., inter/intra dependency between speakers) and global\ninformation (context). The proposed model uses Graph Neural Network (GNN) based\narchitecture to model the complex dependencies (local and global information)\nin a conversation. Our model gives state-of-the-art (SOTA) results on IEMOCAP\nand MOSEI datasets, and detailed ablation experiments show the importance of\nmodeling information at both levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Abhinav Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1\">Ashwani Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Atin Vikram Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog. (arXiv:2205.02471v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02471","description":"<p>A typical end-to-end task-oriented dialog system transfers context into\ndialog state, and upon which generates a response, which usually faces the\nproblem of error propagation from both previously generated inaccurate dialog\nstates and responses, especially in low-resource scenarios. To alleviate these\nissues, we propose BORT, a back and denoising reconstruction approach for\nend-to-end task-oriented dialog system. Squarely, to improve the accuracy of\ndialog states, back reconstruction is used to reconstruct the original input\ncontext from the generated dialog states since inaccurate dialog states cannot\nrecover the corresponding input context. To enhance the denoising capability of\nthe model to reduce the impact of error propagation, denoising reconstruction\nis used to reconstruct the corrupted dialog state and response. Extensive\nexperiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of\nBORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot\ndomain and low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Recognition in the Wild. (arXiv:2205.02475v1 [cs.SD])","link":"http://arxiv.org/abs/2205.02475","description":"<p>In this paper, we propose a pipeline to find the number of speakers, as well\nas audios belonging to each of these now identified speakers in a source of\naudio data where number of speakers or speaker labels are not known a priori.\nWe used this approach as a part of our Data Preparation pipeline for Speech\nRecognition in Indic Languages\n(https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation). To\nunderstand and evaluate the accuracy of our proposed pipeline, we introduce two\nmetrics: Cluster Purity, and Cluster Uniqueness. Cluster Purity quantifies how\n\"pure\" a cluster is. Cluster Uniqueness, on the other hand, quantifies what\npercentage of clusters belong only to a single dominant speaker. We discuss\nmore on these metrics in section \\ref{sec:metrics}. Since we develop this\nutility to aid us in identifying data based on speaker IDs before training an\nAutomatic Speech Recognition (ASR) model, and since most of this data takes\nconsiderable effort to scrape, we also conclude that 98\\% of data gets mapped\nto the top 80\\% of clusters (computed by removing any clusters with less than a\nfixed number of utterances -- we do this to get rid of some very small clusters\nand use this threshold as 30), in the test set chosen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework. (arXiv:2205.02490v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02490","description":"<p>Recent work for extracting relations from texts has achieved excellent\nperformance. However, most existing methods pay less attention to the\nefficiency, making it still challenging to quickly extract relations from\nmassive or streaming text data in realistic scenarios. The main efficiency\nbottleneck is that these methods use a Transformer-based pre-trained language\nmodel for encoding, which heavily affects the training speed and inference\nspeed. To address this issue, we propose a fast relation extraction model\n(FastRE) based on convolutional encoder and improved cascade binary tagging\nframework. Compared to previous work, FastRE employs several innovations to\nimprove efficiency while also keeping promising performance. Concretely, FastRE\nadopts a novel convolutional encoder architecture combined with dilated\nconvolution, gated unit and residual connection, which significantly reduces\nthe computation cost of training and inference, while maintaining the\nsatisfactory performance. Moreover, to improve the cascade binary tagging\nframework, FastRE first introduces a type-relation mapping mechanism to\naccelerate tagging efficiency and alleviate relation redundancy, and then\nutilizes a position-dependent adaptive thresholding strategy to obtain higher\ntagging accuracy and better model generalization. Experimental results\ndemonstrate that FastRE is well balanced between efficiency and performance,\nand achieves 3-10x training speed, 7-15x inference speed faster, and 1/100\nparameters compared to the state-of-the-art models, while the performance is\nstill competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guozheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiafeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiqing Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration. (arXiv:2205.02517v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02517","description":"<p>The cross-entropy objective has proved to be an all-purpose training\nobjective for autoregressive language models (LMs). However, without\nconsidering the penalization of problematic tokens, LMs trained using\ncross-entropy exhibit text degeneration. To address this, unlikelihood training\nhas been proposed to force unlikely tokens to be assigned a low probability by\na LM. But unlikelihood does not consider the relationship between the label\ntokens and the unlikely token candidates, thus showing marginal improvements in\ndegeneration. We propose a new contrastive token learning objective that\ninherits the advantages of cross-entropy and unlikelihood training and avoids\ntheir limitations. The key idea is to force a LM to generate high probabilities\nfor label tokens at each step while low probabilities of negative candidates.\nComprehensive experiments on language modeling and open-domain dialogue\ngeneration tasks show that the proposed contrastive token objective yields less\nrepetitive texts, with a higher generation quality than unlikelihood training,\nachieving the new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shaojie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theories of \"Gender\" in NLP Bias Research. (arXiv:2205.02526v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02526","description":"<p>The rise of concern around Natural Language Processing (NLP) technologies\ncontaining and perpetuating social biases has led to a rich and rapidly growing\narea of research. Gender bias is one of the central biases being analyzed, but\nto date there is no comprehensive analysis of how \"gender\" is theorized in the\nfield. We survey nearly 200 articles concerning gender bias in NLP to discover\nhow the field conceptualizes gender both explicitly (e.g. through definitions\nof terms) and implicitly (e.g. through how gender is operationalized in\npractice). In order to get a better idea of emerging trajectories of thought,\nwe split these articles into two sections by time.\n</p>\n<p>We find that the majority of the articles do not make their theorization of\ngender explicit, even if they clearly define \"bias.\" Almost none use a model of\ngender that is intersectional or inclusive of nonbinary genders; and many\nconflate sex characteristics, social gender, and linguistic gender in ways that\ndisregard the existence and experience of trans, nonbinary, and intersex\npeople. There is an increase between the two time-sections in statements\nacknowledging that gender is a complicated reality, however, very few articles\nmanage to put this acknowledgment into practice. In addition to analyzing these\nfindings, we provide specific recommendations to facilitate interdisciplinary\nwork, and to incorporate theory and methodology from Gender Studies. Our hope\nis that this will produce more inclusive gender bias research in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devinney_H/0/1/0/all/0/1\">Hannah Devinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjorklund_J/0/1/0/all/0/1\">Jenny Bj&#xf6;rklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjorklund_H/0/1/0/all/0/1\">Henrik Bj&#xf6;rklund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing the Welsh Text Summarisation Dataset and Baseline Systems. (arXiv:2205.02545v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02545","description":"<p>Welsh is an official language in Wales and is spoken by an estimated 884,300\npeople (29.2% of the population of Wales). Despite this status and estimated\nincrease in speaker numbers since the last (2011) census, Welsh remains a\nminority language undergoing revitalization and promotion by Welsh Government\nand relevant stakeholders. As part of the effort to increase the availability\nof Welsh digital technology, this paper introduces the first Welsh\nsummarisation dataset, which we provide freely for research purposes to help\nadvance the work on Welsh text summarization. The dataset was created by Welsh\nspeakers by manually summarising Welsh Wikipedia articles. In addition, the\npaper discusses the implementation and evaluation of different summarisation\nsystems for Welsh. The summarization systems and results will serve as\nbenchmarks for the development of summarises in other minority language\ncontexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Haj_M/0/1/0/all/0/1\">Mahmoud El-Haj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">Jonathan Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_D/0/1/0/all/0/1\">Dawn Knight</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Driven Research of Medical Note Generation Software. (arXiv:2205.02549v1 [cs.HC])","link":"http://arxiv.org/abs/2205.02549","description":"<p>A growing body of work uses Natural Language Processing (NLP) methods to\nautomatically generate medical notes from audio recordings of doctor-patient\nconsultations. However, there are very few studies on how such systems could be\nused in clinical practice, how clinicians would adjust to using them, or how\nsystem design should be influenced by such considerations. In this paper, we\npresent three rounds of user studies, carried out in the context of developing\na medical note generation system. We present, analyse and discuss the\nparticipating clinicians' impressions and views of how the system ought to be\nadapted to be of value to them. Next, we describe a three-week test run of the\nsystem in a live telehealth clinical practice, major findings from which\ninclude (i) the emergence of five different note-taking behaviours; (ii) the\nimportance of the system generating notes in real time during the consultation;\nand (iii) the identification of a number of clinical use cases that could prove\nchallenging for automatic note generation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knoll_T/0/1/0/all/0/1\">Tom Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1\">Rachel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_C/0/1/0/all/0/1\">Claudia Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Mark Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perstl_C/0/1/0/all/0/1\">Christian Perstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking. (arXiv:2205.02550v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02550","description":"<p>Dialogue state tracking (DST) aims to predict the current dialogue state\ngiven the dialogue history. Existing methods generally exploit the utterances\nof all dialogue turns to assign value for each slot. This could lead to\nsuboptimal results due to the information introduced from irrelevant utterances\nin the dialogue history, which may be useless and can even cause confusion. To\naddress this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach.\nIt first explicitly aligns each slot with its most relevant utterance, then\nfurther predicts the corresponding value based on this aligned utterance\ninstead of all dialogue utterances. Furthermore, we design a slot ranking\nauxiliary task to learn the temporal correlation among slots which could\nfacilitate the alignment. Comprehensive experiments are conducted on\nmulti-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1,\nand MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art\nresults on these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Size Does Not Fit All: The Case for Personalised Word Complexity Models. (arXiv:2205.02564v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02564","description":"<p>Complex Word Identification (CWI) aims to detect words within a text that a\nreader may find difficult to understand. It has been shown that CWI systems can\nimprove text simplification, readability prediction and vocabulary acquisition\nmodelling. However, the difficulty of a word is a highly idiosyncratic notion\nthat depends on a reader's first language, proficiency and reading experience.\nIn this paper, we show that personal models are best when predicting word\ncomplexity for individual readers. We use a novel active learning framework\nthat allows models to be tailored to individuals and release a dataset of\ncomplexity annotations and models as a benchmark for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gooding_S/0/1/0/all/0/1\">Sian Gooding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tragut_M/0/1/0/all/0/1\">Manuel Tragut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation. (arXiv:2205.02570v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02570","description":"<p>Open-domain conversational systems are assumed to generate equally good\nresponses on multiple domains. Previous work achieved good performance on the\nsingle corpus, but training and evaluating on multiple corpora from different\ndomains are less studied. This paper explores methods of generating relevant\nresponses for each of multiple multi-domain corpora. We first examine\ninterleaved learning which intermingles multiple corpora as the baseline. We\nthen investigate two multi-domain learning methods, labeled learning and\nmulti-task labeled learning, which encode each corpus through a unique corpus\nembedding. Furthermore, we propose Domain-specific Frequency (DF), a novel\nword-level importance weight that measures the relative importance of a word\nfor a specific corpus compared to other corpora. Based on DF, we propose\nweighted learning, a method that integrates DF to the loss function. We also\nadopt DF as a new evaluation metric. Extensive experiments show that our\nmethods gain significant improvements on both automatic and human evaluation.\nWe share our code and data for reproducibility\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yujie Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinglun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlaug_N/0/1/0/all/0/1\">Nils Barlaug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulla_J/0/1/0/all/0/1\">Jon Atle Gulla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation. (arXiv:2205.02593v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02593","description":"<p>Knowing the reasoning chains from knowledge to the predicted answers can help\nconstruct an explainable question answering (QA) system. Advances on QA\nexplanation propose to explain the answers with entailment trees composed of\nmultiple entailment steps. While current work proposes to generate entailment\ntrees with end-to-end generative models, the steps in the generated trees are\nnot constrained and could be unreliable. In this paper, we propose METGEN, a\nModule-based Entailment Tree GENeration framework that has multiple modules and\na reasoning controller. Given a question and several supporting knowledge,\nMETGEN can iteratively generate the entailment tree by conducting single-step\nentailment with separate modules and selecting the reasoning flow with the\ncontroller. As each module is guided to perform a specific type of entailment\nreasoning, the steps generated by METGEN are more reliable and valid.\nExperiment results on the standard benchmark show that METGEN can outperform\nprevious state-of-the-art models with only 9% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xintong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims. (arXiv:2205.02596v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02596","description":"<p>We present a comprehensive work on automated veracity assessment from dataset\ncreation to developing novel methods based on Natural Language Inference (NLI),\nfocusing on misinformation related to the COVID-19 pandemic. We first describe\nthe construction of the novel PANACEA dataset consisting of heterogeneous\nclaims on COVID-19 and their respective information sources. The dataset\nconstruction includes work on retrieval techniques and similarity measurements\nto ensure a unique set of claims. We then propose novel techniques for\nautomated veracity assessment based on Natural Language Inference including\ngraph convolutional networks and attention based approaches. We have carried\nout experiments on evidence retrieval and veracity assessment on the dataset\nusing the proposed techniques and found them competitive with SOTA methods, and\nprovided a detailed discussion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02613","description":"<p>Hierarchical text classification aims to leverage label hierarchy in\nmulti-label text classification. Existing methods encode label hierarchy in a\nglobal view, where label hierarchy is treated as the static hierarchical\nstructure containing all labels. Since global hierarchy is static and\nirrelevant to text samples, it makes these methods hard to exploit hierarchical\ninformation. Contrary to global hierarchy, local hierarchy as the structured\ntarget labels hierarchy corresponding to each text sample is dynamic and\nrelevant to text samples, which is ignored in previous methods. To exploit\nglobal and local hierarchies, we propose Hierarchy-guided BERT with Global and\nLocal hierarchies (HBGL), which utilizes the large-scale parameters and prior\nlanguage knowledge of BERT to model both global and local hierarchies.\nMoreover, HBGL avoids the intentional fusion of semantic and hierarchical\nmodules by directly modeling semantic and hierarchical information with BERT.\nCompared with the state-of-the-art method HGCLR, our method achieves\nsignificant improvement on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Leilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qinghong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WDV: A Broad Data Verbalisation Dataset Built from Wikidata. (arXiv:2205.02627v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02627","description":"<p>Data verbalisation is a task of great importance in the current field of\nnatural language processing, as there is great benefit in the transformation of\nour abundant structured and semi-structured data into human-readable formats.\nVerbalising Knowledge Graph (KG) data focuses on converting interconnected\ntriple-based claims, formed of subject, predicate, and object, into text.\nAlthough KG verbalisation datasets exist for some KGs, there are still gaps in\ntheir fitness for use in many scenarios. This is especially true for Wikidata,\nwhere available datasets either loosely couple claim sets with textual\ninformation or heavily focus on predicates around biographies, cities, and\ncountries. To address these gaps, we propose WDV, a large KG claim\nverbalisation dataset built from Wikidata, with a tight coupling between\ntriples and text, covering a wide variety of entities and predicates. We also\nevaluate the quality of our verbalisations through a reusable workflow for\nmeasuring human-centred fluency and adequacy scores. Our data and code are\nopenly available in the hopes of furthering research towards KG verbalisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amaral_G/0/1/0/all/0/1\">Gabriel Amaral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_O/0/1/0/all/0/1\">Odinaldo Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient yet Competitive Speech Translation: FBK@IWSLT2022. (arXiv:2205.02629v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02629","description":"<p>The primary goal of this FBK's systems submission to the IWSLT 2022 offline\nand simultaneous speech translation tasks is to reduce model training costs\nwithout sacrificing translation quality. As such, we first question the need of\nASR pre-training, showing that it is not essential to achieve competitive\nresults. Second, we focus on data filtering, showing that a simple method that\nlooks at the ratio between source and target characters yields a quality\nimprovement of 1 BLEU. Third, we compare different methods to reduce the\ndetrimental effect of the audio segmentation mismatch between training data\nmanually segmented at sentence level and inference data that is automatically\nsegmented. Towards the same goal of training cost reduction, we participate in\nthe simultaneous task with the same model trained for offline ST. The\neffectiveness of our lightweight training strategy is shown by the high score\nobtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in\nhigh-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test\nset over last year's winning system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1\">Dennis Fucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02655","description":"<p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour. (arXiv:2205.02684v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02684","description":"<p>Forced labour is the most common type of modern slavery, and it is\nincreasingly gaining the attention of the research and social community. Recent\nstudies suggest that artificial intelligence (AI) holds immense potential for\naugmenting anti-slavery action. However, AI tools need to be developed\ntransparently in cooperation with different stakeholders. Such tools are\ncontingent on the availability and access to domain-specific data, which are\nscarce due to the near-invisible nature of forced labour. To the best of our\nknowledge, this paper presents the first openly accessible English corpus\nannotated for multi-class and multi-label forced labour detection. The corpus\nconsists of 989 news articles retrieved from specialised data sources and\nannotated according to risk indicators defined by the International Labour\nOrganization (ILO). Each news article was annotated for two aspects: (1)\nindicators of forced labour as classification labels and (2) snippets of the\ntext that justify labelling decisions. We hope that our data set can help\npromote research on explainability for multi-class and multi-label text\nclassification. In this work, we explain our process for collecting the data\nunderpinning the proposed corpus, describe our annotation guidelines and\npresent some statistical analysis of its content. Finally, we summarise the\nresults of baseline experiments based on different variants of the\nBidirectional Encoder Representation from Transformer (BERT) model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guzman_E/0/1/0/all/0/1\">Erick Mendez Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1\">Riza Batista-Navarro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Language Variation Acoustically with Few Resources. (arXiv:2205.02694v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02694","description":"<p>Deep acoustic models represent linguistic information based on massive\namounts of data. Unfortunately, for regional languages and dialects such\nresources are mostly not available. However, deep acoustic models might have\nlearned linguistic information that transfers to low-resource languages. In\nthis study, we evaluate whether this is the case through the task of\ndistinguishing low-resource (Dutch) regional varieties. By extracting\nembeddings from the hidden layers of various wav2vec 2.0 models (including new\nmodels which are pre-trained and/or fine-tuned on Dutch) and using dynamic time\nwarping, we compute pairwise pronunciation differences averaged over 10 words\nfor over 100 individual dialects from four (regional) languages. We then\ncluster the resulting difference matrix in four groups and compare these to a\ngold standard, and a partitioning on the basis of comparing phonetic\ntranscriptions. Our results show that acoustic models outperform the\n(traditional) transcription-based approach without requiring phonetic\ntranscriptions, with the best performance achieved by the multilingual XLSR-53\nmodel fine-tuned on Dutch. On the basis of only six seconds of speech, the\nresulting clustering closely matches the gold standard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit N-grams Induced by Recurrence. (arXiv:2205.02724v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02724","description":"<p>Although self-attention based models such as Transformers have achieved\nremarkable successes on natural language processing (NLP) tasks, recent studies\nreveal that they have limitations on modeling sequential transformations (Hahn,\n2020), which may prompt re-examinations of recurrent neural networks (RNNs)\nthat demonstrated impressive results on handling sequential data. Despite many\nprior attempts to interpret RNNs, their internal mechanisms have not been fully\nunderstood, and the question on how exactly they capture sequential features\nremains largely unclear. In this work, we present a study that shows there\nactually exist some explainable components that reside within the hidden\nstates, which are reminiscent of the classical n-grams features. We evaluated\nsuch extracted explainable features from trained RNNs on downstream sentiment\nanalysis tasks and found they could be used to model interesting linguistic\nphenomena such as negation and intensification. Furthermore, we examined the\nefficacy of using such n-gram components alone as encoders on tasks such as\nsentiment analysis and language modeling, revealing they could be playing\nimportant roles in contributing to the overall performance of RNNs. We hope our\nfindings could add interpretability to RNN architectures, and also provide\ninspirations for proposing new architectures for sequential data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaobing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic Terms. (arXiv:2205.02728v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02728","description":"<p>Euphemisms have not received much attention in natural language processing,\ndespite being an important element of polite and figurative language.\nEuphemisms prove to be a difficult topic, not only because they are subject to\nlanguage change, but also because humans may not agree on what is a euphemism\nand what is not. Nevertheless, the first step to tackling the issue is to\ncollect and analyze examples of euphemisms. We present a corpus of potentially\neuphemistic terms (PETs) along with example texts from the GloWbE corpus.\nAdditionally, we present a subcorpus of texts where these PETs are not being\nused euphemistically, which may be useful for future applications. We also\ndiscuss the results of multiple analyses run on the corpus. Firstly, we find\nthat sentiment analysis on the euphemistic texts supports that PETs generally\ndecrease negative and offensive sentiment. Secondly, we observe cases of\ndisagreement in an annotation task, where humans are asked to label PETs as\neuphemistic or not in a subset of our corpus text examples. We attribute the\ndisagreement to a variety of potential reasons, including if the PET was a\ncommonly accepted term (CAT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavidia_M/0/1/0/all/0/1\">Martha Gavidia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversifying Neural Dialogue Generation via Negative Distillation. (arXiv:2205.02795v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02795","description":"<p>Generative dialogue models suffer badly from the generic response problem,\nlimiting their applications to a few toy scenarios. Recently, an interesting\napproach, namely negative training, has been proposed to alleviate this problem\nby reminding the model not to generate high-frequency responses during\ntraining. However, its performance is hindered by two issues, ignoring\nlow-frequency but generic responses and bringing low-frequency but meaningless\nresponses. In this paper, we propose a novel negative training paradigm, called\nnegative distillation, to keep the model away from the undesirable generic\nresponses while avoiding the above problems. First, we introduce a negative\nteacher model that can produce query-wise generic responses, and then the\nstudent model is required to maximize the distance with multi-level negative\nknowledge. Empirical results show that our method outperforms previous negative\ntraining methods significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shaoxiong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Cloze By Date: What LMs Know About Unseen Entities. (arXiv:2205.02832v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02832","description":"<p>Language models (LMs) are typically trained once on a large-scale corpus and\nused for years without being updated. However, in a dynamic world, new entities\nconstantly arise. We propose a framework to analyze what LMs can infer about\nnew entities that did not exist when the LMs were pretrained. We derive a\ndataset of entities indexed by their origination date and paired with their\nEnglish Wikipedia articles, from which we can find sentences about each entity.\nWe evaluate LMs' perplexity on masked spans within these sentences. We show\nthat models more informed about the entities, such as those with access to a\ntextual definition of them, achieve lower perplexity on this benchmark. Our\nexperimental results demonstrate that making inferences about new entities\nremains difficult for LMs. Given its wide coverage on entity knowledge and\ntemporal indexing, our dataset can be used to evaluate LMs and techniques\ndesigned to modify or extend their knowledge. Our automatic data collection\npipeline can be easily used to continually update our benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling. (arXiv:2005.06377v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.06377","description":"<p>Canonical automatic summary evaluation metrics, such as ROUGE, focus on\nlexical similarity which cannot well capture semantics nor linguistic quality\nand require a reference summary which is costly to obtain. Recently, there have\nbeen a growing number of efforts to alleviate either or both of the two\ndrawbacks. In this paper, we present a proof-of-concept study to a weakly\nsupervised summary evaluation approach without the presence of reference\nsummaries. Massive data in existing summarization datasets are transformed for\ntraining by pairing documents with corrupted reference summaries. In\ncross-domain tests, our strategy outperforms baselines with promising\nimprovements, and show a great advantage in gauging linguistic qualities over\nall metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1\">Forrest Sheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hebi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Ge Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Youbiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Emotion Detection. (arXiv:2106.06017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06017","description":"<p>Emotion detection can provide us with a window into understanding human\nbehavior. Due to the complex dynamics of human emotions, however, constructing\nannotated datasets to train automated models can be expensive. Thus, we explore\nthe efficacy of cross-lingual approaches that would use data from a source\nlanguage to build models for emotion detection in a target language. We compare\nthree approaches, namely: i) using inherently multilingual models; ii)\ntranslating training data into the target language; and iii) using an\nautomatically tagged parallel corpus. In our study, we consider English as the\nsource language with Arabic and Spanish as target languages. We study the\neffectiveness of different classification models such as BERT and SVMs trained\nwith different features. Our BERT-based monolingual models that are trained on\ntarget language data surpass state-of-the-art (SOTA) by 4% and 5% absolute\nJaccard score for Arabic and Spanish respectively. Next, we show that using\ncross-lingual approaches with English data alone, we can achieve more than 90%\nand 80% relative effectiveness of the Arabic and Spanish BERT models\nrespectively. Lastly, we use LIME to analyze the challenges of training\ncross-lingual models for different language pairs\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy (CE) loss, which encourages an\n\\emph{exact} token-by-token match between a target sequence with a generated\nsequence. Such training objective is sub-optimal when the target sequence is\nnot perfect, e.g., when the target sequence is corrupted with noises, or when\nonly weak sequence supervision is available. To address the challenge, we\npropose a novel Edit-Invariant Sequence Loss (EISL), which computes the\nmatching loss of a target n-gram with all n-grams in the generated sequence.\nEISL is designed to be robust to various noises and edits in the target\nsequences. Moreover, the EISL computation is essentially an approximate\nconvolution operation with target n-grams as kernels, which is easy to\nimplement and efficient to compute with existing libraries. To demonstrate the\neffectiveness of EISL, we conduct experiments on a wide range of tasks,\nincluding machine translation with noisy target sequences, unsupervised text\nstyle transfer with only weak training signals, and non-autoregressive\ngeneration with non-predefined generation order. Experimental results show our\nmethod significantly outperforms the common CE loss and other strong baselines\non all the tasks. EISL has a simple API which can be used as a drop-in\nreplacement of the CE loss: https://anonymous.4open.science/r/EISLLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuSiQue: Multihop Questions via Single-hop Question Composition. (arXiv:2108.00573v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00573","description":"<p>Multihop reasoning remains an elusive goal as existing multihop benchmarks\nare known to be largely solvable via shortcuts. Can we create a question\nanswering (QA) dataset that, by construction, \\emph{requires} proper multihop\nreasoning? To this end, we introduce a bottom-up approach that systematically\nselects composable pairs of single-hop questions that are connected, i.e.,\nwhere one reasoning step critically relies on information from another. This\nbottom-up methodology lets us explore a vast space of questions and add\nstringent filters as well as other mechanisms targeting connected reasoning. It\nprovides fine-grained control over the construction process and the properties\nof the resulting $k$-hop questions. We use this methodology to create\nMuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to\nexisting datasets, MuSiQue-Ans is more difficult overall (3x increase in\nhuman-machine gap), and harder to cheat via disconnected reasoning (e.g., a\nsingle-hop model has a 30 point drop in F1). We further add unanswerable\ncontrast questions to produce a more stringent dataset, MuSiQue-Full. We hope\nour datasets will help the NLP community develop models that perform genuine\nmultihop reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04223","description":"<p>Incorporating factual knowledge into pre-trained language models (PLM) such\nas BERT is an emerging trend in recent NLP studies. However, most of the\nexisting methods combine the external knowledge integration module with a\nmodified pre-training loss and re-implement the pre-training process on the\nlarge-scale corpus. Re-pretraining these models is usually resource-consuming,\nand difficult to adapt to another domain with a different knowledge graph (KG).\nBesides, those works either cannot embed knowledge context dynamically\naccording to textual context or struggle with the knowledge ambiguity issue. In\nthis paper, we propose a novel knowledge-aware language model framework based\non fine-tuning process, which equips PLM with a unified knowledge-enhanced text\ngraph that contains both text and multi-relational sub-graphs extracted from\nKG. We design a hierarchical relational-graph-based message passing mechanism,\nwhich can allow the representations of injected KG and text to mutually update\neach other and can dynamically select ambiguous mentioned entities that share\nthe same text. Our empirical results show that our model can efficiently\nincorporate world knowledge from KGs into existing language models such as\nBERT, and achieve significant improvement on the machine reading comprehension\n(MRC) task compared with other knowledge-enhanced models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yinquan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guirong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07518","description":"<p>Current open-domain conversational models can easily be made to talk in\ninadequate ways. Online learning from conversational feedback given by the\nconversation partner is a promising avenue for a model to improve and adapt, so\nas to generate fewer of these safety failures. However, current\nstate-of-the-art models tend to react to feedback with defensive or oblivious\nresponses. This makes for an unpleasant experience and may discourage\nconversation partners from giving feedback in the future. This work proposes\nSaFeRDialogues, a task and dataset of graceful responses to conversational\nfeedback about safety failures. We collect a dataset of 10k dialogues\ndemonstrating safety failures, feedback signaling them, and a response\nacknowledging the feedback. We show how fine-tuning on this dataset results in\nconversations that human raters deem considerably more likely to lead to a\ncivil conversation, without sacrificing engagingness or general conversational\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02080","description":"<p>Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks. (arXiv:2112.03204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03204","description":"<p>When a neural language model (LM) is adapted to perform a new task, what\naspects of the task predict the eventual performance of the model? In NLP,\nsystematic features of LM generalization to individual examples are well\ncharacterized, but systematic aspects of LM adaptability to new tasks are not\nnearly as well understood. We present a large-scale empirical study of the\nfeatures and limits of LM adaptability using a new benchmark, TaskBench500,\nbuilt from 500 procedurally generated sequence modeling tasks. These tasks\ncombine core aspects of language processing, including lexical semantics,\nsequence processing, memorization, logical reasoning, and world knowledge.\nUsing TaskBench500, we evaluate three facets of adaptability, finding that: (1)\nadaptation procedures differ dramatically in their ability to memorize small\ndatasets; (2) within a subset of task types, adaptation procedures exhibit\ncompositional adaptability to complex tasks; and (3) failure to match training\nlabel distributions is explained by mismatches in the intrinsic difficulty of\npredicting individual labels. Our experiments show that adaptability to new\ntasks, like generalization to new examples, can be systematically described and\nunderstood, and we conclude with a discussion of additional aspects of\nadaptability that could be studied using the new benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Belinda Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention. (arXiv:2112.03254v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03254","description":"<p>Most of today's AI systems focus on using self-attention mechanisms and\ntransformer architectures on large amounts of diverse data to achieve\nimpressive performance gains. In this paper, we propose to augment the\ntransformer architecture with an external attention mechanism to bring external\nknowledge and context to bear. By integrating external information into the\nprediction process, we hope to reduce the need for ever-larger models and\nincrease the democratization of AI systems. We find that the proposed external\nattention mechanism can significantly improve the performance of existing AI\nsystems, allowing practitioners to easily customize foundation AI models to\nmany diverse downstream applications. In particular, we focus on the task of\nCommonsense Reasoning, demonstrating that the proposed external attention\nmechanism can augment existing transformer models and significantly improve the\nmodel's reasoning capabilities. The proposed system, Knowledgeable External\nAttention for commonsense Reasoning (KEAR), reaches human parity on the open\nCommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to\nthe human accuracy of 88.9\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Compositional Generalization with Latent Structure and Data Augmentation. (arXiv:2112.07610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07610","description":"<p>Generic unstructured neural networks have been shown to struggle on\nout-of-distribution compositional generalization. Compositional data\naugmentation via example recombination has transferred some prior knowledge\nabout compositionality to such black-box neural models for several semantic\nparsing tasks, but this often required task-specific engineering or provided\nlimited gains.\n</p>\n<p>We present a more powerful data recombination method using a model called\nCompositional Structure Learner (CSL). CSL is a generative model with a\nquasi-synchronous context-free grammar backbone, which we induce from the\ntraining data. We sample recombined examples from CSL and add them to the\nfine-tuning data of a pre-trained sequence-to-sequence model (T5). This\nprocedure effectively transfers most of CSL's compositional bias to T5 for\ndiagnostic tasks, and results in a model even stronger than a T5-CSL ensemble\non two real world compositional generalization tasks. This results in new\nstate-of-the-art performance for these challenging semantic parsing tasks\nrequiring generalization to both natural language variation and novel\ncompositions of elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_P/0/1/0/all/0/1\">Pawe&#x142; Krzysztof Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textless Speech-to-Speech Translation on Real Data. (arXiv:2112.08352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08352","description":"<p>We present a textless speech-to-speech translation (S2ST) system that can\ntranslate speech from one language into another language and can be built\nwithout the need of any text data. Different from existing work in the\nliterature, we tackle the challenge in modeling multi-speaker target speech and\ntrain the systems with real-world S2ST data. The key to our approach is a\nself-supervised unit-based speech normalization technique, which finetunes a\npre-trained speech encoder with paired audios from multiple speakers and a\nsingle reference speaker to reduce the variations due to accents, while\npreserving the lexical content. With only 10 minutes of paired data for speech\nnormalization, we obtain on average 3.2 BLEU gain when training the S2ST model\non the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized\nspeech target. We also incorporate automatically mined S2ST data and show an\nadditional 2.0 BLEU gain. To our knowledge, we are the first to establish a\ntextless S2ST technique that can be trained with real-world data and works for\nmultiple language pairs. Audio samples are available at\nhttps://facebookresearch.github.io/speech_translation/textless_s2st_real_data/index.html .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language. (arXiv:2112.08614v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08614","description":"<p>The primary focus of recent work with largescale transformers has been on\noptimizing the amount of information packed into the model's parameters. In\nthis work, we ask a different question: Can multimodal transformers leverage\nexplicit knowledge in their reasoning? Existing, primarily unimodal, methods\nhave explored approaches under the paradigm of knowledge retrieval followed by\nanswer prediction, but leave open questions about the quality and relevance of\nthe retrieved knowledge used, and how the reasoning processes over implicit and\nexplicit knowledge should be integrated. To address these challenges, we\npropose a novel model - Knowledge Augmented Transformer (KAT) - which achieves\na strong state-of-the-art result (+6 points absolute) on the open-domain\nmultimodal task of OK-VQA. Our approach integrates implicit and explicit\nknowledge in an end to end encoder-decoder architecture, while still jointly\nreasoning over both knowledge sources during answer generation. An additional\nbenefit of explicit knowledge integration is seen in improved interpretability\nof model predictions in our analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Liangke Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DREAM: Improving Situational QA by First Elaborating the Situation. (arXiv:2112.08656v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08656","description":"<p>When people answer questions about a specific situation, e.g., \"I cheated on\nmy mid-term exam last week. Was that wrong?\", cognitive science suggests that\nthey form a mental picture of that situation before answering. While we do not\nknow how language models (LMs) answer such questions, we conjecture that they\nmay answer more accurately if they are also provided with additional details\nabout the question situation, elaborating the \"scene\". To test this conjecture,\nwe train a new model, DREAM, to answer questions that elaborate the scenes that\nsituated questions are about, and then provide those elaborations as additional\ncontext to a question-answering (QA) model. We find that DREAM is able to\ncreate better scene elaborations (more accurate, useful, and consistent) than a\nrepresentative state-of-the-art, zero-shot model (Macaw). We also find that\nusing the scene elaborations as additional context improves the answer accuracy\nof a downstream QA system, including beyond that obtainable by simply further\nfinetuning the QA system on DREAM's training data. These results suggest that\nadding focused elaborations about a situation can improve a system's reasoning\nabout it, and may serve as an effective way of injecting new scenario based\nknowledge into QA models. Finally, our approach is dataset-neutral; we observe\nimproved QA performance across different models, with even bigger gains on\nmodels with fewer parameters. We make our dataset and model publicly available\nat https://github.com/allenai/dream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Human-AI Collaboration for Generating Free-Text Explanations. (arXiv:2112.08674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08674","description":"<p>Large language models are increasingly capable of generating fluent-appearing\ntext with relatively little task-specific supervision. But can these models\naccurately explain classification decisions? We consider the task of generating\nfree-text explanations using human-written examples in a few-shot manner. We\nfind that (1) authoring higher quality prompts results in higher quality\ngenerations; and (2) surprisingly, in a head-to-head comparison, crowdworkers\noften prefer explanations generated by GPT-3 to crowdsourced explanations in\nexisting datasets. Our human studies also show, however, that while models\noften produce factual, grammatical, and sufficient explanations, they have room\nto improve along axes such as providing novel information and supporting the\nlabel. We create a pipeline that combines GPT-3 with a supervised filter that\nincorporates binary acceptability judgments from humans in the loop. Despite\nthe intrinsic subjectivity of acceptability judgments, we demonstrate that\nacceptability is partially correlated with various fine-grained attributes of\nexplanations. Our approach is able to consistently filter GPT-3-generated\nexplanations deemed acceptable by humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOCmT5: Document-Level Pretraining of Multilingual Language Models. (arXiv:2112.08709v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08709","description":"<p>In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence\nlanguage model pretrained with large scale parallel documents. While previous\napproaches have focused on leveraging sentence-level parallel data, we try to\nbuild a general-purpose pretrained model that can understand and generate long\ndocuments. We propose a simple and effective pretraining objective - Document\nreordering Machine Translation (DrMT), in which the input documents that are\nshuffled and masked need to be translated. DrMT brings consistent improvements\nover strong baselines on a variety of document-level generation tasks,\nincluding over 12 BLEU points for seen-language-pair document-level MT, over 7\nBLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1\npoints for seen-language-pair cross-lingual summarization. We achieve\nstate-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation\ntasks. We also conduct extensive analysis on various factors for document\npretraining, including (1) The effects of pretraining data quality and (2) The\neffects of combining mono-lingual and cross-lingual pretraining. We plan to\nmake our model checkpoints publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnakar_V/0/1/0/all/0/1\">Viresh Ratnakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v4 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction. (arXiv:2202.08316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08316","description":"<p>This paper presents FAMIE, a comprehensive and efficient active learning (AL)\ntoolkit for multilingual information extraction. FAMIE is designed to address a\nfundamental problem in existing AL frameworks where annotators need to wait for\na long time between annotation batches due to the time-consuming nature of\nmodel training and data selection at each AL iteration. This hinders the\nengagement, productivity, and efficiency of annotators. Based on the idea of\nusing a small proxy network for fast data selection, we introduce a novel\nknowledge distillation mechanism to synchronize the proxy network with the main\nlarge model (i.e., BERT-based) to ensure the appropriateness of the selected\nannotation examples for the main model. Our AL framework can support multiple\nlanguages. The experiments demonstrate the advantages of FAMIE in terms of\ncompetitive performance and time efficiency for sequence labeling with AL. We\npublicly release our code (\\url{https://github.com/nlp-uoregon/famie}) and demo\nwebsite (\\url{<a href=\"http://nlp.uoregon.edu:9000/\">this http URL</a>}). A demo video for FAMIE is\nprovided at: \\url{https://youtu.be/I2i8n_jAyrY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1\">Nghia Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17247","description":"<p>Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_E/0/1/0/all/0/1\">Estelle Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Meng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-Yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03958","description":"<p>This paper introduces a model for incomplete utterance restoration (IUR).\nDifferent from prior studies that only work on extraction or abstraction\ndatasets, we design a simple but effective model, working for both scenarios of\nIUR. Our design simulates the nature of IUR, where omitted tokens from the\ncontext contribute to restoration. From this, we construct a Picker that\nidentifies the omitted tokens. To support the picker, we design two label\ncreation methods (soft and hard labels), which can work in cases of no\nannotation of the omitted tokens. The restoration is done by using a Generator\nwith the help of the Picker on joint learning. Promising results on four\nbenchmark datasets in extraction and abstraction scenarios show that our model\nis better than the pretrained T5 and non-generative language model methods in\nboth rich and limited training data settings. The code will be also available\n(https://github.com/shumpei19/jointiur).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shumpei Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tsungwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Hong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada. (arXiv:2204.04862v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04862","description":"<p>Over the last decade, Twitter has emerged as one of the most influential\nforums for social, political, and health discourse. In this paper, we introduce\na massive dataset of more than 45 million geo-located tweets posted between\n2015 and 2021 from US and Canada (TUSC), especially curated for natural\nlanguage analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to\ncapture patterns of emotions associated with tweets over time. We use TED and\nTUSC to explore the use of emotion-associated words across US and Canada;\nacross 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the\nsecond year of the pandemic); and across individual tweeters. We show that\nCanadian tweets tend to have higher valence, lower arousal, and higher\ndominance than the US tweets. Further, we show that the COVID-19 pandemic had a\nmarked impact on the emotional signature of tweets posted in 2020, when\ncompared to the adjoining years. Finally, we determine metrics of TED for\n170,000 tweeters to benchmark characteristics of TED metrics at an aggregate\nlevel. TUSC and the metrics for TED will enable a wide variety of research on\nstudying how we use language to express ourselves, persuade, communicate, and\ninfluence, with particularly promising applications in public health, affective\nscience, social science, and psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification. (arXiv:2204.04952v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04952","description":"<p>Text classification struggles to generalize to unseen classes with very few\nlabeled text instances per class. In such a few-shot learning (FSL) setting,\nmetric-based meta-learning approaches have shown promising results. Previous\nstudies mainly aim to derive a prototype representation for each class.\nHowever, they neglect that it is challenging-yet-unnecessary to construct a\ncompact representation which expresses the entire meaning for each class. They\nalso ignore the importance to capture the inter-dependency between query and\nthe support set for few-shot text classification. To deal with these issues, we\npropose a meta-learning based method MGIMN which performs instance-wise\ncomparison followed by aggregation to generate class-wise matching vectors\ninstead of prototype learning. The key of instance-wise comparison is the\ninteractive matching within the class-specific context and episode-specific\ncontext. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms the existing state-of-the-art approaches, under both\nthe standard FSL and generalized FSL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maimaiti_M/0/1/0/all/0/1\">Mieradilijiang Maimaiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language. (arXiv:2204.07432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07432","description":"<p>This paper describes the system used by the Machine Learning Group of LTU in\nsubtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language\n(PCL) Detection. Our system consists of finetuning a pretrained\nText-to-Text-Transfer Transformer (T5) and innovatively reducing its\nout-of-class predictions. The main contributions of this paper are 1) the\ndescription of the implementation details of the T5 model we used, 2) analysis\nof the successes &amp; struggles of the model in this task, and 3) ablation studies\nbeyond the official submission to ascertain the relative importance of data\nsplit. Our model achieves an F1 score of 0.5452 on the official test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhaled_L/0/1/0/all/0/1\">Lama Alkhaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokayed_H/0/1/0/all/0/1\">Hamam Mokayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08198","description":"<p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Arash Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2204.09781","description":"<p>The COVID-19 pandemic has been severely impacting global society since\nDecember 2019. Massive research has been undertaken to understand the\ncharacteristics of the virus and design vaccines and drugs. The related\nfindings have been reported in biomedical literature at a rate of about 10,000\narticles on COVID-19 per month. Such rapid growth significantly challenges\nmanual curation and interpretation. For instance, LitCovid is a literature\ndatabase of COVID-19-related articles in PubMed, which has accumulated more\nthan 200,000 articles with millions of accesses each month by users worldwide.\nOne primary curation task is to assign up to eight topics (e.g., Diagnosis and\nTreatment) to the articles in LitCovid. Despite the continuing advances in\nbiomedical text mining methods, few have been dedicated to topic annotations in\nCOVID-19 literature. To close the gap, we organized the BioCreative LitCovid\ntrack to call for a community effort to tackle automated topic annotation for\nCOVID-19 literature. The BioCreative LitCovid dataset, consisting of over\n30,000 articles with manually reviewed topics, was created for training and\ntesting. It is one of the largest multilabel classification datasets in\nbiomedical scientific literature. 19 teams worldwide participated and made 80\nsubmissions in total. Most teams used hybrid systems based on transformers. The\nhighest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro\nF1-score, micro F1-score, and instance-based F1-score, respectively. The level\nof participation and results demonstrate a successful track and help close the\ngap between dataset curation and method development. The dataset is publicly\navailable via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for\nbenchmarking and further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_R/0/1/0/all/0/1\">Rezarta Islamaj Do&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Li Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuefu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_P/0/1/0/all/0/1\">Parsa Bagherzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_S/0/1/0/all/0/1\">Sabine Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_A/0/1/0/all/0/1\">Aakash Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhavsar_N/0/1/0/all/0/1\">Nidhir Bhavsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yung-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wentai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavchioski_I/0/1/0/all/0/1\">Ilija Tavchioski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shubo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_N/0/1/0/all/0/1\">Niladri Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_K/0/1/0/all/0/1\">Kushagri Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus Laleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinghang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujari_S/0/1/0/all/0/1\">Subhash Chandra Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chizhikova_M/0/1/0/all/0/1\">Mariia Chizhikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13021","description":"<p>We present NLU++, a novel dataset for natural language understanding (NLU) in\ntask-oriented dialogue (ToD) systems, with the aim to provide a much more\nchallenging evaluation environment for dialogue NLU models, up to date with the\ncurrent application and industry requirements. NLU++ is divided into two\ndomains (BANKING and HOTELS) and brings several crucial improvements over\ncurrent commonly used NLU datasets. 1) NLU++ provides fine-grained domain\nontologies with a large set of challenging multi-intent sentences, introducing\nand validating the idea of intent modules that can be combined into complex\nintents that convey complex user goals, combined with finer-grained and thus\nmore challenging slot sets. 2) The ontology is divided into domain-specific and\ngeneric (i.e., domain-universal) intent modules that overlap across domains,\npromoting cross-domain reusability of annotated examples. 3) The dataset design\nhas been inspired by the problems observed in industrial ToD systems, and 4) it\nhas been collected, filtered and carefully annotated by dialogue NLU experts,\nyielding high-quality annotated data. Finally, we benchmark a series of current\nstate-of-the-art NLU models on NLU++; the results demonstrate the challenging\nnature of the dataset, especially in low-data regimes, the validity of `intent\nmodularisation', and call for further research on ToD NLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spithourakis_G/0/1/0/all/0/1\">Georgios P. Spithourakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning. (arXiv:2204.13957v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13957","description":"<p>Knowledge graph (KG) embedding methods which map entities and relations to\nunique embeddings in the KG have shown promising results on many reasoning\ntasks. However, the same embedding dimension for both dense entities and sparse\nentities will cause either over parameterization (sparse entities) or under\nfitting (dense entities). Normally, a large dimension is set to get better\nperformance. Meanwhile, the inference time grows log-linearly with the number\nof entities for all entities are traversed and compared. Both the parameter and\ninference become challenges when working with huge amounts of entities. Thus,\nwe propose PIE, a \\textbf{p}arameter and \\textbf{i}nference \\textbf{e}fficient\nsolution. Inspired from tensor decomposition methods, we find that decompose\nentity embedding matrix into low rank matrices can reduce more than half of the\nparameters while maintaining comparable performance. To accelerate model\ninference, we propose a self-supervised auxiliary task, which can be seen as\nfine-grained entity typing. By randomly masking and recovering entities'\nconnected relations, the task learns the co-occurrence of entity and relations.\nUtilizing the fine grained typing, we can filter unrelated entities during\ninference and get targets with possibly sub-linear time requirement.\nExperiments on link prediction benchmarks demonstrate the proposed key\ncapabilities. Moreover, we prove effectiveness of the proposed solution on the\nOpen Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the\nstate of the art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Linlin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiexiong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01068","description":"<p>Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_C/0/1/0/all/0/1\">Christopher Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Anjali Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI. (arXiv:2205.01809v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.01809","description":"<p>A fundamental research goal for Explainable AI (XAI) is to build models that\nare capable of reasoning through the generation of natural language\nexplanations. However, the methodologies to design and evaluate\nexplanation-based inference models are still poorly informed by theoretical\naccounts on the nature of explanation. As an attempt to provide an\nepistemologically grounded characterisation for XAI, this paper focuses on the\nscientific domain, aiming to bridge the gap between theory and practice on the\nnotion of a scientific explanation. Specifically, the paper combines a detailed\nsurvey of the modern accounts of scientific explanation in Philosophy of\nScience with a systematic analysis of corpora of natural language explanations,\nclarifying the nature and function of explanatory arguments from both a\ntop-down (categorical) and a bottom-up (corpus-based) perspective. Through a\nmixture of quantitative and qualitative methodologies, the presented study\nallows deriving the following main conclusions: (1) Explanations cannot be\nentirely characterised in terms of inductive or deductive arguments as their\nmain function is to perform unification; (2) An explanation must cite causes\nand mechanisms that are responsible for the occurrence of the event to be\nexplained; (3) While natural language explanations possess an intrinsic\ncausal-mechanistic nature, they are not limited to causes and mechanisms, also\naccounting for pragmatic elements such as definitions, properties and taxonomic\nrelations; (4) Patterns of unification naturally emerge in corpora of\nexplanations even if not intentionally modelled; (5) Unification is realised\nthrough a process of abstraction, whose function is to provide the inference\nsubstrate for subsuming the event to be explained under recurring patterns and\nhigh-level regularities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.01818","description":"<p>Human intelligence is multimodal; we integrate visual, linguistic, and\nacoustic signals to maintain a holistic worldview. Most current pretraining\nmethods, however, are limited to one or two modalities. We present i-Code, a\nself-supervised pretraining framework where users may flexibly combine the\nmodalities of vision, speech, and language into unified and general-purpose\nvector representations. In this framework, data from each modality are first\ngiven to pretrained single-modality encoders. The encoder outputs are then\nintegrated with a multimodal fusion network, which uses novel attention\nmechanisms and other architectural innovations to effectively combine\ninformation from the different modalities. The entire system is pretrained\nend-to-end with new objectives including masked modality unit modeling and\ncross-modality contrastive learning. Unlike previous research using only video\nfor pretraining, the i-Code framework can dynamically process single, dual, and\ntriple-modality data during training and inference, flexibly projecting\ndifferent combinations of modalities into a single representation space.\nExperimental results demonstrate how i-Code can outperform state-of-the-art\ntechniques on five video understanding tasks and the GLUE NLP benchmark,\nimproving by as much as 11% and demonstrating the power of integrative\nmultimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P^3 Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning. (arXiv:2205.01886v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.01886","description":"<p>Compared to other language tasks, applying pre-trained language models (PLMs)\nfor search ranking often requires more nuances and training signals. In this\npaper, we identify and study the two mismatches between pre-training and\nranking fine-tuning: the training schema gap regarding the differences in\ntraining objectives and model architectures, and the task knowledge gap\nconsidering the discrepancy between the knowledge needed in ranking and that\nlearned during pre-training. To mitigate these gaps, we propose Pre-trained,\nPrompt-learned and Pre-finetuned Neural Ranker (P^3 Ranker). P^3 Ranker\nleverages prompt-based learning to convert the ranking task into a pre-training\nlike schema and uses pre-finetuning to initialize the model on intermediate\nsupervised tasks. Experiments on MS MARCO and Robust04 show the superior\nperformances of P^3 Ranker in few-shot ranking. Analyses reveal that P^3 Ranker\nis able to better accustom to the ranking task through prompt-based learning\nand retrieve necessary ranking-oriented knowledge gleaned in pre-finetuning,\nresulting in data-efficient PLM adaptation. Our code is available at\nhttps://github.com/NEUIR/P3Ranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Ge Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning to Social Norms and Values in Interactive Narratives. (arXiv:2205.01975v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01975","description":"<p>We focus on creating agents that act in alignment with socially beneficial\nnorms and values in interactive narratives or text-based games -- environments\nwherein an agent perceives and interacts with a world through natural language.\nSuch interactive agents are often trained via reinforcement learning to\noptimize task performance, even when such rewards may lead to agent behaviors\nthat violate societal norms -- causing harm either to the agent itself or other\nentities in the environment. Social value alignment refers to creating agents\nwhose behaviors conform to expected moral and social norms for a given context\nand group of people -- in our case, it means agents that behave in a manner\nthat is less harmful and more beneficial for themselves and others.\n</p>\n<p>We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25\nannotated interactive narratives containing thousands of morally salient\nscenarios covering everything from theft and bodily harm to altruism. We\nintroduce the GALAD (Game-value ALignment through Action Distillation) agent\nthat uses the social commonsense knowledge present in specially trained\nlanguage models to contextually restrict its action space to only those actions\nthat are aligned with socially beneficial values. An experimental study shows\nthat the GALAD agent makes decisions efficiently enough to improve\nstate-of-the-art task performance by 4% while reducing the frequency of\nsocially harmful behaviors by 25% compared to strong contemporary value\nalignment approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02023","description":"<p>The success of multilingual pre-trained models is underpinned by their\nability to learn representations shared by multiple languages even in absence\nof any explicit supervision. However, it remains unclear how these models learn\nto generalise across languages. In this work, we conjecture that multilingual\npre-trained models can derive language-universal abstractions about grammar. In\nparticular, we investigate whether morphosyntactic information is encoded in\nthe same subset of neurons in different languages. We conduct the first\nlarge-scale empirical study over 43 languages and 14 morphosyntactic categories\nwith a state-of-the-art neuron-level probe. Our findings show that the\ncross-lingual overlap between neurons is significant, but its extent may vary\nacross categories and depends on language proximity and pre-training data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision. (arXiv:2205.02300v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02300","description":"<p>In this paper, we study the problem of procedure planning in instructional\nvideos. Here, an agent must produce a plausible sequence of actions that can\ntransform the environment from a given start to a desired goal state. When\nlearning procedure planning from instructional videos, most recent work\nleverages intermediate visual observations as supervision, which requires\nexpensive annotation efforts to localize precisely all the instructional steps\nin training videos. In contrast, we remove the need for expensive temporal\nvideo annotations and propose a weakly supervised approach by learning from\nnatural language instructions. Our model is based on a transformer equipped\nwith a memory module, which maps the start and goal observations to a sequence\nof plausible actions. Furthermore, we augment our model with a probabilistic\ngenerative module to capture the uncertainty inherent to procedure planning, an\naspect largely overlooked by previous work. We evaluate our model on three\ndatasets and show our weaklysupervised approach outperforms previous fully\nsupervised state-of-the-art models on multiple metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1\">Isma Hadji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dvornik_N/0/1/0/all/0/1\">Nikita Dvornik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jepson_A/0/1/0/all/0/1\">Allan D. Jepson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02301","description":"<p>Estimating human motion from video is an active research area due to its many\npotential applications. Most state-of-the-art methods predict human shape and\nposture estimates for individual images and do not leverage the temporal\ninformation available in video. Many \"in the wild\" sequences of human motion\nare captured by a moving camera, which adds the complication of conflated\ncamera and human motion to the estimation. We therefore present BodySLAM, a\nmonocular SLAM system that jointly estimates the position, shape, and posture\nof human bodies, as well as the camera trajectory. We also introduce a novel\nhuman motion model to constrain sequential body postures and observe the scale\nof the scene. Through a series of experiments on video sequences of human\nmotion captured by a moving monocular camera, we demonstrate that BodySLAM\nimproves estimates of all human body parameters and camera poses when compared\nto estimating these separately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henning_D/0/1/0/all/0/1\">Dorian Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShoeRinsics: Shoeprint Prediction for Forensics with Intrinsic Decomposition. (arXiv:2205.02361v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02361","description":"<p>Shoe tread impressions are one of the most common types of evidence left at\ncrime scenes. However, the utility of such evidence is limited by the lack of\ndatabases of footwear impression patterns that cover the huge and growing\nnumber of distinct shoe models. We propose to address this gap by leveraging\nshoe tread photographs collected by online retailers. The core challenge is to\npredict the impression pattern from the shoe photograph since ground-truth\nimpressions or 3D shapes of tread patterns are not available. We develop a\nmodel that performs intrinsic image decomposition (predicting depth, normal,\nalbedo, and lighting) from a single tread photo. Our approach, which we term\nShoeRinsics, combines domain adaptation and re-rendering losses in order to\nleverage a mix of fully supervised synthetic data and unsupervised retail image\ndata. To validate model performance, we also collected a set of paired\nshoe-sole images and corresponding prints, and define a benchmarking protocol\nto quantify the accuracy of predicted impressions. On this benchmark,\nShoeRinsics outperforms existing methods for depth prediction and\nsynthetic-to-real domain adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafique_S/0/1/0/all/0/1\">Samia Shafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1\">Bailey Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless C. Fowlkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian Detect to Track System for Robust Visual Object Tracking and Semi-Supervised Model Learning. (arXiv:2205.02371v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02371","description":"<p>Object tracking is one of the fundamental problems in visual recognition\ntasks and has achieved significant improvements in recent years. The\nachievements often come with the price of enormous hardware consumption and\nexpensive labor effort for consecutive labeling. A missing ingredient for\nrobust tracking is achieving performance with minimal modification on network\nstructure and semi-supervised learning intermittent labeled frames. In this\npaper, we ad-dress these problems in a Bayesian tracking and detection\nframework parameterized by neural network outputs. In our framework, the\ntracking and detection process is formulated in a probabilistic way as\nmulti-objects dynamics and network detection uncertainties. With our\nformulation, we propose a particle filter-based approximate sampling algorithm\nfor tracking object state estimation. Based on our particle filter inference\nalgorithm, a semi-supervised learn-ing algorithm is utilized for learning\ntracking network on intermittent labeled frames by variational inference. In\nour experiments, we provide both mAP and probability-based detection\nmeasurements for comparison between our algorithm with non-Bayesian solutions.\nWe also train a semi-supervised tracking network on M2Cai16-Tool-Locations\nDataset and compare our results with supervised learning on fully labeled\nframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressive Ptychography using Deep Image and Generative Priors. (arXiv:2205.02397v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02397","description":"<p>Ptychography is a well-established coherent diffraction imaging technique\nthat enables non-invasive imaging of samples at a nanometer scale. It has been\nextensively used in various areas such as the defense industry or materials\nscience. One major limitation of ptychography is the long data acquisition time\ndue to mechanical scanning of the sample; therefore, approaches to reduce the\nscan points are highly desired. However, reconstructions with less number of\nscan points lead to imaging artifacts and significant distortions, hindering a\nquantitative evaluation of the results. To address this bottleneck, we propose\na generative model combining deep image priors with deep generative priors. The\nself-training approach optimizes the deep generative neural network to create a\nsolution for a given dataset. We complement our approach with a prior acquired\nfrom a previously trained discriminator network to avoid a possible divergence\nfrom the desired output caused by the noise in the measurements. We also\nsuggest using the total variation as a complementary before combat artifacts\ndue to measurement noise. We analyze our approach with numerical experiments\nthrough different probe overlap percentages and varying noise levels. We also\ndemonstrate improved reconstruction accuracy compared to the state-of-the-art\nmethod and discuss the advantages and disadvantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barutcu_S/0/1/0/all/0/1\">Semih Barutcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_D/0/1/0/all/0/1\">Do&#x11f;a G&#xfc;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot-adaptive Knowledge Distillation. (arXiv:2205.02399v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02399","description":"<p>Knowledge distillation (KD) has become a well established paradigm for\ncompressing deep neural networks. The typical way of conducting knowledge\ndistillation is to train the student network under the supervision of the\nteacher network to harness the knowledge at one or multiple spots (i.e.,\nlayers) in the teacher network. The distillation spots, once specified, will\nnot change for all the training samples, throughout the whole distillation\nprocess. In this work, we argue that distillation spots should be adaptive to\ntraining samples and distillation epochs. We thus propose a new distillation\nstrategy, termed spot-adaptive KD (SAKD), to adaptively determine the\ndistillation spots in the teacher network per sample, at every training\niteration during the whole distillation period. As SAKD actually focuses on\n\"where to distill\" instead of \"what to distill\" that is widely investigated by\nmost existing works, it can be seamlessly integrated into existing distillation\nmethods to further improve their performance. Extensive experiments with 10\nstate-of-the-art distillers are conducted to demonstrate the effectiveness of\nSAKD for improving their distillation performance, under both homogeneous and\nheterogeneous distillation settings. Code is available at\nhttps://github.com/zju-vipa/spot-adaptive-pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Reconstruction from Point Clouds: A Survey and a Benchmark. (arXiv:2205.02413v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02413","description":"<p>Reconstruction of a continuous surface of two-dimensional manifold from its\nraw, discrete point cloud observation is a long-standing problem. The problem\nis technically ill-posed, and becomes more difficult considering that various\nsensing imperfections would appear in the point clouds obtained by practical\ndepth scanning. In literature, a rich set of methods has been proposed, and\nreviews of existing methods are also provided. However, existing reviews are\nshort of thorough investigations on a common benchmark. The present paper aims\nto review and benchmark existing methods in the new era of deep learning\nsurface reconstruction. To this end, we contribute a large-scale benchmarking\ndataset consisting of both synthetic and real-scanned data; the benchmark\nincludes object- and scene-level surfaces and takes into account various\nsensing imperfections that are commonly encountered in practical depth\nscanning. We conduct thorough empirical studies by comparing existing methods\non the constructed benchmark, and pay special attention on robustness of\nexisting methods against various scanning imperfections; we also study how\ndifferent methods generalize in terms of reconstructing complex surface shapes.\nOur studies help identify the best conditions under which different methods\nwork, and suggest some empirical findings. For example, while deep learning\nmethods are increasingly popular, our systematic studies suggest that,\nsurprisingly, a few classical methods perform even better in terms of both\nrobustness and generalization; our studies also suggest that the practical\nchallenges of misalignment of point sets from multi-view scanning, missing of\nsurface points, and point outliers remain unsolved by all the existing surface\nreconstruction methods. We expect that the benchmark and our studies would be\nvaluable both for practitioners and as a guidance for new innovations in future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhangjin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinjuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Real-time Traffic Sign and Traffic Light Detection on Embedded Systems. (arXiv:2205.02421v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02421","description":"<p>Recent work done on traffic sign and traffic light detection focus on\nimproving detection accuracy in complex scenarios, yet many fail to deliver\nreal-time performance, specifically with limited computational resources. In\nthis work, we propose a simple deep learning based end-to-end detection\nframework, which effectively tackles challenges inherent to traffic sign and\ntraffic light detection such as small size, large number of classes and complex\nroad scenarios. We optimize the detection models using TensorRT and integrate\nwith Robot Operating System to deploy on an Nvidia Jetson AGX Xavier as our\nembedded device. The overall system achieves a high inference speed of 63\nframes per second, demonstrating the capability of our system to perform in\nreal-time. Furthermore, we introduce CeyRo, which is the first ever large-scale\ntraffic sign and traffic light detection dataset for the Sri Lankan context.\nOur dataset consists of 7984 total images with 10176 traffic sign and traffic\nlight instances covering 70 traffic sign and 5 traffic light classes. The\nimages have a high resolution of 1920 x 1080 and capture a wide range of\nchallenging road scenarios with different weather and lighting conditions. Our\nwork is publicly available at https://github.com/oshadajay/CeyRo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayasinghe_O/0/1/0/all/0/1\">Oshada Jayasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sahan Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anhettigama_D/0/1/0/all/0/1\">Damith Anhettigama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_S/0/1/0/all/0/1\">Shenali Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickremasinghe_T/0/1/0/all/0/1\">Tharindu Wickremasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_C/0/1/0/all/0/1\">Chalani Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasekara_P/0/1/0/all/0/1\">Peshala Jayasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to artistic image generation. (arXiv:2205.02439v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02439","description":"<p>Painting is one of the ways for people to express their ideas, but what if\npeople with disabilities in hands want to paint? To tackle this challenge, we\ncreate an end-to-end solution that can generate artistic images from text\ndescriptions. However, due to the lack of datasets with paired text description\nand artistic images, it is hard to directly train an algorithm which can create\nart based on text input. To address this issue, we split our task into three\nsteps: (1) Generate a realistic image from a text description by using Dynamic\nMemory Generative Adversarial Network (<a href=\"/abs/1904.01310\">arXiv:1904.01310</a>), (2) Classify the\nimage as a genre that exists in the WikiArt dataset using Resnet (arXiv:\n<a href=\"/abs/1512.03385\">1512.03385</a>), (3) Select a style that is compatible with the genre and transfer\nit to the generated image by using neural artistic stylization network\n(<a href=\"/abs/1705.06830\">arXiv:1705.06830</a>).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qinghe Tian</a> (University of Waterloo), <a href=\"http://arxiv.org/find/cs/1/au:+Franchitti_J/0/1/0/all/0/1\">Jean-Claude Franchitti</a> (New York University Courant Institute)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Declaration-based Prompt Tuning for Visual Question Answering. (arXiv:2205.02456v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02456","description":"<p>In recent years, the pre-training-then-fine-tuning paradigm has yielded\nimmense success on a wide spectrum of cross-modal tasks, such as visual\nquestion answering (VQA), in which a visual-language (VL) model is first\noptimized via self-supervised task objectives, e.g., masked language modeling\n(MLM) and image-text matching (ITM), and then fine-tuned to adapt to downstream\ntask (e.g., VQA) via a brand-new objective function, e.g., answer prediction.\nThe inconsistency of the objective forms not only severely limits the\ngeneralization of pre-trained VL models to downstream tasks, but also requires\na large amount of labeled data for fine-tuning. To alleviate the problem, we\npropose an innovative VL fine-tuning paradigm (named Declaration-based Prompt\nTuning, abbreviated as DPT), which jointly optimizes the objectives of\npre-training and fine-tuning of VQA model, boosting the effective adaptation of\npre-trained VL models to the downstream task. Specifically, DPT reformulates\nthe objective form of VQA task via (1) textual adaptation, which converts the\ngiven questions into declarative sentence-form for prompt-tuning, and (2) task\nadaptation, which optimizes the objective function of VQA problem in the manner\nof pre-training phase. Experimental results on GQA dataset show that DPT\noutperforms the fine-tuned counterpart by a large margin regarding accuracy in\nboth fully-supervised (2.68%) and zero-shot/few-shot (over 31%) settings. All\nthe data and codes will be available to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daowan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMINR: Multi-frame-to-Multi-frame Inference with Noise Resistance for Precipitation Nowcasting with Radar. (arXiv:2205.02457v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02457","description":"<p>Precipitation nowcasting based on radar echo maps is essential in\nmeteorological research. Recently, Convolutional RNNs based methods dominate\nthis field, but they cannot be solved by parallel computation resulting in\nlonger inference time. FCN based methods adopt a multi-frame-to-single-frame\ninference (MSI) strategy to avoid this problem. They feedback into the model\nagain to predict the next time step to get multi-frame nowcasting results in\nthe prediction phase, which will lead to the accumulation of prediction errors.\nIn addition, precipitation noise is a crucial factor contributing to high\nprediction errors because of its unpredictability. To address this problem, we\npropose a novel Multi-frame-to-Multi-frame Inference (MMI) model with Noise\nResistance (NR) named MMINR. It avoids error accumulation and resists\nprecipitation noise\\'s negative effect in parallel computation. NR contains a\nNoise Dropout Module (NDM) and a Semantic Restore Module (SRM). NDM\ndeliberately dropout noise simple yet efficient, and SRM supplements semantic\ninformation of features to alleviate the problem of semantic information\nmistakenly lost by NDM. Experimental results demonstrate that MMINR can attain\ncompetitive scores compared with other SOTAs. The ablation experiments show\nthat the proposed NDM and SRM can solve the aforementioned problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Feng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Cong Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Correspondences with All-pairs Correlations for Multi-view Depth Estimation. (arXiv:2205.02481v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02481","description":"<p>Multi-view depth estimation plays a critical role in reconstructing and\nunderstanding the 3D world. Recent learning-based methods have made significant\nprogress in it. However, multi-view depth estimation is fundamentally a\ncorrespondence-based optimization problem, but previous learning-based methods\nmainly rely on predefined depth hypotheses to build correspondence as the cost\nvolume and implicitly regularize it to fit depth prediction, deviating from the\nessence of iterative optimization based on stereo correspondence. Thus, they\nsuffer unsatisfactory precision and generalization capability. In this paper,\nwe are the first to explore more general image correlations to establish\ncorrespondences dynamically for depth estimation. We design a novel iterative\nmulti-view depth estimation framework mimicking the optimization process, which\nconsists of 1) a correlation volume construction module that models the pixel\nsimilarity between a reference image and source images as all-to-all\ncorrelations; 2) a flow-based depth initialization module that estimates the\ndepth from the 2D optical flow; 3) a novel correlation-guided depth refinement\nmodule that reprojects points in different views to effectively fetch relevant\ncorrelations for further fusion and integrate the fused correlation for\niterative depth update. Without predefined depth hypotheses, the fused\ncorrelations establish multi-view correspondence in an efficient way and guide\nthe depth refinement heuristically. We conduct sufficient experiments on\nScanNet, DeMoN, ETH3D, and 7Scenes to demonstrate the superiority of our method\non multi-view depth estimation and its best generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are GAN-based Morphs Threatening Face Recognition?. (arXiv:2205.02496v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02496","description":"<p>Morphing attacks are a threat to biometric systems where the biometric\nreference in an identity document can be altered. This form of attack presents\nan important issue in applications relying on identity documents such as border\nsecurity or access control. Research in generation of face morphs and their\ndetection is developing rapidly, however very few datasets with morphing\nattacks and open-source detection toolkits are publicly available. This paper\nbridges this gap by providing two datasets and the corresponding code for four\ntypes of morphing attacks: two that rely on facial landmarks based on OpenCV\nand FaceMorpher, and two that use StyleGAN 2 to generate synthetic morphs. We\nalso conduct extensive experiments to assess the vulnerability of four\nstate-of-the-art face recognition systems, including FaceNet, VGG-Face,\nArcFace, and ISV. Surprisingly, the experiments demonstrate that, although\nvisually more appealing, morphs based on StyleGAN 2 do not pose a significant\nthreat to the state to face recognition systems, as these morphs were\noutmatched by the simple morphs that are based facial landmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_E/0/1/0/all/0/1\">Eklavya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korshunov_P/0/1/0/all/0/1\">Pavel Korshunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colbois_L/0/1/0/all/0/1\">Laurent Colbois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View-labels Are Indispensable: A Multifacet Complementarity Study of Multi-view Clustering. (arXiv:2205.02507v1 [cs.LG])","link":"http://arxiv.org/abs/2205.02507","description":"<p>Consistency and complementarity are two key ingredients for boosting\nmulti-view clustering (MVC). Recently with the introduction of popular\ncontrastive learning, the consistency learning of views has been further\nenhanced in MVC, leading to promising performance. However, by contrast, the\ncomplementarity has not received sufficient attention except just in the\nfeature facet, where the Hilbert Schmidt Independence Criterion (HSIC) term or\nthe independent encoder-decoder network is usually adopted to capture\nview-specific information. This motivates us to reconsider the complementarity\nlearning of views comprehensively from multiple facets including the feature-,\nview-label- and contrast- facets, while maintaining the view consistency. We\nempirically find that all the facets contribute to the complementarity\nlearning, especially the view-label facet, which is usually neglected by\nexisting methods. Based on this, we develop a novel \\underline{M}ultifacet\n\\underline{C}omplementarity learning framework for\n\\underline{M}ulti-\\underline{V}iew \\underline{C}lustering (MCMVC), which fuses\nmultifacet complementarity information, especially explicitly embedding the\nview-label information. To our best knowledge, it is the first time to use\nview-labels explicitly to guide the complementarity learning of views. Compared\nwith the SOTA baseline, MCMVC achieves remarkable improvements, e.g., by\naverage margins over $5.00\\%$ and $7.00\\%$ respectively in complete and\nincomplete MVC settings on Caltech101-20 in terms of three evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_C/0/1/0/all/0/1\">Chuanxing Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1\">Aiyang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Picture is Worth a Thousand Words: A New Wallet Recovery Process. (arXiv:2205.02511v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02511","description":"<p>We introduce a new wallet recovery process. Our solution associates 1) visual\npasswords: a photograph of a secretly picked object (Chabanne et al., 2013)\nwith 2) ImageNet classifiers transforming images into binary vectors and, 3)\nobfuscated fuzzy matching (Galbraith and Zobernig, 2019) for the storage of\nvisual passwords/retrieval of wallet seeds. Our experiments show that the\nreplacement of long seed phrases by a photograph is possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chabannne_H/0/1/0/all/0/1\">Herv&#xe9; Chabannne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Despiegel_V/0/1/0/all/0/1\">Vincent Despiegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guiga_L/0/1/0/all/0/1\">Linda Guiga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOPose: Transformer-based Multi-Object 6D Pose Estimation using Keypoint Regression. (arXiv:2205.02536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02536","description":"<p>6D object pose estimation is a crucial prerequisite for autonomous robot\nmanipulation applications. The state-of-the-art models for pose estimation are\nconvolutional neural network (CNN)-based. Lately, Transformers, an architecture\noriginally proposed for natural language processing, is achieving\nstate-of-the-art results in many computer vision tasks as well. Equipped with\nthe multi-head self-attention mechanism, Transformers enable simple\nsingle-stage end-to-end architectures for learning object detection and 6D\nobject pose estimation jointly. In this work, we propose YOLOPose (short form\nfor You Only Look Once Pose estimation), a Transformer-based multi-object 6D\npose estimation method based on keypoint regression. In contrast to the\nstandard heatmaps for predicting keypoints in an image, we directly regress the\nkeypoints. Additionally, we employ a learnable orientation estimation module to\npredict the orientation from the keypoints. Along with a separate translation\nestimation module, our model is end-to-end differentiable. Our method is\nsuitable for real-time applications and achieves results comparable to\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Arash Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1\">Arul Selvam Periyasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Reshaping of Portraits in Videos. (arXiv:2205.02538v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02538","description":"<p>Sharing short personalized videos to various social media networks has become\nquite popular in recent years. This raises the need for digital retouching of\nportraits in videos. However, applying portrait image editing directly on\nportrait video frames cannot generate smooth and stable video sequences. To\nthis end, we present a robust and easy-to-use parametric method to reshape the\nportrait in a video to produce smooth retouched results. Given an input\nportrait video, our method consists of two main stages: stabilized face\nreconstruction, and continuous video reshaping. In the first stage, we start by\nestimating face rigid pose transformations across video frames. Then we jointly\noptimize multiple frames to reconstruct an accurate face identity, followed by\nrecovering face expressions over the entire video. In the second stage, we\nfirst reshape the reconstructed 3D face using a parametric reshaping model\nreflecting the weight change of the face, and then utilize the reshaped 3D face\nto guide the warping of video frames. We develop a novel signed distance\nfunction based dense mapping method for the warping between face contours\nbefore and after reshaping, resulting in stable warped video frames with\nminimum distortions. In addition, we use the 3D structure of the face to\ncorrect the dense mapping to achieve temporal consistency. We generate the\nfinal result by minimizing the background distortion through optimizing a\ncontent-aware warping mesh. Extensive experiments show that our method is able\nto create visually pleasing results by adjusting a simple reshaping parameter,\nwhich facilitates portrait video editing for social media and visual effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangjun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong-Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Controllable Motion Transition for Characters. (arXiv:2205.02540v1 [cs.GR])","link":"http://arxiv.org/abs/2205.02540","description":"<p>Real-time in-between motion generation is universally required in games and\nhighly desirable in existing animation pipelines. Its core challenge lies in\nthe need to satisfy three critical conditions simultaneously: quality,\ncontrollability and speed, which renders any methods that need offline\ncomputation (or post-processing) or cannot incorporate (often unpredictable)\nuser control undesirable. To this end, we propose a new real-time transition\nmethod to address the aforementioned challenges. Our approach consists of two\nkey components: motion manifold and conditional transitioning. The former\nlearns the important low-level motion features and their dynamics; while the\nlatter synthesizes transitions conditioned on a target frame and the desired\ntransition duration. We first learn a motion manifold that explicitly models\nthe intrinsic transition stochasticity in human motions via a multi-modal\nmapping mechanism. Then, during generation, we design a transition model which\nis essentially a sampling strategy to sample from the learned manifold, based\non the target frame and the aimed transition duration. We validate our method\non different datasets in tasks where no post-processing or offline computation\nis allowed. Through exhaustive evaluation and comparison, we show that our\nmethod is able to generate high-quality motions measured under multiple\nmetrics. Our method is also robust under various target frames (with extreme\ncases).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangjun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ruifan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_Q/0/1/0/all/0/1\">Qilong Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCR Synthetic Benchmark Dataset for Indic Languages. (arXiv:2205.02543v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02543","description":"<p>We present the largest publicly available synthetic OCR benchmark dataset for\nIndic languages. The collection contains a total of 90k images and their ground\ntruth for 23 Indic languages. OCR model validation in Indic languages require a\ngood amount of diverse data to be processed in order to create a robust and\nreliable model. Generating such a huge amount of data would be difficult\notherwise but with synthetic data, it becomes far easier. It can be of great\nimportance to fields like Computer Vision or Image Processing where once an\ninitial synthetic data is developed, model creation becomes easier. Generating\nsynthetic data comes with the flexibility to adjust its nature and environment\nas and when required in order to improve the performance of the model. Accuracy\nfor labeled real-time data is sometimes quite expensive while accuracy for\nsynthetic data can be easily achieved with a good score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1\">Naresh Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_P/0/1/0/all/0/1\">Promodh Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bheemaraj_A/0/1/0/all/0/1\">Aravinth Bheemaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daga_D/0/1/0/all/0/1\">Dhiraj Daga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Saurabh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaraj_S/0/1/0/all/0/1\">Srihari Nagaraj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biologically inspired deep residual networks for computer vision applications. (arXiv:2205.02551v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02551","description":"<p>Deep neural network has been ensured as a key technology in the field of many\nchallenging and vigorously researched computer vision tasks. Furthermore,\nclassical ResNet is thought to be a state-of-the-art convolutional neural\nnetwork (CNN) and was observed to capture features which can have good\ngeneralization ability. In this work, we propose a biologically inspired deep\nresidual neural network where the hexagonal convolutions are introduced along\nthe skip connections. The performance of different ResNet variants using square\nand hexagonal convolution are evaluated with the competitive training strategy\nmentioned by [1]. We show that the proposed approach advances the baseline\nimage classification accuracy of vanilla ResNet architectures on CIFAR-10 and\nthe same was observed over multiple subsets of the ImageNet 2012 dataset. We\nobserved an average improvement by 1.35% and 0.48% on baseline top-1 accuracies\nfor ImageNet 2012 and CIFAR-10, respectively. The proposed biologically\ninspired deep residual networks were observed to have improved generalized\nperformance and this could be a potential research direction to improve the\ndiscriminative ability of state-of-the-art image classification networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varghese_P/0/1/0/all/0/1\">Prathibha Varghese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saroja_D/0/1/0/all/0/1\">Dr. G. Arockia Selva Saroja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropTrack -- automatic droplet tracking using deep learning for microfluidic applications. (arXiv:2205.02568v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02568","description":"<p>Deep neural networks are rapidly emerging as data analysis tools, often\noutperforming the conventional techniques used in complex microfluidic systems.\nOne fundamental analysis frequently desired in microfluidic experiments is\ncounting and tracking the droplets. Specifically, droplet tracking in dense\nemulsions is challenging as droplets move in tightly packed configurations.\nSometimes the individual droplets in these dense clusters are hard to resolve,\neven for a human observer. Here, two deep learning-based cutting-edge\nalgorithms for object detection (YOLO) and object tracking (DeepSORT) are\ncombined into a single image analysis tool, DropTrack, to track droplets in\nmicrofluidic experiments. DropTrack analyzes input videos, extracts droplets'\ntrajectories, and infers other observables of interest, such as droplet\nnumbers. Training an object detector network for droplet recognition with\nmanually annotated images is a labor-intensive task and a persistent\nbottleneck. This work partly resolves this problem by training object detector\nnetworks (YOLOv5) with hybrid datasets containing real and synthetic images. We\npresent an analysis of a double emulsion experiment as a case study to measure\nDropTrack's performance. For our test case, the YOLO networks trained with 60%\nsynthetic images show similar performance in droplet counting as with the one\ntrained using 100% real images, meanwhile saving the image annotation work by\n60%. DropTrack's performance is measured in terms of mean average precision\n(mAP), mean square error in counting the droplets, and inference speed. The\nfastest configuration of DropTrack runs inference at about 30 frames per\nsecond, well within the standards for real-time image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durve_M/0/1/0/all/0/1\">Mihir Durve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiribocchi_A/0/1/0/all/0/1\">Adriano Tiribocchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonaccorso_F/0/1/0/all/0/1\">Fabio Bonaccorso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montessori_A/0/1/0/all/0/1\">Andrea Montessori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauricella_M/0/1/0/all/0/1\">Marco Lauricella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdan_M/0/1/0/all/0/1\">Michal Bogdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzowski_J/0/1/0/all/0/1\">Jan Guzowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Succi_S/0/1/0/all/0/1\">Sauro Succi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intra and Cross-spectrum Iris Presentation Attack Detection in the NIR and Visible Domains Using Attention-based and Pixel-wise Supervised Learning. (arXiv:2205.02573v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02573","description":"<p>Iris Presentation Attack Detection (PAD) is essential to secure iris\nrecognition systems. Recent iris PAD solutions achieved good performance by\nleveraging deep learning techniques. However, most results were reported under\nintra-database scenarios and it is unclear if such solutions can generalize\nwell across databases and capture spectra. These PAD methods run the risk of\noverfitting because of the binary label supervision during the network\ntraining, which serves global information learning but weakens the capture of\nlocal discriminative features. This chapter presents a novel attention-based\ndeep pixel-wise binary supervision (A-PBS) method. A-PBS utilizes pixel-wise\nsupervision to capture the fine-grained pixel/patch-level cues and attention\nmechanism to guide the network to automatically find regions where most\ncontribute to an accurate PAD decision. Extensive experiments are performed on\nsix NIR and one visible-light iris databases to show the effectiveness and\nrobustness of proposed A-PBS methods. We additionally conduct extensive\nexperiments under intra-/cross-database and intra-/cross-spectrum for detailed\nanalysis. The results of our experiments indicates the generalizability of the\nA-PBS iris PAD approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Approach to Measure Sample-level Adversarial Vulnerability and its Utility in Building Trustworthy Systems. (arXiv:2205.02604v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02604","description":"<p>Adversarial attack perturbs an image with an imperceptible noise, leading to\nincorrect model prediction. Recently, a few works showed inherent bias\nassociated with such attack (robustness bias), where certain subgroups in a\ndataset (e.g. based on class, gender, etc.) are less robust than others. This\nbias not only persists even after adversarial training, but often results in\nsevere performance discrepancies across these subgroups. Existing works\ncharacterize the subgroup's robustness bias by only checking individual\nsample's proximity to the decision boundary. In this work, we argue that this\nmeasure alone is not sufficient and validate our argument via extensive\nexperimental analysis. It has been observed that adversarial attacks often\ncorrupt the high-frequency components of the input image. We, therefore,\npropose a holistic approach for quantifying adversarial vulnerability of a\nsample by combining these different perspectives, i.e., degree of model's\nreliance on high-frequency features and the (conventional) sample-distance to\nthe decision boundary. We demonstrate that by reliably estimating adversarial\nvulnerability at the sample level using the proposed holistic metric, it is\npossible to develop a trustworthy system where humans can be alerted about the\nincoming samples that are highly likely to be misclassified at test time. This\nis achieved with better precision when our holistic metric is used over\nindividual measures. To further corroborate the utility of the proposed\nholistic approach, we perform knowledge distillation in a limited-sample\nsetting. We observe that the student network trained with the subset of samples\nselected using our combined metric performs better than both the competing\nbaselines, viz., where samples are selected randomly or based on their\ndistances to the decision boundary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1\">Ruchit Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1\">Rohit Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Himanshu Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANimator: Neural Motion Synthesis from a Single Sequence. (arXiv:2205.02625v1 [cs.GR])","link":"http://arxiv.org/abs/2205.02625","description":"<p>We present GANimator, a generative model that learns to synthesize novel\nmotions from a single, short motion sequence. GANimator generates motions that\nresemble the core elements of the original motion, while simultaneously\nsynthesizing novel and diverse movements. Existing data-driven techniques for\nmotion synthesis require a large motion dataset which contains the desired and\nspecific skeletal structure. By contrast, GANimator only requires training on a\nsingle motion sequence, enabling novel motion synthesis for a variety of\nskeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework\ncontains a series of generative and adversarial neural networks, each\nresponsible for generating motions in a specific frame rate. The framework\nprogressively learns to synthesize motion from random noise, enabling\nhierarchical control over the generated motion content across varying levels of\ndetail. We show a number of applications, including crowd simulation, key-frame\nediting, style transfer, and interactive control, which all learn from a single\ninput sequence. Code and data for this paper are at\nhttps://peizhuoli.github.io/ganimator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peizhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImPosIng: Implicit Pose Encoding for Efficient Camera Pose Estimation. (arXiv:2205.02638v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02638","description":"<p>We propose a novel learning-based formulation for camera pose estimation that\ncan perform relocalization accurately and in real-time in city-scale\nenvironments. Camera pose estimation algorithms determine the position and\norientation from which an image has been captured, using a set of\ngeo-referenced images or 3D scene representation. Our new localization\nparadigm, named Implicit Pose Encoding (ImPosing), embeds images and camera\nposes into a common latent representation with 2 separate neural networks, such\nthat we can compute a similarity score for each image-pose pair. By evaluating\ncandidates through the latent space in a hierarchical manner, the camera\nposition and orientation are not directly regressed but incrementally refined.\nCompared to the representation used in structure-based relocalization methods,\nour implicit map is memory bounded and can be properly explored to improve\nlocalization performances against learning-based regression approaches. In this\npaper, we describe how to effectively optimize our learned modules, how to\ncombine them to achieve real-time localization, and demonstrate results on\ndiverse large scale scenarios that significantly outperform prior work in\naccuracy and computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreau_A/0/1/0/all/0/1\">Arthur Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1\">Nathan Piasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortelle_A/0/1/0/all/0/1\">Arnaud de La Fortelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02655","description":"<p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Detection on Technical Drawings for the Digitization of Brown-field Processes. (arXiv:2205.02659v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02659","description":"<p>This paper addresses the issue of autonomously detecting text on technical\ndrawings. The detection of text on technical drawings is a critical step\ntowards autonomous production machines, especially for brown-field processes,\nwhere no closed CAD-CAM solutions are available yet. Automating the process of\nreading and detecting text on technical drawings reduces the effort for\nhandling inefficient media interruptions due to paper-based processes, which\nare often todays quasi-standard in brown-field processes. However, there are no\nreliable methods available yet to solve the issue of automatically detecting\ntext on technical drawings. The unreliable detection of the contents on\ntechnical drawings using classical detection and object character recognition\n(OCR) tools is mainly due to the limited number of technical drawings and the\ncaptcha-like structure of the contents. Text is often combined with unknown\nsymbols and interruptions by lines. Additionally, due to intellectual property\nrights and technical know-how issues, there are no out-of-the box training\ndatasets available in the literature to train such models. This paper combines\na domain knowledge-based generator to generate realistic technical drawings\nwith a state-of-the-art object detection model to solve the issue of detecting\ntext on technical drawings. The generator yields artificial technical drawings\nin a large variety and can be considered as a data augmentation generator.\nThese artificial drawings are used for training, while the model is tested on\nreal data. The authors show that artificially generated data of technical\ndrawings improve the detection quality with an increasing number of drawings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netzer_M/0/1/0/all/0/1\">Markus Netzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillinger_J/0/1/0/all/0/1\">Jan Hillinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2205.02671v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02671","description":"<p>Understanding spatial relations is essential for intelligent agents to act\nand communicate in the physical world. Relative directions are spatial\nrelations that describe the relative positions of target objects with regard to\nthe intrinsic orientation of reference objects. Grounding relative directions\nis more difficult than grounding absolute directions because it not only\nrequires a model to detect objects in the image and to identify spatial\nrelation based on this information, but it also needs to recognize the\norientation of objects and integrate this information into the reasoning\nprocess. We investigate the challenging problem of grounding relative\ndirections with end-to-end neural networks. To this end, we provide GRiD-3D, a\nnovel dataset that features relative directions and complements existing visual\nquestion answering (VQA) datasets, such as CLEVR, that involve only absolute\ndirections. We also provide baselines for the dataset with two established\nend-to-end VQA models. Experimental evaluations show that answering questions\non relative directions is feasible when questions in the dataset simulate the\nnecessary subtasks for grounding relative directions. We discover that those\nsubtasks are learned in an order that reflects the steps of an intuitive\npipeline for processing relative directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1\">Kyra Ahrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware System Implementation for Human Detection using HOG and SVM Algorithm. (arXiv:2205.02689v1 [cs.AR])","link":"http://arxiv.org/abs/2205.02689","description":"<p>Human detection is a popular issue and has been widely used in many\napplications. However, including complexities in computation, leading to the\nhuman detection system implemented hardly in real-time applications. This paper\npresents the architecture of hardware, a human detection system that was\nsimulated in the ModelSim tool. As a co-processor, this system was built to\noff-load to Central Processor Unit (CPU) and speed up the computation timing.\nThe 130x66 RGB pixels of static input image attracted features and classify by\nusing the Histogram of Oriented Gradient (HOG) algorithm and Support Vector\nMachine (SVM) algorithm, respectively. As a result, the accuracy rate of this\nsystem reaches 84.35 percent. And the timing for detection decreases to 0.757\nms at 50MHz frequency (54 times faster when this system was implemented in\nsoftware by using the Matlab tool).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van-Cam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hong-Tuan-Dinh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_H/0/1/0/all/0/1\">Huu-Thuan Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Batch Artifact Scanning Protocol: A new method using computed tomography (CT) to rapidly create three-dimensional models of objects from large collections en masse. (arXiv:2205.02691v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02691","description":"<p>Within anthropology, the use of three-dimensional (3D) imaging has become\nincreasingly standard and widespread since it broadens the available avenues\nfor addressing a wide range of key issues. The ease with which 3D models can be\nshared has had major impacts for research, cultural heritage, education,\nscience communication, and public engagement, as well as contributing to the\npreservation of the physical specimens and archiving collections in widely\naccessible data bases. Current scanning protocols have the ability to create\nthe required research quality 3D models; however, they tend to be time and\nlabor intensive and not practical when working with large collections. Here we\ndescribe a streamlined, Batch Artifact Scanning Protocol we have developed to\nrapidly create 3D models using a medical CT scanner. Though this method can be\nused on a variety of material types, we use a large collection of\nexperimentally broken ungulate limb bones. Using the Batch Artifact Scanning\nProtocol, we were able to efficiently create 3D models of 2,474 bone fragments\nat a rate of less than $3$ minutes per specimen, as opposed to an average of 50\nminutes per specimen using structured light scanning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_Woodley_K/0/1/0/all/0/1\">Katrina Yezzi-Woodley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1\">Jeff Calder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweno_M/0/1/0/all/0/1\">Mckenzie Sweno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siewert_C/0/1/0/all/0/1\">Chloe Siewert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olver_P/0/1/0/all/0/1\">Peter J. Olver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gait Recognition in the Wild: A Benchmark. (arXiv:2205.02692v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02692","description":"<p>Gait benchmarks empower the research community to train and evaluate\nhigh-performance gait recognition systems. Even though growing efforts have\nbeen devoted to cross-view recognition, academia is restricted by current\nexisting databases captured in the controlled environment. In this paper, we\ncontribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW\ndataset is constructed from natural videos, which contains hundreds of cameras\nand thousands of hours streams in open systems. With tremendous manual\nannotations, the GREW consists of 26K identities and 128K sequences with rich\nattributes for unconstrained gait recognition. Moreover, we add a distractor\nset of over 233K sequences, making it more suitable for real-world\napplications. Compared with prevailing predefined cross-view datasets, the GREW\nhas diverse and practical view variations, as well as more natural challenging\nfactors. To the best of our knowledge, this is the first large-scale dataset\nfor gait recognition in the wild. Equipped with this benchmark, we dissect the\nunconstrained gait recognition problem. Representative appearance-based and\nmodel-based methods are explored, and comprehensive baselines are established.\nExperimental results show (1) The proposed GREW benchmark is necessary for\ntraining and evaluating gait recognizer in the wild. (2) For state-of-the-art\ngait recognition approaches, there is a lot of room for improvement. (3) The\nGREW benchmark can be used as effective pre-training for controlled gait\nrecognition. Benchmark website is https://www.grew-benchmark.org/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xianda Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Different Deep Metric Learning Losses Lead to Similar Learned Features?. (arXiv:2205.02698v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02698","description":"<p>Recent studies have shown that many deep metric learning loss functions\nperform very similarly under the same experimental conditions. One potential\nreason for this unexpected result is that all losses let the network focus on\nsimilar image regions or properties. In this paper, we investigate this by\nconducting a two-step analysis to extract and compare the learned visual\nfeatures of the same model architecture trained with different loss functions:\nFirst, we compare the learned features on the pixel level by correlating\nsaliency maps of the same input images. Second, we compare the clustering of\nembeddings for several image properties, e.g. object color or illumination. To\nprovide independent control over these properties, photo-realistic 3D car\nrenders similar to images in the Cars196 dataset are generated. In our\nanalysis, we compare 14 pretrained models from a recent study and find that,\neven though all models perform similarly, different loss functions can guide\nthe model to learn different features. We especially find differences between\nclassification and ranking based losses. Our analysis also shows that some\nseemingly irrelevant properties can have significant influence on the resulting\nembedding. We encourage researchers from the deep metric learning community to\nuse our methods to get insights into the features learned by their proposed\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobs_K/0/1/0/all/0/1\">Konstantin Kobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steininger_M/0/1/0/all/0/1\">Michael Steininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulny_A/0/1/0/all/0/1\">Andrzej Dulny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid CNN Based Attention with Category Prior for User Image Behavior Modeling. (arXiv:2205.02711v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02711","description":"<p>User historical behaviors are proved useful for Click Through Rate (CTR)\nprediction in online advertising system. In Meituan, one of the largest\ne-commerce platform in China, an item is typically displayed with its image and\nwhether a user clicks the item or not is usually influenced by its image, which\nimplies that user's image behaviors are helpful for understanding user's visual\npreference and improving the accuracy of CTR prediction. Existing user image\nbehavior models typically use a two-stage architecture, which extracts visual\nembeddings of images through off-the-shelf Convolutional Neural Networks (CNNs)\nin the first stage, and then jointly trains a CTR model with those visual\nembeddings and non-visual features. We find that the two-stage architecture is\nsub-optimal for CTR prediction. Meanwhile, precisely labeled categories in\nonline ad systems contain abundant visual prior information, which can enhance\nthe modeling of user image behaviors. However, off-the-shelf CNNs without\ncategory prior may extract category unrelated features, limiting CNN's\nexpression ability. To address the two issues, we propose a hybrid CNN based\nattention module, unifying user's image behaviors and category prior, for CTR\nprediction. Our approach achieves significant improvements in both online and\noffline experiments on a billion scale real serving dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qingtao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shihang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jun Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Rendering in a Room: Amodal 3D Understanding and Free-Viewpoint Rendering for the Closed Scene Composed of Pre-Captured Objects. (arXiv:2205.02714v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02714","description":"<p>We, as human beings, can understand and picture a familiar scene from\narbitrary viewpoints given a single image, whereas this is still a grand\nchallenge for computers. We hereby present a novel solution to mimic such human\nperception capability based on a new paradigm of amodal 3D scene understanding\nwith neural rendering for a closed scene. Specifically, we first learn the\nprior knowledge of the objects in a closed scene via an offline stage, which\nfacilitates an online stage to understand the room with unseen furniture\narrangement. During the online stage, given a panoramic image of the scene in\ndifferent layouts, we utilize a holistic neural-rendering-based optimization\nframework to efficiently estimate the correct 3D scene layout and deliver\nrealistic free-viewpoint rendering. In order to handle the domain gap between\nthe offline and online stage, our method exploits compositional neural\nrendering techniques for data augmentation in the offline training. The\nexperiments on both synthetic and real datasets demonstrate that our two-stage\ndesign achieves robust 3D scene understanding and outperforms competing methods\nby a large margin, and we also show that our realistic free-viewpoint rendering\nenables various applications, including scene touring and editing. Code and\ndata are available on the project webpage:\nhttps://zju3dv.github.io/nr_in_a_room/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bangbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1\">Sean Fanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection. (arXiv:2205.02717v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02717","description":"<p>Temporal action detection (TAD) is extensively studied in the video\nunderstanding community by following the object detection pipelines in images.\nHowever, complex designs are not uncommon in TAD, such as two-stream feature\nextraction, multi-stage training, complex temporal modeling, and global context\nfusion. In this paper, we do not aim to introduce any novel technique for TAD.\nInstead, we study a simple, straightforward, yet must-known baseline given the\ncurrent status of complex design and low efficiency in TAD. In our simple\nbaseline (BasicTAD), we decompose the TAD pipeline into several essential\ncomponents: data sampling, backbone design, neck construction, and detection\nhead. We empirically investigate the existing techniques in each component for\nthis baseline and, more importantly, perform end-to-end training over the\nentire pipeline thanks to the simplicity in design. Our BasicTAD yields an\nastounding RGB-Only baseline very close to the state-of-the-art methods with\ntwo-stream inputs. In addition, we further improve the BasicTAD by preserving\nmore temporal and spatial information in network representation (termed as\nBasicTAD Plus). Empirical results demonstrate that our BasicTAD Plus is very\nefficient and significantly outperforms the previous methods on the datasets of\nTHUMOS14 and FineAction. Our approach can serve as a strong baseline for TAD.\nThe code will be released at https://github.com/MCG-NJU/BasicTAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yin-Dong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Koopman pose predictions for temporally consistent human walking estimations. (arXiv:2205.02737v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02737","description":"<p>We tackle the problem of tracking the human lower body as an initial step\ntoward an automatic motion assessment system for clinical mobility evaluation,\nusing a multimodal system that combines Inertial Measurement Unit (IMU) data,\nRGB images, and point cloud depth measurements. This system applies the factor\ngraph representation to an optimization problem that provides 3-D skeleton\njoint estimations. In this paper, we focus on improving the temporal\nconsistency of the estimated human trajectories to greatly extend the range of\noperability of the depth sensor. More specifically, we introduce a new factor\ngraph factor based on Koopman theory that embeds the nonlinear dynamics of\nseveral lower-limb movement activities. This factor performs a two-step\nprocess: first, a custom activity recognition module based on spatial temporal\ngraph convolutional networks recognizes the walking activity; then, a Koopman\npose prediction of the subsequent skeleton is used as an a priori estimation to\ndrive the optimization problem toward more consistent results. We tested the\nperformance of this module on datasets composed of multiple clinical lowerlimb\nmobility tests, and we show that our approach reduces outliers on the skeleton\nform by almost 1 m, while preserving natural walking trajectories at depths up\nto more than 10 m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitjans_M/0/1/0/all/0/1\">Marc Mitjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_D/0/1/0/all/0/1\">David M. Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_L/0/1/0/all/0/1\">Louis N. Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tron_R/0/1/0/all/0/1\">Roberto Tron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activity Detection in Long Surgical Videos using Spatio-Temporal Models. (arXiv:2205.02805v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02805","description":"<p>Automatic activity detection is an important component for developing\ntechnologies that enable next generation surgical devices and workflow\nmonitoring systems. In many application, the videos of interest are long and\ninclude several activities; hence, the deep models designed for such purposes\nconsist of a backbone and a temporal sequence modeling architecture. In this\npaper, we investigate both the state-of-the-art activity recognition and\ntemporal models to find the architectures that yield the highest performance.\nWe first benchmark these models on a large-scale activity recognition dataset\nin the operating room with over 800 full-length surgical videos. However, since\nmost other medical applications lack such a large dataset, we further evaluate\nour models on the Cholec80 surgical phase segmentation dataset, consisting of\nonly 40 training videos. For backbone architectures, we investigate both 3D\nConvNets and most recent transformer-based models; for temporal modeling, we\ninclude temporal ConvNets, RNNs, and transformer models for a comprehensive and\nthorough study. We show that even in the case of limited labeled data, we can\noutperform the existing work by benefiting from models pre-trained on other\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharghi_A/0/1/0/all/0/1\">Aidean Sharghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zooey He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1\">Omid Mohareri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Octree Graph Networks for Learning Adaptive Volumetric Shape Representations. (arXiv:2205.02825v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02825","description":"<p>We present an adaptive deep representation of volumetric fields of 3D shapes\nand an efficient approach to learn this deep representation for high-quality 3D\nshape reconstruction and auto-encoding. Our method encodes the volumetric field\nof a 3D shape with an adaptive feature volume organized by an octree and\napplies a compact multilayer perceptron network for mapping the features to the\nfield value at each 3D position. An encoder-decoder network is designed to\nlearn the adaptive feature volume based on the graph convolutions over the dual\ngraph of octree nodes. The core of our network is a new graph convolution\noperator defined over a regular grid of features fused from irregular\nneighboring octree nodes at different levels, which not only reduces the\ncomputational and memory cost of the convolutions over irregular neighboring\noctree nodes, but also improves the performance of feature learning. Our method\neffectively encodes shape details, enables fast 3D shape reconstruction, and\nexhibits good generality for modeling 3D shapes out of training categories. We\nevaluate our method on a set of reconstruction tasks of 3D shapes and scenes\nand validate its superiority over other existing approaches. Our code, data,\nand trained models are available at https://wang-ps.github.io/dualocnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually plausible human-object interaction capture from wearable sensors. (arXiv:2205.02830v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02830","description":"<p>In everyday lives, humans naturally modify the surrounding environment\nthrough interactions, e.g., moving a chair to sit on it. To reproduce such\ninteractions in virtual spaces (e.g., metaverse), we need to be able to capture\nand model them, including changes in the scene geometry, ideally from\nego-centric input alone (head camera and body-worn inertial sensors). This is\nan extremely hard problem, especially since the object/scene might not be\nvisible from the head camera (e.g., a human not looking at a chair while\nsitting down, or not looking at the door handle while opening a door). In this\npaper, we present HOPS, the first method to capture interactions such as\ndragging objects and opening doors from ego-centric data alone. Central to our\nmethod is reasoning about human-object interactions, allowing to track objects\neven when they are not visible from the head camera. HOPS localizes and\nregisters both the human and the dynamic object in a pre-scanned static scene.\nHOPS is an important first step towards advanced AR/VR applications based on\nimmersive virtual universes, and can provide human-centric training data to\nteach machines to interact with their surroundings. The supplementary video,\ndata, and code will be available on our project page at\n<a href=\"http://virtualhumans.mpi-inf.mpg.de/hops/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guzov_V/0/1/0/all/0/1\">Vladimir Guzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-view Transformers for real-time Map-view Semantic Segmentation. (arXiv:2205.02833v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02833","description":"<p>We present cross-view transformers, an efficient attention-based model for\nmap-view semantic segmentation from multiple cameras. Our architecture\nimplicitly learns a mapping from individual camera views into a canonical\nmap-view representation using a camera-aware cross-view attention mechanism.\nEach camera uses positional embeddings that depend on its intrinsic and\nextrinsic calibration. These embeddings allow a transformer to learn the\nmapping across different views without ever explicitly modeling it\ngeometrically. The architecture consists of a convolutional image encoder for\neach view and cross-view transformer layers to infer a map-view semantic\nsegmentation. Our model is simple, easily parallelizable, and runs in\nreal-time. The presented architecture performs at state-of-the-art on the\nnuScenes dataset, with 4x faster inference speeds. Code is available at\nhttps://github.com/bradyz/cross_view_transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Brady Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction. (arXiv:2205.02834v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02834","description":"<p>This paper studies the problem of fixing malfunctional 3D objects. While\nprevious works focus on building passive perception models to learn the\nfunctionality from static 3D objects, we argue that functionality is reckoned\nwith respect to the physical interactions between the object and the user.\nGiven a malfunctional object, humans can perform mental simulations to reason\nabout its functionality and figure out how to fix it. Inspired by this, we\npropose FixIt, a dataset that contains about 5k poorly-designed 3D physical\nobjects paired with choices to fix them. To mimic humans' mental simulation\nprocess, we present FixNet, a novel framework that seamlessly incorporates\nperception and physical dynamics. Specifically, FixNet consists of a perception\nmodule to extract the structured representation from the 3D point cloud, a\nphysical dynamics prediction module to simulate the results of interactions on\n3D objects, and a functionality prediction module to evaluate the functionality\nand choose the correct fix. Experimental results show that our framework\noutperforms baseline models by a large margin, and can generalize well to\nobjects with similar interaction types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics. (arXiv:2205.02835v1 [cs.RO])","link":"http://arxiv.org/abs/2205.02835","description":"<p>Differentiable physics has recently been shown as a powerful tool for solving\nsoft-body manipulation tasks. However, the differentiable physics solver often\ngets stuck when the initial contact points of the end effectors are sub-optimal\nor when performing multi-stage tasks that require contact point switching,\nwhich often leads to local minima. To address this challenge, we propose a\ncontact point discovery approach (CPDeform) that guides the stand-alone\ndifferentiable physics solver to deform various soft-body plasticines. The key\nidea of our approach is to integrate optimal transport-based contact points\ndiscovery into the differentiable physics solver to overcome the local minima\nfrom initial contact points or contact switching. On single-stage tasks, our\nmethod can automatically find suitable initial contact points based on\ntransport priorities. On complex multi-stage tasks, we can iteratively switch\nthe contact points of end-effectors based on transport priorities. To evaluate\nthe effectiveness of our method, we introduce PlasticineLab-M that extends the\nexisting differentiable physics benchmark PlasticineLab to seven new\nchallenging multi-stage soft-body manipulation tasks. Extensive experimental\nresults suggest that: 1) on multi-stage tasks that are infeasible for the\nvanilla differentiable physics solver, our approach discovers contact points\nthat efficiently guide the solver to completion; 2) on tasks where the vanilla\nsolver performs sub-optimally or near-optimally, our contact point discovery\nmethod performs better than or on par with the manipulation performance\nobtained with handcrafted contact points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural 3D Scene Reconstruction with the Manhattan-world Assumption. (arXiv:2205.02836v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02836","description":"<p>This paper addresses the challenge of reconstructing 3D indoor scenes from\nmulti-view images. Many previous works have shown impressive reconstruction\nresults on textured objects, but they still have difficulty in handling\nlow-textured planar regions, which are common in indoor scenes. An approach to\nsolving this issue is to incorporate planer constraints into the depth map\nestimation in multi-view stereo-based methods, but the per-view plane\nestimation and depth optimization lack both efficiency and multi-view\nconsistency. In this work, we show that the planar constraints can be\nconveniently integrated into the recent implicit neural representation-based\nreconstruction methods. Specifically, we use an MLP network to represent the\nsigned distance function as the scene geometry. Based on the Manhattan-world\nassumption, planar constraints are employed to regularize the geometry in floor\nand wall regions predicted by a 2D semantic segmentation network. To resolve\nthe inaccurate segmentation, we encode the semantics of 3D points with another\nMLP and design a novel loss that jointly optimizes the scene geometry and\nsemantics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that\nthe proposed method outperforms previous methods by a large margin on 3D\nreconstruction quality. The code is available at\nhttps://zju3dv.github.io/manhattan_sdf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haotong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlobGAN: Spatially Disentangled Scene Representations. (arXiv:2205.02837v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02837","description":"<p>We propose an unsupervised, mid-level representation for a generative model\nof scenes. The representation is mid-level in that it is neither per-pixel nor\nper-image; rather, scenes are modeled as a collection of spatial, depth-ordered\n\"blobs\" of features. Blobs are differentiably placed onto a feature grid that\nis decoded into an image by a generative adversarial network. Due to the\nspatial uniformity of blobs and the locality inherent to convolution, our\nnetwork learns to associate different blobs with different entities in a scene\nand to arrange these blobs to capture scene layout. We demonstrate this\nemergent behavior by showing that, despite training without any supervision,\nour method enables applications such as easy manipulation of objects within a\nscene (e.g., moving, removing, and restyling furniture), creation of feasible\nscenes given constraints (e.g., plausible rooms with drawers at a particular\nlocation), and parsing of real-world images into constituent parts. On a\nchallenging multi-category dataset of indoor scenes, BlobGAN outperforms\nStyleGAN2 in image quality as measured by FID. See our project page for video\nresults and interactive demo: <a href=\"http://www.dave.ml/blobgan\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taesung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/0901.3590","description":"<p>We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanxi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID. (arXiv:2003.06650v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06650","description":"<p>Unsupervised domain adaptation (UDA) aims at adapting the model trained on a\nlabeled source-domain dataset to an unlabeled target-domain dataset. The task\nof UDA on open-set person re-identification (re-ID) is even more challenging as\nthe identities (classes) do not have overlap between the two domains. One major\nresearch direction was based on domain translation, which, however, has fallen\nout of favor in recent years due to inferior performance compared to\npseudo-label-based methods. We argue that the domain translation has great\npotential on exploiting the valuable source-domain data but existing methods\ndid not provide proper regularization on the translation process. Specifically,\nprevious methods only focus on maintaining the identities of the translated\nimages while ignoring the inter-sample relations during translation. To tackle\nthe challenges, we propose an end-to-end structured domain adaptation framework\nwith an online relation-consistency regularization term. During training, the\nperson feature encoder is optimized to model inter-sample relations on-the-fly\nfor supervising relation-consistency domain translation, which in turn,\nimproves the encoder with informative translated images. The encoder can be\nfurther improved with pseudo labels, where the source-to-target translated\nimages with ground-truth identities and target-domain images with pseudo\nidentities are jointly used for training. In the experiments, our proposed\nframework is shown to achieve state-of-the-art performance on multiple UDA\ntasks of person re-ID. With the synthetic-to-real translated images from our\nstructured domain-translation network, we achieved second place in the Visual\nDomain Adaptation Challenge (VisDA) in 2020.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2007.13083v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.13083","description":"<p>Semantic segmentation of remotely sensed images plays an important role in\nland resource management, yield estimation, and economic assessment. U-Net, a\ndeep encoder-decoder architecture, has been used frequently for image\nsegmentation with high accuracy. In this Letter, we incorporate multi-scale\nfeatures generated by different layers of U-Net and design a multi-scale skip\nconnected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation\nusing fine-resolution remotely sensed images. Our design has the following\nadvantages: (1) The multi-scale skip connections combine and realign semantic\nfeatures contained in both low-level and high-level feature maps; (2) the\nasymmetric convolution block strengthens the feature representation and feature\nextraction capability of a standard convolution layer. Experiments conducted on\ntwo remotely sensed datasets captured by different satellite sensors\ndemonstrate that the proposed MACU-Net transcends the U-Net, U-NetPPL, U-Net\n3+, amongst other benchmark approaches. Code is available at\nhttps://github.com/lironui/MACU-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pneumonia Detection on Chest X-ray using Radiomic Features and Contrastive Learning. (arXiv:2101.04269v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04269","description":"<p>Chest X-ray becomes one of the most common medical diagnoses due to its\nnoninvasiveness. The number of chest X-ray images has skyrocketed, but reading\nchest X-rays still have been manually performed by radiologists, which creates\nhuge burnouts and delays. Traditionally, radiomics, as a subfield of radiology\nthat can extract a large number of quantitative features from medical images,\ndemonstrates its potential to facilitate medical imaging diagnosis before the\ndeep learning era. With the rise of deep learning, the explainability of deep\nneural networks on chest X-ray diagnosis remains opaque. In this study, we\nproposed a novel framework that leverages radiomics features and contrastive\nlearning to detect pneumonia in chest X-ray. Experiments on the RSNA Pneumonia\nDetection Challenge dataset show that our model achieves superior results to\nseveral state-of-the-art models (&gt; 10% in F1-score) and increases the model's\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed H Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-rays with Radiomics using a Feedback Loop. (arXiv:2104.04968v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04968","description":"<p>Building a highly accurate predictive model for classification and\nlocalization of abnormalities in chest X-rays usually requires a large number\nof manually annotated labels and pixel regions (bounding boxes) of\nabnormalities. However, it is expensive to acquire such annotations, especially\nthe bounding boxes. Recently, contrastive learning has shown strong promise in\nleveraging unlabeled natural images to produce highly generalizable and\ndiscriminative features. However, extending its power to the medical image\ndomain is under-explored and highly non-trivial, since medical images are much\nless amendable to data augmentations. In contrast, their prior knowledge, as\nwell as radiomic features, is often crucial. To bridge this gap, we propose an\nend-to-end semi-supervised knowledge-augmented contrastive learning framework,\nthat simultaneously performs disease classification and localization tasks. The\nkey knob of our framework is a unique positive sampling approach tailored for\nthe medical images, by seamlessly integrating radiomic features as a knowledge\naugmentation. Specifically, we first apply an image encoder to classify the\nchest X-rays and to generate the image features. We next leverage Grad-CAM to\nhighlight the crucial (abnormal) regions for chest X-rays (even when\nunannotated), from which we extract radiomic features. The radiomic features\nare then passed through another dedicated encoder to act as the positive sample\nfor the image features generated from the same chest X-ray. In this way, our\nframework constitutes a feedback loop for image and radiomic modality features\nto mutually reinforce each other. Their contrasting yields knowledge-augmented\nrepresentations that are both robust and interpretable. Extensive experiments\non the NIH Chest X-ray dataset demonstrate that our approach outperforms\nexisting baselines in both classification and localization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glicksberg_B/0/1/0/all/0/1\">Benjamin Glicksberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Green View Index and Green View Index best path using Google Street View and deep learning. (arXiv:2104.12627v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12627","description":"<p>As an important part of the urban landscape research, analysing and studying\nstreetscape can increase the understanding of the cities' infrastructure, which\ncontributes to better planning and design of the urban living environment. In\nthis paper, we used Google Street View to obtain street view images of Osaka\nCity. The semantic segmentation model is adopted to segment the Osaka City\nstreet view images and analyse the Green View Index (GVI). Based on the GVI\nvalue, we take advantage of adjacency matrix and Floyd algorithm is used to\ncalculate Green View Index best path, solving the limitations of ArcGIS\nsoftware. Our analysis not only allows the calculation of specific routes for\nthe GVI best paths, but also realize the visualization and integration of\nneighbourhood landscape. By summarising all the data, a more specific and\nobjective analysis of the landscape in the study area can be carried out, and\nbased on this, the available natural resources can be maximized for a better\nlife.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anqi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding. (arXiv:2106.10634v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10634","description":"<p>We propose an effective two-stage approach to tackle the problem of\nlanguage-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In\nthe first stage, we propose an Augmented 2D Temporal Adjacent Network\n(Augmented 2D-TAN) to temporally ground the target moment corresponding to the\ngiven description. Primarily, we improve the original 2D-TAN from two aspects:\nFirst, a temporal context-aware Bi-LSTM Aggregation Module is developed to\naggregate clip-level representations, replacing the original max-pooling.\nSecond, we propose to employ Random Concatenation Augmentation (RCA) mechanism\nduring the training phase. In the second stage, we use pretrained MDETR model\nto generate per-frame bounding boxes via language query, and design a set of\nhand-crafted rules to select the best matching bounding box outputted by MDETR\nfor each frame within the grounded moment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaolei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian-Fang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08621","description":"<p>In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before &amp; after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02671","description":"<p>Real-world perception systems in many cases build on hardware with limited\nresources to adhere to cost and power limitations of their carrying system.\nDeploying deep neural networks on resource-constrained hardware became possible\nwith model compression techniques, as well as efficient and hardware-aware\narchitecture design. However, model adaptation is additionally required due to\nthe diverse operation environments. In this work, we address the problem of\ntraining deep neural networks on resource-constrained hardware in the context\nof visual domain adaptation. We select the task of monocular depth estimation\nwhere our goal is to transform a pre-trained model to the target's domain data.\nWhile the source domain includes labels, we assume an unlabelled target domain,\nas it happens in real-world applications. Then, we present an adversarial\nlearning approach that is adapted for training on the device with limited\nresources. Since visual domain adaptation, i.e. neural network training, has\nnot been previously explored for resource-constrained hardware, we present the\nfirst feasibility study for image-based depth estimation. Our experiments show\nthat visual domain adaptation is relevant only for efficient network\narchitectures and training sets at the order of a few hundred samples. Models\nand code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1\">Julia Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1\">Lazaros Nalpantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Neighborhood Deep Fusion Network for Point Cloud Analysis. (arXiv:2108.09228v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09228","description":"<p>Recently, deep neural networks have made remarkable achievements in 3D point\ncloud classification. However, existing classification methods are mainly\nimplemented on idealized point clouds and suffer heavy degradation of\nper-formance on non-idealized scenarios. To handle this prob-lem, a feature\nrepresentation learning method, named Dual-Neighborhood Deep Fusion Network\n(DNDFN), is proposed to serve as an improved point cloud encoder for the task\nof non-idealized point cloud classification. DNDFN utilizes a trainable\nneighborhood learning method called TN-Learning to capture the global key\nneighborhood. Then, the global neighborhood is fused with the local\nneighbor-hood to help the network achieve more powerful reasoning ability.\nBesides, an Information Transfer Convolution (IT-Conv) is proposed for DNDFN to\nlearn the edge infor-mation between point-pairs and benefits the feature\ntransfer procedure. The transmission of information in IT-Conv is similar to\nthe propagation of information in the graph which makes DNDFN closer to the\nhuman reasoning mode. Extensive experiments on existing benchmarks especially\nnon-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the\nstate of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovery-and-Selection: Towards Optimal Multiple Instance Learning for Weakly Supervised Object Detection. (arXiv:2110.09060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09060","description":"<p>Weakly supervised object detection (WSOD) is a challenging task that requires\nsimultaneously learn object classifiers and estimate object locations under the\nsupervision of image category labels. A major line of WSOD methods roots in\nmultiple instance learning which regards images as bags of instances and\nselects positive instances from each bag to learn the detector. However, a\ngrand challenge emerges when the detector inclines to converge to\ndiscriminative parts of objects rather than the whole objects. In this paper,\nunder the hypothesis that optimal solutions are included in local minima, we\npropose a discovery-and-selection approach fused with multiple instance\nlearning (DS-MIL), which finds rich local minima and select optimal solution\nfrom multiple local minima. To implement DS-MIL, an attention module is\nproposed so that more context information can be captured by feature maps and\nmore valuable proposals can be collected during training. With proposal\ncandidates, a selection module is proposed to select informative instances for\nobject detector. Experimental results on commonly used benchmarks show that our\nproposed DS-MIL approach can consistently improve the baselines, reporting\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleAlign: Analysis and Applications of Aligned StyleGAN Models. (arXiv:2110.11323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11323","description":"<p>In this paper, we perform an in-depth study of the properties and\napplications of aligned generative models. We refer to two models as aligned if\nthey share the same architecture, and one of them (the child) is obtained from\nthe other (the parent) via fine-tuning to another domain, a common practice in\ntransfer learning. Several works already utilize some basic properties of\naligned StyleGAN models to perform image-to-image translation. Here, we perform\nthe first detailed exploration of model alignment, also focusing on StyleGAN.\nFirst, we empirically analyze aligned models and provide answers to important\nquestions regarding their nature. In particular, we find that the child model's\nlatent spaces are semantically aligned with those of the parent, inheriting\nincredibly rich semantics, even for distant data domains such as human faces\nand churches. Second, equipped with this better understanding, we leverage\naligned models to solve a diverse set of tasks. In addition to image\ntranslation, we demonstrate fully automatic cross-domain image morphing. We\nfurther show that zero-shot vision tasks may be performed in the child domain,\nwhile relying exclusively on supervision in the parent domain. We demonstrate\nqualitatively and quantitatively that our approach yields state-of-the-art\nresults, while requiring only simple fine-tuning and inversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zongze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Rich Features for Gait Recognition by Integrating Skeletons and Silhouettes. (arXiv:2110.13408v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13408","description":"<p>Gait recognition captures gait patterns from the walking sequence of an\nindividual for identification. Most existing gait recognition methods learn\nfeatures from silhouettes or skeletons for the robustness to clothing,\ncarrying, and other exterior factors. The combination of the two data\nmodalities, however, is not fully exploited. Previous multimodal gait\nrecognition methods mainly employ the skeleton to assist the local feature\nextraction where the intrinsic discrimination of the skeleton data is ignored.\nThis paper proposes a simple yet effective Bimodal Fusion (BiFusion) network\nwhich mines discriminative gait patterns in skeletons and integrates with\nsilhouette representations to learn rich features for identification.\nParticularly, the inherent hierarchical semantics of body joints in a skeleton\nis leveraged to design a novel Multi-Scale Gait Graph (MSGG) network for the\nfeature extraction of skeletons. Extensive experiments on CASIA-B and OUMVLP\ndemonstrate both the superiority of the proposed MSGG network in modeling\nskeletons and the effectiveness of the bimodal fusion for gait recognition.\nUnder the most challenging condition of walking in different clothes on\nCASIA-B, our method achieves the rank-1 accuracy of 92.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yunjie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Perceptual Quality of 2D Animation Interpolation. (arXiv:2111.12792v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12792","description":"<p>Traditional 2D animation is labor-intensive, often requiring animators to\nmanually draw twelve illustrations per second of movement. While automatic\nframe interpolation may ease this burden, the artistic effects inherent to 2D\nanimation make video synthesis particularly challenging compared to in the\nphotorealistic domain. Lower framerates result in larger displacements and\nocclusions, discrete perceptual elements (e.g. lines and solid-color regions)\npose difficulties for texture-oriented convolutional networks, and exaggerated\nnonlinear movements hinder training data collection. Previous work tried\naddressing these issues, but used unscalable methods and focused on\npixel-perfect performance. In contrast, we build a scalable system more\nappropriately centered on perceptual quality for this artistic domain. Firstly,\nwe propose a lightweight architecture with a simple yet effective\nocclusion-inpainting technique to improve convergence on perceptual metrics\nwith fewer trainable parameters. Secondly, we design a novel auxiliary module\nthat leverages the Euclidean distance transform to improve the preservation of\nkey line and region structures. Thirdly, we automatically double the existing\nmanually-collected dataset for this task by quantitatively filtering out\nmovement nonlinearities, allowing us to improve model generalization. Finally,\nwe establish LPIPS and chamfer distance as strongly preferable to PSNR and SSIM\nthrough a user study, validating our system's emphasis on perceptual quality in\nthe 2D animation domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion. (arXiv:2111.14690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14690","description":"<p>A typical pipeline for multi-object tracking (MOT) is to use a detector for\nobject localization, and following re-identification (re-ID) for object\nassociation. This pipeline is partially motivated by recent progress in both\nobject detection and re-ID, and partially motivated by biases in existing\ntracking datasets, where most objects tend to have distinguishing appearance\nand re-ID models are sufficient for establishing associations. In response to\nsuch bias, we would like to re-emphasize that methods for multi-object tracking\nshould also work when object appearance is not sufficiently discriminative. To\nthis end, we propose a large-scale dataset for multi-human tracking, where\nhumans have similar appearance, diverse motion and extreme articulation. As the\ndataset contains mostly group dancing videos, we name it \"DanceTrack\". We\nexpect DanceTrack to provide a better platform to develop more MOT algorithms\nthat rely less on visual discrimination and depend more on motion analysis. We\nbenchmark several state-of-the-art trackers on our dataset and observe a\nsignificant performance drop on DanceTrack when compared against existing\nbenchmarks. The dataset, project code and competition server are released at:\n\\url{https://github.com/DanceTrack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jinkun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Large Vessel Occlusions using Deep Learning by Deforming Vessel Tree Segmentations. (arXiv:2112.01797v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.01797","description":"<p>Computed Tomography Angiography is a key modality providing insights into the\ncerebrovascular vessel tree that are crucial for the diagnosis and treatment of\nischemic strokes, in particular in cases of large vessel occlusions (LVO).\nThus, the clinical workflow greatly benefits from an automated detection of\npatients suffering from LVOs. This work uses convolutional neural networks for\ncase-level classification trained with elastic deformation of the vessel tree\nsegmentation masks to artificially augment training data. Using only masks as\nthe input to our model uniquely allows us to apply such deformations much more\naggressively than one could with conventional image volumes while retaining\nsample realism. The neural network classifies the presence of an LVO and the\naffected hemisphere. In a 5-fold cross validated ablation study, we demonstrate\nthat the use of the suggested augmentation enables us to train robust models\neven from few data sets. Training the EfficientNetB1 architecture on 100 data\nsets, the proposed augmentation scheme was able to raise the ROC AUC to 0.85\nfrom a baseline value of 0.56 using no augmentation. The best performance was\nachieved using a 3D-DenseNet yielding an AUC of 0.87. The augmentation had\npositive impact in classification of the affected hemisphere as well, where the\n3D-DenseNet reached an AUC of 0.93 on both sides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thamm_F/0/1/0/all/0/1\">Florian Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jurgens_M/0/1/0/all/0/1\">Markus J&#xfc;rgens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ditt_H/0/1/0/all/0/1\">Hendrik Ditt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth-Swap: A Simple Enhancement for Face-Swapping with Smoothness. (arXiv:2112.05907v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05907","description":"<p>Face-swapping models have been drawing attention for their compelling\ngeneration quality, but their complex architectures and loss functions often\nrequire careful tuning for successful training. We propose a new face-swapping\nmodel called `Smooth-Swap', which excludes complex handcrafted designs and\nallows fast and stable training. The main idea of Smooth-Swap is to build\nsmooth identity embedding that can provide stable gradients for identity\nchange. Unlike the one used in previous models trained for a purely\ndiscriminative task, the proposed embedding is trained with a supervised\ncontrastive loss promoting a smoother space. With improved smoothness,\nSmooth-Swap suffices to be composed of a generic U-Net-based generator and\nthree basic loss functions, a far simpler design compared with the previous\nmodels. Extensive experiments on face-swapping benchmarks (FFHQ,\nFaceForensics++) and face images in the wild show that our model is also\nquantitatively and qualitatively comparable or even superior to the existing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function. (arXiv:2201.01016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01016","description":"<p>Recovering detailed facial geometry from a set of calibrated multi-view\nimages is valuable for its wide range of applications. Traditional multi-view\nstereo (MVS) methods adopt an optimization-based scheme to regularize the\nmatching cost. Recently, learning-based methods integrate all these into an\nend-to-end neural network and show superiority of efficiency. In this paper, we\npropose a novel architecture to recover extremely detailed 3D faces within\ndozens of seconds. Unlike previous learning-based methods that regularize the\ncost volume via 3D CNN, we propose to learn an implicit function for regressing\nthe matching cost. By fitting a 3D morphable model from multi-view images, the\nfeatures of multiple images are extracted and aggregated in the mesh-attached\nUV space, which makes the implicit function more effective in recovering\ndetailed facial shape. Our method outperforms SOTA learning-based MVS in\naccuracy by a large margin on the FaceScape dataset. The code and data are\nreleased in https://github.com/zhuhao-nju/mvfr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunze Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Z/0/1/0/all/0/1\">Zhengyu Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02445","description":"<p>Using only global image-class labels, weakly-supervised learning methods,\nsuch as class activation mapping, allow training CNNs to jointly classify an\nimage, and locate regions of interest associated with the predicted class.\nHowever, without any guidance at the pixel level, such methods may yield\ninaccurate regions. This problem is known to be more challenging with histology\nimages than with natural ones, since objects are less salient, structures have\nmore variations, and foreground and background regions have stronger\nsimilarities. Therefore, computer vision methods for visual interpretation of\nCNNs may not directly apply. In this paper, a simple yet efficient method based\non a composite loss is proposed to learn information from the fully negative\nsamples (i.e., samples without positive regions), and thereby reduce false\npositives/negatives. Our new loss function contains two complementary terms:\nthe first exploits positive evidence collected from the CNN classifier, while\nthe second leverages the fully negative samples from training data. In\nparticular, a pre-trained CNN is equipped with a decoder that allows refining\nthe regions of interest. The CNN is exploited to collect both positive and\nnegative evidence at the pixel level to train the decoder. Our method called\nNEGEV benefits from the fully negative samples that naturally occur in the\ndata, without any additional supervision signals beyond image-class labels.\nExtensive experiments show that our proposed method can substantial outperform\nrelated state-of-art methods on GlaS (public benchmark for colon cancer), and\nCamelyon16 (patch-based benchmark for breast cancer using three different\nbackbones). Our results highlight the benefits of using both positive and\nnegative evidence, the first obtained from a classifier, and the other\nnaturally available in datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution. (arXiv:2201.08157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08157","description":"<p>Exploiting image patches instead of whole images have proved to be a powerful\napproach to tackle various problems in image processing. Recently, Wasserstein\npatch priors (WPP), which are based on the comparison of the patch\ndistributions of the unknown image and a reference image, were successfully\nused as data-driven regularizers in the variational formulation of\nsuperresolution. However, for each input image, this approach requires the\nsolution of a non-convex minimization problem which is computationally costly.\nIn this paper, we propose to learn two kinds of neural networks in an\nunsupervised way based on WPP loss functions. First, we show how convolutional\nneural networks (CNNs) can be incorporated. Once the network, called WPPNet, is\nlearned, it can very efficiently applied to any input image. Second, we\nincorporate conditional normalizing flows to provide a tool for uncertainty\nquantification. Numerical examples demonstrate the very good performance of\nWPPNets for superresolution in various image classes even if the forward\noperator is known only approximately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1\">Fabian Altekr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1\">Johannes Hertrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets. (arXiv:2202.00273v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.00273","description":"<p>Computer graphics has experienced a recent surge of data-centric approaches\nfor photorealistic and controllable content creation. StyleGAN in particular\nsets new standards for generative modeling regarding image quality and\ncontrollability. However, StyleGAN's performance severely degrades on large\nunstructured datasets such as ImageNet. StyleGAN was designed for\ncontrollability; hence, prior works suspect its restrictive design to be\nunsuitable for diverse datasets. In contrast, we find the main limiting factor\nto be the current training strategy. Following the recently introduced\nProjected GAN paradigm, we leverage powerful neural network priors and a\nprogressive growing strategy to successfully train the latest StyleGAN3\ngenerator on ImageNet. Our final model, StyleGAN-XL, sets a new\nstate-of-the-art on large-scale image synthesis and is the first to generate\nimages at a resolution of $1024^2$ at such a dataset scale. We demonstrate that\nthis model can invert and edit images beyond the narrow domain of portraits or\nspecific object classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1\">Axel Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1\">Katja Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object discovery and representation networks. (arXiv:2203.08777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08777","description":"<p>The promise of self-supervised learning (SSL) is to leverage large amounts of\nunlabeled data to solve complex tasks. While there has been excellent progress\nwith simple, image-level learning, recent methods have shown the advantage of\nincluding knowledge of image structure. However, by introducing hand-crafted\nimage segmentations to define regions of interest, or specialized augmentation\nstrategies, these methods sacrifice the simplicity and generality that makes\nSSL so powerful. Instead, we propose a self-supervised learning paradigm that\ndiscovers this image structure by itself. Our method, Odin, couples object\ndiscovery and representation networks to discover meaningful image\nsegmentations without any supervision. The resulting learning paradigm is\nsimpler, less brittle, and more general, and achieves state-of-the-art transfer\nlearning results for object detection and instance segmentation on COCO, and\nsemantic segmentation on PASCAL and Cityscapes, while strongly surpassing\nsupervised pre-training for video segmentation on DAVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier J. H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1\">Relja Arandjelovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13167","description":"<p>In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Saurav Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17247","description":"<p>Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_E/0/1/0/all/0/1\">Estelle Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Meng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-Yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection. (arXiv:2204.01349v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01349","description":"<p>The Facial Action Coding System (FACS) encodes the action units (AUs) in\nfacial images, which has attracted extensive research attention due to its wide\nuse in facial expression analysis. Many methods that perform well on automatic\nfacial action unit (AU) detection primarily focus on modeling various types of\nAU relations between corresponding local muscle areas, or simply mining global\nattention-aware facial features, however, neglect the dynamic interactions\namong local-global features. We argue that encoding AU features just from one\nperspective may not capture the rich contextual information between regional\nand global face features, as well as the detailed variability across AUs,\nbecause of the diversity in expression and individual characteristics. In this\npaper, we propose a novel Multi-level Graph Relational Reasoning Network\n(termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a\nmulti-level (i.e., region-level, pixel-wise and channel-wise level) feature\nlearning. While the region-level feature learning from local face patches\nfeatures via graph neural network can encode the correlation across different\nAUs, the pixel-wise and channel-wise feature learning via graph attention\nnetwork can enhance the discrimination ability of AU features from global face\nfeatures. The fused features from the three levels lead to improved AU\ndiscriminative ability. Extensive experiments on DISFA and BP4D AU datasets\nshow that the proposed approach achieves superior performance than the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songpei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Quasi-AutoRegression: Forecasting the visual popularity of new fashion products. (arXiv:2204.04014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04014","description":"<p>Estimating the preferences of consumers is of utmost importance for the\nfashion industry as appropriately leveraging this information can be beneficial\nin terms of profit. Trend detection in fashion is a challenging task due to the\nfast pace of change in the fashion industry. Moreover, forecasting the visual\npopularity of new garment designs is even more demanding due to lack of\nhistorical data. To this end, we propose MuQAR, a Multimodal\nQuasi-AutoRegressive deep learning architecture that combines two modules: (1)\na multi-modal multi-layer perceptron processing categorical, visual and textual\nfeatures of the product and (2) a quasi-autoregressive neural network modelling\nthe \"target\" time series of the product's attributes along with the \"exogenous\"\ntime series of all other attributes. We utilize computer vision, image\nclassification and image captioning, for automatically extracting visual\nfeatures and textual descriptions from the images of new products. Product\ndesign in fashion is initially expressed visually and these features represent\nthe products' unique characteristics without interfering with the creative\nprocess of its designers by requiring additional inputs (e.g manually written\ntexts). We employ the product's target attributes time series as a proxy of\ntemporal popularity patterns, mitigating the lack of historical data, while\nexogenous time series help capture trends among interrelated attributes. We\nperform an extensive ablation analysis on two large scale image fashion\ndatasets, Mallzee and SHIFT15m to assess the adequacy of MuQAR and also use the\nAmazon Reviews: Home and Kitchen dataset to assess generalisability to other\ndomains. A comparative study on the VISUELLE dataset, shows that MuQAR is\ncapable of competing and surpassing the domain's current state of the art by\n4.65% and 4.8% in terms of WAPE and MAE respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Stefanos I. Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation. (arXiv:2204.05084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05084","description":"<p>Generating a new font library is a very labor-intensive and time-consuming\njob for glyph-rich scripts. Few-shot font generation is thus required, as it\nrequires only a few glyph references without fine-tuning during test. Existing\nmethods follow the style-content disentanglement paradigm and expect novel\nfonts to be produced by combining the style codes of the reference glyphs and\nthe content representations of the source. However, these few-shot font\ngeneration methods either fail to capture content-independent style\nrepresentations, or employ localized component-wise style representations,\nwhich is insufficient to model many Chinese font styles that involve\nhyper-component features such as inter-component spacing and\n\"connected-stroke\". To resolve these drawbacks and make the style\nrepresentations more reliable, we propose a self-supervised cross-modality\npre-training strategy and a cross-modality transformer-based encoder that is\nconditioned jointly on the glyph image and the corresponding stroke labels. The\ncross-modality encoder is pre-trained in a self-supervised manner to allow\neffective capture of cross- and intra-modality correlations, which facilitates\nthe content-style disentanglement and modeling style representations of all\nscales (stroke-level, component-level and character-level). The pre-trained\nencoder is then applied to the downstream font generation task without\nfine-tuning. Experimental comparisons of our method with state-of-the-art\nmethods demonstrate our method successfully transfers styles of all scales. In\naddition, it only requires one reference glyph and achieves the lowest rate of\nbad cases in the few-shot font generation task 28% lower than the second best\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid ISTA: Unfolding ISTA With Convergence Guarantees Using Free-Form Deep Neural Networks. (arXiv:2204.11640v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11640","description":"<p>It is promising to solve linear inverse problems by unfolding iterative\nalgorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep\nneural networks (DNNs) with learnable parameters. However, existing ISTA-based\nunfolded algorithms restrict the network architectures for iterative updates\nwith the partial weight coupling structure to guarantee convergence. In this\npaper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned\nparameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible\nand reasonable network architectures), while ensuring theoretical convergence.\nWe first develop HCISTA to improve the efficiency and flexibility of classical\nISTA (with pre-computed parameters) without compromising the convergence rate\nin theory. Furthermore, the DNN-based hybrid algorithm is generalized to\npopular variants of learned ISTA, dubbed HLISTA, to enable a free architecture\nof learned parameters with a guarantee of linear convergence. To our best\nknowledge, this paper is the first to provide a convergence-provable framework\nthat enables free-form DNNs in ISTA-based unfolded algorithms. This framework\nis general to endow arbitrary DNNs for solving linear inverse problems with\nconvergence guarantees. Extensive experiments demonstrate that hybrid ISTA can\nreduce the reconstruction error with an improved convergence rate in the tasks\nof sparse recovery and compressive sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Ziyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1\">Duoduo Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Logit Adjustment. (arXiv:2204.11822v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11822","description":"<p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses\nchallenges in recognizing novel classes in the test phase. The development of\ngenerative models enables current GZSL techniques to probe further into the\nsemantic-visual link, culminating in a two-stage form that includes a generator\nand a classifier. However, existing generation-based methods focus on enhancing\nthe generator's effect while neglecting the improvement of the classifier. In\nthis paper, we first analyze of two properties of the generated pseudo unseen\nsamples: bias and homogeneity. Then, we perform variational Bayesian inference\nto back-derive the evaluation metrics, which reflects the balance of the seen\nand unseen classes. As a consequence of our derivation, the aforementioned two\nproperties are incorporated into the classifier training as seen-unseen priors\nvia logit adjustment. The Zero-Shot Logit Adjustment further puts\nsemantic-based classifiers into effect in generation-based GZSL. Our\nexperiments demonstrate that the proposed technique achieves state-of-the-art\nwhen combined with the basic generator, and it can improve various generative\nZero-Shot Learning frameworks. Our codes are available on\nhttps://github.com/cdb342/IJCAI-2022-ZLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dubing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12037","description":"<p>Spatial-temporal representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks an unified guidance and analysis about\nwhy modern spatial-temporal representation learning methods are easily collapse\ninto data bias and have limited generalization and cognitive abilities.\nInspired by the strong inference ability of human-level agents, recent years\nhave therefore witnessed great effort in developing causal reasoning paradigms\nto realize robust representation and model learning with good cognitive\nability. In this paper, we conduct a comprehensive review of existing causal\nreasoning methods for spatial-temporal representation learning, covering\nfundamental theories, models, and datasets. The limitations of current methods\nand datasets are also discussed. Moreover, we propose some primary challenges,\nopportunities, and future research directions for benchmarking causal reasoning\nalgorithms in spatial-temporal representation learning. This paper aims to\nprovide a comprehensive overview of this emerging field, attract attention,\nencourage discussions, bring to the forefront the urgency of developing novel\ncausal reasoning methods, publicly available benchmarks, and consensus-building\nstandards for reliable spatial-temporal representation learning and related\nreal-world applications more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yushen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-Aware Evaluation and Model Scaling for LiDAR-Based 3D Object Detection. (arXiv:2205.01142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01142","description":"<p>Considerable research efforts have been devoted to LiDAR-based 3D object\ndetection and its empirical performance has been significantly improved. While\nthe progress has been encouraging, we observe an overlooked issue: it is not\nyet common practice to compare different 3D detectors under the same cost,\ne.g., inference latency. This makes it difficult to quantify the true\nperformance gain brought by recently proposed architecture designs. The goal of\nthis work is to conduct a cost-aware evaluation of LiDAR-based 3D object\ndetectors. Specifically, we focus on SECOND, a simple grid-based one-stage\ndetector, and analyze its performance under different costs by scaling its\noriginal architecture. Then we compare the family of scaled SECOND with recent\n3D detection methods, such as Voxel R-CNN and PV-RCNN++. The results are\nsurprising. We find that, if allowed to use the same latency, SECOND can match\nthe performance of PV-RCNN++, the current state-of-the-art method on the Waymo\nOpen Dataset. Scaled SECOND also easily outperforms many recent 3D detection\nmethods published during the past year. We recommend future research control\nthe inference cost in their empirical comparison and include the family of\nscaled SECOND as a strong baseline when presenting novel 3D detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The scope for AI-augmented interpretation of building blueprints in commercial and industrial property insurance. (arXiv:2205.01671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01671","description":"<p>This report, commissioned by the WTW research network, investigates the use\nof AI in property risk assessment. It (i) reviews existing work on risk\nassessment in commercial and industrial properties and automated information\nextraction from building blueprints; and (ii) presents an exploratory 'proof-of\nconcept-solution' exploring the feasibility of using machine learning for the\nautomated extraction of information from building blueprints to support\ninsurance risk assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milne_A/0/1/0/all/0/1\">Alistair Milne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillier_J/0/1/0/all/0/1\">John Hillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oglesby_F/0/1/0/all/0/1\">Frances Oglesby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.01818","description":"<p>Human intelligence is multimodal; we integrate visual, linguistic, and\nacoustic signals to maintain a holistic worldview. Most current pretraining\nmethods, however, are limited to one or two modalities. We present i-Code, a\nself-supervised pretraining framework where users may flexibly combine the\nmodalities of vision, speech, and language into unified and general-purpose\nvector representations. In this framework, data from each modality are first\ngiven to pretrained single-modality encoders. The encoder outputs are then\nintegrated with a multimodal fusion network, which uses novel attention\nmechanisms and other architectural innovations to effectively combine\ninformation from the different modalities. The entire system is pretrained\nend-to-end with new objectives including masked modality unit modeling and\ncross-modality contrastive learning. Unlike previous research using only video\nfor pretraining, the i-Code framework can dynamically process single, dual, and\ntriple-modality data during training and inference, flexibly projecting\ndifferent combinations of modalities into a single representation space.\nExperimental results demonstrate how i-Code can outperform state-of-the-art\ntechniques on five video understanding tasks and the GLUE NLP benchmark,\nimproving by as much as 11% and demonstrating the power of integrative\nmultimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gmyr_R/0/1/0/all/0/1\">Robert Gmyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANUBIS: Review and Benchmark Skeleton-Based Action Recognition Methods with a New Dataset. (arXiv:2205.02071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02071","description":"<p>Skeleton-based action recognition, as a subarea of action recognition, is\nswiftly accumulating attention and popularity. The task is to recognize actions\nperformed by human articulation points. Compared with other data modalities, 3D\nhuman skeleton representations have extensive unique desirable characteristics,\nincluding succinctness, robustness, racial-impartiality, and many more. We aim\nto provide a roadmap for new and existing researchers a on the landscapes of\nskeleton-based action recognition for new and existing researchers. To this\nend, we present a review in the form of a taxonomy on existing works of\nskeleton-based action recognition. We partition them into four major\ncategories: (1) datasets; (2) extracting spatial features; (3) capturing\ntemporal patterns; (4) improving signal quality. For each method, we provide\nconcise yet informatively-sufficient descriptions. To promote more fair and\ncomprehensive evaluation on existing approaches of skeleton-based action\nrecognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared\nwith previously collected dataset, ANUBIS are advantageous in the following\nfour aspects: (1) employing more recently released sensors; (2) containing\nnovel back view; (3) encouraging high enthusiasm of subjects; (4) including\nactions of the COVID pandemic era. Using ANUBIS, we comparably benchmark\nperformance of current skeleton-based action recognizers. At the end of this\npaper, we outlook future development of skeleton-based action recognition by\nlisting several new technical problems. We believe they are valuable to solve\nin order to commercialize skeleton-based action recognition in the near future.\nThe dataset of ANUBIS is available at:\n<a href=\"http://hcc-workshop.anu.edu.au/webs/anu101/home.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Madhawa Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Extrapolation in Space and Time. (arXiv:2205.02084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02084","description":"<p>Novel view synthesis (NVS) and video prediction (VP) are typically considered\ndisjoint tasks in computer vision. However, they can both be seen as ways to\nobserve the spatial-temporal world: NVS aims to synthesize a scene from a new\npoint of view, while VP aims to see a scene from a new point of time. These two\ntasks provide complementary signals to obtain a scene representation, as\nviewpoint changes from spatial observations inform depth, and temporal\nobservations inform the motion of cameras and individual objects. Inspired by\nthese observations, we propose to study the problem of Video Extrapolation in\nSpace and Time (VEST). We propose a model that leverages the self-supervision\nand the complementary cues from both tasks, while existing methods can only\nsolve one of them. Experiments show that our method achieves performance better\nthan or comparable to several state-of-the-art NVS and VP methods on indoor and\noutdoor real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2205.02105","description":"<p>The incentive for using Evolutionary Algorithms (EAs) for the automated\noptimization and training of deep neural networks (DNNs), a process referred to\nas neuroevolution, has gained momentum in recent years. The configuration and\ntraining of these networks can be posed as optimization problems. Indeed, most\nof the recent works on neuroevolution have focused their attention on\nsingle-objective optimization. Moreover, from the little research that has been\ndone at the intersection of neuroevolution and evolutionary multi-objective\noptimization (EMO), all the research that has been carried out has focused\npredominantly on the use of one type of DNN: convolutional neural networks\n(CNNs), using well-established standard benchmark problems such as MNIST. In\nthis work, we make a leap in the understanding of these two areas\n(neuroevolution and EMO), regarded in this work as neuroevolutionary\nmulti-objective, by using and studying a rich DNN composed of a CNN and\nLong-short Term Memory network. Moreover, we use a robust and challenging\nvehicle trajectory prediction problem. By using the well-known Non-dominated\nSorting Genetic Algorithm-II, we study the effects of five different\nobjectives, tested in categories of three, allowing us to show how these\nobjectives have either a positive or detrimental effect in neuroevolution for\ntrajectory prediction in autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_F/0/1/0/all/0/1\">Fergal Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvan_E/0/1/0/all/0/1\">Edgar Galv&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}