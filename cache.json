{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Nonverbal Sound Detection for Disordered Speech. (arXiv:2202.07750v1 [eess.AS])","link":"http://arxiv.org/abs/2202.07750","description":"<p>Voice assistants have become an essential tool for people with various\ndisabilities because they enable complex phone- or tablet-based interactions\nwithout the need for fine-grained motor control, such as with touchscreens.\nHowever, these systems are not tuned for the unique characteristics of\nindividuals with speech disorders, including many of those who have a\nmotor-speech disorder, are deaf or hard of hearing, have a severe stutter, or\nare minimally verbal. We introduce an alternative voice-based input system\nwhich relies on sound event detection using fifteen nonverbal mouth sounds like\n\"pop,\" \"click,\" or \"eh.\" This system was designed to work regardless of ones'\nspeech abilities and allows full access to existing technology. In this paper,\nwe describe the design of a dataset, model considerations for real-world\ndeployment, and efforts towards model personalization. Our fully-supervised\nmodel achieves segment-level precision and recall of 88.6% and 88.4% on an\ninternal dataset of 710 adults, while achieving 0.31 false positives per hour\non aggressors such as speech. Five-shot personalization enables satisfactory\nperformance in 84.5% of cases where the generic model fails.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jain_D/0/1/0/all/0/1\">Dhruv Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liaghat_Z/0/1/0/all/0/1\">Zeinab Liaghat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Findlater_L/0/1/0/all/0/1\">Leah Findlater</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Russian SuperGLUE 1.1: Revising the Lessons not Learned by Russian NLP models. (arXiv:2202.07791v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07791","description":"<p>In the last year, new neural architectures and multilingual pre-trained\nmodels have been released for Russian, which led to performance evaluation\nproblems across a range of language understanding tasks.\n</p>\n<p>This paper presents Russian SuperGLUE 1.1, an updated benchmark styled after\nGLUE for Russian NLP models. The new version includes a number of technical,\nuser experience and methodological improvements, including fixes of the\nbenchmark vulnerabilities unresolved in the previous version: novel and\nimproved tests for understanding the meaning of a word in context (RUSSE) along\nwith reading comprehension and common sense reasoning (DaNetQA, RuCoS, MuSeRC).\nTogether with the release of the updated datasets, we improve the benchmark\ntoolkit based on \\texttt{jiant} framework for consistent training and\nevaluation of NLP-models of various architectures which now supports the most\nrecent models for Russian. Finally, we provide the integration of Russian\nSuperGLUE with a framework for industrial evaluation of the open-source models,\nMOROCCO (MOdel ResOurCe COmparison), in which the models are evaluated\naccording to the weighted average metric over all tasks, the inference speed,\nand the occupied amount of RAM. Russian SuperGLUE is publicly available at\nhttps://russiansuperglue.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1\">Alena Fenogenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1\">Maria Tikhonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emelyanov_A/0/1/0/all/0/1\">Anton Emelyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevelev_D/0/1/0/all/0/1\">Denis Shevelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukushkin_A/0/1/0/all/0/1\">Alexandr Kukushkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Generation for Unknown Libraries via Reading API Documentations. (arXiv:2202.07806v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07806","description":"<p>Open-domain code generation is a challenging problem because the set of\nfunctions and classes that we use are frequently changed and extended in\nprogramming communities. We consider the challenge of code generation for\nunknown libraries without additional training. In this paper, we explore a\nframework of code generation that can refer to relevant API documentations like\nhuman programmers to handle unknown libraries. As a first step of this\ndirection, we implement a model that can extract relevant code signatures from\nAPI documentations based on a natural language intent and copy primitives from\nthe extracted signatures. Moreover, to evaluate code generation for unknown\nlibraries and our framework, we extend an existing dataset of open-domain code\ngeneration and resplit it so that the evaluation data consist of only examples\nusing the libraries that do not appear in the training data. Experiments on our\nnew split show that baseline encoder-decoder models cannot generate code using\nprimitives of unknown libraries as expected. In contrast, our model outperforms\nthe baseline on the new split and can properly generate unknown primitives when\nextracted code signatures are noiseless.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Washio_K/0/1/0/all/0/1\">Koki Washio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyao_Y/0/1/0/all/0/1\">Yusuke Miyao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech. (arXiv:2202.07816v1 [eess.AS])","link":"http://arxiv.org/abs/2202.07816","description":"<p>Expressive text-to-speech (TTS) has become a hot research topic recently,\nmainly focusing on modeling prosody in speech. Prosody modeling has several\nchallenges: 1) the extracted pitch used in previous prosody modeling works have\ninevitable errors, which hurts the prosody modeling; 2) different attributes of\nprosody (e.g., pitch, duration and energy) are dependent on each other and\nproduce the natural prosody together; and 3) due to high variability of prosody\nand the limited amount of high-quality data for TTS training, the distribution\nof prosody cannot be fully shaped. To tackle these issues, we propose\nProsoSpeech, which enhances the prosody using quantized latent vectors\npre-trained on large-scale unpaired and low-quality text and speech data.\nSpecifically, we first introduce a word-level prosody encoder, which quantizes\nthe low-frequency band of the speech and compresses prosody attributes in the\nlatent prosody vector (LPV). Then we introduce an LPV predictor, which predicts\nLPV given word sequence. We pre-train the LPV predictor on large-scale text and\nlow-quality speech data and fine-tune it on the high-quality TTS dataset.\nFinally, our model can generate expressive speech conditioned on the predicted\nLPV. Experimental results show that ProsoSpeech can generate speech with richer\nprosody compared with baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_M/0/1/0/all/0/1\">Ming Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Speech Recognition By Learning Conversation-level Characteristics. (arXiv:2202.07855v1 [cs.SD])","link":"http://arxiv.org/abs/2202.07855","description":"<p>Conversational automatic speech recognition (ASR) is a task to recognize\nconversational speech including multiple speakers. Unlike sentence-level ASR,\nconversational ASR can naturally take advantages from specific characteristics\nof conversation, such as role preference and topical coherence. This paper\nproposes a conversational ASR model which explicitly learns conversation-level\ncharacteristics under the prevalent end-to-end neural framework. The highlights\nof the proposed model are twofold. First, a latent variational module (LVM) is\nattached to a conformer-based encoder-decoder ASR backbone to learn role\npreference and topical coherence. Second, a topic model is specifically adopted\nto bias the outputs of the decoder to words in the predicted topics.\nExperiments on two Mandarin conversational ASR tasks show that the proposed\nmodel achieves a maximum 12% relative character error rate (CER) reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sining Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NLP Task Effectiveness of Long-Range Transformers. (arXiv:2202.07856v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07856","description":"<p>Transformer models cannot easily scale to long sequences due to their O(N^2)\ntime and space complexity. This has led to Transformer variants seeking to\nlessen computational complexity, such as Longformer and Performer. While such\nmodels have theoretically greater efficiency, their effectiveness on real NLP\ntasks has not been well studied. We benchmark 7 variants of Transformer models\non 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the\neffect of pretraining and hyperparameter settings, to focus on their capacity\nfor long-range attention. Moreover, we present various methods to investigate\nattention behaviors, to illuminate model details beyond metric scores. We find\nthat attention of long-range transformers has advantages on content selection\nand query-guided decoding, but they come with previously unrecognized drawbacks\nsuch as insufficient attention to distant tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITTC @ TREC 2021 Clinical Trials Track. (arXiv:2202.07858v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07858","description":"<p>This paper describes the submissions of the Natural Language Processing (NLP)\nteam from the Australian Research Council Industrial Transformation Training\nCentre (ITTC) for Cognitive Computing in Medical Technologies to the TREC 2021\nClinical Trials Track. The task focuses on the problem of matching eligible\nclinical trials to topics constituting a summary of a patient's admission\nnotes. We explore different ways of representing trials and topics using NLP\ntechniques, and then use a common retrieval model to generate the ranked list\nof relevant trials for each topic. The results from all our submitted runs are\nwell above the median scores for all topics, but there is still plenty of scope\nfor improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thinh Hung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavedon_L/0/1/0/all/0/1\">Lawrence Cavedon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIS2: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07880","description":"<p>Transformers have been showing near-human performance on a variety of tasks,\nbut they are not without their limitations. We discuss the issue of conflating\nresults of transformers that are instructed to do multiple tasks\nsimultaneously. In particular, we focus on the domain of commonsense reasoning\nwithin story prose, which we call contextual commonsense inference (CCI). We\nlook at the GLUCOSE (Mostafazadeh et al 2020) dataset and task for predicting\nimplicit commonsense inferences between story sentences. Since the GLUCOSE task\nsimultaneously generates sentences and predicts the CCI relation, there is a\nconflation in the results. Is the model really measuring CCI or is its ability\nto generate grammatical text carrying the results? In this paper, we introduce\nthe task contextual commonsense inference in sentence selection (CIS$^2$), a\nsimplified task that avoids conflation by eliminating language generation\naltogether. Our findings emphasize the necessity of future work to disentangle\nlanguage generation from the desired NLP tasks at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers. (arXiv:2202.07894v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07894","description":"<p>End-to-end speech recognition is a promising technology for enabling compact\nautomatic speech recognition (ASR) systems since it can unify the acoustic and\nlanguage model into a single neural network. However, as a drawback, training\nof end-to-end speech recognizers always requires transcribed utterances. Since\nend-to-end models are also known to be severely data hungry, this constraint is\ncrucial especially because obtaining transcribed utterances is costly and can\npossibly be impractical or impossible. This paper proposes a method for\nalleviating this issue by transferring knowledge from a language model neural\nnetwork that can be pretrained with text-only data. Specifically, this paper\nattempts to transfer semantic knowledge acquired in embedding vectors of\nlarge-scale language models. Since embedding vectors can be assumed as implicit\nrepresentations of linguistic information such as part-of-speech, intent, and\nso on, those are also expected to be useful modeling cues for ASR decoders.\nThis paper extends two types of ASR decoders, attention-based decoders and\nneural transducers, by modifying training loss functions to include embedding\nprediction terms. The proposed systems were shown to be effective for error\nrate reduction without incurring extra computational costs in the decoding\nphase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kubo_Y/0/1/0/all/0/1\">Yotaro Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1\">Shigeki Karita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacchiani_M/0/1/0/all/0/1\">Michiel Bacchiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation. (arXiv:2202.07922v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07922","description":"<p>There is a growing interest in dataset generation recently due to the\nsuperior generative capacity of large pre-trained language models (PLMs). In\nthis paper, we study a flexible and efficient zero-short learning method,\nZeroGen. Given a zero-shot task, we first generate a dataset from scratch using\nPLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM)\nunder the supervision of the synthesized dataset. This approach allows highly\nefficient inference as the final task model only has orders of magnitude fewer\nparameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free\nand efficient, we argue that ZeroGen can also provide useful insights from the\nperspective of data-free model-agnostic knowledge distillation, and\nunreferenced text generation evaluation. Experiments and analysis on different\nNLP tasks, namely, text classification, question answering, and natural\nlanguage inference), show the effectiveness of ZeroGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. (arXiv:2202.07959v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07959","description":"<p>We propose EdgeFormer -- a parameter-efficient Transformer of the\nencoder-decoder architecture for on-device seq2seq generation, which is\ncustomized under the strict computation and memory constraints. EdgeFormer\nproposes two novel principles for cost-effective parameterization and further\nenhance the model with efficient layer adaptation. We conduct extensive\nexperiments on two practical on-device seq2seq tasks: Machine Translation and\nGrammatical Error Correction, and show that EdgeFormer can effectively\noutperform previous parameter-efficient Transformer baselines and achieve very\ncompetitive results with knowledge distillation under both the computation and\nmemory constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Parameter-Efficient Tuning: Are We Really There Yet?. (arXiv:2202.07962v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07962","description":"<p>Parameter-efficient tuning (PETuning) methods have been deemed by many as the\nnew paradigm for using pretrained language models (PLMs). By tuning just a\nfraction amount of parameters comparing to full model finetuning, PETuning\nmethods claim to have achieved performance on par with or even better than\nfinetuning. In this work, we take a step back and re-examine these PETuning\nmethods by conducting the first comprehensive investigation into the training\nand evaluation of PETuning methods. We found the problematic validation and\ntesting practice in current studies, when accompanied by the instability nature\nof PETuning methods, has led to unreliable conclusions. When being compared\nunder a truly fair evaluation protocol, PETuning cannot yield consistently\ncompetitive performance while finetuning remains to be the best-performing\nmethod in medium- and high-resource settings. We delve deeper into the cause of\nthe instability and observed that model size does not explain the phenomenon\nbut training iteration positively correlates with the stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shangsong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"m-Nearly k-Universal Words -- Investigating Simon Congruence. (arXiv:2202.07981v1 [math.CO])","link":"http://arxiv.org/abs/2202.07981","description":"<p>Determining the index of the Simon congruence is a long outstanding open\nproblem. Two words $u$ and $v$ are called Simon congruent if they have the same\nset of scattered factors, which are parts of the word in the correct order but\nnot necessarily consecutive, e.g., $\\mathtt{oath}$ is a scattered factor of\n$\\mathtt{logarithm}$. Following the idea of scattered factor $k$-universality,\nwe investigate $m$-nearly $k$-universality, i.e., words where $m$ scattered\nfactors of length $k$ are absent, w.r.t. Simon congruence. We present a full\ncharacterisation as well as the index of the congruence for $m=1$. For $m\\neq\n1$, we show some results if in addition $w$ is $(k-1)$-universal as well as\nsome further insights for different $m$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Fleischmann_P/0/1/0/all/0/1\">Pamela Fleischmann</a>, <a href=\"http://arxiv.org/find/math/1/au:+Haschke_L/0/1/0/all/0/1\">Lukas Haschke</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huch_A/0/1/0/all/0/1\">Annika Huch</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mayrock_A/0/1/0/all/0/1\">Annika Mayrock</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Self Shuffle Language. (arXiv:2202.07988v1 [math.CO])","link":"http://arxiv.org/abs/2202.07988","description":"<p>The shuffle product \\(u\\shuffle v\\) of two words \\(u\\) and \\(v\\) is the set\nof all words which can be obtained by interleaving \\(u\\) and \\(v\\). Motivated\nby the paper \\emph{The Shuffle Product: New Research Directions} by Restivo\n(2015) we investigate a special case of the shuffle product. In this work we\nconsider the shuffle of a word with itself called the \\emph{self shuffle} or\n\\emph{shuffle square}, showing first that the self shuffle language and the\nshuffle of the language are in general different sets. We prove that the\nlanguage of all words arising as a self shuffle of some word is context\nsensitive but not context free. Furthermore, we show that the self shuffle \\(w\n\\shuffle w\\) uniquely determines \\(w\\).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Fleischmann_P/0/1/0/all/0/1\">Pamela Fleischmann</a>, <a href=\"http://arxiv.org/find/math/1/au:+Harju_T/0/1/0/all/0/1\">Tero Harju</a>, <a href=\"http://arxiv.org/find/math/1/au:+Haschke_L/0/1/0/all/0/1\">Lukas Haschke</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hofer_J/0/1/0/all/0/1\">Jonas H&#xf6;fer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADIMA: Abuse Detection In Multilingual Audio. (arXiv:2202.07991v1 [cs.SD])","link":"http://arxiv.org/abs/2202.07991","description":"<p>Abusive content detection in spoken text can be addressed by performing\nAutomatic Speech Recognition (ASR) and leveraging advancements in natural\nlanguage processing. However, ASR models introduce latency and often perform\nsub-optimally for profane words as they are underrepresented in training\ncorpora and not spoken clearly or completely. Exploration of this problem\nentirely in the audio domain has largely been limited by the lack of audio\ndatasets. Building on these challenges, we propose ADIMA, a novel,\nlinguistically diverse, ethically sourced, expert annotated and well-balanced\nmultilingual profanity detection audio dataset comprising of 11,775 audio\nsamples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique\nusers. Through quantitative experiments across monolingual and cross-lingual\nzero-shot settings, we take the first step in democratizing audio based content\nmoderation in Indic languages and set forth our dataset to pave future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vikram Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharon_R/0/1/0/all/0/1\">Rini Sharon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawhney_R/0/1/0/all/0/1\">Ramit Sawhney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Debdoot Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should You Mask 15% in Masked Language Modeling?. (arXiv:2202.08005v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08005","description":"<p>Masked language models conventionally use a masking rate of 15% due to the\nbelief that more masking would provide insufficient context to learn good\nrepresentations, and less masking would make training too expensive.\nSurprisingly, we find that masking up to 40% of input tokens can outperform the\n15% baseline, and even masking 80% can preserve most of the performance, as\nmeasured by fine-tuning on downstream tasks. Increasing the masking rates has\ntwo distinct effects, which we investigate through careful ablations: (1) A\nlarger proportion of input tokens are corrupted, reducing the context size and\ncreating a harder task, and (2) models perform more predictions, which benefits\ntraining. We observe that larger models in particular favor higher masking\nrates, as they have more capacity to perform the harder task. We also connect\nour findings to sophisticated masking schemes such as span masking and PMI\nmasking, as well as BERT's curious 80-10-10 corruption strategy, and find that\nsimple uniform masking with [MASK] replacements can be competitive at higher\nmasking rates. Our results contribute to a better understanding of masked\nlanguage modeling and point to new avenues for efficient pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks. (arXiv:2202.08011v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08011","description":"<p>The research of open-domain dialog systems has been greatly prospered by\nneural models trained on large-scale corpora, however, such corpora often\nintroduce various safety problems (e.g., offensive languages, biases, and toxic\nbehaviors) that significantly hinder the deployment of dialog systems in\npractice. Among all these unsafe issues, addressing social bias is more complex\nas its negative impact on marginalized populations is usually expressed\nimplicitly, thus requiring normative reasoning and rigorous analysis. In this\npaper, we focus our investigation on social bias detection of dialog safety\nproblems. We first propose a novel Dial-Bias Frame for analyzing the social\nbias in conversations pragmatically, which considers more comprehensive\nbias-related analyses rather than simple dichotomy annotations. Based on the\nproposed framework, we further introduce CDail-Bias Dataset that, to our\nknowledge, is the first well-annotated Chinese social bias dialog dataset. In\naddition, we establish several dialog bias detection benchmarks at different\nlabel granularities and input types (utterance-level and context-level). We\nshow that the proposed in-depth analyses together with these benchmarks in our\nDial-Bias Frame are necessary and essential to bias detection tasks and can\nbenefit building safe dialog systems in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective. (arXiv:2202.08048v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08048","description":"<p>Natural language understanding (NLU) models tend to rely on spurious\ncorrelations (\\emph{i.e.}, dataset bias) to achieve high performance on\nin-distribution datasets but poor performance on out-of-distribution ones. Most\nof the existing debiasing methods often identify and weaken these samples with\nbiased features (\\emph{i.e.}, superficial surface features that cause such\nspurious correlations). However, down-weighting these samples obstructs the\nmodel in learning from the non-biased parts of these samples. To tackle this\nchallenge, in this paper, we propose to eliminate spurious correlations in a\nfine-grained manner from a feature space perspective. Specifically, we\nintroduce Random Fourier Features and weighted re-sampling to decorrelate the\ndependencies between features to mitigate spurious correlations. After\nobtaining decorrelated features, we further design a mutual-information-based\nmethod to purify them, which forces the model to learn features that are more\nrelevant to tasks. Extensive experiments on two well-studied NLU tasks\nincluding Natural Language Inference and Fact Verification demonstrate that our\nmethod is superior to other comparative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08063","description":"<p>Knowledge Extraction (KE) which aims to extract structural information from\nunstructured texts often suffers from data scarcity and emerging unseen types,\ni.e., low-resource scenarios. Many neural approaches on low-resource KE have\nbeen widely investigated and achieved impressive performance. In this paper, we\npresent a literature review towards KE in low-resource scenarios, and\nsystematically categorize existing works into three paradigms: (1) exploiting\nhigher-resource data, (2) exploiting stronger models, and (3) exploiting data\nand models together. In addition, we describe promising applications and\noutline some potential directions for future research. We hope that our survey\ncan help both the academic and industrial community to better understand this\nfield, inspire more ideas and boost broader applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XFBoost: Improving Text Generation with Controllable Decoders. (arXiv:2202.08124v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08124","description":"<p>Multimodal conditionality in transformer-based natural language models has\ndemonstrated state-of-the-art performance in the task of product description\ngeneration. Recent approaches condition a language model on one or more images\nand other textual metadata to achieve near-human performance for describing\nproducts from e-commerce stores. However, generated descriptions may exhibit\ndegrees of inaccuracy or even contradictory claims relative to the inputs of a\ngiven product. In this paper, we propose a controllable language generation\nframework called Extract-Finetune-Boost (XFBoost), which addresses the problem\nof inaccurate low-quality inference. By using visual semantic attributes as\nconstraints at the decoding stage of the generation process and finetuning the\nlanguage model with policy gradient techniques, the XFBoost framework is found\nto produce significantly more descriptive text with higher image relevancy,\noutperforming baselines and lowering the frequency of factually inaccurate\ndescriptions. We further demonstrate the application of XFBoost to online\nlearning wherein human-in-the-loop critics improve language models with active\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sollami_M/0/1/0/all/0/1\">Michael Sollami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Processing the structure of documents: Logical Layout Analysis of historical newspapers in French. (arXiv:2202.08125v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08125","description":"<p>Background. In recent years, libraries and archives led important\ndigitisation campaigns that opened the access to vast collections of historical\ndocuments. While such documents are often available as XML ALTO documents, they\nlack information about their logical structure. In this paper, we address the\nproblem of Logical Layout Analysis applied to historical documents in French.\nWe propose a rule-based method, that we evaluate and compare with two\nMachine-Learning models, namely RIPPER and Gradient Boosting. Our data set\ncontains French newspapers, periodicals and magazines, published in the first\nhalf of the twentieth century in the Franche-Comt\\'e Region. Results. Our\nrule-based system outperforms the two other models in nearly all evaluations.\nIt has especially better Recall results, indicating that our system covers more\ntypes of every logical label than the other two models. When comparing RIPPER\nwith Gradient Boosting, we can observe that Gradient Boosting has better\nPrecision scores but RIPPER has better Recall scores. Conclusions. The\nevaluation shows that our system outperforms the two Machine Learning models,\nand provides significantly higher Recall. It also confirms that our system can\nbe used to produce annotated data sets that are large enough to envisage\nMachine Learning or Deep Learning approaches for the task of Logical Layout\nAnalysis. Combining rules and Machine Learning models into hybrid systems could\npotentially provide even better performances. Furthermore, as the layout in\nhistorical documents evolves rapidly, one possible solution to overcome this\nproblem would be to apply Rule Learning algorithms to bootstrap rule sets\nadapted to different publication periods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutehrle_N/0/1/0/all/0/1\">Nicolas Gutehrl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanassova_I/0/1/0/all/0/1\">Iana Atanassova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08138","description":"<p>We consider the task of temporal human action localization in lifestyle\nvlogs. We introduce a novel dataset consisting of manual annotations of\ntemporal localization for 13,000 narrated actions in 1,200 video clips. We\npresent an extensive analysis of this data, which allows us to better\nunderstand how the language and visual modalities interact throughout the\nvideos. We propose a simple yet effective method to localize the narrated\nactions based on their expected duration. Through several experiments and\nanalyses, we show that our method brings complementary information with respect\nto previous methods, and leads to improvements over previous work for the task\nof temporal action localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jiajun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_D/0/1/0/all/0/1\">Dandan Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Filter: Few-shot text-to-speech speaker adaptation using voice conversion as a post-processing module. (arXiv:2202.08164v1 [eess.AS])","link":"http://arxiv.org/abs/2202.08164","description":"<p>State-of-the-art text-to-speech (TTS) systems require several hours of\nrecorded speech data to generate high-quality synthetic speech. When using\nreduced amounts of training data, standard TTS models suffer from speech\nquality and intelligibility degradations, making training low-resource TTS\nsystems problematic. In this paper, we propose a novel extremely low-resource\nTTS method called Voice Filter that uses as little as one minute of speech from\na target speaker. It uses voice conversion (VC) as a post-processing module\nappended to a pre-existing high-quality TTS system and marks a conceptual shift\nin the existing TTS paradigm, framing the few-shot TTS problem as a VC task.\nFurthermore, we propose to use a duration-controllable TTS system to create a\nparallel speech corpus to facilitate the VC task. Results show that the Voice\nFilter outperforms state-of-the-art few-shot speech synthesis techniques in\nterms of objective and subjective metrics on one minute of speech on a diverse\nset of voices, while being competitive against a TTS model built on 30 times\nmore data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gabrys_A/0/1/0/all/0/1\">Adam Gabry&#x15b;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huybrechts_G/0/1/0/all/0/1\">Goeric Huybrechts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manuel Sam Ribeiro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chien_C/0/1/0/all/0/1\">Chung-Ming Chien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_J/0/1/0/all/0/1\">Julian Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Comini_G/0/1/0/all/0/1\">Giulia Comini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perz_B/0/1/0/all/0/1\">Bartek Perz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1\">Jaime Lorenzo-Trueba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model. (arXiv:2202.08171v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08171","description":"<p>Capitalization normalization (truecasing) is the task of restoring the\ncorrect case (uppercase or lowercase) of noisy text. We propose a fast,\naccurate and compact two-level hierarchical word-and-character-based recurrent\nneural network model. We use the truecaser to normalize user-generated text in\na Federated Learning framework for language modeling. A case-aware language\nmodel trained on this normalized text achieves the same perplexity as a model\ntrained on text with gold capitalization. In a real user A/B experiment, we\ndemonstrate that the improvement translates to reduced prediction error rates\nin a virtual keyboard application. Similarly, in an ASR language model fusion\nexperiment, we show reduction in uppercase character error rate and word error\nrate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">You-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphNLI: A Graph-based Natural Language Inference Model for Polarity Prediction in Online Debates. (arXiv:2202.08175v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08175","description":"<p>Online forums that allow participatory engagement between users have been\ntransformative for public discussion of important issues. However, debates on\nsuch forums can sometimes escalate into full blown exchanges of hate or\nmisinformation. An important tool in understanding and tackling such problems\nis to be able to infer the argumentative relation of whether a reply is\nsupporting or attacking the post it is replying to. This so called polarity\nprediction task is difficult because replies may be based on external context\nbeyond a post and the reply whose polarity is being predicted. We propose\nGraphNLI, a novel graph-based deep learning architecture that uses graph walk\ntechniques to capture the wider context of a discussion thread in a principled\nfashion. Specifically, we propose methods to perform root-seeking graph walks\nthat start from a post and captures its surrounding context to generate\nadditional embeddings for the post. We then use these embeddings to predict the\npolarity relation between a reply and the post it is replying to. We evaluate\nthe performance of our models on a curated debate dataset from Kialo, an online\ndebating platform. Our model outperforms relevant baselines, including S-BERT,\nwith an overall accuracy of 83%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhor Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_S/0/1/0/all/0/1\">Sagar Joglekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">Anthony P. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_N/0/1/0/all/0/1\">Nishanth Sastry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to summarize from human feedback. (arXiv:2009.01325v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.01325","description":"<p>As language models become more powerful, training and evaluation are\nincreasingly bottlenecked by the data and metrics used for a particular task.\nFor example, summarization models are often trained to predict human reference\nsummaries and evaluated using ROUGE, but both of these metrics are rough\nproxies for what we really care about -- summary quality. In this work, we show\nthat it is possible to significantly improve summary quality by training a\nmodel to optimize for human preferences. We collect a large, high-quality\ndataset of human comparisons between summaries, train a model to predict the\nhuman-preferred summary, and use that model as a reward function to fine-tune a\nsummarization policy using reinforcement learning. We apply our method to a\nversion of the TL;DR dataset of Reddit posts and find that our models\nsignificantly outperform both human reference summaries and much larger models\nfine-tuned with supervised learning alone. Our models also transfer to CNN/DM\nnews articles, producing summaries nearly as good as the human reference\nwithout any news-specific fine-tuning. We conduct extensive analyses to\nunderstand our human feedback dataset and fine-tuned models We establish that\nour reward model generalizes to new datasets, and that optimizing our reward\nmodel results in better summaries than optimizing ROUGE according to humans. We\nhope the evidence from our paper motivates machine learning researchers to pay\ncloser attention to how their training loss affects the model behavior they\nactually want.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stiennon_N/0/1/0/all/0/1\">Nisan Stiennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voss_C/0/1/0/all/0/1\">Chelsea Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning for on-line Sequence Transformation. (arXiv:2105.14097v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14097","description":"<p>A number of problems in the processing of sound and natural language, as well\nas in other areas, can be reduced to simultaneously reading an input sequence\nand writing an output sequence of generally different length. There are well\ndeveloped methods that produce the output sequence based on the entirely known\ninput. However, efficient methods that enable such transformations on-line do\nnot exist. In this paper we introduce an architecture that learns with\nreinforcement to make decisions about whether to read a token or write another\ntoken. This architecture is able to transform potentially infinite sequences\non-line. In an experimental study we compare it with state-of-the-art methods\nfor neural machine translation. While it produces slightly worse translations\nthan Transformer, it outperforms the autoencoder with attention, even though\nour architecture translates texts on-line thereby solving a more difficult\nproblem than both reference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rypesc_G/0/1/0/all/0/1\">Grzegorz Rype&#x15b;&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepak_L/0/1/0/all/0/1\">&#x141;ukasz Lepak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1\">Pawe&#x142; Wawrzy&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v8 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique for doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v5 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Automated Unit Tests for Unsupervised Code Translation. (arXiv:2110.06773v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2110.06773","description":"<p>With little to no parallel data available for programming languages,\nunsupervised methods are well-suited to source code translation. However, the\nmajority of unsupervised machine translation approaches rely on\nback-translation, a method developed in the context of natural language\ntranslation and one that inherently involves training on noisy inputs.\nUnfortunately, source code is highly sensitive to small changes; a single token\ncan result in compilation failures or erroneous programs, unlike natural\nlanguages where small inaccuracies may not change the meaning of a sentence. To\naddress this issue, we propose to leverage an automated unit-testing system to\nfilter out invalid translations, thereby creating a fully tested parallel\ncorpus. We found that fine-tuning an unsupervised model with this filtered data\nset significantly reduces the noise in the translations so-generated,\ncomfortably outperforming the state-of-the-art for all language pairs studied.\nIn particular, for Java $\\to$ Python and Python $\\to$ C++ we outperform the\nbest previous methods by more than 16% and 24% respectively, reducing the error\nrate by more than 35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie M. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harman_M/0/1/0/all/0/1\">Mark Harman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1\">Guillaume Lample</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Language Model Integration for RNN Transducer based Speech Recognition. (arXiv:2110.06841v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06841","description":"<p>The mismatch between an external language model (LM) and the implicitly\nlearned internal LM (ILM) of RNN-Transducer (RNN-T) can limit the performance\nof LM integration such as simple shallow fusion. A Bayesian interpretation\nsuggests to remove this sequence prior as ILM correction. In this work, we\nstudy various ILM correction-based LM integration methods formulated in a\ncommon RNN-T framework. We provide a decoding interpretation on two major\nreasons for performance improvement with ILM correction, which is further\nexperimentally verified with detailed analysis. We also propose an exact-ILM\ntraining framework by extending the proof given in the hybrid autoregressive\ntransducer, which enables a theoretical justification for other ILM approaches.\nSystematic comparison is conducted for both in-domain and cross-domain\nevaluation on the Librispeech and TED-LIUM Release 2 corpora, respectively. Our\nproposed exact-ILM training can further improve the best ILM method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14532","description":"<p>Our society produces and shares overwhelming amounts of information through\nOnline Social Networks (OSNs). Within this environment, misinformation and\ndisinformation have proliferated, becoming a public safety concern in most\ncountries. Allowing the public and professionals to efficiently find reliable\nevidences about the factual veracity of a claim is a crucial step to mitigate\nthis harmful spread. To this end, we propose FacTeR-Check, a multilingual\narchitecture for semi-automated fact-checking that can be used for either\napplications designed for the general public and by fact-checking\norganisations. FacTeR-Check enables retrieving fact-checked information,\nunchecked claims verification and tracking dangerous information over social\nmedia. This architectures involves several modules developed to evaluate\nsemantic similarity, to calculate natural language inference and to retrieve\ninformation from Online Social Networks. The union of all these components\nbuilds a semi-automated fact-checking tool able of verifying new claims, to\nextract related evidence, and to track the evolution of a hoax on a OSN. While\nindividual modules are validated on related benchmarks (mainly MSTS and SICK),\nthe complete architecture is validated using a new dataset called NLI19-SP that\nis publicly released with COVID-19 related hoaxes and tweets from Spanish\nsocial media. Our results show state-of-the-art performance on the individual\nbenchmarks, as well as producing a useful analysis of the evolution over time\nof 61 different hoaxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Garcia_A/0/1/0/all/0/1\">&#xc1;lvaro Huertas-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villar_Rodriguez_G/0/1/0/all/0/1\">Guillermo Villar-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Learn Commonsense Knowledge?. (arXiv:2111.00607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00607","description":"<p>Language models (LMs) trained on large amounts of data have shown impressive\nperformance on many NLP tasks under the zero-shot and few-shot setup. Here we\naim to better understand the extent to which such models learn commonsense\nknowledge -- a critical component of many NLP applications. To that end, we\nconduct a systematic and rigorous zero-shot and few-shot commonsense evaluation\nof pre-trained LMs, where we: (i) carefully control for the LM's ability to\nexploit potential surface cues and annotation artefacts, and (ii) account for\nvariations in model performance that arise from non-commonsense related\nfactors. Our findings highlight the limitations of pre-trained LMs in acquiring\ncommonsense knowledge without task-specific supervision; furthermore, using\nlarger models -- or augmenting the LMs with commonsense knowledge bases at\ntest-time -- did not substantially improve their performance. More broadly, our\nfindings offer valuable lessons and best practices for conducting more rigorous\nmultiple-choice evaluations of pre-trained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAutume_C/0/1/0/all/0/1\">Cyprien de Masson d&#x27;Autume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone. (arXiv:2112.02418v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.02418","description":"<p>YourTTS brings the power of a multilingual approach to the task of zero-shot\nmulti-speaker TTS. Our method builds upon the VITS model and adds several novel\nmodifications for zero-shot multi-speaker and multilingual training. We\nachieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and\nresults comparable to SOTA in zero-shot voice conversion on the VCTK dataset.\nAdditionally, our approach achieves promising results in a target language with\na single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS\nand zero-shot voice conversion systems in low-resource languages. Finally, it\nis possible to fine-tune the YourTTS model with less than 1 minute of speech\nand achieve state-of-the-art results in voice similarity and with reasonable\nquality. This is important to allow synthesis for speakers with a very\ndifferent voice or recording characteristics from those seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_J/0/1/0/all/0/1\">Julian Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golge_E/0/1/0/all/0/1\">Eren G&#xf6;lge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink the Evaluation for Attack Strength of Backdoor Attacks in Natural Language Processing. (arXiv:2201.02993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02993","description":"<p>It has been shown that natural language processing (NLP) models are\nvulnerable to a kind of security threat called the Backdoor Attack, which\nutilizes a `backdoor trigger' paradigm to mislead the models. The most\nthreatening backdoor attack is the stealthy backdoor, which defines the\ntriggers as text style or syntactic. Although they have achieved an incredible\nhigh attack success rate (ASR), we find that the principal factor contributing\nto their ASR is not the `backdoor trigger' paradigm. Thus the capacity of these\nstealthy backdoor attacks is overestimated when categorized as backdoor\nattacks. Therefore, to evaluate the real attack power of backdoor attacks, we\npropose a new metric called attack successful rate difference (ASRD), which\nmeasures the ASR difference between clean state and poison state models.\nBesides, since the defenses against stealthy backdoor attacks are absent, we\npropose Trigger Breaker, consisting of two too simple tricks that can defend\nagainst stealthy backdoor attacks effectively. Experiments show that our method\nachieves significantly better performance than state-of-the-art defense methods\nagainst stealthy backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ABBE Corpus: Animate Beings Being Emotional. (arXiv:2201.10618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10618","description":"<p>Emotion detection is an established NLP task of demonstrated utility for text\nunderstanding. However, basic emotion detection leaves out key information,\nnamely, who is experiencing the emotion in question. For example, it may be the\nauthor, the narrator, or a character; or the emotion may correspond to\nsomething the audience is supposed to feel, or even be unattributable to a\nspecific being, e.g., when emotions are being discussed per se. We provide the\nABBE corpus -- Animate Beings Being Emotional -- a new double-annotated corpus\nof texts that captures this key information for one class of emotion\nexperiencer, namely, animate beings in the world described by the text. Such a\ncorpus is useful for developing systems that seek to model or understand this\nspecific type of expressed emotion. Our corpus contains 30 chapters, comprising\n134,513 words, drawn from the Corpus of English Novels, and contains 2,010\nunique emotion expressions attributable to 2,227 animate beings. The emotion\nexpressions are categorized according to Plutchik's 8-category emotion model,\nand the overall inter-annotator agreement for the annotations was 0.83 Cohen's\nKappa, indicating excellent agreement. We describe in detail our annotation\nscheme and procedure, and also release the corpus for use by other researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zad_S/0/1/0/all/0/1\">Samira Zad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_J/0/1/0/all/0/1\">Joshuan Jimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Mark A. Finlayson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Scaling and Transfer of Language Model Architectures for Machine Translation. (arXiv:2202.00528v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00528","description":"<p>Natural language understanding and generation models follow one of the two\ndominant architectural paradigms: language models (LMs) that process\nconcatenated sequences in a single stack of layers, and encoder-decoder models\n(EncDec) that utilize separate layer stacks for input and output processing. In\nmachine translation, EncDec has long been the favoured approach, but with few\nstudies investigating the performance of LMs. In this work, we thoroughly\nexamine the role of several architectural design choices on the performance of\nLMs on bilingual, (massively) multilingual and zero-shot translation tasks,\nunder systematic variations of data conditions and model sizes. Our results\nshow that: (i) Different LMs have different scaling properties, where\narchitectural differences often have a significant impact on model performance\nat small scales, but the performance gap narrows as the number of parameters\nincreases, (ii) Several design choices, including causal masking and\nlanguage-modeling objectives for the source sequence, have detrimental effects\non translation quality, and (iii) When paired with full-visible masking for\nsource sequences, LMs could perform on par with EncDec on supervised bilingual\nand multilingual translation tasks, and improve greatly on zero-shot directions\nby facilitating the reduction of off-target translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jonathan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05331","description":"<p>Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Daniel Louzada Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marcos Henrique Fonseca Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_F/0/1/0/all/0/1\">Fabio Ribeiro Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Michel Melo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Task Framework for Improving Persona-grounded Dialogue Dataset. (arXiv:2202.05435v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05435","description":"<p>This paper introduces a simple yet effective data-centric approach for the\ntask of improving persona-conditioned dialogue agents. Prior model-centric\napproaches unquestioningly depend on the raw crowdsourced benchmark datasets\nsuch as Persona-Chat. In contrast, we aim to fix annotation artifacts in\nbenchmarking, which is orthogonally applicable to any dialogue model.\nSpecifically, we augment relevant personas to improve dialogue dataset/agent,\nby leveraging the primal-dual structure of the two tasks, predicting dialogue\nresponses and personas based on each other. Experiments on Persona-Chat show\nthat our approach outperforms pre-trained LMs by an 11.7 point gain in terms of\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Beong-woo Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hong-in Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models. (arXiv:2202.06539v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2202.06539","description":"<p>Past work has shown that large language models are susceptible to privacy\nattacks, where adversaries generate sequences from a trained model and detect\nwhich sequences are memorized from the training set. In this work, we show that\nthe success of these attacks is largely due to duplication in commonly used\nweb-scraped training sets. We first show that the rate at which language models\nregenerate training sequences is superlinearly related to a sequence's count in\nthe training set. For instance, a sequence that is present 10 times in the\ntraining data is on average generated ~1000 times more often than a sequence\nthat is present only once. We next show that existing methods for detecting\nmemorized sequences have near-chance accuracy on non-duplicated training\nsequences. Finally, we find that after applying methods to deduplicate training\ndata, language models are considerably more secure against these types of\nprivacy attacks. Taken together, our results motivate an increased focus on\ndeduplication in privacy-sensitive applications and a reevaluation of the\npracticality of existing privacy attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandpal_N/0/1/0/all/0/1\">Nikhil Kandpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06991","description":"<p>In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Misinformation Detection in Social Media Video Posts. (arXiv:2202.07706v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07706","description":"<p>With the growing adoption of short-form video by social media platforms,\nreducing the spread of misinformation through video posts has become a critical\nchallenge for social media providers. In this paper, we develop methods to\ndetect misinformation in social media posts, exploiting modalities such as\nvideo and text. Due to the lack of large-scale public data for misinformation\ndetection in multi-modal datasets, we collect 160,000 video posts from Twitter,\nand leverage self-supervised learning to learn expressive representations of\njoint visual and textual data. In this work, we propose two new methods for\ndetecting semantic inconsistencies within short-form social media video posts,\nbased on contrastive learning and masked language modeling. We demonstrate that\nour new approaches outperform current state-of-the-art methods on both\nartificial data generated by random-swapping of positive samples and in the\nwild on a new manually-labeled test set for semantic misinformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kehan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Seth Z. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1\">Avideh Zakhor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Preserving Visual Question Answering. (arXiv:2202.07712v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07712","description":"<p>We introduce a novel privacy-preserving methodology for performing Visual\nQuestion Answering on the edge. Our method constructs a symbolic representation\nof the visual scene, using a low-complexity computer vision model that jointly\npredicts classes, attributes and predicates. This symbolic representation is\nnon-differentiable, which means it cannot be used to recover the original\nimage, thereby keeping the original image private. Our proposed hybrid solution\nuses a vision model which is more than 25 times smaller than the current\nstate-of-the-art (SOTA) vision models, and 100 times smaller than end-to-end\nSOTA VQA models. We report detailed error analysis and discuss the trade-offs\nof using a distilled vision model and a symbolic representation of the visual\nscene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Abhinav Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MV_R/0/1/0/all/0/1\">Rohith MV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav S. Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Subjective Quality Study for Video Frame Interpolation. (arXiv:2202.07727v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07727","description":"<p>Video frame interpolation (VFI) is one of the fundamental research areas in\nvideo processing and there has been extensive research on novel and enhanced\ninterpolation algorithms. The same is not true for quality assessment of the\ninterpolated content. In this paper, we describe a subjective quality study for\nVFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36\nreference sequences at three different frame rates and 180 distorted videos\ngenerated using five conventional and learning based VFI algorithms. Subjective\nopinion scores have been collected from 60 human participants, and then\nemployed to evaluate eight popular quality metrics, including PSNR, SSIM and\nLPIPS which are all commonly used for assessing VFI methods. The results\nindicate that none of these metrics provide acceptable correlation with the\nperceived quality on interpolated content, with the best-performing metric,\nLPIPS, offering a SROCC value below 0.6. Our findings show that there is an\nurgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFI\ndataset is publicly available and can be accessed at\nhttps://danielism97.github.io/BVI-VFI/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Danier_D/0/1/0/all/0/1\">Duolikun Danier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis. (arXiv:2202.07728v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07728","description":"<p>A variety of methods have been proposed to try to explain how deep neural\nnetworks make their decisions. Key to those approaches is the need to sample\nthe pixel space efficiently in order to derive importance maps. However, it has\nbeen shown that the sampling methods used to date introduce biases and other\nartifacts, leading to inaccurate estimates of the importance of individual\npixels and severely limit the reliability of current explainability methods.\nUnfortunately, the alternative -- to exhaustively sample the image space is\ncomputationally prohibitive. In this paper, we introduce EVA (Explaining using\nVerified perturbation Analysis) -- the first explainability method guarantee to\nhave an exhaustive exploration of a perturbation space. Specifically, we\nleverage the beneficial properties of verified perturbation analysis -- time\nefficiency, tractability and guaranteed complete coverage of a manifold -- to\nefficiently characterize the input variables that are most likely to drive the\nmodel decision. We evaluate the approach systematically and demonstrate\nstate-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ducoffe_M/0/1/0/all/0/1\">Melanie Ducoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vigouroux_D/0/1/0/all/0/1\">David Vigouroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capelle_M/0/1/0/all/0/1\">Mikael Capelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicodeme_C/0/1/0/all/0/1\">Claire Nicodeme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN. (arXiv:2202.07731v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07731","description":"<p>This paper presents a new deformable convolution-based video frame\ninterpolation (VFI) method, using a coarse to fine 3D CNN to enhance the\nmulti-flow prediction. This model first extracts spatio-temporal features at\nmultiple scales using a 3D CNN, and estimates multi-flows using these features\nin a coarse-to-fine manner. The estimated multi-flows are then used to warp the\noriginal input frames as well as context maps, and the warped results are fused\nby a synthesis network to produce the final output. This VFI approach has been\nfully evaluated against 12 state-of-the-art VFI methods on three commonly used\ntest databases. The results evidently show the effectiveness of the proposed\nmethod, which offers superior interpolation performance over other state of the\nart algorithms, with PSNR gains up to 0.19dB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1\">Duolikun Danier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ab-initio Contrast Estimation and Denoising of Cryo-EM Images. (arXiv:2202.07737v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07737","description":"<p>Background and Objective: The contrast of cryo-EM images vary from one to\nanother, primarily due to the uneven thickness of ice layers. The variation of\ncontrast can affect the quality of 2-D class averaging, 3-D ab-initio modeling,\nand 3-D heterogeneity analysis. Contrast estimation is currently performed\nduring 3-D iterative refinement. As a result, the estimates are not available\nfor class averaging and ab-initio modeling. However, these methods require good\ninitial estimates of 3-D volumes and 3-D rotations of molecules. This paper\naims to solve the contrast estimation problem in the ab-initio stage, without\nestimating the 3-D volume.\n</p>\n<p>Methods: The key observation underlying our analysis is that the 2-D\ncovariance matrix of the raw images is related to the covariance of the\nunderlying clean images, the noise variance, and the contrast variability\nbetween images. We show that the contrast variability can be derived from the\n2-D covariance matrix and use the existing Covariance Wiener Filtering (CWF)\nframework to estimate it. We also demonstrate a modification of CWF to estimate\nthe contrast of individual images.\n</p>\n<p>Results: Our method improves the contrast estimation by a large margin,\ncompared to the previous CWF method. Its estimation accuracy is often\ncomparable to that of an oracle that knows the ground truth covariance of the\nclean images. The more accurate contrast estimation also improves the quality\nof image denoising as demonstrated in both synthetic and experimental datasets.\n</p>\n<p>Conclusions: This paper proposes an effective method for contrast estimation\ndirectly from noisy images without using any 3-D volume information. It enables\ncontrast correction in the earlier stage of single particle analysis, and may\nimprove the accuracy of downstream processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normalized K-Means for Noise-Insensitive Multi-Dimensional Feature Learning. (arXiv:2202.07754v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07754","description":"<p>Many measurement modalities which perform imaging by probing an object\npixel-by-pixel, such as via Photoacoustic Microscopy, produce a\nmulti-dimensional feature (typically a time-domain signal) at each pixel. In\nprinciple, the many degrees of freedom in the time-domain signal would admit\nthe possibility of significant multi-modal information being implicitly\npresent, much more than a single scalar \"brightness\", regarding the underlying\ntargets being observed. However, the measured signal is neither a weighted-sum\nof basis functions (such as principal components) nor one of a set of\nprototypes (K-means), which has motivated the novel clustering method proposed\nhere, capable of learning centroids (signal shapes) that are related to the\nunderlying, albeit unknown, target characteristics in a scalable and\nnoise-robust manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1\">Nicholas Pellegrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1\">Paul Fieguth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_P/0/1/0/all/0/1\">Parsin Haji Reza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images. (arXiv:2202.07755v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07755","description":"<p>Autofluorescence lifetime images reveal unique characteristics of endogenous\nfluorescence in biological samples. Comprehensive understanding and clinical\ndiagnosis rely on co-registration with the gold standard, histology images,\nwhich is extremely challenging due to the difference of both images. Here, we\nshow an unsupervised image-to-image translation network that significantly\nimproves the success of the co-registration using a conventional\noptimisation-based regression network, applicable to autofluorescence lifetime\nimages at different emission wavelengths. A preliminary blind comparison by\nexperienced researchers shows the superiority of our method on co-registration.\nThe results also indicate that the approach is applicable to various image\nformats, like fluorescence intensity images. With the registration, stitching\noutcomes illustrate the distinct differences of the spectral lifetime across an\nunstained tissue, enabling macro-level rapid visual identification of lung\ncancer and cellular-level characterisation of cell variants and common types.\nThe approach could be effortlessly extended to lifetime images beyond this\nrange and other staining technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandes_S/0/1/0/all/0/1\">Susan Fernandes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williams_G/0/1/0/all/0/1\">Gareth O. S. Williams</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Finlayson_N/0/1/0/all/0/1\">Neil Finlayson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_A/0/1/0/all/0/1\">Ahsan R. Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhaliwal_K/0/1/0/all/0/1\">Kevin Dhaliwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hopgood_J/0/1/0/all/0/1\">James R. Hopgood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vallejo_M/0/1/0/all/0/1\">Marta Vallejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General-purpose, long-context autoregressive modeling with Perceiver AR. (arXiv:2202.07765v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07765","description":"<p>Real-world data is high-dimensional: a book, image, or musical performance\ncan easily contain hundreds of thousands of elements even after compression.\nHowever, the most commonly used autoregressive models, Transformers, are\nprohibitively expensive to scale to the number of inputs and layers needed to\ncapture this long-range structure. We develop Perceiver AR, an autoregressive,\nmodality-agnostic architecture which uses cross-attention to map long-range\ninputs to a small number of latents while also maintaining end-to-end causal\nmasking. Perceiver AR can directly attend to over a hundred thousand tokens,\nenabling practical long-context density estimation without the need for\nhand-crafted sparsity patterns or memory mechanisms. When trained on images or\nmusic, Perceiver AR generates outputs with clear long-term coherence and\nstructure. Our architecture also obtains state-of-the-art likelihood on\nlong-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1\">C&#x103;t&#x103;lina Cangea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1\">Charlie Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1\">Sander Dieleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1\">Ian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheahan_H/0/1/0/all/0/1\">Hannah Sheahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeghidour_N/0/1/0/all/0/1\">Neil Zeghidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1\">Jesse Engel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Deterministic Translation for Unsupervised Domain Adaptation. (arXiv:2202.07778v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07778","description":"<p>In this work we challenge the common approach of using a one-to-one mapping\n('translation') between the source and target domains in unsupervised domain\nadaptation (UDA). Instead, we rely on stochastic translation to capture\ninherent translation ambiguities. This allows us to (i) train more accurate\ntarget networks by generating multiple outputs conditioned on the same source\nimage, leveraging both accurate translation and data augmentation for\nappearance variability, (ii) impute robust pseudo-labels for the target data by\naveraging the predictions of a source network on multiple translated versions\nof a single target image and (iii) train and ensemble diverse networks in the\ntarget domain by modulating the degree of stochasticity in the translations. We\nreport improvements over strong recent baselines, leading to state-of-the-art\nUDA results on two challenging semantic segmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_E/0/1/0/all/0/1\">Eleni Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagiotaki_E/0/1/0/all/0/1\">Eleftheria Panagiotaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkinos_I/0/1/0/all/0/1\">Iasonas Kokkinos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. (arXiv:2202.07800v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07800","description":"<p>Vision Transformers (ViTs) take all the image patches as tokens and construct\nmulti-head self-attention (MHSA) among them. Complete leverage of these image\ntokens brings redundant computations since not all the tokens are attentive in\nMHSA. Examples include that tokens containing semantically meaningless or\ndistractive image backgrounds do not positively contribute to the ViT\npredictions. In this work, we propose to reorganize image tokens during the\nfeed-forward process of ViT models, which is integrated into ViT during\ntraining. For each forward inference, we identify the attentive image tokens\nbetween MHSA and FFN (i.e., feed-forward network) modules, which is guided by\nthe corresponding class token attention. Then, we reorganize image tokens by\npreserving attentive image tokens and fusing inattentive ones to expedite\nsubsequent MHSA and FFN computations. To this end, our method EViT improves\nViTs from two perspectives. First, under the same amount of input image tokens,\nour method reduces MHSA and FFN computation for efficient inference. For\ninstance, the inference speed of DeiT-S is increased by 50% while its\nrecognition accuracy is decreased by only 0.3% for ImageNet classification.\nSecond, by maintaining the same computational cost, our method empowers ViTs to\ntake more image tokens as input for recognition accuracy improvement, where the\nimage tokens are from higher resolution images. An example is that we improve\nthe recognition accuracy of DeiT-S by 1% for ImageNet classification at the\nsame computational cost of a vanilla DeiT-S. Meanwhile, our method does not\nintroduce more parameters to ViTs. Experiments on the standard benchmarks show\nthe effectiveness of our method. The code is available at\nhttps://github.com/youweiliang/evit\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying adversarial networks to increase the data efficiency and reliability of Self-Driving Cars. (arXiv:2202.07815v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07815","description":"<p>Convolutional Neural Networks (CNNs) are vulnerable to misclassifying images\nwhen small perturbations are present. With the increasing prevalence of CNNs in\nself-driving cars, it is vital to ensure these algorithms are robust to prevent\ncollisions from occurring due to failure in recognizing a situation. In the\nAdversarial Self-Driving framework, a Generative Adversarial Network (GAN) is\nimplemented to generate realistic perturbations in an image that cause a\nclassifier CNN to misclassify data. This perturbed data is then used to train\nthe classifier CNN further. The Adversarial Self-driving framework is applied\nto an image classification algorithm to improve the classification accuracy on\nperturbed images and is later applied to train a self-driving car to drive in a\nsimulation. A small-scale self-driving car is also built to drive around a\ntrack and classify signs. The Adversarial Self-driving framework produces\nperturbed images through learning a dataset, as a result removing the need to\ntrain on significant amounts of data. Experiments demonstrate that the\nAdversarial Self-driving framework identifies situations where CNNs are\nvulnerable to perturbations and generates new examples of these situations for\nthe CNN to train on. The additional data generated by the Adversarial\nSelf-driving framework provides sufficient data for the CNN to generalize to\nthe environment. Therefore, it is a viable tool to increase the resilience of\nCNNs to perturbations. Particularly, in the real-world self-driving car, the\napplication of the Adversarial Self-Driving framework resulted in an 18 %\nincrease in accuracy, and the simulated self-driving model had no collisions in\n30 minutes of driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aakash Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images. (arXiv:2202.07817v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07817","description":"<p>Cross-view image matches have been widely explored on terrestrial image\nlocalization using aerial images from drones or satellites. This study expands\nthe cross-view image match idea and proposes a cross-domain and cross-view\nlocalization framework. The method identifies the correlation between color\naerial images and underwater acoustic images to improve the localization of\nunderwater vehicles that travel in partially structured environments such as\nharbors and marinas. The approach is validated on a real dataset acquired by an\nunderwater vehicle in a marina. The results show an improvement in the\nlocalization when compared to the dead reckoning of the vehicle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Matheus M. Dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1\">Giovanni G. De Giacomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+J%2E_P/0/1/0/all/0/1\">Paulo L. J. Drews-Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_S/0/1/0/all/0/1\">Silvia S. C. Botelho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis. (arXiv:2202.07820v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07820","description":"<p>The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male\nreproductive health diagnosis and Infertility treatment. With the development\nof the computer industry in recent years, a great of accurate algorithms are\nproposed. With the assistance of those novel algorithms, it is possible for\nCASA to achieve a faster and higher quality result. Since image processing is\nthe technical basis of CASA, including pre-processing,feature extraction,\ntarget detection and tracking, these methods are important technical steps in\ndealing with CASA. The various works related to Computer Assisted Sperm\nAnalysis methods in the last 30 years (since 1988) are comprehensively\nintroduced and analysed in this survey. To facilitate understanding, the\nmethods involved are analysed in the sequence of general steps in sperm\nanalysis. In other words, the methods related to sperm detection (localization)\nare first analysed, and then the methods of sperm tracking are analysed. Beside\nthis, we analyse and prospect the present situation and future of CASA.\nAccording to our work, the feasible for applying in sperm microscopic video of\nmethods mentioned in this review is explained. Moreover, existing challenges of\nobject detection and tracking in microscope video are potential to be solved\ninspired by this survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wenwei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bu_X/0/1/0/all/0/1\">Xiaoning Bu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_S/0/1/0/all/0/1\">Shuojia Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_T/0/1/0/all/0/1\">Tao Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation and Risk Score Prediction of Head and Neck Cancers in PET/CT Volumes with 3D U-Net and Cox Proportional Hazard Neural Networks. (arXiv:2202.07823v1 [physics.med-ph])","link":"http://arxiv.org/abs/2202.07823","description":"<p>We utilized a 3D nnU-Net model with residual layers supplemented by squeeze\nand excitation (SE) normalization for tumor segmentation from PET/CT images\nprovided by the Head and Neck Tumor segmentation chal-lenge (HECKTOR). Our\nproposed loss function incorporates the Unified Fo-cal and Mumford-Shah losses\nto take the advantage of distribution, region, and boundary-based loss\nfunctions. The results of leave-one-out-center-cross-validation performed on\ndifferent centers showed a segmentation performance of 0.82 average Dice score\n(DSC) and 3.16 median Hausdorff Distance (HD), and our results on the test set\nachieved 0.77 DSC and 3.01 HD. Following lesion segmentation, we proposed\ntraining a case-control proportional hazard Cox model with an MLP neural net\nbackbone to predict the hazard risk score for each discrete lesion. This hazard\nrisk prediction model (CoxCC) was to be trained on a number of PET/CT radiomic\nfeatures extracted from the segmented lesions, patient and lesion demographics,\nand encoder features provided from the penultimate layer of a multi-input 2D\nPET/CT convolutional neural network tasked with predicting time-to-event for\neach lesion. A 10-fold cross-validated CoxCC model resulted in a c-index\nvalidation score of 0.89, and a c-index score of 0.61 on the HECKTOR challenge\ntest dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Janzen_I/0/1/0/all/0/1\">Ian Janzen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dubljevic_N/0/1/0/all/0/1\">Natalia Dubljevic</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-En Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hill_C/0/1/0/all/0/1\">Chloe Hill</a>, <a href=\"http://arxiv.org/find/physics/1/au:+MacAulay_C/0/1/0/all/0/1\">Calum MacAulay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RNGDet: Road Network Graph Detection by Transformer in Aerial Images. (arXiv:2202.07824v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07824","description":"<p>Road network graphs provide critical information for autonomous vehicle\napplications, such as motion planning on drivable areas. However, manually\nannotating road network graphs is inefficient and labor-intensive.\nAutomatically detecting road network graphs could alleviate this issue, but\nexisting works are either segmentation-based approaches that could not ensure\nsatisfactory topology correctness, or graph-based approaches that could not\npresent precise enough detection results. To provide a solution to these\nproblems, we propose a novel approach based on transformer and imitation\nlearning named RNGDet (\\underline{R}oad \\underline{N}etwork \\underline{G}raph\n\\underline{Det}ection by Transformer) in this paper. In view of that\nhigh-resolution aerial images could be easily accessed all over the world\nnowadays, we make use of aerial images in our approach. Taken as input an\naerial image, our approach iteratively generates road network graphs\nvertex-by-vertex. Our approach can handle complicated intersection points of\nvarious numbers of road segments. We evaluate our approach on a publicly\navailable dataset. The superiority of our approach is demonstrated through the\ncomparative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Overconfidence Predictions for Autonomous Driving Perception. (arXiv:2202.07825v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07825","description":"<p>In state-of-the-art deep learning for object recognition, SoftMax and Sigmoid\nfunctions are most commonly employed as the predictor outputs. Such layers\noften produce overconfident predictions rather than proper probabilistic\nscores, which can thus harm the decision-making of `critical' perception\nsystems applied in autonomous driving and robotics. Given this, the experiments\nin this work propose a probabilistic approach based on distributions calculated\nout of the Logit layer scores of pre-trained networks. We demonstrate that\nMaximum Likelihood (ML) and Maximum a-Posteriori (MAP) functions are more\nsuitable for probabilistic interpretations than SoftMax and Sigmoid-based\npredictions for object recognition. We explore distinct sensor modalities via\nRGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5\ndatasets, where our approach shows promising performance compared to the usual\nSoftMax and Sigmoid layers, with the benefit of enabling interpretable\nprobabilistic predictions. Another advantage of the approach introduced in this\npaper is that the ML and MAP functions can be implemented in existing trained\nnetworks, that is, the approach benefits from the output of the Logit layer of\npre-trained networks. Thus, there is no need to carry out a new training phase\nsince the ML and MAP functions are used in the test/prediction phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melotti_G/0/1/0/all/0/1\">Gledson Melotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_D/0/1/0/all/0/1\">Diego R. Faria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_N/0/1/0/all/0/1\">Nuno Gon&#xe7;alves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Transformer K-Means. (arXiv:2202.07829v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07829","description":"<p>K-means defines one of the most employed centroid-based clustering algorithms\nwith performances tied to the data's embedding. Intricate data embeddings have\nbeen designed to push $K$-means performances at the cost of reduced theoretical\nguarantees and interpretability of the results. Instead, we propose preserving\nthe intrinsic data space and augment K-means with a similarity measure\ninvariant to non-rigid transformations. This enables (i) the reduction of\nintrinsic nuisances associated with the data, reducing the complexity of the\nclustering task and increasing performances and producing state-of-the-art\nresults, (ii) clustering in the input space of the data, leading to a fully\ninterpretable clustering algorithm, and (iii) the benefit of convergence\nguarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cosentino_R/0/1/0/all/0/1\">Romain Cosentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahroun_Y/0/1/0/all/0/1\">Yanis Bahroun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1\">Anirvan Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aazhang_B/0/1/0/all/0/1\">Behnaam Aazhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCRP: Unsupervised Point Cloud Object Retrieval and Pose Estimation. (arXiv:2202.07843v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07843","description":"<p>An unsupervised point cloud object retrieval and pose estimation method,\ncalled PCRP, is proposed in this work. It is assumed that there exists a\ngallery point cloud set that contains point cloud objects with given pose\norientation information. PCRP attempts to register the unknown point cloud\nobject with those in the gallery set so as to achieve content-based object\nretrieval and pose estimation jointly, where the point cloud registration task\nis built upon an enhanced version of the unsupervised R-PointHop method.\nExperiments on the ModelNet40 dataset demonstrate the superior performance of\nPCRP in comparison with traditional and learning based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Pranav Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeply-Supervised Knowledge Distillation. (arXiv:2202.07846v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07846","description":"<p>Knowledge distillation aims to enhance the performance of a lightweight\nstudent model by exploiting the knowledge from a pre-trained cumbersome teacher\nmodel. However, in the traditional knowledge distillation, teacher predictions\nare only used to provide the supervisory signal for the last layer of the\nstudent model, which may result in those shallow student layers lacking\naccurate training guidance in the layer-by-layer back propagation and thus\nhinders effective knowledge transfer. To address this issue, we propose\nDeeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class\npredictions and feature maps of the teacher model to supervise the training of\nshallow student layers. A loss-based weight allocation strategy is developed in\nDSKD to adaptively balance the learning process of each shallow layer, so as to\nfurther improve the student performance. Extensive experiments show that the\nperformance of DSKD consistently exceeds state-of-the-art methods on various\nteacher-student models, confirming the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shiya Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Network Acceleration with Tiny Sets. (arXiv:2202.07861v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07861","description":"<p>Network compression is effective in accelerating the inference of deep neural\nnetworks, but often requires finetuning with all the training data to recover\nfrom the accuracy loss. It is impractical in some applications, however, due to\ndata privacy issues or constraints in compression time budget. To deal with the\nabove issues, we propose a method named PRACTISE to accelerate the network with\ntiny sets of training images. By considering both the pruned part and the\nunpruned part of a compressed model, PRACTISE alleviates layer-wise error\naccumulation, which is the main drawback of previous methods. Furthermore,\nexisting methods are confined to few compression schemes, have limited speedup\nin terms of latency, and are unstable. In contrast, PRACTISE is stable, fast to\ntrain, versatile to handle various compression schemes, and achieves low\nlatency. We also propose that dropping entire blocks is a better way than\nexisting compression schemes when only tiny sets of training data are\navailable. Extensive experiments demonstrate that PRACTISE achieves much higher\naccuracy and more stable models than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives. (arXiv:2202.07870v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07870","description":"<p>DBSCAN is a fundamental density-based clustering technique that identifies\nany arbitrary shape of the clusters. However, it becomes infeasible while\nhandling big data. On the other hand, centroid-based clustering is important\nfor detecting patterns in a dataset since unprocessed data points can be\nlabeled to their nearest centroid. However, it can not detect non-spherical\nclusters. For a large data, it is not feasible to store and compute labels of\nevery samples. These can be done as and when the information is required. The\npurpose can be accomplished when clustering act as a tool to identify cluster\nrepresentatives and query is served by assigning cluster labels of nearest\nrepresentative. In this paper, we propose an Incremental Prototype-based DBSCAN\n(IPD) algorithm which is designed to identify arbitrary-shaped clusters for\nlarge-scale data. Additionally, it chooses a set of representatives for each\ncluster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_J/0/1/0/all/0/1\">Jayasree Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_J/0/1/0/all/0/1\">Jayanta Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Common Representation Learning with Triplet Loss Functions. (arXiv:2202.07901v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07901","description":"<p>Common representation learning (CRL) learns a shared embedding between two or\nmore modalities to improve in a given task over using only one of the\nmodalities. CRL from different data types such as images and time-series data\n(e.g., audio or text data) requires a deep metric learning loss that minimizes\nthe distance between the modality embeddings. In this paper, we propose to use\nthe triplet loss, which uses positive and negative identities to create sample\npairs with different labels, for CRL between image and time-series modalities.\nBy adapting the triplet loss for CRL, higher accuracy in the main (time-series\nclassification) task can be achieved by exploiting additional information of\nthe auxiliary (image classification) task. Our experiments on synthetic data\nand handwriting recognition data from sensor-enhanced pens show an improved\nclassification accuracy, faster convergence, and a better generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1\">Felix Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1\">Lucas Heublein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1\">Christopher Mutschler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Deep Learning be Applied to Model-Based Multi-Object Tracking?. (arXiv:2202.07909v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07909","description":"<p>Multi-object tracking (MOT) is the problem of tracking the state of an\nunknown and time-varying number of objects using noisy measurements, with\nimportant applications such as autonomous driving, tracking animal behavior,\ndefense systems, and others. In recent years, deep learning (DL) has been\nincreasingly used in MOT for improving tracking performance, but mostly in\nsettings where the measurements are high-dimensional and there are no available\nmodels of the measurement likelihood and the object dynamics. The model-based\nsetting instead has not attracted as much attention, and it is still unclear if\nDL methods can outperform traditional model-based Bayesian methods, which are\nthe state of the art (SOTA) in this context. In this paper, we propose a\nTransformer-based DL tracker and evaluate its performance in the model-based\nsetting, comparing it to SOTA model-based Bayesian methods in a variety of\ndifferent tasks. Our results show that the proposed DL method can match the\nperformance of the model-based methods in simple tasks, while outperforming\nthem when the task gets more complicated, either due to an increase in the data\nassociation complexity, or to stronger nonlinearities of the models of the\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Juliano Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1\">Georg Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljungbergh_W/0/1/0/all/0/1\">William Ljungbergh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wymeersch_H/0/1/0/all/0/1\">Henk Wymeersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActionFormer: Localizing Moments of Actions with Transformers. (arXiv:2202.07925v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07925","description":"<p>Self-attention based Transformer models have demonstrated impressive results\nfor image classification and object detection, and more recently for video\nunderstanding. Inspired by this success, we investigate the application of\nTransformer networks for temporal action localization in videos. To this end,\nwe present ActionFormer -- a simple yet powerful model to identify actions in\ntime and recognize their categories in a single shot, without using action\nproposals or relying on pre-defined anchor windows. ActionFormer combines a\nmultiscale feature representation with local self-attention, and uses a\nlight-weighted decoder to classify every moment in time and estimate the\ncorresponding action boundaries. We show that this orchestrated design results\nin major improvements upon prior works. Without bells and whistles,\nActionFormer achieves 65.6% mAP at tIoU=0.5 on THUMOS14, outperforming the best\nprior model by 8.7 absolute percentage points and crossing the 60% mAP for the\nfirst time. Further, ActionFormer demonstrates strong results on ActivityNet\n1.3 (36.0% average mAP) and the more recent EPIC-Kitchens 100 (+13.5% average\nmAP over prior works). Our code is available at\n<a href=\"http://github.com/happyharrycn/actionformer_release\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Knowledge Distillation. (arXiv:2202.07940v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07940","description":"<p>Recent studies pointed out that knowledge distillation (KD) suffers from two\ndegradation problems, the teacher-student gap and the incompatibility with\nstrong data augmentations, making it not applicable to training\nstate-of-the-art models, which are trained with advanced augmentations.\nHowever, we observe that a key factor, i.e., the temperatures in the softmax\nfunctions for generating probabilities of both the teacher and student models,\nwas mostly overlooked in previous methods. With properly tuned temperatures,\nsuch degradation problems of KD can be much mitigated. However, instead of\nrelying on a naive grid search, which shows poor transferability, we propose\nMeta Knowledge Distillation (MKD) to meta-learn the distillation with learnable\nmeta temperature parameters. The meta parameters are adaptively adjusted during\ntraining according to the gradients of the learning objective. We validate that\nMKD is robust to different dataset scales, different teacher/student\narchitectures, and different types of data augmentation. With MKD, we achieve\nthe best performance with popular ViT architectures among compared methods that\nuse only ImageNet-1K as training data, ranging from tiny to large models. With\nViT-L, we achieve 86.5% with 600 epochs of training, 0.6% better than MAE that\ntrains for 1,650 epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified smoke and fire detection in an evolutionary framework with self-supervised progressive data augment. (arXiv:2202.07954v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07954","description":"<p>Few researches have studied simultaneous detection of smoke and flame\naccompanying fires due to their different physical natures that lead to\nuncertain fluid patterns. In this study, we collect a large image data set to\nre-label them as a multi-label image classification problem so as to identify\nsmoke and flame simultaneously. In order to solve the generalization ability of\nthe detection model on account of the movable fluid objects with uncertain\nshapes like fire and smoke, and their not compactible natures as well as the\ncomplex backgrounds with high variations, we propose a data augment method by\nrandom image stitch to deploy resizing, deforming, position variation, and\nbackground altering so as to enlarge the view of the learner. Moreover, we\npropose a self-learning data augment method by using the class activation map\nto extract the highly trustable region as new data source of positive examples\nto further enhance the data augment. By the mutual reinforcement between the\ndata augment and the detection model that are performed iteratively, both\nmodules make progress in an evolutionary manner. Experiments show that the\nproposed method can effectively improve the generalization performance of the\nmodel for concurrent smoke and fire detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Su Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lu_z/0/1/0/all/0/1\">zhongyan lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+sun_h/0/1/0/all/0/1\">helin sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07983","description":"<p>Age-related macular degeneration (AMD) is the leading cause of visual\nimpairment among elderly in the world. Early detection of AMD is of great\nimportance as the vision loss caused by AMD is irreversible and permanent.\nColor fundus photography is the most cost-effective imaging modality to screen\nfor retinal disorders. \\textcolor{red}{Recently, some algorithms based on deep\nlearning had been developed for fundus image analysis and automatic AMD\ndetection. However, a comprehensive annotated dataset and a standard evaluation\nbenchmark are still missing.} To deal with this issue, we set up the Automatic\nDetection challenge on Age-related Macular degeneration (ADAM) for the first\ntime, held as a satellite event of the ISBI 2020 conference. The ADAM challenge\nconsisted of four tasks which cover the main topics in detecting AMD from\nfundus images, including classification of AMD, detection and segmentation of\noptic disc, localization of fovea, and detection and segmentation of lesions.\nThe ADAM challenge has released a comprehensive dataset of 1200 fundus images\nwith the category labels of AMD, the pixel-wise segmentation masks of the full\noptic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well\nas the location coordinates of the macular fovea. A uniform evaluation\nframework has been built to make a fair comparison of different models. During\nthe ADAM challenge, 610 results were submitted for online evaluation, and\nfinally, 11 teams participated in the onsite challenge. This paper introduces\nthe challenge, dataset, and evaluation methods, as well as summarizes the\nmethods and analyzes the results of the participating teams of each task. In\nparticular, we observed that ensembling strategy and clinical prior knowledge\ncan better improve the performances of the deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xingxing Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1\">Fengbin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_J/0/1/0/all/0/1\">Jaemin Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sunho Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenole Quellec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matta_S/0/1/0/all/0/1\">Sarah Matta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shankaranarayana_S/0/1/0/all/0/1\">Sharath M Shankaranarayana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuen-heng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Yen Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hai Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Innani_S/0/1/0/all/0/1\">Shubham Innani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wenxiu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamble_R/0/1/0/all/0/1\">Ravi Kamble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1\">Nitin Singhal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planckian jitter: enhancing the color quality of self-supervised visual representations. (arXiv:2202.07993v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07993","description":"<p>Several recent works on self-supervised learning are trained by mapping\ndifferent augmentations of the same image to the same feature representation.\nThe set of used data augmentations is of crucial importance for the quality of\nthe learned feature representation. We analyze how the traditionally used color\njitter negatively impacts the quality of the color features in the learned\nfeature representation. To address this problem, we replace this module with\nphysics-based color augmentation, called Planckian jitter, which creates\nrealistic variations in chromaticity, producing a model robust to llumination\nchanges that can be commonly observed in real life, while maintaining the\nability to discriminate the image content based on color information. We\nfurther improve the performance by introducing a latent space combination of\ncolor-sensitive and non-color-sensitive features. These are found to be\ncomplementary and the combination leads to large absolute performance gains\nover the default data augmentation on color classification tasks, including on\nFlowers-102 (+15%), Cub200 (+11%), VegFru (+15%), and T1K+ (+12%). Finally, we\npresent a color sensitivity analysis to document the impact of different\ntraining methods on the model neurons and we show that the performance of the\nlearned features is robust with respect to illuminant variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zini_S/0/1/0/all/0/1\">Simone Zini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzzelli_M/0/1/0/all/0/1\">Marco Buzzelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360 Depth Estimation in the Wild -- The Depth360 Dataset and the SegFuse Network. (arXiv:2202.08010v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08010","description":"<p>Single-view depth estimation from omnidirectional images has gained\npopularity with its wide range of applications such as autonomous driving and\nscene reconstruction. Although data-driven learning-based methods demonstrate\nsignificant potential in this field, scarce training data and ineffective 360\nestimation algorithms are still two key limitations hindering accurate\nestimation across diverse domains. In this work, we first establish a\nlarge-scale dataset with varied settings called Depth360 to tackle the training\ndata problem. This is achieved by exploring the use of a plenteous source of\ndata, 360 videos from the internet, using a test-time training method that\nleverages unique information in each omnidirectional sequence. With novel\ngeometric and temporal constraints, our method generates consistent and\nconvincing depth samples to facilitate single-view estimation. We then propose\nan end-to-end two-branch multi-task learning network, SegFuse, that mimics the\nhuman eye to effectively learn from the dataset and estimate high-quality depth\nmaps from diverse monocular RGB images. With a peripheral branch that uses\nequirectangular projection for depth estimation and a foveal branch that uses\ncubemap projection for semantic segmentation, our method predicts consistent\nglobal depth while maintaining sharp details at local regions. Experimental\nresults show favorable performance against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Detect People on the Fly: A Bio-inspired Event-based Visual System for Drones. (arXiv:2202.08023v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08023","description":"<p>We demonstrate for the first time that a biologicallyplausible spiking neural\nnetwork (SNN) equipped with Spike- Timing-Dependent Plasticity (STDP) learning\ncan continuously learn to detect walking people on the fly using\nretina-inspired, event-based camera data. Our pipeline works as follows. First,\na short sequence of event data (&lt; 2 minutes), capturing a walking human from a\nflying drone, is shown to a convolutional SNNSTDP system which also receives\nteacher spiking signals from a convolutional readout (forming a semi-supervised\nsystem). Then, STDP adaptation is stopped and the learned system is assessed on\ntesting sequences. We conduct several experiments to study the effect of key\nmechanisms in our system and we compare our precision-recall performance to\nconventionally-trained CNNs working with either RGB or event-based camera\nframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing Batch Normalization in Class Incremental Learning. (arXiv:2202.08025v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08025","description":"<p>Extensive researches have applied deep neural networks (DNNs) in class\nincremental learning (Class-IL). As building blocks of DNNs, batch\nnormalization (BN) standardizes intermediate feature maps and has been widely\nvalidated to improve training stability and convergence. However, we claim that\nthe direct use of standard BN in Class-IL models is harmful to both the\nrepresentation learning and the classifier training, thus exacerbating\ncatastrophic forgetting. In this paper we investigate the influence of BN on\nClass-IL models by illustrating such BN dilemma. We further propose BN Tricks\nto address the issue by training a better feature extractor while eliminating\nclassification bias. Without inviting extra hyperparameters, we apply BN Tricks\nto three baseline rehearsal-based methods, ER, DER++ and iCaRL. Through\ncomprehensive experiments conducted on benchmark datasets of Seq-CIFAR-10,\nSeq-CIFAR-100 and Seq-Tiny-ImageNet, we show that BN Tricks can bring\nsignificant performance gains to all adopted baselines, revealing its potential\ngenerality along this line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Minghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1\">Jun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generalize across Domains on Single Test Samples. (arXiv:2202.08045v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08045","description":"<p>We strive to learn a model from a set of source domains that generalizes well\nto unseen target domains. The main challenge in such a domain generalization\nscenario is the unavailability of any target domain data during training,\nresulting in the learned model not being explicitly adapted to the unseen\ntarget domains. We propose learning to generalize across domains on single test\nsamples. We leverage a meta-learning paradigm to learn our model to acquire the\nability of adaptation with single samples at training time so as to further\nadapt itself to each single test sample at test time. We formulate the\nadaptation to the single test sample as a variational Bayesian inference\nproblem, which incorporates the test sample as a conditional into the\ngeneration of model parameters. The adaptation to each test sample requires\nonly one feed-forward computation at test time without any fine-tuning or\nself-supervised training on additional data from the unseen domains. Extensive\nablation studies demonstrate that our model learns the ability to adapt models\nto each single sample by mimicking domain shifts during training. Further, our\nmodel achieves at least comparable -- and often better -- performance than\nstate-of-the-art methods on multiple benchmarks for domain generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image translation of Ultrasound to Pseudo Anatomical Display Using Artificial Intelligence. (arXiv:2202.08053v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08053","description":"<p>Ultrasound is the second most used modality in medical imaging. It is cost\neffective, hazardless, portable and implemented routinely in numerous clinical\nprocedures. Nonetheless, image quality is characterized by granulated\nappearance, poor SNR and speckle noise. Specific for malignant tumors, the\nmargins are blurred and indistinct. Thus, there is a great need for improving\nultrasound image quality. We hypothesize that this can be achieved by\ntranslation into a more realistic anatomic display, using neural networks. In\norder to achieve this goal, the preferable approach would be to use a set of\npaired images. However, this is practically impossible in our case. Therefore,\nCycleGAN was used, to learn each domain properties separately and enforce cross\ndomain cycle consistency. The two datasets which were used for training the\nmodel were \"Breast Ultrasound Images\" (BUSI) and a set of optic images of\npoultry breast tissue samples acquired at our lab. The generated pseudo\nanatomical images provide improved visual discrimination of the lesions with\nclearer border definition and pronounced contrast. Furthermore, the algorithm\nmanages to overcome the acoustic shadows artifacts commonly appearing in\nultrasonic images. In order to evaluate the preservation of the anatomical\nfeatures, the lesions in the ultrasonic images and the generated pseudo\nanatomical images were both automatically segmented and compared. This\ncomparison yielded median dice score of 0.78 for the benign tumors and 0.43 for\nthe malignancies. Median lesion center error of 2.38% and 8.42% for the benign\nand malignancies respectively and median area error index of 0.77% and 5.06%\nfor the benign and malignancies respectively. In conclusion, these generated\npseudo anatomical images, which are presented in a more intuitive way, preserve\ntissue anatomy and can potentially simplify the diagnosis and improve the\nclinical outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barkat_L/0/1/0/all/0/1\">Lilach Barkat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azhari_H/0/1/0/all/0/1\">Haim Azhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adapt to Light. (arXiv:2202.08098v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08098","description":"<p>Light adaptation or brightness correction is a key step in improving the\ncontrast and visual appeal of an image. There are multiple light-related tasks\n(for example, low-light enhancement and exposure correction) and previous\nstudies have mainly investigated these tasks individually. However, it is\ninteresting to consider whether these light-related tasks can be executed by a\nunified model, especially considering that our visual system adapts to external\nlight in such way. In this study, we propose a biologically inspired method to\nhandle light-related image-enhancement tasks with a unified network (called\nLA-Net). First, a frequency-based decomposition module is designed to decouple\nthe common and characteristic sub-problems of light-related tasks into two\npathways. Then, a new module is built inspired by biological visual adaptation\nto achieve unified light adaptation in the low-frequency pathway. In addition,\nnoise suppression or detail enhancement is achieved effectively in the\nhigh-frequency pathway regardless of the light levels. Extensive experiments on\nthree tasks -- low-light enhancement, exposure correction, and tone mapping --\ndemonstrate that the proposed method almost obtains state-of-the-art\nperformance compared with recent methods designed for these individual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai-Fu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Cheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shi-Xuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xian-Shi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Jie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reversible data hiding with dual pixel-value-ordering and1minimum prediction error expansion. (arXiv:2202.08100v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08100","description":"<p>Pixel Value Ordering (PVO) holds an impressive property for high fidelity\nReversible Data Hiding (RDH). In this paper, we introduce a dual-PVO (dPVO) for\nPrediction Error Expansion(PEE), and thereby develop a new RDH scheme to offer\na better rate-distortion performance. Particularly, we propose to embed in two\nphases: forward and backward. In the forward phase, PVO with classic PEE is\napplied to every non-overlapping image block of size 1x3. In the backward\nphase,minimum-set and maximum-set of pixels are determined from the pixels\npredicted in the forward phase. The minimum set only contains the lowest\npredicted pixels and the maximum set contains the largest predicted pixels of\neach image block. Proposed dPVO withPEE is then applied to both sets, so that\nthe pixel values of the minimum set are increased and that of the maximum set\nare decreased by a unit value. Thereby, the pixels predicted in the forward\nembedding can partially be restored to their original values resulting in both\nbetter-embedded image quality and a higher embedding rate. Experimental results\nhave recorded a promising rate-distortion performance of our scheme with a\nsignificant improvement of embedded image quality at higher embedding rates\ncompared to the popular and state-of-the-art PVO-based RDHschemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahed_M/0/1/0/all/0/1\">Md. Abdul Wahed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeem_H/0/1/0/all/0/1\">Hussain Nyeem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrasing Magritte's Observation. (arXiv:2202.08103v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08103","description":"<p>Contrast Sensitivity of the human visual system can be explained from certain\nlow-level vision tasks (like retinal noise and optical blur removal), but not\nfrom others (like chromatic adaptation or pure reconstruction after simple\nbottlenecks). This conclusion still holds even under substantial change in\nstimulus statistics, as for instance considering cartoon-like images as opposed\nto natural images (Li et al. Journal of Vision, 2022, Preprint\n<a href=\"/abs/2103.00481\">arXiv:2103.00481</a>).\n</p>\n<p>In this note we present a method to generate original cartoon-like images\ncompatible with the statistical training used in (Li et al., 2022). Following\nthe classical observation in (Magritte, 1929), the stimuli generated by the\nproposed method certainly are not what they represent: Ceci n'est pas une pipe.\nThe clear distinction between representation (the stimuli generated by the\nproposed method) and reality (the actual object) avoids eventual problems for\nthe use of the generated stimuli in academic, non-profit, publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jesus Malo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typography-MNIST (TMNIST): an MNIST-Style Image Dataset to Categorize Glyphs and Font-Styles. (arXiv:2202.08112v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08112","description":"<p>We present Typography-MNIST (TMNIST), a dataset comprising of 565,292\nMNIST-style grayscale images representing 1,812 unique glyphs in varied styles\nof 1,355 Google-fonts. The glyph-list contains common characters from over 150\nof the modern and historical language scripts with symbol sets, and each\nfont-style represents varying subsets of the total unique glyphs. The dataset\nhas been developed as part of the CognitiveType project which aims to develop\neye-tracking tools for real-time mapping of type to cognition and to create\ncomputational tools that allow for the easy design of typefaces with cognitive\nproperties such as readability. The dataset and scripts to generate MNIST-style\nimages for glyphs in different font styles are freely available at\nhttps://github.com/aiskunks/CognitiveType.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magre_N/0/1/0/all/0/1\">Nimish Magre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Nicholas Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Navigational Information to Learn Visual Representations. (arXiv:2202.08114v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08114","description":"<p>Children learn to build a visual representation of the world from\nunsupervised exploration and we hypothesize that a key part of this learning\nability is the use of self-generated navigational information as a similarity\nlabel to drive a learning objective for self-supervised learning. The goal of\nthis work is to exploit navigational information in a visual environment to\nprovide performance in training that exceeds the state-of-the-art\nself-supervised training. Here, we show that using spatial and temporal\ninformation in the pretraining stage of contrastive learning can improve the\nperformance of downstream classification relative to conventional contrastive\nlearning approaches that use instance discrimination to discriminate between\ntwo alterations of the same image or two different images. We designed a\npipeline to generate egocentric-vision images from a photorealistic ray-tracing\nenvironment (ThreeDWorld) and record relevant navigational information for each\nimage. Modifying the Momentum Contrast (MoCo) model, we introduced spatial and\ntemporal information to evaluate the similarity of two views in the pretraining\nstage instead of instance discrimination. This work reveals the effectiveness\nand efficiency of contextual information for improving representation learning.\nThe work informs our understanding of the means by which children might learn\nto see the world without external supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lizhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyble_B/0/1/0/all/0/1\">Brad Wyble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08138","description":"<p>We consider the task of temporal human action localization in lifestyle\nvlogs. We introduce a novel dataset consisting of manual annotations of\ntemporal localization for 13,000 narrated actions in 1,200 video clips. We\npresent an extensive analysis of this data, which allows us to better\nunderstand how the language and visual modalities interact throughout the\nvideos. We propose a simple yet effective method to localize the narrated\nactions based on their expected duration. Through several experiments and\nanalyses, we show that our method brings complementary information with respect\nto previous methods, and leads to improvements over previous work for the task\nof temporal action localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jiajun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_D/0/1/0/all/0/1\">Dandan Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUN-SIS: a Fully UNsupervised approach for Surgical Instrument Segmentation. (arXiv:2202.08141v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08141","description":"<p>Automatic surgical instrument segmentation of endoscopic images is a crucial\nbuilding block of many computer-assistance applications for minimally invasive\nsurgery. So far, state-of-the-art approaches completely rely on the\navailability of a ground-truth supervision signal, obtained via manual\nannotation, thus expensive to collect at large scale. In this paper, we present\nFUN-SIS, a Fully-UNsupervised approach for binary Surgical Instrument\nSegmentation. FUN-SIS trains a per-frame segmentation model on completely\nunlabelled endoscopic videos, by solely relying on implicit motion information\nand instrument shape-priors. We define shape-priors as realistic segmentation\nmasks of the instruments, not necessarily coming from the same dataset/domain\nas the videos. The shape-priors can be collected in various and convenient\nways, such as recycling existing annotations from other datasets. We leverage\nthem as part of a novel generative-adversarial approach, allowing to perform\nunsupervised instrument segmentation of optical-flow images during training. We\nthen use the obtained instrument masks as pseudo-labels in order to train a\nper-frame segmentation model; to this aim, we develop a\nlearning-from-noisy-labels architecture, designed to extract a clean\nsupervision signal from these pseudo-labels, leveraging their peculiar noise\nproperties. We validate the proposed contributions on three surgical datasets,\nincluding the MICCAI 2017 EndoVis Robotic Instrument Segmentation Challenge\ndataset. The obtained fully-unsupervised results for surgical instrument\nsegmentation are almost on par with the ones of fully-supervised\nstate-of-the-art approaches. This suggests the tremendous potential of the\nproposed method to leverage the great amount of unlabelled data produced in the\ncontext of minimally invasive surgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sestini_L/0/1/0/all/0/1\">Luca Sestini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1\">Benoit Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrigno_G/0/1/0/all/0/1\">Giancarlo Ferrigno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias in Automated Image Colorization: Metrics and Error Types. (arXiv:2202.08143v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08143","description":"<p>We measure the color shifts present in colorized images from the ADE20K\ndataset, when colorized by the automatic GAN-based DeOldify model. We introduce\nfine-grained local and regional bias measurements between the original and the\ncolorized images, and observe many colorization effects. We confirm a general\ndesaturation effect, and also provide novel observations: a shift towards the\ntraining average, a pervasive blue shift, different color shifts among image\ncategories, and a manual categorization of colorization errors in three\nclasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stapel_F/0/1/0/all/0/1\">Frank Stapel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weers_F/0/1/0/all/0/1\">Floris Weers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1\">Doina Bucur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Class-Cognizant Few-Shot Classification. (arXiv:2202.08149v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08149","description":"<p>Unsupervised learning is argued to be the dark matter of human intelligence.\nTo build in this direction, this paper focuses on unsupervised learning from an\nabundance of unlabeled data followed by few-shot fine-tuning on a downstream\nclassification task. To this aim, we extend a recent study on adopting\ncontrastive learning for self-supervised pre-training by incorporating\nclass-level cognizance through iterative clustering and re-ranking and by\nexpanding the contrastive optimization loss to account for it. To our\nknowledge, our experimentation both in standard and cross-domain scenarios\ndemonstrate that we set a new state-of-the-art (SoTA) in (5-way, 1 and 5-shot)\nsettings of standard mini-ImageNet benchmark as well as the (5-way, 5 and\n20-shot) settings of cross-domain CDFSL benchmark. Our code and experimentation\ncan be found in our GitHub repository: https://github.com/ojss/c3lr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirekar_O/0/1/0/all/0/1\">Ojas Kishore Shirekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamali_Rad_H/0/1/0/all/0/1\">Hadi Jamali-Rad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative modeling with projected entangled-pair states. (arXiv:2202.08177v1 [quant-ph])","link":"http://arxiv.org/abs/2202.08177","description":"<p>We argue and demonstrate that projected entangled-pair states (PEPS)\noutperform matrix product states significantly for the task of generative\nmodeling of datasets with an intrinsic two-dimensional structure such as\nimages. Our approach builds on a recently introduced algorithm for sampling\nPEPS, which allows for the efficient optimization and sampling of the\ndistributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Vieijra_T/0/1/0/all/0/1\">Tom Vieijra</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Vanderstraeten_L/0/1/0/all/0/1\">Laurens Vanderstraeten</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Verstraete_F/0/1/0/all/0/1\">Frank Verstraete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible-Modal Face Anti-Spoofing: A Benchmark. (arXiv:2202.08192v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08192","description":"<p>Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems from presentation attacks. Benefitted from the maturing camera sensors,\nsingle-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in\nvarious scenarios with different configurations of sensors/modalities. Existing\nsingle- and multi-modal FAS methods usually separately train and deploy models\nfor each possible modality scenario, which might be redundant and inefficient.\nCan we train a unified model, and flexibly deploy it under various modality\nscenarios? In this paper, we establish the first flexible-modal FAS benchmark\nwith the principle `train one for all'. To be specific, with trained\nmulti-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings\nare conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and\nRGB+Depth+IR). We also investigate prevalent deep models and feature fusion\nstrategies for flexible-modal FAS. We hope this new benchmark will facilitate\nthe future research of the multi-modal FAS. The protocols and codes are\navailable at https://github.com/ZitongYu/Flex-Modal-FAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kevin H. M. Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Propagation for Annotation-Efficient Nuclei Segmentation from Pathology Images. (arXiv:2202.08195v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08195","description":"<p>Nuclei segmentation is a crucial task for whole slide image analysis in\ndigital pathology. Generally, the segmentation performance of fully-supervised\nlearning heavily depends on the amount and quality of the annotated data.\nHowever, it is time-consuming and expensive for professional pathologists to\nprovide accurate pixel-level ground truth, while it is much easier to get\ncoarse labels such as point annotations. In this paper, we propose a\nweakly-supervised learning method for nuclei segmentation that only requires\npoint annotations for training. The proposed method achieves label propagation\nin a coarse-to-fine manner as follows. First, coarse pixel-level labels are\nderived from the point annotations based on the Voronoi diagram and the k-means\nclustering method to avoid overfitting. Second, a co-training strategy with an\nexponential moving average method is designed to refine the incomplete\nsupervision of the coarse labels. Third, a self-supervised visual\nrepresentation learning method is tailored for nuclei segmentation of pathology\nimages that transforms the hematoxylin component images into the H\\&amp;E stained\nimages to gain better understanding of the relationship between the nuclei and\ncytoplasm. We comprehensively evaluate the proposed method using two public\ndatasets. Both visual and quantitative results demonstrate the superiority of\nour method to the state-of-the-art methods, and its competitive performance\ncompared to the fully-supervised methods. The source codes for implementing the\nexperiments will be released after acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_Z/0/1/0/all/0/1\">Zhiyong Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhongke Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_L/0/1/0/all/0/1\">Lili Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Surgical Phase Recognition from Timestamp Supervision. (arXiv:2202.08199v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08199","description":"<p>Surgical phase recognition is a fundamental task in computer-assisted surgery\nsystems. Most existing works require expensive frame-wise annotations, which is\nvery time-consuming. In this paper, we introduce timestamp supervision to\nsurgical phase recognition for the first time, which only requires randomly\nlabeling one frame for each phase in a video. With timestamp supervision,\ncurrent methods in natural videos aim to generate pseudo labels of full frames.\nHowever, due to the surgical videos containing ambiguous boundaries, these\nmethods would generate many noisy and inconsistent pseudo labels, leading to\nlimited performance. We argue that less is more in surgical phase\nrecognition,~\\ie, less but discriminative pseudo labels outperform full but\nambiguous frames. To this end, we propose a novel method called\nuncertainty-aware temporal diffusion to generate trustworthy pseudo labels. Our\napproach evaluates the confidence of generated pseudo labels based on\nuncertainty estimation. Then, we treat the annotated frames as anchors and make\npseudo labels diffuse to both sides, starting from anchors and stopping at the\nhigh-uncertainty frames. In this way, our proposed method can generate\ncontiguous confident pseudo labels while discarding the uncertain ones.\nExtensive experiments demonstrate that our method not only significantly save\nannotation cost, but also outperforms fully supervised methods. Moreover, our\nproposed approach can be used to clean noisy labels near boundaries and improve\nthe performance of the current surgical phase recognition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinpeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08227","description":"<p>Digitizing physical objects into the virtual world has the potential to\nunlock new research and applications in embodied AI and mixed reality. This\nwork focuses on recreating interactive digital twins of real-world articulated\nobjects, which can be directly imported into virtual environments. We introduce\nDitto to learn articulation model estimation and 3D geometry reconstruction of\nan articulated object through interactive perception. Given a pair of visual\nobservations of an articulated object before and after interaction, Ditto\nreconstructs part-level geometry and estimates the articulation model of the\nobject. We employ implicit neural representations for joint geometry and\narticulation modeling. Our experiments show that Ditto effectively builds\ndigital twins of articulated objects in a category-agnostic way. We also apply\nDitto to real-world objects and deploy the recreated digital twins in physical\nsimulation. Code and additional results are available at\nhttps://ut-austin-rpl.github.io/Ditto\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Cheng-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08238","description":"<p>Breast density estimation is one of the key tasks in recognizing individuals\npredisposed to breast cancer. It is often challenging because of low contrast\nand fluctuations in mammograms' fatty tissue background. Most of the time, the\nbreast density is estimated manually where a radiologist assigns one of the\nfour density categories decided by the Breast Imaging and Reporting Data\nSystems (BI-RADS). There have been efforts in the direction of automating a\nbreast density classification pipeline.\n</p>\n<p>Breast density estimation is one of the key tasks performed during a\nscreening exam. Dense breasts are more susceptible to breast cancer. The\ndensity estimation is challenging because of low contrast and fluctuations in\nmammograms' fatty tissue background. Traditional mammograms are being replaced\nby tomosynthesis and its other low radiation dose variants (for example\nHologic' Intelligent 2D and C-View). Because of the low-dose requirement,\nincreasingly more screening centers are favoring the Intelligent 2D view and\nC-View. Deep-learning studies for breast density estimation use only a single\nmodality for training a neural network. However, doing so restricts the number\nof images in the dataset. In this paper, we show that a neural network trained\non all the modalities at once performs better than a neural network trained on\nany single modality. We discuss these results using the area under the receiver\noperator characteristics curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_V/0/1/0/all/0/1\">Vikash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirer_M/0/1/0/all/0/1\">Mutlu Demirer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maxwell_R/0/1/0/all/0/1\">Robert W. Maxwell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+White_R/0/1/0/all/0/1\">Richard D. White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erdal_B/0/1/0/all/0/1\">Barabaros Selnur Erdal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09147","description":"<p>Convolutional neural network (CNN) has surpassed traditional methods for\nmedical image classification. However, CNN is vulnerable to adversarial attacks\nwhich may lead to disastrous consequences in medical applications. Although\nadversarial noises are usually generated by attack algorithms,\nwhite-noise-induced adversarial samples can exist, and therefore the threats\nare real. In this study, we propose a novel training method, named IMA, to\nimprove the robust-ness of CNN against adversarial noises. During training, the\nIMA method increases the margins of training samples in the input space, i.e.,\nmoving CNN decision boundaries far away from the training samples to improve\nrobustness. The IMA method is evaluated on publicly available datasets under\nstrong 100-PGD white-box adversarial attacks, and the results show that the\nproposed method significantly improved CNN classification and segmentation\naccuracy on noisy data while keeping a high accuracy on clean data. We hope our\napproach may facilitate the development of robust applications in medical\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.01441","description":"<p>We propose neural network layers that explicitly combine frequency and image\nfeature representations and show that they can be used as a versatile building\nblock for reconstruction from frequency space data. Our work is motivated by\nthe challenges arising in MRI acquisition where the signal is a corrupted\nFourier transform of the desired image. The proposed joint learning schemes\nenable both correction of artifacts native to the frequency space and\nmanipulation of image space representations to reconstruct coherent image\nstructures at every layer of the network. This is in contrast to most current\ndeep learning approaches for image reconstruction that treat frequency and\nimage space features separately and often operate exclusively in one of the two\nspaces. We demonstrate the advantages of joint convolutional learning for a\nvariety of tasks, including motion correction, denoising, reconstruction from\nundersampled acquisitions, and combined undersampling and motion correction on\nsimulated and real world multicoil MRI data. The joint models produce\nconsistently high quality output images across all tasks and datasets. When\nintegrated into a state of the art unrolled optimization network with\nphysics-inspired data consistency constraints for undersampled reconstruction,\nthe proposed architectures significantly improve the optimization landscape,\nwhich yields an order of magnitude reduction of training time. This result\nsuggests that joint representations are particularly well suited for MRI\nsignals in deep learning networks. Our code and pretrained models are publicly\navailable at https://github.com/nalinimsingh/interlacer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nalini M. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Learning: A Survey. (arXiv:2007.08745v5 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2007.08745","description":"<p>Backdoor attack intends to embed hidden backdoor into deep neural networks\n(DNNs), so that the attacked models perform well on benign samples, whereas\ntheir predictions will be maliciously changed if the hidden backdoor is\nactivated by attacker-specified triggers. This threat could happen when the\ntraining process is not fully controlled, such as training on third-party\ndatasets or adopting third-party models, which poses a new and realistic\nthreat. Although backdoor learning is an emerging and rapidly growing research\narea, its systematic review, however, remains blank. In this paper, we present\nthe first comprehensive survey of this realm. We summarize and categorize\nexisting backdoor attacks and defenses based on their characteristics, and\nprovide a unified framework for analyzing poisoning-based backdoor attacks.\nBesides, we also analyze the relation between backdoor attacks and relevant\nfields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely\nadopted benchmark datasets. Finally, we briefly outline certain future research\ndirections relying upon reviewed works. A curated list of backdoor-related\nresources is also available at\n\\url{https://github.com/THUYimingLi/backdoor-learning-resources}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Generation Networks with Structure Prediction for Recipe Generation. (arXiv:2007.13374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.13374","description":"<p>Recipe generation from food images and ingredients is a challenging task,\nwhich requires the interpretation of the information from another modality.\nDifferent from the image captioning task, where the captions usually have one\nsentence, cooking instructions contain multiple sentences and have obvious\nstructures. To help the model capture the recipe structure and avoid missing\nsome cooking details, we propose a novel framework: Decomposing Generation\nNetworks (DGN) with structure prediction, to get more structured and complete\nrecipe generation outputs. Specifically, we split each cooking instruction into\nseveral phases, and assign different sub-generators to each phase. Our approach\nincludes two novel ideas: (i) learning the recipe structures with the global\nstructure prediction component and (ii) producing recipe phases in the\nsub-generator output component based on the predicted structure. Extensive\nexperiments on the challenging large-scale Recipe1M dataset validate the\neffectiveness of our proposed model, which improves the performance over the\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-valued Iris Recognition Network. (arXiv:2011.11198v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11198","description":"<p>In this work, we design a fully complex-valued neural network for the task of\niris recognition. Unlike the problem of general object recognition, where\nreal-valued neural networks can be used to extract pertinent features, iris\nrecognition depends on the extraction of both phase and magnitude information\nfrom the input iris texture in order to better represent its biometric content.\nThis necessitates the extraction and processing of phase information that\ncannot be effectively handled by a real-valued neural network. In this regard,\nwe design a fully complex-valued neural network that can better capture the\nmulti-scale, multi-resolution, and multi-orientation phase and amplitude\nfeatures of the iris texture. We show a strong correspondence of the proposed\ncomplex-valued iris recognition network with Gabor wavelets that are used to\ngenerate the classical IrisCode; however, the proposed method enables a new\ncapability of automatic complex-valued feature learning that is tailored for\niris recognition. We conduct experiments on three benchmark datasets -\nND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit\nof the proposed network for the task of iris recognition. We exploit\nvisualization schemes to convey how the complex-valued network, when compared\nto standard real-valued networks, extracts fundamentally different features\nfrom the iris texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16938","description":"<p>Real-time estimation of actual environment depth is an essential module for\nvarious autonomous system tasks such as localization, obstacle detection and\npose estimation. During the last decade of machine learning, extensive\ndeployment of deep learning methods to computer vision tasks yielded successful\napproaches for realistic depth synthesis out of a simple RGB modality. While\nmost of these models rest on paired depth data or availability of video\nsequences and stereo images, there is a lack of methods facing single-image\ndepth synthesis in an unsupervised manner. Therefore, in this study, latest\nadvancements in the field of generative neural networks are leveraged to fully\nunsupervised single-image depth synthesis. To be more exact, two\ncycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are\nimplemented and simultaneously optimized using the Wasserstein-1 distance. To\nensure plausibility of the proposed method, we apply the models to a self\nacquised industrial data set as well as to the renown NYU Depth v2 data set,\nwhich allows comparison with existing approaches. The observed success in this\nstudy suggests high potential for unpaired single-image depth estimation in\nreal world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1\">Ad&#xe9;la Moravov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics. (arXiv:2106.01981v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01981","description":"<p>Our work focuses on the development of a learnable neural representation of\nhuman pose for advanced AI assisted animation tooling. Specifically, we tackle\nthe problem of constructing a full static human pose based on sparse and\nvariable user inputs (e.g. locations and/or orientations of a subset of body\njoints). To solve this problem, we propose a novel neural architecture that\ncombines residual connections with prototype encoding of a partially specified\npose to create a new complete pose from the learned latent space. We show that\nour architecture outperforms a baseline based on Transformer, both in terms of\naccuracy and computational efficiency. Additionally, we develop a user\ninterface to integrate our neural model in Unity, a real-time 3D development\nplatform. Furthermore, we introduce two new datasets representing the static\nhuman pose modeling problem, based on high-quality human motion capture data,\nwhich will be released publicly along with model code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1\">Boris N. Oreshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1\">Florent Bocquelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1\">F&#xe9;lix G. Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1\">Bay Raitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1\">Dominic Laflamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preservation of the Global Knowledge by Not-True Self Knowledge Distillation in Federated Learning. (arXiv:2106.03097v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03097","description":"<p>In federated learning, a strong global model is collaboratively learned by\naggregating clients' locally trained models. Although this precludes the need\nto access clients' data directly, the global model's convergence often suffers\nfrom data heterogeneity. This study starts from an analogy to continual\nlearning and suggests that forgetting could be the bottleneck of federated\nlearning. We observe that the global model forgets the knowledge from previous\nrounds, and the local training induces forgetting the knowledge outside of the\nlocal distribution. Based on our findings, we hypothesize that tackling down\nforgetting will relieve the data heterogeneity problem. To this end, we propose\na novel and effective algorithm, Federated Not-True Distillation (FedNTD),\nwhich preserves the global perspective on locally available data only for the\nnot-true classes. In the experiments, FedNTD shows state-of-the-art performance\non various setups without compromising data privacy or incurring additional\ncommunication costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minchan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yongjin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-based Style Randomization for Domain Generalization. (arXiv:2106.03171v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03171","description":"<p>As a recent noticeable topic, domain generalization (DG) aims to first learn\na generic model on multiple source domains and then directly generalize to an\narbitrary unseen target domain without any additional adaption. In previous DG\nmodels, by generating virtual data to supplement observed source domains, the\ndata augmentation based methods have shown its effectiveness. To simulate the\npossible unseen domains, most of them enrich the diversity of original data via\nimage-level style transformation. However, we argue that the potential styles\nare hard to be exhaustively illustrated and fully augmented due to the limited\nreferred styles, leading the diversity could not be always guaranteed. Unlike\nimage-level augmentation, we in this paper develop a simple yet effective\nfeature-based style randomization module to achieve feature-level augmentation,\nwhich can produce random styles via integrating random noise into the original\nstyle. Compared with existing image-level augmentation, our feature-level\naugmentation favors a more goal-oriented and sample-diverse way. Furthermore,\nto sufficiently explore the efficacy of the proposed module, we design a novel\nprogressive training strategy to enable all parameters of the network to be\nfully trained. Extensive experiments on three standard benchmark datasets,\ni.e., PACS, VLCS and Office-Home, highlight the superiority of our method\ncompared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable 3D Spherical Neurons. (arXiv:2106.13863v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of neurons with spherical\ndecision surfaces and operates on point clouds. Such spherical neurons are\nobtained by conformal embedding of Euclidean space and have recently been\nrevisited in the context of learning representations of point sets. Focusing on\n3D geometry, we exploit the isometry property of spherical neurons and derive a\n3D steerability constraint. After training spherical neurons to classify point\nclouds in a canonical orientation, we use a tetrahedron basis to quadruplicate\nthe neurons and construct rotation-equivariant spherical filter banks. We then\napply the derived constraint to interpolate the filter bank outputs and, thus,\nobtain a rotation-invariant network. Finally, we use a synthetic point set and\nreal-world 3D skeleton data to verify our theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11941","description":"<p>Accurately segmenting teeth and identifying the corresponding anatomical\nlandmarks on dental mesh models are essential in computer-aided orthodontic\ntreatment. Manually performing these two tasks is time-consuming, tedious, and,\nmore importantly, highly dependent on orthodontists' experiences due to the\nabnormality and large-scale variance of patients' teeth. Some machine\nlearning-based methods have been designed and applied in the orthodontic field\nto automatically segment dental meshes (e.g., intraoral scans). In contrast,\nthe number of studies on tooth landmark localization is still limited. This\npaper proposes a two-stage framework based on mesh deep learning (called\nTS-MDL) for joint tooth labeling and landmark identification on raw intraoral\nscans. Our TS-MDL first adopts an end-to-end \\emph{i}MeshSegNet method (i.e., a\nvariant of the existing MeshSegNet with both improved accuracy and efficiency)\nto label each tooth on the downsampled scan. Guided by the segmentation\noutputs, our TS-MDL further selects each tooth's region of interest (ROI) on\nthe original mesh to construct a light-weight variant of the pioneering\nPointNet (i.e., PointNet-Reg) for regressing the corresponding landmark\nheatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing\npromising segmentation and localization performance. Specifically,\n\\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice\nsimilarity coefficient (DSC) at $0.964\\pm0.054$, significantly outperforming\nthe original MeshSegNet. In the second stage, PointNet-Reg achieved a mean\nabsolute error (MAE) of $0.597\\pm0.761 \\, mm$ in distances between the\nprediction and ground truth for $66$ landmarks, which is superior compared with\nother networks for landmark detection. All these results suggest the potential\nusage of our TS-MDL in clinical practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tai-Hsien Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastewait_M/0/1/0/all/0/1\">Matthew Pastewait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piers_C/0/1/0/all/0/1\">Christian Piers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chiung-Ying Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenchi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_C/0/1/0/all/0/1\">Christina Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Chang Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-theoretic Algorithm for Small Bowel Path Tracking in CT Scans. (arXiv:2110.00466v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.00466","description":"<p>We present a novel graph-theoretic method for small bowel path tracking. It\nis formulated as finding the minimum cost path between given start and end\nnodes on a graph that is constructed based on the bowel wall detection. We\nobserved that a trivial solution with many short-cuts is easily made even with\nthe wall detection, where the tracked path penetrates indistinct walls around\nthe contact between different parts of the small bowel. Thus, we propose to\ninclude must-pass nodes in finding the path to better cover the entire course\nof the small bowel. The proposed method does not entail training with\nground-truth paths while the previous methods do. We acquired ground-truth\npaths that are all connected from start to end of the small bowel for 10\nabdominal CT scans, which enables the evaluation of the path tracking for the\nentire course of the small bowel. The proposed method showed clear improvements\nin terms of several metrics compared to the baseline method. The maximum length\nof the path that is tracked without an error per scan, by the proposed method,\nis above 800mm on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sungwon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Objects in Semantic Topology. (arXiv:2110.02687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02687","description":"<p>A more realistic object detection paradigm, Open-World Object Detection, has\narisen increasing research interests in the community recently. A qualified\nopen-world object detector can not only identify objects of known categories,\nbut also discover unknown objects, and incrementally learn to categorize them\nwhen their annotations progressively arrive. Previous works rely on independent\nmodules to recognize unknown categories and perform incremental learning,\nrespectively. In this paper, we provide a unified perspective: Semantic\nTopology. During the life-long learning of an open-world object detector, all\nobject instances from the same category are assigned to their corresponding\npre-defined node in the semantic topology, including the `unknown' category.\nThis constraint builds up discriminative feature representations and consistent\nrelationships among objects, thus enabling the detector to distinguish unknown\nobjects out of the known categories, as well as making learned features of\nknown objects undistorted when learning new categories incrementally. Extensive\nexperiments demonstrate that semantic topology, either randomly-generated or\nderived from a well-trained language model, could outperform the current\nstate-of-the-art open-world object detectors by a large margin, e.g., the\nabsolute open-set error is reduced from 7832 to 2546, exhibiting the inherent\nsuperiority of semantic topology on open-world object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Strong Gravitational Lenses Through Self-Attention. (arXiv:2110.09202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09202","description":"<p>The upcoming large scale surveys are expected to find approximately $10^5$\nstrong gravitational systems by analyzing data of many orders of magnitude than\nthe current era. In this scenario, non-automated techniques will be highly\nchallenging and time-consuming. We propose a new automated architecture based\non the principle of self-attention to find strong gravitational lensing. The\nadvantages of self-attention based encoder models over convolution neural\nnetworks are investigated and encoder models are analyzed to optimize\nperformance. We constructed 21 self-attention based encoder models and four\nconvolution neural networks trained to identify gravitational lenses from the\nBologna Lens Challenge. Each model is trained separately using 18,000 simulated\nimages, cross-validated using 2 000 images, and then applied to a test set with\n100 000 images. We used four different metrics for evaluation: classification\naccuracy, the area under the receiver operating characteristic curve (AUROC),\nthe $TPR_0$ score and the $TPR_{10}$ score. The performance of the\nself-attention based encoder models and CNN's participated in the challenge are\ncompared. The encoder models performed better than the CNNs and surpassed the\nCNN models that participated in the bologna lens challenge by a high margin for\nthe $TPR_0$ and $TPR_{10}$. In terms of the AUROC, the encoder models scored\nequivalent to the top CNN model by only using one-sixth parameters to that of\nthe CNN. Self-Attention based models have a clear advantage compared to simpler\nCNNs. A low computational cost and complexity make it a highly competing\narchitecture to currently used residual neural networks. Moreover, introducing\nthe encoder layers can also tackle the over-fitting problem present in the\nCNN's by acting as effective filters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thuruthipilly_H/0/1/0/all/0/1\">Hareesh Thuruthipilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_A/0/1/0/all/0/1\">Adam Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollo_A/0/1/0/all/0/1\">Agnieszka Pollo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung Cancer Lesion Detection in Histopathology Images Using Graph-Based Sparse PCA Network. (arXiv:2110.14728v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14728","description":"<p>Early detection of lung cancer is critical for improvement of patient\nsurvival. To address the clinical need for efficacious treatments, genetically\nengineered mouse models (GEMM) have become integral in identifying and\nevaluating the molecular underpinnings of this complex disease that may be\nexploited as therapeutic targets. Assessment of GEMM tumor burden on\nhistopathological sections performed by manual inspection is both time\nconsuming and prone to subjective bias. Therefore, an interplay of needs and\nchallenges exists for computer-aided diagnostic tools, for accurate and\nefficient analysis of these histopathology images. In this paper, we propose a\nsimple machine learning approach called the graph-based sparse principal\ncomponent analysis (GS-PCA) network, for automated detection of cancerous\nlesions on histological lung slides stained by hematoxylin and eosin (H&amp;E). Our\nmethod comprises four steps: 1) cascaded graph-based sparse PCA, 2) PCA binary\nhashing, 3) block-wise histograms, and 4) support vector machine (SVM)\nclassification. In our proposed architecture, graph-based sparse PCA is\nemployed to learn the filter banks of the multiple stages of a convolutional\nnetwork. This is followed by PCA hashing and block histograms for indexing and\npooling. The meaningful features extracted from this GS-PCA are then fed to an\nSVM classifier. We evaluate the performance of the proposed algorithm on H&amp;E\nslides obtained from an inducible K-rasG12D lung cancer mouse model using\nprecision/recall rates, F-score, Tanimoto coefficient, and area under the curve\n(AUC) of the receiver operator characteristic (ROC) and show that our algorithm\nis efficient and provides improved detection accuracy compared to existing\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ram_S/0/1/0/all/0/1\">Sundaresh Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1\">Wenfei Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_A/0/1/0/all/0/1\">Alexander J. Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spencer_C/0/1/0/all/0/1\">Cara Spencer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buschhaus_A/0/1/0/all/0/1\">Alexander Buschhaus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatt_C/0/1/0/all/0/1\">Charles R. Hatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+diMagliano_M/0/1/0/all/0/1\">Marina Pasca diMagliano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_J/0/1/0/all/0/1\">Jeffrey J. Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galban_S/0/1/0/all/0/1\">Stefanie Galban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galban_C/0/1/0/all/0/1\">Craig J. Galban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The self-supervised spectral-spatial attention-based transformer network for automated, accurate prediction of crop nitrogen status from UAV imagery. (arXiv:2111.06839v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06839","description":"<p>Nitrogen (N) fertilizer is routinely applied by farmers to increase crop\nyields. At present, farmers often over-apply N fertilizer in some locations or\nat certain times because they do not have high-resolution crop N status data.\nN-use efficiency can be low, with the remaining N lost to the environment,\nresulting in higher production costs and environmental pollution. Accurate and\ntimely estimation of N status in crops is crucial to improving cropping\nsystems' economic and environmental sustainability. Destructive approaches\nbased on plant tissue analysis are time consuming and impractical over large\nfields. Recent advances in remote sensing and deep learning have shown promise\nin addressing the aforementioned challenges in a non-destructive way. In this\nwork, we propose a novel deep learning framework: a self-supervised\nspectral-spatial attention-based vision transformer (SSVT). The proposed SSVT\nintroduces a Spectral Attention Block (SAB) and a Spatial Interaction Block\n(SIB), which allows for simultaneous learning of both spatial and spectral\nfeatures from UAV digital aerial imagery, for accurate N status prediction in\nwheat fields. Moreover, the proposed framework introduces local-to-global\nself-supervised learning to help train the model from unlabelled data. The\nproposed SSVT has been compared with five state-of-the-art models including:\nResNet, RegNet, EfficientNet, EfficientNetV2 and the original vision\ntransformer on both testing and independent datasets. The proposed approach\nachieved high accuracy (0.96) with good generalizability and reproducibility\nfor wheat N status estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobeih_T/0/1/0/all/0/1\">Tam Sobeih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lappin_L/0/1/0/all/0/1\">Lewis Lappin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mark Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andew Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kisdi_A/0/1/0/all/0/1\">Aron Kisdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hamiltonian Operator Disentanglement of Content and Motion in Image Sequences. (arXiv:2112.01641v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01641","description":"<p>We introduce a deep generative model for image sequences that reliably\nfactorise the latent space into content and motion variables. To model the\ndiverse dynamics, we split the motion space into subspaces and introduce a\nunique Hamiltonian operator for each subspace. The Hamiltonian formulation\nprovides reversible dynamics that constrain the evolution of the motion path\nalong the low-dimensional manifold and conserves learnt invariant properties.\nThe explicit split of the motion space decomposes the Hamiltonian into symmetry\ngroups and gives long-term separability of the dynamics. This split also means\nwe can learn content representations that are easy to interpret and control. We\ndemonstrate the utility of our model by swapping the motion of two videos,\ngenerating long term sequences of various actions from a given image,\nunconditional sequence generation and image rotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asif Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Interpretation of Public Scenes Using Computer Vision and Temporal Graphs to Identify Social Distancing Violations. (arXiv:2112.06428v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06428","description":"<p>The COVID-19 pandemic has caused an unprecedented global public health\ncrisis. Given its inherent nature, social distancing measures are proposed as\nthe primary strategies to curb the spread of this pandemic. Therefore,\nidentifying situations where these protocols are violated, has implications for\ncurtailing the spread of the disease and promoting a sustainable lifestyle.\nThis paper proposes a novel computer vision-based system to analyze CCTV\nfootage to provide a threat level assessment of COVID-19 spread. The system\nstrives to holistically capture and interpret the information content of CCTV\nfootage spanning multiple frames to recognize instances of various violations\nof social distancing protocols, across time and space, as well as\nidentification of group behaviors. This functionality is achieved primarily by\nutilizing a temporal graph-based structure to represent the information of the\nCCTV footage and a strategy to holistically interpret the graph and quantify\nthe threat level of the given scene. The individual components are tested and\nvalidated on a range of scenarios and the complete system is tested against\nhuman expert opinion. The results reflect the dependence of the threat level on\npeople, their physical proximity, interactions, protective clothing, and group\ndynamics. The system performance has an accuracy of 76%, thus enabling a\ndeployable threat monitoring system in cities, to permit normalcy and\nsustainability in the society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_J/0/1/0/all/0/1\">Jameel Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senananayaka_J/0/1/0/all/0/1\">Janith Bandara Senananayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1\">Harshana Weligampola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_J/0/1/0/all/0/1\">Janaka Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmaratne_S/0/1/0/all/0/1\">Samath Dharmaratne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation. (arXiv:2201.08093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08093","description":"<p>In this letter, we present a novel markerless 3D human motion capture (MoCap)\nsystem for unstructured, outdoor environments that uses a team of autonomous\nunmanned aerial vehicles (UAVs) with on-board RGB cameras and computation.\nExisting methods are limited by calibrated cameras and off-line processing.\nThus, we present the first method (AirPose) to estimate human pose and shape\nusing images captured by multiple extrinsically uncalibrated flying cameras.\nAirPose itself calibrates the cameras relative to the person instead of relying\non any pre-calibration. It uses distributed neural networks running on each UAV\nthat communicate viewpoint-independent information with each other about the\nperson (i.e., their 3D shape and articulated pose). The person's shape and pose\nare parameterized using the SMPL-X body model, resulting in a compact\nrepresentation, that minimizes communication between the UAVs. The network is\ntrained using synthetic images of realistic virtual environments, and\nfine-tuned on a small set of real images. We also introduce an\noptimization-based post-processing method (AirPose$^{+}$) for offline\napplications that require higher MoCap quality. We make our method's code and\ndata available for research at\nhttps://github.com/robot-perception-group/AirPose. A video describing the\napproach and results is available at https://youtu.be/xLYe1TNHsfs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1\">Nitin Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonetto_E/0/1/0/all/0/1\">Elia Bonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aamir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Array Network for Low-light Image Enhancement. (arXiv:2201.08996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08996","description":"<p>Convolution neural networks (CNNs) based methods have dominated the low-light\nimage enhancement tasks due to their outstanding performance. However, the\nconvolution operation is based on a local sliding window mechanism, which is\ndifficult to construct the long-range dependencies of the feature maps.\nMeanwhile, the self-attention based global relationship aggregation methods\nhave been widely used in computer vision, but these methods are difficult to\nhandle high-resolution images because of the high computational complexity. To\nsolve this problem, this paper proposes a Linear Array Self-attention (LASA)\nmechanism, which uses only two 2-D feature encodings to construct 3-D global\nweights and then refines feature maps generated by convolution layers. Based on\nLASA, Linear Array Network (LAN) is proposed, which is superior to the existing\nstate-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement\ntasks with a smaller amount of parameters. The code is released in\nhttps://github.com/cuiziteng/LASA_enhancement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jieru Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Ge Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiguo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuhua Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12083","description":"<p>Recently, MLP-like vision models have achieved promising performances on\nmainstream visual recognition tasks. In contrast with vision transformers and\nCNNs, the success of MLP-like models shows that simple information fusion\noperations among tokens and channels can yield a good representation power for\ndeep recognition models. However, existing MLP-like models fuse tokens through\nstatic fusion operations, lacking adaptability to the contents of the tokens to\nbe mixed. Thus, customary information fusion procedures are not effective\nenough. To this end, this paper presents an efficient MLP-like network\narchitecture, dubbed DynaMixer, resorting to dynamic information fusion.\nCritically, we propose a procedure, on which the DynaMixer model relies, to\ndynamically generate mixing matrices by leveraging the contents of all the\ntokens to be mixed. To reduce the time complexity and improve the robustness, a\ndimensionality reduction technique and a multi-segment fusion mechanism are\nadopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\\% top-1\naccuracy on the ImageNet-1K dataset without extra training data, performing\nfavorably against the state-of-the-art vision MLP models. When the number of\nparameters is reduced to 26M, it still achieves 82.7\\% top-1 accuracy,\nsurpassing the existing MLP-like models with a similar capacity. The\nimplementation of DynaMixer will be made available to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O-ViT: Orthogonal Vision Transformer. (arXiv:2201.12133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12133","description":"<p>Inspired by the tremendous success of the self-attention mechanism in natural\nlanguage processing, the Vision Transformer (ViT) creatively applies it to\nimage patch sequences and achieves incredible performance. However, the scaled\ndot-product self-attention of ViT brings about scale ambiguity to the structure\nof the original feature space. To address this problem, we propose a novel\nmethod named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the\ngeometric perspective. O-ViT limits parameters of self-attention blocks to be\non the norm-keeping orthogonal manifold, which can keep the geometry of the\nfeature space. Moreover, O-ViT achieves both orthogonal constraints and cheap\noptimization overhead by adopting a surjective mapping between the orthogonal\ngroup and its Lie algebra.We have conducted comparative experiments on image\nrecognition tasks to demonstrate O-ViT's validity and experiments show that\nO-ViT can boost the performance of ViT by up to 3.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yanhong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingsong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12170","description":"<p>Real-time estimation of actual object depth is a module that is essential to\nperforming various autonomous system tasks such as 3D reconstruction, scene\nunderstanding and condition assessment of machinery parts. During the last\ndecade of machine learning, extensive deployment of deep learning methods to\ncomputer vision tasks has yielded approaches that succeed in achieving\nrealistic depth synthesis out of a simple RGB modality. While most of these\nmodels are based on paired depth data or availability of video sequences and\nstereo images, methods for single-view depth synthesis in a fully unsupervised\nsetting have hardly been explored. This study presents the most recent advances\nin the field of generative neural networks, leveraging them to perform fully\nunsupervised single-shot depth synthesis. Two generators for RGB-to-depth and\ndepth-to-RGB transfer are implemented and simultaneously optimized using the\nWasserstein-1 distance and a novel perceptual reconstruction term. To ensure\nthat the proposed method is plausible, we comprehensively evaluate the models\nusing industrial surface depth data as well as the Texas 3D Face Recognition\nDatabase and the SURREAL dataset that records body depth. The success observed\nin this study suggests the great potential for unsupervised single-shot depth\nestimation in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.12589","description":"<p>The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in the diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical or even\nluxurious, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. Previously, the misaligned\nunpaired neuroimaging data (termed as MUD) are generally treated as noisy\nlabel. However, such a noisy label-based method could not work very well when\nmisaligned data occurs distortions severely, for example, different angles of\nrotation. In this paper, we propose a novel federated self-supervised learning\n(FedMed) for brain image synthesis. An affine transform loss (ATL) was\nformulated to make use of severely distorted images without violating privacy\nlegislation for the hospital. We then introduce a new data augmentation\nprocedure for self-supervised training and fed it into three auxiliary heads,\nnamely auxiliary rotation, auxiliary translation, and auxiliary scaling heads.\nThe proposed method demonstrates advanced performance in both the quality of\nsynthesized results under a severely misaligned and unpaired data setting, and\nbetter stability than other GAN-based algorithms. The proposed method also\nreduces the demand for deformable registration while encouraging to realize the\nusage of those misaligned and unpaired data. Experimental results verify the\noutstanding ability of our learning paradigm compared to other state-of-the-art\napproaches. Our code is available on the website:\nhttps://github.com/FedMed-Meta/FedMed-ATL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms. (arXiv:2201.12728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12728","description":"<p>Unlike the conventional facial expressions, micro-expressions are involuntary\nand transient facial expressions capable of revealing the genuine emotions that\npeople attempt to hide. Therefore, they can provide important information in a\nbroad range of applications such as lie detection, criminal detection, etc.\nSince micro-expressions are transient and of low intensity, however, their\ndetection and recognition is difficult and relies heavily on expert\nexperiences. Due to its intrinsic particularity and complexity, video-based\nmicro-expression analysis is attractive but challenging, and has recently\nbecome an active area of research. Although there have been numerous\ndevelopments in this area, thus far there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences between macro- and micro-expressions, then use these\ndifferences to guide our research survey of video-based micro-expression\nanalysis in a cascaded structure, encompassing the neuropsychological basis,\ndatasets, features, spotting algorithms, recognition algorithms, applications\nand evaluation of state-of-the-art approaches. For each aspect, the basic\ntechniques, advanced developments and major challenges are addressed and\ndiscussed. Furthermore, after considering the limitations of existing\nmicro-expression datasets, we present and release a new dataset - called\nmicro-and-macro expression warehouse (MMEW) - containing more video samples and\nmore labeled emotion types. We then perform a unified comparison of\nrepresentative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for\nrecognition, respectively. Finally, some potential future research directions\nare explored and outlined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_X/0/1/0/all/0/1\">Xianye Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su-Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Weixiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.13392","description":"<p>The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation. (arXiv:2202.02371v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02371","description":"<p>Unsupervised pre-training has been proven as an effective approach to boost\nvarious downstream tasks given limited labeled data. Among various methods,\ncontrastive learning learns a discriminative representation by constructing\npositive and negative pairs. However, it is not trivial to build reasonable\npairs for a segmentation task in an unsupervised way. In this work, we propose\na novel unsupervised pre-training framework that avoids the drawback of\ncontrastive learning. Our framework consists of two principles: unsupervised\nover-segmentation as a pre-train task using mutual information maximization and\nboundary-aware preserving learning. Experimental results on two benchmark\nmedical segmentation datasets reveal our method's effectiveness in improving\nsegmentation performance when few annotated images are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Computational Cytology: A Survey. (arXiv:2202.05126v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05126","description":"<p>Computational cytology is a critical, rapid-developing, yet challenging topic\nin the field of medical image computing which analyzes the digitized cytology\nimage by computer-aided technologies for cancer screening. Recently, an\nincreasing number of deep learning (DL) algorithms have made significant\nprogress in medical image analysis, leading to the boosting publications of\ncytological studies. To investigate the advanced methods and comprehensive\napplications, we survey more than 120 publications of DL-based cytology image\nanalysis in this article. We first introduce various deep learning methods,\nincluding fully supervised, weakly supervised, unsupervised, and transfer\nlearning. Then, we systematically summarize the public datasets, evaluation\nmetrics, versatile cytology image analysis applications including\nclassification, detection, segmentation, and other related tasks. Finally, we\ndiscuss current challenges and potential research directions of computational\ncytology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_R/0/1/0/all/0/1\">Ronald CK Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05331","description":"<p>Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Daniel Louzada Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marcos Henrique Fonseca Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_F/0/1/0/all/0/1\">Fabio Ribeiro Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Michel Melo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges. (arXiv:2202.06511v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06511","description":"<p>Color fundus photography and Optical Coherence Tomography (OCT) are the two\nmost cost-effective tools for glaucoma screening. Both two modalities of images\nhave prominent biomarkers to indicate glaucoma suspected. Clinically, it is\noften recommended to take both of the screenings for a more accurate and\nreliable diagnosis. However, although numerous algorithms are proposed based on\nfundus images or OCT volumes in computer-aided diagnosis, there are still few\nmethods leveraging both of the modalities for the glaucoma assessment. Inspired\nby the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held\npreviously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)\nChallenge to encourage the development of fundus \\&amp; OCT-based glaucoma grading.\nThe primary task of the challenge is to grade glaucoma from both the 2D fundus\nimages and 3D OCT scanning volumes. As part of GAMMA, we have publicly released\na glaucoma annotated dataset with both 2D fundus color photography and 3D OCT\nvolumes, which is the first multi-modality dataset for glaucoma grading. In\naddition, an evaluation framework is also established to evaluate the\nperformance of the submitted methods. During the challenge, 1272 results were\nsubmitted, and finally, top-10 teams were selected to the final stage. We\nanalysis their results and summarize their methods in the paper. Since all\nthese teams submitted their source code in the challenge, a detailed ablation\nstudy is also conducted to verify the effectiveness of the particular modules\nproposed. We find many of the proposed techniques are practical for the\nclinical diagnosis of glaucoma. As the first in-depth study of fundus \\&amp; OCT\nmulti-modality glaucoma grading, we believe the GAMMA Challenge will be an\nessential starting point for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fengbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiongcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lexing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qinji Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wensai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_C/0/1/0/all/0/1\">Chubin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xifei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbi_R/0/1/0/all/0/1\">Riadh Kobbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Cross-Modality Brain Image Synthesis. (arXiv:2202.06997v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06997","description":"<p>The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical or even\nluxurious, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. A realistic solution is to\nexplore either an unsupervised learning or a semi-supervised learning to\nsynthesize the absent neuroimaging data. In this paper, we tend to approach\nmulti-modality brain image synthesis task from different perspectives, which\ninclude the level of supervision, the range of modality synthesis, and the\nsynthesis-based downstream tasks. Particularly, we provide in-depth analysis on\nhow cross-modality brain image synthesis can improve the performance of\ndifferent downstream tasks. Finally, we evaluate the challenges and provide\nseveral open directions for this community. All resources are available at\nhttps://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Domain Experts for Long-Tailed Camera-Trap Recognition. (arXiv:2202.07215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07215","description":"<p>Label distributions in camera-trap images are highly imbalanced and\nlong-tailed, resulting in neural networks tending to be biased towards\nhead-classes that appear frequently. Although long-tail learning has been\nextremely explored to address data imbalances, few studies have been conducted\nto consider camera-trap characteristics, such as multi-domain and multi-frame\nsetup. Here, we propose a unified framework and introduce two datasets for\nlong-tailed camera-trap recognition. We first design domain experts, where each\nexpert learns to balance imperfect decision boundaries caused by data\nimbalances and complement each other to generate domain-balanced decision\nboundaries. Also, we propose a flow consistency loss to focus on moving\nobjects, expecting class activation maps of multi-frame matches the flow with\noptical flow maps for input images. Moreover, two long-tailed camera-trap\ndatasets, WCS-LT and DMZ-LT, are introduced to validate our methods.\nExperimental results show the effectiveness of our framework, and proposed\nmethods outperform previous methods on recessive domain samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Byeongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seungju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07261","description":"<p>3D dynamic point clouds provide a discrete representation of real-world\nobjects or scenes in motion, which have been widely applied in immersive\ntelepresence, autonomous driving, surveillance, \\textit{etc}. However, point\nclouds acquired from sensors are usually perturbed by noise, which affects\ndownstream tasks such as surface reconstruction and analysis. Although many\nefforts have been made for static point cloud denoising, few works address\ndynamic point cloud denoising. In this paper, we propose a novel gradient-based\ndynamic point cloud denoising method, exploiting the temporal correspondence\nfor the estimation of gradient fields -- also a fundamental problem in dynamic\npoint cloud processing and analysis. The gradient field is the gradient of the\nlog-probability function of the noisy point cloud, based on which we perform\ngradient ascent so as to converge each point to the underlying clean surface.\nWe estimate the gradient of each surface patch by exploiting the temporal\ncorrespondence, where the temporally corresponding patches are searched\nleveraging on rigid motion in classical mechanics. In particular, we treat each\npatch as a rigid object, which moves in the gradient field of an adjacent frame\nvia force until reaching a balanced state, i.e., when the sum of gradients over\nthe patch reaches 0. Since the gradient would be smaller when the point is\ncloser to the underlying surface, the balanced patch would fit the underlying\nsurface well, thus leading to the temporal correspondence. Finally, the\nposition of each point in the patch is updated along the direction of the\ngradient averaged from corresponding patches in adjacent frames. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qianjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}