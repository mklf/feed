{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations. (arXiv:2109.12174v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12174","description":"<p>Fine-tuning pretrained models for automatically summarizing doctor-patient\nconversation transcripts presents many challenges: limited training data,\nsignificant domain shift, long and noisy transcripts, and high target summary\nvariability. In this paper, we explore the feasibility of using pretrained\ntransformer models for automatically summarizing doctor-patient conversations\ndirectly from transcripts. We show that fluent and adequate summaries can be\ngenerated with limited training data by fine-tuning BART on a specially\nconstructed dataset. The resulting models greatly surpass the performance of an\naverage human annotator and the quality of previous published work for the\ntask. We evaluate multiple methods for handling long conversations, comparing\nthem to the obvious baseline of truncating the conversation to fit the\npretrained model length limit. We introduce a multistage approach that tackles\nthe task by learning two fine-tuned models: one for summarizing conversation\nchunks into partial summaries, followed by one for rewriting the collection of\npartial summaries into a complete summary. Using a carefully chosen fine-tuning\ndataset, this method is shown to be effective at handling longer conversations,\nimproving the quality of generated summaries. We conduct both an automatic\nevaluation (through ROUGE and two concept-based metrics focusing on medical\nfindings) and a human evaluation (through qualitative examples from literature,\nassessing hallucination, generalization, fluency, and general quality of the\ngenerated summaries).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrinho_R/0/1/0/all/0/1\">Renato Negrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arindam Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagannathan_V/0/1/0/all/0/1\">Vasudevan Jagannathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanzadeh_H/0/1/0/all/0/1\">Hamid Reza Hassanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_T/0/1/0/all/0/1\">Thomas Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Attention Sparsity in Transformers. (arXiv:2109.12188v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12188","description":"<p>A bottleneck in transformer architectures is their quadratic complexity with\nrespect to the input sequence, which has motivated a body of work on efficient\nsparse approximations to softmax. An alternative path, used by entmax\ntransformers, consists of having built-in exact sparse attention; however this\napproach still requires quadratic computation. In this paper, we propose\nSparsefinder, a simple model trained to identify the sparsity pattern of entmax\nattention before computing it. We experiment with three variants of our method,\nbased on distances, quantization, and clustering, on two tasks: machine\ntranslation (attention in the decoder) and masked language modeling\n(encoder-only). Our work provides a new angle to study model efficiency by\ndoing extensive analysis of the tradeoff between the sparsity and recall of the\npredicted attention graph. This allows for detailed comparison between\ndifferent models, and may guide future benchmarks for sparse models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gois_A/0/1/0/all/0/1\">Ant&#xf3;nio G&#xf3;is</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Truly Matters? Using Linguistic Cues for Analyzing the #BlackLivesMatter Movement and its Counter Protests: 2013 to 2020. (arXiv:2109.12192v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12192","description":"<p>Since the fatal shooting of 17-year old Black teenager Trayvon Martin in\nFebruary 2012 by a White neighborhood watchman, George Zimmerman in Sanford,\nFlorida, there has been a significant increase in digital activism addressing\npolice-brutality related and racially-motivated incidents in the United States.\nIn this work, we administer an innovative study of digital activism by\nexploiting social media as an authoritative tool to examine and analyze the\nlinguistic cues and thematic relationships in these three mediums. We conduct a\nmulti-level text analysis on 36,984,559 tweets to investigate users' behaviors\nto examine the language used and understand the impact of digital activism on\nsocial media within each social movement on a sentence-level, word-level, and\ntopic-level. Our results show that excessive use of racially-related or\nprejudicial hashtags were used by the counter protests which portray potential\ndiscriminatory tendencies. Consequently, our findings highlight that social\nactivism done by Black Lives Matter activists does not diverge from the social\nissues and topics involving police-brutality related and racially-motivated\nkillings of Black individuals due to the shape of its topical graph that topics\nand conversations encircling the largest component directly relate to the topic\nof Black Lives Matter. Finally, we see that both Blue Lives Matter and All\nLives Matter movements depict a different directive, as the topics of Blue\nLives Matter or All Lives Matter do not reside in the center. These findings\nsuggest that topics and conversations within each social movement are skewed,\nrandom or possessed racially-related undertones, and thus, deviating from the\nprominent social injustice issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dacon_J/0/1/0/all/0/1\">Jamell Dacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Control for Schema-Guided Natural Language Generation. (arXiv:2109.12211v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12211","description":"<p>Natural Language Generation (NLG) for task-oriented dialogue systems focuses\non communicating specific content accurately, fluently, and coherently. While\nthese attributes are crucial for a successful dialogue, it is also desirable to\nsimultaneously accomplish specific stylistic goals, such as response length,\npoint-of-view, descriptiveness, sentiment, formality, and empathy. In this\nwork, we focus on stylistic control and evaluation for schema-guided NLG, with\njoint goals of achieving both semantic and stylistic control. We experiment in\ndetail with various controlled generation methods for large pretrained language\nmodels: specifically, conditional training, guided fine-tuning, and guided\ndecoding. We discuss their advantages and limitations, and evaluate them with a\nbroad range of automatic and human evaluation metrics. Our results show that\nwhile high style accuracy and semantic correctness are easier to achieve for\nmore lexically-defined styles with conditional training, stylistic control is\nalso achievable for more semantically complex styles using discriminator-based\nguided decoding methods. The results also suggest that methods that are more\nscalable (with less hyper-parameters tuning) and that disentangle content\ngeneration and stylistic variations are more effective at achieving semantic\ncorrectness and style accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_A/0/1/0/all/0/1\">Alicia Y. Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_V/0/1/0/all/0/1\">Vittorio Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Jiun-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. (arXiv:2109.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12242","description":"<p>Radiology report generation aims at generating descriptive text from\nradiology images automatically, which may present an opportunity to improve\nradiology reporting and interpretation. A typical setting consists of training\nencoder-decoder models on image-report pairs with a cross entropy loss, which\nstruggles to generate informative sentences for clinical diagnoses since normal\nfindings dominate the datasets. To tackle this challenge and encourage more\nclinically-accurate text outputs, we propose a novel weakly supervised\ncontrastive loss for medical report generation. Experimental results\ndemonstrate that our method benefits from contrasting target reports with\nincorrect but semantically-close ones. It outperforms previous work on both\nclinical correctness and text generation metrics for two public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eric Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1\">Amilcare Gentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?. (arXiv:2109.12243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12243","description":"<p>We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed\nto study systematic generalization for grounded language understanding. First,\nwe study which aspects of the original benchmark can be solved by commonly used\nmethods in multi-modal research. We find that a general-purpose\nTransformer-based model with cross-modal attention achieves strong performance\non a majority of the gSCAN splits, surprisingly outperforming more specialized\napproaches from prior work. Furthermore, our analysis suggests that many of the\nremaining errors reveal the same fundamental challenge in systematic\ngeneralization of linguistic constructs regardless of visual context. Second,\ninspired by this finding, we propose challenging new tasks for gSCAN by\ngenerating data to incorporate relations between objects in the visual\nenvironment. Finally, we find that current models are surprisingly data\ninefficient given the narrow scope of commands in gSCAN, suggesting another\nchallenge for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features. (arXiv:2109.12258v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12258","description":"<p>We report two essential improvements in readability assessment: 1. three\nnovel features in advanced semantics and 2. the timely evidence that\ntraditional ML models (e.g. Random Forest, using handcrafted features) can\ncombine with transformers (e.g. RoBERTa) to augment model performance. First,\nwe explore suitable transformers and traditional ML models. Then, we extract\n255 handcrafted linguistic features using self-developed extraction software.\nFinally, we assemble those to create several hybrid models, achieving\nstate-of-the-art (SOTA) accuracy on popular datasets in readability assessment.\nThe use of handcrafted features help model performance on smaller datasets.\nNotably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification\naccuracy of 99%, a 20.3% increase from the previous SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoo Sung Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. (arXiv:2109.12264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12264","description":"<p>Textual Question Answering (QA) aims to provide precise answers to user's\nquestions in natural language using unstructured data. One of the most popular\napproaches to this goal is machine reading comprehension(MRC). In recent years,\nmany novel datasets and evaluation metrics based on classical MRC tasks have\nbeen proposed for broader textual QA tasks. In this paper, we survey 47 recent\ntextual QA benchmark datasets and propose a new taxonomy from an application\npoint of view. In addition, We summarize 8 evaluation metrics of textual QA\ntasks. Finally, we discuss current trends in constructing textual QA benchmarks\nand suggest directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Learning to Repair Code and Generate Commit Message. (arXiv:2109.12296v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12296","description":"<p>We propose a novel task of jointly repairing program codes and generating\ncommit messages. Code repair and commit message generation are two essential\nand related tasks for software development. However, existing work usually\nperforms the two tasks independently. We construct a multilingual triple\ndataset including buggy code, fixed code, and commit messages for this novel\ntask. We provide the cascaded models as baseline, which are enhanced with\ndifferent training approaches, including the teacher-student method, the\nmulti-task method, and the back-translation method. To deal with the error\npropagation problem of the cascaded method, the joint model is proposed that\ncan both repair the code and generate the commit message in a unified\nframework. Experimental results show that the enhanced cascaded model with\nteacher-student method and multitask-learning method achieves the best score on\ndifferent metrics of automated code repair, and the joint model behaves better\nthan the cascaded model on commit message generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_A/0/1/0/all/0/1\">Ambrosio Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Transformer Models to Build ASAG System. (arXiv:2109.12300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12300","description":"<p>Research towards creating systems for automatic grading of student answers to\nquiz and exam questions in educational settings has been ongoing since 1966.\nOver the years, the problem was divided into many categories. Among them,\ngrading text answers were divided into short answer grading, and essay grading.\nThe goal of this work was to develop an ML-based short answer grading system. I\nhence built a system which uses finetuning on Roberta Large Model pretrained on\nSTS benchmark dataset and have also created an interface to show the production\nreadiness of the system. I evaluated the performance of the system on the\nMohler extended dataset and SciEntsBank Dataset. The developed system achieved\na Pearsons Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which\nbeats the SOTA performance on this dataset which is correlation of 0.805 and\nRMSE of 0.793. Additionally, Pearsons Correlation of 0.79 and RMSE of 0.56 was\nachieved on the SciEntsBank Dataset, which only reconfirms the robustness of\nthe system. A few observations during achieving these results included usage of\nbatch size of 1 produced better results than using batch size of 16 or 32 and\nusing huber loss as loss function performed well on this regression task. The\nsystem was tried and tested on train and validation splits using various random\nseeds and still has been tweaked to achieve a minimum of 0.76 of correlation\nand a maximum 0.15 (out of 1) RMSE on any dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Mithun Thakkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Templates for Recommender Dialogue System. (arXiv:2109.12302v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12302","description":"<p>Though recent end-to-end neural models have shown promising progress on\nConversational Recommender System (CRS), two key challenges still remain.\nFirst, the recommended items cannot be always incorporated into the generated\nreplies precisely and appropriately. Second, only the items mentioned in the\ntraining corpus have a chance to be recommended in the conversation. To tackle\nthese challenges, we introduce a novel framework called NTRD for recommender\ndialogue system that decouples the dialogue generation from the item\nrecommendation. NTRD has two key components, i.e., response template generator\nand item selector. The former adopts an encoder-decoder model to generate a\nresponse template with slot locations tied to target items, while the latter\nfills in slot locations with the proper items using a sufficient attention\nmechanism. Our approach combines the strengths of both classical slot filling\napproaches (that are generally controllable) and modern neural NLG approaches\n(that are generally more natural and accurate). Extensive experiments on the\nbenchmark ReDial show our NTRD significantly outperforms the previous\nstate-of-the-art methods. Besides, our approach has the unique advantage to\nproduce novel items that do not appear in the training set of dialogue corpus.\nThe code is available at \\url{https://github.com/jokieleung/NTRD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jian Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yingying He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-Based Neural Model for End-to-End Frame Semantic Parsing. (arXiv:2109.12319v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12319","description":"<p>Frame semantic parsing is a semantic analysis task based on FrameNet which\nhas received great attention recently. The task usually involves three subtasks\nsequentially: (1) target identification, (2) frame classification and (3)\nsemantic role labeling. The three subtasks are closely related while previous\nstudies model them individually, which ignores their intern connections and\nmeanwhile induces error propagation problem. In this work, we propose an\nend-to-end neural model to tackle the task jointly. Concretely, we exploit a\ngraph-based method, regarding frame semantic parsing as a graph construction\nproblem. All predicates and roles are treated as graph nodes, and their\nrelations are taken as graph edges. Experiment results on two benchmark\ndatasets of frame semantic parsing show that our method is highly competitive,\nresulting in better performance than pipeline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect. (arXiv:2109.12346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12346","description":"<p>Pre-trained transformers are now the de facto models in Natural Language\nProcessing given their state-of-the-art results in many tasks and languages.\nHowever, most of the current models have been trained on languages for which\nlarge text resources are already available (such as English, French, Arabic,\netc.). Therefore, there is still a number of low-resource languages that need\nmore attention from the community. In this paper, we study the Algerian dialect\nwhich has several specificities that make the use of Arabic or multilingual\nmodels inappropriate. To address this issue, we collected more than one Million\nAlgerian tweets, and pre-trained the first Algerian language model: DziriBERT.\nWhen compared to existing models, DziriBERT achieves the best results on two\nAlgerian downstream datasets. The obtained results show that pre-training a\ndedicated model on a small dataset (150 MB) can outperform existing models that\nhave been trained on much more data (hundreds of GB). Finally, our model is\npublicly available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdaoui_A/0/1/0/all/0/1\">Amine Abdaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrimi_M/0/1/0/all/0/1\">Mohamed Berrimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1\">Mourad Oussalah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussaoui_A/0/1/0/all/0/1\">Abdelouahab Moussaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification. (arXiv:2109.12349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12349","description":"<p>This paper presents an end-to-end system for fact extraction and verification\nusing textual and tabular evidence, the performance of which we demonstrate on\nthe FEVEROUS dataset. We experiment with both a multi-task learning paradigm to\njointly train a graph attention network for both the task of evidence\nextraction and veracity prediction, as well as a single objective graph model\nfor solely learning veracity prediction and separate evidence extraction. In\nboth instances, we employ a framework for per-cell linearization of tabular\nevidence, thus allowing us to treat evidence from tables as sequences. The\ntemplates we employ for linearizing tables capture the context as well as the\ncontent of table data. We furthermore provide a case study to show the\ninterpretability our approach. Our best performing system achieves a FEVEROUS\nscore of 0.23 and 53% label accuracy on the blind test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotonya_N/0/1/0/all/0/1\">Neema Kotonya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1\">Daniele Magazzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Priming for Cross-Lingual Event Extraction. (arXiv:2109.12383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12383","description":"<p>We present a novel, language-agnostic approach to \"priming\" language models\nfor the task of event extraction, providing particularly effective performance\nin low-resource and zero-shot cross-lingual settings. With priming, we augment\nthe input to the transformer stack's language model differently depending on\nthe question(s) being asked of the model at runtime. For instance, if the model\nis being asked to identify arguments for the trigger \"protested\", we will\nprovide that trigger as part of the input to the language model, allowing it to\nproduce different representations for candidate arguments than when it is asked\nabout arguments for the trigger \"arrest\" elsewhere in the same sentence. We\nshow that by enabling the language model to better compensate for the deficits\nof sparse and noisy training data, our approach improves both trigger and\nargument detection and classification significantly over the state of the art\nin a zero-shot cross-lingual setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fincke_S/0/1/0/all/0/1\">Steven Fincke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sorting through the noise: Testing robustness of information processing in pre-trained language models. (arXiv:2109.12393v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12393","description":"<p>Pre-trained LMs have shown impressive performance on downstream NLP tasks,\nbut we have yet to establish a clear understanding of their sophistication when\nit comes to processing, retaining, and applying information presented in their\ninput. In this paper we tackle a component of this question by examining\nrobustness of models' ability to deploy relevant context information in the\nface of distracting content. We present models with cloze tasks requiring use\nof critical context information, and introduce distracting content to test how\nrobustly the models retain and use that critical information for prediction. We\nalso systematically manipulate the nature of these distractors, to shed light\non dynamics of models' use of contextual cues. We find that although models\nappear in simple contexts to make predictions based on understanding and\napplying relevant facts from prior context, the presence of distracting but\nirrelevant content has clear impact in confusing model predictions. In\nparticular, models appear particularly susceptible to factors of semantic\nsimilarity and word position. The findings are consistent with the conclusion\nthat LM predictions are driven in large part by superficial contextual cues,\nrather than by robust representations of context meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandia_L/0/1/0/all/0/1\">Lalchand Pandia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Latent Space Clustering in Multi-filter Seq2Seq Model: A Reinforcement Learning Approach. (arXiv:2109.12399v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12399","description":"<p>In sequence-to-sequence language processing tasks, sentences with\nheterogeneous semantics or grammatical structures may increase the difficulty\nof convergence while training the network. To resolve this problem, we\nintroduce a model that concentrates the each of the heterogeneous features in\nthe input-output sequences. Build upon the encoder-decoder architecture, we\ndesign a latent-enhanced multi-filter seq2seq model (LMS2S) that analyzes the\nlatent space representations using a clustering algorithm. The representations\nare generated from an encoder and a latent space enhancer. A cluster classifier\nis applied to group the representations into clusters. A soft actor-critic\nreinforcement learning algorithm is applied to the cluster classifier to\nenhance the clustering quality by maximizing the Silhouette score. Then,\nmultiple filters are trained by the features only from their corresponding\nclusters, the heterogeneity of the training data can be resolved accordingly.\nOur experiments on semantic parsing and machine translation demonstrate the\npositive correlation between the clustering quality and the model's\nperformance, as well as show the enhancement our model has made with respect to\nthe ordinary encoder-decoder model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhaokun Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINIMAL: Mining Models for Data Free Universal Adversarial Triggers. (arXiv:2109.12406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12406","description":"<p>It is well known that natural language models are vulnerable to adversarial\nattacks, which are mostly input-specific in nature. Recently, it has been shown\nthat there also exist input-agnostic attacks in NLP models, called universal\nadversarial triggers. However, existing methods to craft universal triggers are\ndata intensive. They require large amounts of data samples to generate\nadversarial triggers, which are typically inaccessible by attackers. For\ninstance, previous works take 3000 data samples per class for the SNLI dataset\nto generate adversarial triggers. In this paper, we present a novel data-free\napproach, MINIMAL, to mine input-agnostic adversarial triggers from models.\nUsing the triggers produced with our data-free algorithm, we reduce the\naccuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%.\nSimilarly, for the Stanford Natural Language Inference (SNLI), our single-word\ntrigger reduces the accuracy of the entailment class from 90.95% to less than\n0.6\\%. Despite being completely data-free, we get equivalent accuracy drops as\ndata-dependent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Singla Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference Resolution for the Biomedical Domain: A Survey. (arXiv:2109.12424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12424","description":"<p>Issues with coreference resolution are one of the most frequently mentioned\nchallenges for information extraction from the biomedical literature. Thus, the\nbiomedical genre has long been the second most researched genre for coreference\nresolution after the news domain, and the subject of a great deal of research\nfor NLP in general. In recent years this interest has grown enormously leading\nto the development of a number of substantial datasets, of domain-specific\ncontextual language models, and of several architectures. In this paper we\nreview the state-of-the-art of coreference in the biomedical domain with a\nparticular attention on these most recent developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pengcheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deciding Whether to Ask Clarifying Questions in Large-Scale Spoken Language Understanding. (arXiv:2109.12451v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12451","description":"<p>A large-scale conversational agent can suffer from understanding user\nutterances with various ambiguities such as ASR ambiguity, intent ambiguity,\nand hypothesis ambiguity. When ambiguities are detected, the agent should\nengage in a clarifying dialog to resolve the ambiguities before committing to\nactions. However, asking clarifying questions for all the ambiguity occurrences\ncould lead to asking too many questions, essentially hampering the user\nexperience. To trigger clarifying questions only when necessary for the user\nsatisfaction, we propose a neural self-attentive model that leverages the\nhypotheses with ambiguities and contextual signals. We conduct extensive\nexperiments on five common ambiguity types using real data from a large-scale\ncommercial conversational agent and demonstrate significant improvement over a\nset of baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joo-Kyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Selectively Learn for Weakly-supervised Paraphrase Generation. (arXiv:2109.12457v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12457","description":"<p>Paraphrase generation is a longstanding NLP task that has diverse\napplications for downstream NLP tasks. However, the effectiveness of existing\nefforts predominantly relies on large amounts of golden labeled data. Though\nunsupervised endeavors have been proposed to address this issue, they may fail\nto generate meaningful paraphrases due to the lack of supervision signals. In\nthis work, we go beyond the existing paradigms and propose a novel approach to\ngenerate high-quality paraphrases with weak supervision data. Specifically, we\ntackle the weakly-supervised paraphrase generation problem by: (1) obtaining\nabundant weakly-labeled parallel sentences via retrieval-based pseudo\nparaphrase expansion; and (2) developing a meta-learning framework to\nprogressively select valuable samples for fine-tuning a pre-trained language\nmodel, i.e., BART, on the sentential paraphrasing task. We demonstrate that our\napproach achieves significant improvements over existing unsupervised\napproaches, and is even comparable in performance with supervised\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kaize Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Refinements for Lexically Constrained Text Generation with BART. (arXiv:2109.12487v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12487","description":"<p>Lexically constrained text generation aims to control the generated text by\nincorporating some pre-specified keywords into the output. Previous work\ninjects lexical constraints into the output by controlling the decoding process\nor refining the candidate output iteratively, which tends to generate generic\nor ungrammatical sentences, and has high computational complexity. To address\nthese challenges, we propose Constrained BART (CBART) for lexically constrained\ntext generation. CBART leverages the pre-trained model BART and transfers part\nof the generation burden from the decoder to the encoder by decomposing this\ntask into two sub-tasks, thereby improving the sentence quality. Concretely, we\nextend BART by adding a token-level classifier over the encoder, aiming at\ninstructing the decoder where to replace and insert. Guided by the encoder, the\ndecoder refines multiple tokens of the input in one step by inserting tokens\nbefore specific positions and re-predicting tokens with low confidence. To\nfurther reduce the inference latency, the decoder predicts all tokens in\nparallel. Experiment results on One-Billion-Word and Yelp show that CBART can\ngenerate plausible text with high quality and diversity while significantly\naccelerating inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Electoral Programs of German Parties 2021: A Computational Analysis Of Their Comprehensibility and Likeability Based On SentiArt. (arXiv:2109.12500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12500","description":"<p>The electoral programs of six German parties issued before the parliamentary\nelections of 2021 are analyzed using state-of-the-art computational tools for\nquantitative narrative, topic and sentiment analysis. We compare different\nmethods for computing the textual similarity of the programs, Jaccard Bag\nsimilarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational\nand computational complexity increasing from the 1st to the 4th method. A new\nsimilarity measure for entire documents derived from the Fowlkes Mallows Score\nis applied to kmeans clustering of sBERT transformed sentences. Using novel\nindices of the readability and emotion potential of texts computed via SentiArt\n(Jacobs, 2019), our data shed light on the similarities and differences of the\nprograms regarding their length, main ideas, comprehensibility, likeability,\nand semantic complexity. Among others, they reveal that the programs of the SPD\nand CDU have the best chances to be comprehensible and likeable -all other\nthings being equal-, and they raise the important issue of which similarity\nmeasure is optimal for comparing texts such as electoral programs which\nnecessarily share a lot of words. While such analyses can not replace\nqualitative analyses or a deep reading of the texts, they offer predictions\nthat can be verified in empirical studies and may serve as a motivation for\nchanging aspects of future electoral programs potentially making them more\ncomprehensible and/or likeable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_A/0/1/0/all/0/1\">Arthur M. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_A/0/1/0/all/0/1\">Annette Kinder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Linking Meets Deep Learning: Techniques and Solutions. (arXiv:2109.12520v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12520","description":"<p>Entity linking (EL) is the process of linking entity mentions appearing in\nweb text with their corresponding entities in a knowledge base. EL plays an\nimportant role in the fields of knowledge engineering and data mining,\nunderlying a variety of downstream applications such as knowledge base\npopulation, content analysis, relation extraction, and question answering. In\nrecent years, deep learning (DL), which has achieved tremendous success in\nvarious domains, has also been leveraged in EL methods to surpass traditional\nmachine learning based methods and yield the state-of-the-art performance. In\nthis survey, we present a comprehensive review and analysis of existing DL\nbased EL methods. First of all, we propose a new taxonomy, which organizes\nexisting DL based EL methods using three axes: embedding, feature, and\nalgorithm. Then we systematically survey the representative EL methods along\nthe three axes of the taxonomy. Later, we introduce ten commonly used EL data\nsets and give a quantitative performance analysis of DL based EL methods over\nthese data sets. Finally, we discuss the remaining limitations of existing\nmethods and highlight some promising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models. (arXiv:2109.12533v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12533","description":"<p>Copy mechanisms explicitly obtain unchanged tokens from the source (input)\nsequence to generate the target (output) sequence under the neural seq2seq\nframework. However, most of the existing copy mechanisms only consider single\nword copying from the source sentences, which results in losing essential\ntokens while copying long spans. In this work, we propose a plug-and-play\narchitecture, namely BioCopy, to alleviate the problem aforementioned.\nSpecifically, in the training stage, we construct a BIO tag for each token and\ntrain the original model with BIO tags jointly. In the inference stage, the\nmodel will firstly predict the BIO tag at each time step, then conduct\ndifferent mask strategies based on the predicted BIO label to diminish the\nscope of the probability distributions over the vocabulary list. Experimental\nresults on two separate generative tasks show that they all outperform the\nbaseline models by adding our BioCopy to the original model structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Puning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge. (arXiv:2109.12573v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12573","description":"<p>Cross-lingual pre-training has achieved great successes using monolingual and\nbilingual plain text corpora. However, existing pre-trained models neglect\nmultilingual knowledge, which is language agnostic but comprises abundant\ncross-lingual structure alignment. In this paper, we propose XLM-K, a\ncross-lingual language model incorporating multilingual knowledge in\npre-training. XLM-K augments existing multilingual pre-training with two\nknowledge tasks, namely Masked Entity Prediction Task and Object Entailment\nTask. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly\ndemonstrate significant improvements over existing multilingual language\nmodels. The results on MLQA and NER exhibit the superiority of XLM-K in\nknowledge related tasks. The success in XNLI shows a better cross-lingual\ntransferability obtained in XLM-K. What is more, we provide a detailed probing\nanalysis to confirm the desired knowledge captured in our pre-training regimen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paradigm Shift in Natural Language Processing. (arXiv:2109.12575v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12575","description":"<p>In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, is it imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents. (arXiv:2109.12595v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12595","description":"<p>We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented\ndialogues grounded in multiple documents. Most previous works treat\ndocument-grounded dialogue modeling as a machine reading comprehension task\nbased on a single given document or passage. In this work, we aim to address\nmore realistic scenarios where a goal-oriented information-seeking conversation\ninvolves multiple topics, and hence is grounded on different documents. To\nfacilitate such a task, we introduce a new dataset that contains dialogues\ngrounded in multiple documents from four different domains. We also explore\nmodeling the dialogue-based and document-based context in the dataset. We\npresent strong baseline approaches and various experimental results, aiming to\nsupport further research efforts on such a task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Song Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings. (arXiv:2109.12599v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12599","description":"<p>Learning sentence embeddings from dialogues has drawn increasing attention\ndue to its low annotation cost and high domain adaptability. Conventional\napproaches employ the siamese-network for this task, which obtains the sentence\nembeddings through modeling the context-response semantic relevance by applying\na feed-forward network on top of the sentence encoders. However, as the\nsemantic textual similarity is commonly measured through the element-wise\ndistance metrics (e.g. cosine and L2 distance), such architecture yields a\nlarge gap between training and evaluating. In this paper, we propose\nDialogueCSE, a dialogue-based contrastive learning approach to tackle this\nissue. DialogueCSE first introduces a novel matching-guided embedding (MGE)\nmechanism, which generates a context-aware embedding for each candidate\nresponse embedding (i.e. the context-free embedding) according to the guidance\nof the multi-turn context-response matching matrices. Then it pairs each\ncontext-aware embedding with its corresponding context-free embedding and\nfinally minimizes the contrastive loss across all pairs. We evaluate our model\non three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing\nDong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results\nshow that our approach significantly outperforms the baselines across all three\ndatasets in terms of MAP and Spearman's correlation measures, demonstrating its\neffectiveness. Further quantitative experiments show that our approach achieves\nbetter performance when leveraging more dialogue context and remains robust\nwhen less training data is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces. (arXiv:2109.12640v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12640","description":"<p>Much recent work in bilingual lexicon induction (BLI) views word embeddings\nas vectors in Euclidean space. As such, BLI is typically solved by finding a\nlinear transformation that maps embeddings to a common space. Alternatively,\nword embeddings may be understood as nodes in a weighted graph. This framing\nallows us to examine a node's graph neighborhood without assuming a linear\ntransform, and exploits new techniques from the graph matching optimization\nliterature. These contrasting approaches have not been compared in BLI so far.\nIn this work, we study the behavior of Euclidean versus graph-based approaches\nto BLI under differing data conditions and show that they complement each other\nwhen combined. We release our code at\nhttps://github.com/kellymarchisio/euc-v-graph-bli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Eldin_A/0/1/0/all/0/1\">Ali Saad-Eldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyakin_A/0/1/0/all/0/1\">Anton Alyakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey Priebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions. (arXiv:2109.12655v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12655","description":"<p>Multi-text applications, such as multi-document summarization, are typically\nrequired to model redundancies across related texts. Current methods\nconfronting consolidation struggle to fuse overlapping information. In order to\nexplicitly represent content overlap, we propose to align predicate-argument\nrelations across texts, providing a potential scaffold for information\nconsolidation. We go beyond clustering coreferring mentions, and instead model\noverlap with respect to redundancy at a propositional level, rather than merely\ndetecting shared referents. Our setting exploits QA-SRL, utilizing\nquestion-answer pairs to capture predicate-argument relations, facilitating\nlaymen annotation of cross-text alignments. We employ crowd-workers for\nconstructing a dataset of QA-based alignments, and present a baseline QA\nalignment model trained over our dataset. Analyses show that our new task is\nsemantically challenging, capturing content overlap beyond lexical similarity\nand complements cross-document coreference with proposition-level links,\noffering potential use for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">Daniela Brook Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1\">Ayal Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Performance Using Knowledge Distillation and Active Learning. (arXiv:2109.12662v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12662","description":"<p>Contemporary question answering (QA) systems, including transformer-based\narchitectures, suffer from increasing computational and model complexity which\nrender them inefficient for real-world applications with limited resources.\nFurther, training or even fine-tuning such models requires a vast amount of\nlabeled data which is often not available for the task at hand. In this\nmanuscript, we conduct a comprehensive analysis of the mentioned challenges and\nintroduce suitable countermeasures. We propose a novel knowledge distillation\n(KD) approach to reduce the parameter and model complexity of a pre-trained\nBERT system and utilize multiple active learning (AL) strategies for immense\nreduction in annotation efforts. In particular, we demonstrate that our model\nachieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using\nonly 2% of their total parameters. Finally, by the integration of our AL\napproaches into the BERT framework, we show that state-of-the-art results on\nthe SQuAD dataset can be achieved when we only use 20% of the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boreshban_Y/0/1/0/all/0/1\">Yasaman Boreshban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirbostani_S/0/1/0/all/0/1\">Seyed Morteza Mirbostani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassem_Sani_G/0/1/0/all/0/1\">Gholamreza Ghassem-Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Prunability of Attention Heads in Multilingual BERT. (arXiv:2109.12683v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12683","description":"<p>Large multilingual models, such as mBERT, have shown promise in crosslingual\ntransfer. In this work, we employ pruning to quantify the robustness and\ninterpret layer-wise importance of mBERT. On four GLUE tasks, the relative\ndrops in accuracy due to pruning have almost identical results on mBERT and\nBERT suggesting that the reduced attention capacity of the multilingual models\ndoes not affect robustness to pruning. For the crosslingual task XNLI, we\nreport higher drops in accuracy with pruning indicating lower robustness in\ncrosslingual transfer. Also, the importance of the encoder layers sensitively\ndepends on the language family and the pre-training corpus size. The top\nlayers, which are relatively more influenced by fine-tuning, encode important\ninformation for languages similar to English (SVO) while the bottom layers,\nwhich are relatively less influenced by fine-tuning, are particularly important\nfor agglutinative and low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budhraja_A/0/1/0/all/0/1\">Aakriti Budhraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_M/0/1/0/all/0/1\">Madhura Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting and Inferring Personal Attributes from Dialogue. (arXiv:2109.12702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12702","description":"<p>Personal attributes represent structured information about a person, such as\ntheir hobbies, pets, family, likes and dislikes. In this work, we introduce the\ntasks of extracting and inferring personal attributes from human-human\ndialogue. We first demonstrate the benefit of incorporating personal attributes\nin a social chit-chat dialogue model and task-oriented dialogue setting. Thus\nmotivated, we propose the tasks of personal attribute extraction and inference,\nand then analyze the linguistic demands of these tasks. To meet these\nchallenges, we introduce a simple and extensible model that combines an\nautoregressive language model utilizing constrained attribute generation with a\ndiscriminative reranker. Our model outperforms strong baselines on extracting\npersonal attributes as well as inferring personal attributes that are not\ncontained verbatim in utterances and instead requires commonsense reasoning and\nlexical inferences, which occur frequently in everyday conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_A/0/1/0/all/0/1\">Alex Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. (arXiv:2109.12742v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12742","description":"<p>The few-shot natural language understanding (NLU) task has attracted much\nrecent attention. However, prior methods have been evaluated under a disparate\nset of protocols, which hinders fair comparison and measuring progress of the\nfield. To address this issue, we introduce an evaluation framework that\nimproves previous evaluation procedures in three key aspects, i.e., test\nperformance, dev-test correlation, and stability. Under this new evaluation\nframework, we re-evaluate several state-of-the-art few-shot methods for NLU\ntasks. Our framework reveals new insights: (1) both the absolute performance\nand relative gap of the methods were not accurately estimated in prior\nliterature; (2) no single method dominates most tasks with consistent\nperformance; (3) improvements of some methods diminish with a larger pretrained\nmodel; and (4) gains from different methods are often complementary and the\nbest combined model performs close to a strong fully-supervised baseline. We\nopen-source our toolkit, FewNLU, that implements our evaluation framework along\nwith a number of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Insight: Accelerating Organic Materials Knowledge Extraction via Deep Learning. (arXiv:2109.12758v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12758","description":"<p>Scientific literature is one of the most significant resources for sharing\nknowledge. Researchers turn to scientific literature as a first step in\ndesigning an experiment. Given the extensive and growing volume of literature,\nthe common approach of reading and manually extracting knowledge is too time\nconsuming, creating a bottleneck in the research cycle. This challenge spans\nnearly every scientific domain. For the materials science, experimental data\ndistributed across millions of publications are extremely helpful for\npredicting materials properties and the design of novel materials. However,\nonly recently researchers have explored computational approaches for knowledge\nextraction primarily for inorganic materials. This study aims to explore\nknowledge extraction for organic materials. We built a research dataset\ncomposed of 855 annotated and 708,376 unannotated sentences drawn from 92,667\nabstracts. We used named-entity-recognition (NER) with BiLSTM-CNN-CRF deep\nlearning model to automatically extract key knowledge from literature.\nEarly-phase results show a high potential for automated knowledge extraction.\nThe paper presents our findings and a framework for supervised knowledge\nextraction that can be adapted to other scientific domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xintong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_S/0/1/0/all/0/1\">Steven Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikin_S/0/1/0/all/0/1\">Semion Saikin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_J/0/1/0/all/0/1\">Jane Greenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12761","description":"<p>In order to better simulate the real human conversation process, models need\nto generate dialogue utterances based on not only preceding textual contexts\nbut also visual contexts. However, with the development of multi-modal dialogue\nlearning, the dataset scale gradually becomes a bottleneck. In this report, we\nrelease OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset\ncompared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a\ntotal number of 5.6 million dialogue turns extracted from either movies or TV\nseries from different resources, and each dialogue turn is paired with its\ncorresponding visual context. We hope this large-scale dataset can help\nfacilitate future researches on open-domain multi-modal dialog generation,\ne.g., multi-modal pretraining for dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumour Detection via Zero-shot Cross-lingual Transfer Learning. (arXiv:2109.12773v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12773","description":"<p>Most rumour detection models for social media are designed for one specific\nlanguage (mostly English). There are over 40 languages on Twitter and most\nlanguages lack annotated resources to build rumour detection models. In this\npaper we propose a zero-shot cross-lingual transfer learning framework that can\nadapt a rumour detection model trained for a source language to another target\nlanguage. Our framework utilises pretrained multilingual language models (e.g.\\\nmultilingual BERT) and a self-training loop to iteratively bootstrap the\ncreation of ''silver labels'' in the target language to adapt the model from\nthe source language to the target language. We evaluate our methodology on\nEnglish and Chinese rumour datasets and demonstrate that our model\nsubstantially outperforms competitive benchmarks in both source and target\nlanguage rumour detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural Network for Reliable Intelligence Identification on Vietnamese SNSs. (arXiv:2109.12777v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12777","description":"<p>The overwhelming abundance of data has created a misinformation crisis.\nUnverified sensationalism that is designed to grab the readers' short attention\nspan, when crafted with malice, has caused irreparable damage to our society's\nstructure. As a result, determining the reliability of an article has become a\ncrucial task. After various ablation studies, we propose a multi-input model\nthat can effectively leverage both tabular metadata and post content for the\ntask. Applying state-of-the-art finetuning techniques for the pretrained\ncomponent and training strategies for our complete model, we have achieved a\n0.9462 ROC-score on the VLSP private test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trinh_H/0/1/0/all/0/1\">Hoang Viet Trinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tung Tien Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_H/0/1/0/all/0/1\">Huy Quang Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang Huu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Ngoc N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanh_T/0/1/0/all/0/1\">Ta Minh Thanh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Use of Graph Convolution Network and Contextual Sub-Tree forCommodity News Event Extraction. (arXiv:2109.12781v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12781","description":"<p>Event extraction in commodity news is a less researched area as compared to\ngeneric event extraction. However, accurate event extraction from commodity\nnews is useful in abroad range of applications such as under-standing event\nchains and learning event-event relations, which can then be used for commodity\nprice prediction. The events found in commodity news exhibit characteristics\ndifferent from generic events, hence posing a unique challenge in event\nextraction using existing methods. This paper proposes an effective use of\nGraph Convolutional Networks(GCN) with a pruned dependency parse tree, termed\ncontextual sub-tree, for better event ex-traction in commodity news. The event\nex-traction model is trained using feature embed-dings from ComBERT, a\nBERT-based masked language model that was produced through domain-adaptive\npre-training on a commodity news corpus. Experimental results show the\nefficiency of the proposed solution, which out-performs existing methods with\nF1 scores as high as 0.90. Furthermore, our pre-trained language model\noutperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles\nclassification. For the goal of re-producibility, the code and trained models\nare made publicly available1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplicative Position-aware Transformer Models for Language Understanding. (arXiv:2109.12788v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12788","description":"<p>Transformer models, which leverage architectural improvements like\nself-attention, perform remarkably well on Natural Language Processing (NLP)\ntasks. The self-attention mechanism is position agnostic. In order to capture\npositional ordering information, various flavors of absolute and relative\nposition embeddings have been proposed. However, there is no systematic\nanalysis on their contributions and a comprehensive comparison of these methods\nis missing in the literature. In this paper, we review major existing position\nembedding methods and compare their accuracy on downstream NLP tasks, using our\nown implementations. We also propose a novel multiplicative embedding method\nwhich leads to superior accuracy when compared to existing methods. Finally, we\nshow that our proposed embedding method, served as a drop-in replacement of the\ndefault absolute position embedding, can improve the RoBERTa-base and\nRoBERTa-large models on SQuAD1.1 and SQuAD2.0 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates. (arXiv:2109.12804v1 [eess.AS])","link":"http://arxiv.org/abs/2109.12804","description":"<p>The multi-decoder (MD) end-to-end speech translation model has demonstrated\nhigh translation quality by searching for better intermediate automatic speech\nrecognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass\ndecoding model decomposing the overall task into ASR and machine translation\nsub-tasks. However, the decoding speed is not fast enough for real-world\napplications because it conducts beam search for both sub-tasks during\ninference. We propose Fast-MD, a fast MD model that generates HI by\nnon-autoregressive (NAR) decoding based on connectionist temporal\nclassification (CTC) outputs followed by an ASR decoder. We investigated two\ntypes of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR\ndecoder and (2) masked HI by using Mask-CTC, which combines CTC and the\nconditional masked language model. To reduce a mismatch in the ASR decoder\nbetween teacher-forcing during training and conditioning on CTC outputs during\ntesting, we also propose sampling CTC outputs during training. Experimental\nevaluations on three corpora show that Fast-MD achieved about 2x and 4x faster\ndecoding speed than that of the na\\\"ive MD model on GPU and CPU with comparable\ntranslation quality. Adopting the Conformer encoder and intermediate CTC loss\nfurther boosts its quality without sacrificing decoding speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Non-local Features for Neural Constituency Parsing. (arXiv:2109.12814v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12814","description":"<p>Thanks to the strong representation power of neural encoders, neural\nchart-based parsers have achieved highly competitive performance by using local\nfeatures. Recently, it has been shown that non-local features in CRF structures\nlead to improvements. In this paper, we investigate injecting non-local\nfeatures into the training process of a local span-based parser, by predicting\nconstituent n-gram non-local patterns and ensuring consistency between\nnon-local patterns and local constituents. Results show that our simple method\ngives better results than the CRF parser on both PTB and CTB. Besides, our\nmethod achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and\nstrong performance on CTB (92.31 F1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Statements Considered Useful. (arXiv:2001.04425v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2001.04425","description":"<p>Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering\nsystems that are mainly geared for positive questions; they can also contribute\nto informative summaries of entities. Due to the abundance of such invalid\nstatements, any effort to compile them needs to address ranking by saliency. We\npresent a statisticalinference method for compiling and ranking negative\nstatements, based on expectations from positive statements of related entities\nin peer groups. Experimental results, with a variety of datasets, show that the\nmethod can effectively discover notable negative statements, and extrinsic\nstudies underline their usefulness for entity summarization. Datasets and code\nare released as resources for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06666","description":"<p>The de-facto approach to many vision tasks is to start from pretrained visual\nrepresentations, typically learned via supervised training on ImageNet. Recent\nmethods have explored unsupervised pretraining to scale to vast quantities of\nunlabeled images. In contrast, we aim to learn high-quality visual\nrepresentations from fewer images. To this end, we revisit supervised\npretraining, and seek data-efficient alternatives to classification-based\npretraining. We propose VirTex -- a pretraining approach using semantically\ndense captions to learn visual representations. We train convolutional networks\nfrom scratch on COCO Captions, and transfer them to downstream recognition\ntasks including image classification, object detection, and instance\nsegmentation. On all tasks, VirTex yields features that match or exceed those\nlearned on ImageNet -- supervised or unsupervised -- despite using up to ten\ntimes fewer images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Karan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions Attributes. (arXiv:2007.06954v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.06954","description":"<p>This paper describes a large global dataset on people's social media\nresponses to the COVID-19 pandemic over the Twitter platform. From 28 January\n2020 to 1 September 2021, we collected over 198 million Twitter posts from more\nthan 25 million unique users using four keywords: \"corona\", \"wuhan\", \"nCov\" and\n\"covid\". Leveraging topic modeling techniques and pre-trained machine\nlearning-based emotion analytic algorithms, we labeled each tweet with\nseventeen semantic attributes, including a) ten binary attributes indicating\nthe tweet's relevance or irrelevance to the top ten detected topics, b) five\nquantitative emotion attributes indicating the degree of intensity of the\nvalence or sentiment (from 0: very negative to 1: very positive), and the\ndegree of intensity of fear, anger, happiness and sadness emotions (from 0: not\nat all to 1: extremely intense), and c) two qualitative attributes indicating\nthe sentiment category (very negative, negative, neutral or mixed, positive,\nvery positive) and the dominant emotion category (fear, anger, happiness,\nsadness, no specific emotion) the tweet is mainly expressing. We report the\ndescriptive statistics around these new attributes, their temporal\ndistributions, and the overall geographic representation of the dataset. The\npaper concludes with an outline of the dataset's possible usage in\ncommunication, psychology, public health, economics, and epidemiology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_A/0/1/0/all/0/1\">Ajay Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Pretrained Language Models for Graph-to-Text Generation. (arXiv:2007.08426v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.08426","description":"<p>Graph-to-text generation aims to generate fluent texts from graph-based data.\nIn this paper, we investigate two recently proposed pretrained language models\n(PLMs) and analyze the impact of different task-adaptive pretraining strategies\nfor PLMs in graph-to-text generation. We present a study across three graph\ndomains: meaning representations, Wikipedia knowledge graphs (KGs) and\nscientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art\nresults and that task-adaptive pretraining strategies improve their performance\neven further. In particular, we report new state-of-the-art BLEU scores of\n49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative\nimprovement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,\nwe identify possible reasons for the PLMs' success on graph-to-text tasks. We\nfind evidence that their knowledge about true facts helps them perform well\neven when the input graph representation is reduced to a simple bag of node and\nedge labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Conflicting Data when Updating a Neural Semantic Parser. (arXiv:2010.12675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12675","description":"<p>In this paper, we explore how to use a small amount of new data to update a\ntask-oriented semantic parsing model when the desired output for some examples\nhas changed. When making updates in this way, one potential problem that arises\nis the presence of conflicting data, or out-of-date labels in the original\ntraining set. To evaluate the impact of this understudied problem, we propose\nan experimental setup for simulating changes to a neural semantic parser. We\nshow that the presence of conflicting data greatly hinders learning of an\nupdate, then explore several methods to mitigate its effect. Our multi-task and\ndata selection methods lead to large improvements in model accuracy compared to\na naive data-mixing strategy, and our best method closes 86% of the accuracy\ngap between this baseline and an oracle upper bound.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaddy_D/0/1/0/all/0/1\">David Gaddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzemtchenko_A/0/1/0/all/0/1\">Alex Kouzemtchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muddireddy_P/0/1/0/all/0/1\">Pavankumar Reddy Muddireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhar_P/0/1/0/all/0/1\">Prateek Kolhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Panoramic Survey of Natural Language Processing in the Arab World. (arXiv:2011.12631v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12631","description":"<p>The term natural language refers to any system of symbolic communication\n(spoken, signed or written) without intentional human planning and design. This\ndistinguishes natural languages such as Arabic and Japanese from artificially\nconstructed languages such as Esperanto or Python. Natural language processing\n(NLP) is the sub-field of artificial intelligence (AI) focused on modeling\nnatural languages to build applications such as speech recognition and\nsynthesis, machine translation, optical character recognition (OCR), sentiment\nanalysis (SA), question answering, dialogue systems, etc. NLP is a highly\ninterdisciplinary field with connections to computer science, linguistics,\ncognitive science, psychology, mathematics and others. Some of the earliest AI\napplications were in NLP (e.g., machine translation); and the last decade\n(2010-2020) in particular has witnessed an incredible increase in quality,\nmatched with a rise in public awareness, use, and expectations of what may have\nseemed like science fiction in the past. NLP researchers pride themselves on\ndeveloping language independent models and tools that can be applied to all\nhuman languages, e.g. machine translation systems can be built for a variety of\nlanguages using the same basic mechanisms and models. However, the reality is\nthat some languages do get more attention (e.g., English and Chinese) than\nothers (e.g., Hindi and Swahili). Arabic, the primary language of the Arab\nworld and the religious language of millions of non-Arab Muslims is somewhere\nin the middle of this continuum. Though Arabic NLP has many challenges, it has\nseen many successes and developments. Next we discuss Arabic's main challenges\nas a necessary background, and we present a brief history of Arabic NLP. We\nthen survey a number of its research areas, and close with a critical\ndiscussion of the future of Arabic NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1\">Mourad Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khalifa_H/0/1/0/all/0/1\">Hend Al-Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Natsheh_H/0/1/0/all/0/1\">Huseein T. Al-Natsheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Beltagy_S/0/1/0/all/0/1\">Samhaa R. El-Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouzoubaa_K/0/1/0/all/0/1\">Karim Bouzoubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavalli_Sforza_V/0/1/0/all/0/1\">Violetta Cavalli-Sforza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hajj_W/0/1/0/all/0/1\">Wassim El-Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of Language Model Interpretability via Sentence Editing. (arXiv:2011.14039v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.14039","description":"<p>Pre-trained language models (PLMs) like BERT are being used for almost all\nlanguage-related tasks, but interpreting their behavior still remains a\nsignificant challenge and many important questions remain largely unanswered.\nIn this work, we re-purpose a sentence editing dataset, where faithful\nhigh-quality human rationales can be automatically extracted and compared with\nextracted model rationales, as a new testbed for interpretability. This enables\nus to conduct a systematic investigation on an array of questions regarding\nPLMs' interpretability, including the role of pre-training procedure,\ncomparison of rationale extraction methods, and different layers in the PLM.\nThe investigation generates new insights, for example, contrary to the common\nunderstanding, we find that attention weights correlate well with human\nrationales and work better than gradient-based saliency in extracting model\nrationales. Both the dataset and code are available at\nhttps://github.com/samuelstevens/sentence-editing-interpretability to\nfacilitate future interpretability research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Variable Models for Visual Question Answering. (arXiv:2101.06399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06399","description":"<p>Current work on Visual Question Answering (VQA) explore deterministic\napproaches conditioned on various types of image and question features. We\nposit that, in addition to image and question pairs, other modalities are\nuseful for teaching machine to carry out question answering. Hence in this\npaper, we propose latent variable models for VQA where extra information (e.g.\ncaptions and answer categories) are incorporated as latent variables, which are\nobserved during training but in turn benefit question-answering performance at\ntest time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the\neffectiveness of our proposed models: they improve over strong baselines,\nespecially those that do not rely on extensive language-vision pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models. (arXiv:2102.05126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05126","description":"<p>Attention-based pre-trained language models such as GPT-2 brought\nconsiderable progress to end-to-end dialogue modelling. However, they also\npresent considerable risks for task-oriented dialogue, such as lack of\nknowledge grounding or diversity. To address these issues, we introduce\nmodified training objectives for language model finetuning, and we employ\nmassive data augmentation via back-translation to increase the diversity of the\ntraining data. We further examine the possibilities of combining data from\nmultiples sources to improve performance on the target dataset. We carefully\nevaluate our contributions with both human and automatic methods. Our model\nsubstantially outperforms the baseline on the MultiWOZ data and shows\ncompetitive performance with state of the art in both automatic and human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nekvinda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Nekvinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01287","description":"<p>Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1\">Olivia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwaha_A/0/1/0/all/0/1\">Akruti Kushwaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Saloni Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">William Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce. (arXiv:2104.06960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06960","description":"<p>Existing pre-trained language models (PLMs) have demonstrated the\neffectiveness of self-supervised learning for a broad range of natural language\nprocessing (NLP) tasks. However, most of them are not explicitly aware of\ndomain-specific knowledge, which is essential for downstream tasks in many\ndomains, such as tasks in e-commerce scenarios. In this paper, we propose\nK-PLUG, a knowledge-injected pre-trained language model based on the\nencoder-decoder transformer that can be transferred to both natural language\nunderstanding and generation tasks. We verify our method in a diverse range of\ne-commerce scenarios that require domain-specific knowledge. Specifically, we\npropose five knowledge-aware self-supervised pre-training objectives to\nformulate the learning of domain-specific knowledge, including e-commerce\ndomain-specific knowledge-bases, aspects of product entities, categories of\nproduct entities, and unique selling propositions of product entities. K-PLUG\nachieves new state-of-the-art results on a suite of domain-specific NLP tasks,\nincluding product knowledge base completion, abstractive product summarization,\nand multi-turn dialogue, significantly outperforms baselines across the board,\nwhich demonstrates that the proposed method effectively learns a diverse set of\ndomain-specific knowledge for both language understanding and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Song Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1\">Peng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation-Based Human Debugging of NLP Models: A Survey. (arXiv:2104.15135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.15135","description":"<p>Debugging a machine learning model is hard since the bug usually involves the\ntraining data and the learning process. This becomes even harder for an opaque\ndeep learning model if we have no clue about how the model actually works. In\nthis survey, we review papers that exploit explanations to enable humans to\ngive feedback and debug NLP models. We call this problem explanation-based\nhuman debugging (EBHD). In particular, we categorize and discuss existing work\nalong three dimensions of EBHD (the bug context, the workflow, and the\nexperimental setting), compile findings on how EBHD components affect the\nfeedback providers, and highlight open problems that could be future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2105.04727","description":"<p>We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.14371","description":"<p>Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Luyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.03200","description":"<p>The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Robotic Vision for Space Mining. (arXiv:2109.12109v1 [cs.RO])","link":"http://arxiv.org/abs/2109.12109","description":"<p>Future Moon bases will likely be constructed using resources mined from the\nsurface of the Moon. The difficulty of maintaining a human workforce on the\nMoon and communications lag with Earth means that mining will need to be\nconducted using collaborative robots with a high degree of autonomy. In this\npaper, we explore the utility of robotic vision towards addressing several\nmajor challenges in autonomous mining in the lunar environment: lack of\nsatellite positioning systems, navigation in hazardous terrain, and delicate\nrobot interactions. Specifically, we describe and report the results of robotic\nvision algorithms that we developed for Phase 2 of the NASA Space Robotics\nChallenge, which was framed in the context of autonomous collaborative robots\nfor mining on the Moon. The competition provided a simulated lunar environment\nthat exhibits the complexities alluded to above. We show how machine\nlearning-enabled vision could help alleviate the challenges posed by the lunar\nenvironment. A robust multi-robot coordinator was also developed to achieve\nlong-term operation and effective collaboration between robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Ragav Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_R/0/1/0/all/0/1\">Ravi Hammond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bockman_J/0/1/0/all/0/1\">James Bockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_A/0/1/0/all/0/1\">Alec Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smart_B/0/1/0/all/0/1\">Brandon Smart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craggs_D/0/1/0/all/0/1\">Dustin Craggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowntree_T/0/1/0/all/0/1\">Thomas Rowntree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutz_E/0/1/0/all/0/1\">Elijah Schutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_A/0/1/0/all/0/1\">Adrian Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Andy Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Women with Mammographically-Occult Breast Cancer Leveraging GAN-Simulated Mammograms. (arXiv:2109.12113v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12113","description":"<p>Our objective is to show the feasibility of using simulated mammograms to\ndetect mammographically-occult (MO) cancer in women with dense breasts and a\nnormal screening mammogram who could be triaged for additional screening with\nmagnetic resonance imaging (MRI) or ultrasound. We developed a Conditional\nGenerative Adversarial Network (CGAN) to simulate a mammogram with normal\nappearance using the opposite mammogram as the condition. We used a\nConvolutional Neural Network (CNN) trained on Radon Cumulative Distribution\nTransform (RCDT) processed mammograms to detect MO cancer. For training CGAN,\nwe used screening mammograms of 1366 women. For MO cancer detection, we used\nscreening mammograms of 333 women (97 MO cancer) with dense breasts. We\nsimulated the right mammogram for normal controls and the cancer side for MO\ncancer cases. We created two RCDT images, one from a real mammogram pair and\nanother from a real-simulated mammogram pair. We finetuned a VGG16 on resulting\nRCDT images to classify the women with MO cancer. We compared the\nclassification performance of the CNN trained on fused RCDT images, CNN_{Fused}\nto that of trained only on real RCDT images, CNN_{Real}, and to that of trained\nonly on simulated RCDT images, CNN_{Simulated}. The test AUC for CNN_{Fused}\nwas 0.77 with a 95% confidence interval (95CI) of [0.71, 0.83], which was\nstatistically better (p-value &lt; 0.02) than the CNN_{Real} AUC of 0.70 with a\n95CI of [0.64, 0.77] and CNN_{Simulated} AUC of 0.68 with a 95CI of [0.62,\n0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Juhun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishikawa_R/0/1/0/all/0/1\">Robert M. Nishikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use of the Deep Learning Approach to Measure Alveolar Bone Level. (arXiv:2109.12115v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12115","description":"<p>Abstract:\n</p>\n<p>Aim: The goal was to use a Deep Convolutional Neural Network to measure the\nradiographic alveolar bone level to aid periodontal diagnosis.\n</p>\n<p>Material and methods: A Deep Learning (DL) model was developed by integrating\nthree segmentation networks (bone area, tooth, cementoenamel junction) and\nimage analysis to measure the radiographic bone level and assign radiographic\nbone loss (RBL) stages. The percentage of RBL was calculated to determine the\nstage of RBL for each tooth. A provisional periodontal diagnosis was assigned\nusing the 2018 periodontitis classification. RBL percentage, staging, and\npresumptive diagnosis were compared to the measurements and diagnoses made by\nthe independent examiners.\n</p>\n<p>Results: The average Dice Similarity Coefficient (DSC) for segmentation was\nover 0.91. There was no significant difference in RBL percentage measurements\ndetermined by DL and examiners (p=0.65). The Area Under the Receiver Operating\nCharacteristics Curve of RBL stage assignment for stage I, II and III was 0.89,\n0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85.\n</p>\n<p>Conclusion: The proposed DL model provides reliable RBL measurements and\nimage-based periodontal diagnosis using periapical radiographic images.\nHowever, this model has to be further optimized and validated by a larger\nnumber of images to facilitate its application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Teh Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kabir_T/0/1/0/all/0/1\">Tanjida Kabir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nelson_J/0/1/0/all/0/1\">Jiman Nelson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_S/0/1/0/all/0/1\">Sally Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Hsiu-Wan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dyke_T/0/1/0/all/0/1\">Thomas E. Van Dyke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walji_M/0/1/0/all/0/1\">Muhammad F. Walji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shams_S/0/1/0/all/0/1\">Shayan Shams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12131","description":"<p>Autonomous driving requires 3D maps that provide accurate and up-to-date\ninformation about semantic landmarks. Due to the wider availability and lower\ncost of cameras compared with laser scanners, vision-based mapping has\nattracted much attention from academia and industry. Among the existing\nsolutions, Structure-from-Motion (SfM) technology has proved to be feasible for\nbuilding 3D maps from crowdsourced data, since it allows unordered images as\ninput. Previous works on SfM have mainly focused on issues related to building\n3D point clouds and calculating camera poses, leaving the issues of automatic\nchange detection and localization open.\n</p>\n<p>We propose in this paper an SfM-based solution for automatic map update, with\na focus on real-time change detection and localization. Our solution builds on\ncomparison of semantic map data (e.g. types and locations of traffic signs).\nThrough a novel design of the pixel-wise 3D localization algorithm, our system\ncan locate the objects detected from 2D images in a 3D space, utilizing sparse\nSfM point clouds. Experiments with dashcam videos collected from two urban\nareas prove that the system is able to locate visible traffic signs in front\nalong the driving direction with a median distance error of 1.52 meters.\nMoreover, it can detect up to 80\\% of the changes with a median distance error\nof 2.21 meters. The result analysis also shows the potential of significantly\nimproving the system performance in the future by increasing the accuracy of\nthe background technology in use, including in particularly the object\ndetection and point cloud geo-registration algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhanabatyrova_A/0/1/0/all/0/1\">Aziza Zhanabatyrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_C/0/1/0/all/0/1\">Clayton Souza Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Contractive Flow: Improved Contractive Flows with Lipschitz-constrained Self-Attention. (arXiv:2109.12135v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12135","description":"<p>Normalizing flows provide an elegant method for obtaining tractable density\nestimates from distributions by using invertible transformations. The main\nchallenge is to improve the expressivity of the models while keeping the\ninvertibility constraints intact. We propose to do so via the incorporation of\nlocalized self-attention. However, conventional self-attention mechanisms don't\nsatisfy the requirements to obtain invertible flows and can't be naively\nincorporated into normalizing flows. To address this, we introduce a novel\napproach called Attentive Contractive Flow (ACF) which utilizes a special\ncategory of flow-based generative models - contractive flows. We demonstrate\nthat ACF can be introduced into a variety of state of the art flow models in a\nplug-and-play manner. This is demonstrated to not only improve the\nrepresentation power of these models (improving on the bits per dim metric),\nbut also to results in significantly faster convergence in training them.\nQualitative results, including interpolations between test images, demonstrate\nthat samples are more realistic and capture local correlations in the data\nwell. We evaluate the results further by performing perturbation analysis using\nAWGN demonstrating that ACF models (especially the dot-product variant) show\nbetter and more consistent resilience to additive noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Avideep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1\">Badri Narayan Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1\">Sahil Sidheekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks for Blind Image Quality Assessment: Addressing the Data Challenge. (arXiv:2109.12161v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12161","description":"<p>The enormous space and diversity of natural images is usually represented by\na few small-scale human-rated image quality assessment (IQA) datasets. This\ncasts great challenges to deep neural network (DNN) based blind IQA (BIQA),\nwhich requires large-scale training data that is representative of the natural\nimage distribution. It is extremely difficult to create human-rated IQA\ndatasets composed of millions of images due to constraints of subjective\ntesting. While a number of efforts have focused on design innovations to\nenhance the performance of DNN based BIQA, attempts to address the scarcity of\nlabeled IQA data remain surprisingly missing. To address this data challenge,\nwe construct so far the largest IQA database, namely Waterloo Exploration-II,\nwhich contains 3,570 pristine reference and around 3.45 million singly and\nmultiply distorted images. Since subjective testing for such a large dataset is\nnearly impossible, we develop a novel mechanism that synthetically assigns\nperceptual quality labels to the distorted images. We construct a DNN-based\nBIQA model called EONSS, train it on Waterloo Exploration-II, and test it on\nnine subject-rated IQA datasets, without any retraining or fine-tuning. The\nresults show that with a straightforward DNN architecture, EONSS is able to\noutperform the very state-of-the-art in BIQA, both in terms of quality\nprediction performance and execution speed. This study strongly supports the\nview that the quantity and quality of meaningfully annotated training data,\nrather than a sophisticated network architecture or training strategy, is the\ndominating factor that determines the performance of DNN-based BIQA models.\n(Note: Since this is an ongoing project, the final versions of Waterloo\nExploration-II database, quality annotations, and EONSS, will be made publicly\navailable in the future when it culminates.)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Athar_S/0/1/0/all/0/1\">Shahrukh Athar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongling Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble. (arXiv:2109.12169v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12169","description":"<p>Magnetic resonance images (MRIs) are widely used to quantify vestibular\nschwannoma and the cochlea. Recently, deep learning methods have shown\nstate-of-the-art performance for segmenting these structures. However, training\nsegmentation models may require manual labels in target domain, which is\nexpensive and time-consuming. To overcome this problem, domain adaptation is an\neffective way to leverage information from source domain to obtain accurate\nsegmentations without requiring manual labels in target domain. In this paper,\nwe propose an unsupervised learning framework to segment the VS and cochlea.\nOur framework leverages information from contrast-enhanced T1-weighted (ceT1-w)\nMRIs and its labels, and produces segmentations for T2-weighted MRIs without\nany labels in the target domain. We first applied a generator to achieve\nimage-to-image translation. Next, we ensemble outputs from an ensemble of\ndifferent models to obtain final segmentations. To cope with MRIs from\ndifferent sites/scanners, we applied various 'online' augmentations during\ntraining to better capture the geometric variability and the variability in\nimage appearance and quality. Our method is easy to build and produces\npromising segmentations, with a mean Dice score of 0.7930 and 0.7432 for VS and\ncochlea respectively in the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qibang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larson_K/0/1/0/all/0/1\">Kathleen E. Larson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huahong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling. (arXiv:2109.12178v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12178","description":"<p>Vision-and-Language Pre-training (VLP) improves model performance for\ndownstream tasks that require image and text inputs. Current VLP approaches\ndiffer on (i) model architecture (especially image embedders), (ii) loss\nfunctions, and (iii) masking policies. Image embedders are either deep models\nlike ResNet or linear projections that directly feed image-pixels into the\ntransformer. Typically, in addition to the Masked Language Modeling (MLM) loss,\nalignment-based objectives are used for cross-modality interaction, and RoI\nfeature regression and classification tasks for Masked Image-Region Modeling\n(MIRM). Both alignment and MIRM objectives mostly do not have ground truth.\nAlignment-based objectives require pairings of image and text and heuristic\nobjective functions. MIRM relies on object detectors. Masking policies either\ndo not take advantage of multi-modality or are strictly coupled with alignments\ngenerated by other models. In this paper, we present Masked Language and Image\nModeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling\n(MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware\nMasking (MAM) to boost cross-modality interaction and take advantage of MLM and\nRECON losses that separately capture text and image reconstruction quality.\nUsing MLM + RECON tasks coupled with MAM, we present a simplified VLP\nmethodology and show that it has better downstream task performance on a\nproprietary e-commerce multi-modal dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arici_T/0/1/0/all/0/1\">Tarik Arici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1\">Mehmet Saygin Seyfioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiman_T/0/1/0/all/0/1\">Tal Neiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Train_S/0/1/0/all/0/1\">Son Train</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutar_I/0/1/0/all/0/1\">Ismail Tutar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NanoBatch DPSGD: Exploring Differentially Private learning on ImageNet with low batch sizes on the IPU. (arXiv:2109.12191v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12191","description":"<p>Differentially private SGD (DPSGD) has recently shown promise in deep\nlearning. However, compared to non-private SGD, the DPSGD algorithm places\ncomputational overheads that can undo the benefit of batching in GPUs.\nMicrobatching is a standard method to alleviate this and is fully supported in\nthe TensorFlow Privacy library (TFDP). However, this technique, while improving\ntraining times also reduces the quality of the gradients and degrades the\nclassification accuracy. Recent works that for example use the JAX framework\nshow promise in also alleviating this but still show degradation in throughput\nfrom non-private to private SGD on CNNs, and have not yet shown ImageNet\nimplementations. In our work, we argue that low batch sizes using group\nnormalization on ResNet-50 can yield high accuracy and privacy on Graphcore\nIPUs. This enables DPSGD training of ResNet-50 on ImageNet in just 6 hours (100\nepochs) on an IPU-POD16 system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Edward H. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krell_M/0/1/0/all/0/1\">Mario Michael Krell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsyplikhin_A/0/1/0/all/0/1\">Alexander Tsyplikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rege_V/0/1/0/all/0/1\">Victoria Rege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1\">Errol Colak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_K/0/1/0/all/0/1\">Kristen W. Yeom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ground material classification and for UAV-based photogrammetric 3D data A 2D-3D Hybrid Approach. (arXiv:2109.12221v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12221","description":"<p>In recent years, photogrammetry has been widely used in many areas to create\nphotorealistic 3D virtual data representing the physical environment. The\ninnovation of small unmanned aerial vehicles (sUAVs) has provided additional\nhigh-resolution imaging capabilities with low cost for mapping a relatively\nlarge area of interest. These cutting-edge technologies have caught the US Army\nand Navy's attention for the purpose of rapid 3D battlefield reconstruction,\nvirtual training, and simulations. Our previous works have demonstrated the\nimportance of information extraction from the derived photogrammetric data to\ncreate semantic-rich virtual environments (Chen et al., 2019). For example, an\nincrease of simulation realism and fidelity was achieved by segmenting and\nreplacing photogrammetric trees with game-ready tree models. In this work, we\nfurther investigated the semantic information extraction problem and focused on\nthe ground material segmentation and object detection tasks. The main\ninnovation of this work was that we leveraged both the original 2D images and\nthe derived 3D photogrammetric data to overcome the challenges faced when using\neach individual data source. For ground material segmentation, we utilized an\nexisting convolutional neural network architecture (i.e., 3DMV) which was\noriginally designed for segmenting RGB-D sensed indoor data. We improved its\nperformance for outdoor photogrammetric data by introducing a depth pooling\nlayer in the architecture to take into consideration the distance between the\nsource images and the reconstructed terrain model. To test the performance of\nour improved 3DMV, a ground truth ground material database was created using\ndata from the One World Terrain (OWT) data repository. Finally, a workflow for\nimporting the segmented ground materials into a virtual simulation scene was\nintroduced, and visual results are reported in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meida Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Andrew Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCullough_K/0/1/0/all/0/1\">Kyle McCullough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_P/0/1/0/all/0/1\">Pratusha Bhuvana Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soibelman_L/0/1/0/all/0/1\">Lucio Soibelman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Generalization to Deep Multi-view Detection. (arXiv:2109.12227v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12227","description":"<p>Multi-view Detection (MVD) is highly effective for occlusion reasoning and is\na mainstream solution in various applications that require accurate top-view\noccupancy maps. While recent works using deep learning have made significant\nadvances in the field, they have overlooked the generalization aspect, which\nmakes them \\emph{impractical for real-world deployment}. The key novelty of our\nwork is to \\emph{formalize} three critical forms of generalization and\n\\emph{propose experiments to investigate them}: i) generalization across a\nvarying number of cameras, ii) generalization with varying camera positions,\nand finally, iii) generalization to new scenes. We find that existing \\sota\nmodels show poor generalization by overfitting to a single scene and camera\nconfiguration. We propose modifications in terms of pre-training, pooling\nstrategy, regularization, and loss function to an existing state-of-the-art\nframework, leading to successful generalization across new camera\nconfigurations and new scenes. We perform a comprehensive set of experiments on\nthe \\wildtrack and \\multiviewx datasets to (a) motivate the necessity to\nevaluate MVD methods on generalization abilities and (b) demonstrate the\nefficacy of the proposed approach. The code is publicly available at\n\\url{https://github.com/jeetv/GMVD}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vora_J/0/1/0/all/0/1\">Jeet Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Swetanjal Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Range Feature Propagating for Natural Image Matting. (arXiv:2109.12252v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12252","description":"<p>Natural image matting estimates the alpha values of unknown regions in the\ntrimap. Recently, deep learning based methods propagate the alpha values from\nthe known regions to unknown regions according to the similarity between them.\nHowever, we find that more than 50\\% pixels in the unknown regions cannot be\ncorrelated to pixels in known regions due to the limitation of small effective\nreception fields of common convolutional neural networks, which leads to\ninaccurate estimation when the pixels in the unknown regions cannot be inferred\nonly with pixels in the reception fields. To solve this problem, we propose\nLong-Range Feature Propagating Network (LFPNet), which learns the long-range\ncontext features outside the reception fields for alpha matte estimation.\nSpecifically, we first design the propagating module which extracts the context\nfeatures from the downsampled image. Then, we present Center-Surround Pyramid\nPooling (CSPP) that explicitly propagates the context features from the\nsurrounding context image patch to the inner center image patch. Finally, we\nuse the matting module which takes the image, trimap and context features to\nestimate the alpha matte. Experimental results demonstrate that the proposed\nmethod performs favorably against the state-of-the-art methods on the\nAlphaMatting and Adobe Image Matting datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haozhe Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_B/0/1/0/all/0/1\">Bineng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery. (arXiv:2109.12257v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12257","description":"<p>Tensor sparse modeling as a promising approach, in the whole of science and\nengineering has been a huge success. As is known to all, various data in\npractical application are often generated by multiple factors, so the use of\ntensors to represent the data containing the internal structure of multiple\nfactors came into being. However, different from the matrix case, constructing\nreasonable sparse measure of tensor is a relatively difficult and very\nimportant task. Therefore, in this paper, we propose a new tensor sparsity\nmeasure called Tensor Full Feature Measure (FFM). It can simultaneously\ndescribe the feature information of each dimension of the tensor and the\nrelated features between two dimensions, and connect the Tucker rank with the\ntensor tube rank. This measurement method can describe the sparse features of\nthe tensor more comprehensively. On this basis, we establish its non-convex\nrelaxation, and apply FFM to low rank tensor completion (LRTC) and tensor\nrobust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM\nare proposed, and two efficient Alternating Direction Multiplier Method (ADMM)\nalgorithms are developed to solve the proposed model. A variety of real\nnumerical experiments substantiate the superiority of the proposed methods\nbeyond state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An embarrassingly simple comparison of machine learning algorithms for indoor scene classification. (arXiv:2109.12261v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12261","description":"<p>With the emergence of autonomous indoor robots, the computer vision task of\nindoor scene recognition has gained the spotlight. Indoor scene recognition is\na challenging problem in computer vision that relies on local and global\nfeatures in a scene. This study aims to compare the performance of five machine\nlearning algorithms on the task of indoor scene classification to identify the\npros and cons of each classifier. It also provides a comparison of low latency\nfeature extractors versus enormous feature extractors to understand the\nperformance effects. Finally, a simple MnasNet based indoor classification\nsystem is proposed, which can achieve 72% accuracy at 23 ms latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gamage_B/0/1/0/all/0/1\">Bhanuka Manesha Samarasekara Vitharana Gamage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data, Assemble: Leveraging Multiple Datasets with Heterogeneous and Partial Labels. (arXiv:2109.12265v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12265","description":"<p>The success of deep learning relies heavily on large datasets with extensive\nlabels, but we often only have access to several small, heterogeneous datasets\nassociated with partial labels, particularly in the field of medical imaging.\nWhen learning from multiple datasets, existing challenges include incomparable,\nheterogeneous, or even conflicting labeling protocols across datasets. In this\npaper, we propose a new initiative--\"data, assemble\"--which aims to unleash the\nfull potential of partially labeled data and enormous unlabeled data from an\nassembly of datasets. To accommodate the supervised learning paradigm to\npartial labels, we introduce a dynamic adapter that encodes multiple visual\ntasks and aggregates image features in a question-and-answer manner.\nFurthermore, we employ pseudo-labeling and consistency constraints to harness\nimages with missing labels and to mitigate the domain gap across datasets. From\nproof-of-concept studies on three natural imaging datasets and rigorous\nevaluations on two large-scale thorax X-ray benchmarks, we discover that\nlearning from \"negative examples\" facilitates both classification and\nsegmentation of classes of interest. This sheds new light on the computer-aided\ndiagnosis of rare diseases and emerging pandemics, wherein \"positive examples\"\nare hard to collect, yet \"negative examples\" are relatively easier to assemble.\nAs a result, besides exceeding the prior art in the NIH ChestXray benchmark,\nour model is particularly strong in identifying diseases of minority classes,\nyielding over 3-point improvement on average. Remarkably, when using existing\npartial labels, our model performance is on-par (p&gt;0.05) with that using a\nfully curated dataset with exhaustive labels, eliminating the need for\nadditional 40% annotation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mintong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan L. Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation. (arXiv:2109.12266v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12266","description":"<p>Current monocular-based 6D object pose estimation methods generally achieve\nless competitive results than RGBD-based methods, mostly due to the lack of 3D\ninformation. To make up this gap, this paper proposes a 3D geometric volume\nbased pose estimation method with a short baseline two-view setting. By\nconstructing a geometric volume in the 3D space, we combine the features from\ntwo adjacent images to the same 3D space. Then a network is trained to learn\nthe distribution of the position of object keypoints in the volume, and a\nrobust soft RANSAC solver is deployed to solve the pose in closed form. To\nbalance accuracy and cost, we propose a coarse-to-fine framework to improve the\nperformance in an iterative way. The experiments show that our method\noutperforms state-of-the-art monocular-based methods, and is robust in\ndifferent objects and scenes, especially in serious occlusion situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lilu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12271","description":"<p>Convolutional neural networks (CNNs) have recently achieved remarkable\nsuccess in automatically identifying organs or lesions on 3D medical images.\nMeanwhile, vision transformer networks have exhibited exceptional performance\nin 2D image classification tasks. Compared with CNNs, transformer networks have\nan obvious advantage of extracting long-range features due to their\nself-attention algorithm. Therefore, in this paper we present a CNN-Transformer\ncombined model called BiTr-Unet for brain tumor segmentation on multi-modal MRI\nscans. The proposed BiTr-Unet achieves good performance on the BraTS 2021\nvalidation dataset with mean Dice score 0.9076, 0.8392 and 0.8231, and mean\nHausdorff distance 4.5322, 13.4592 and 14.9963 for the whole tumor, tumor core,\nand enhancing tumor, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qiran Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Hai Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Differentiable and Interpretable Model for VIO with 4 Trainable Parameters. (arXiv:2109.12292v1 [cs.RO])","link":"http://arxiv.org/abs/2109.12292","description":"<p>Monocular visual-inertial odometry (VIO) is a critical problem in robotics\nand autonomous driving. Traditional methods solve this problem based on\nfiltering or optimization. While being fully interpretable, they rely on manual\ninterference and empirical parameter tuning. On the other hand, learning-based\napproaches allow for end-to-end training but require a large number of training\ndata to learn millions of parameters. However, the non-interpretable and heavy\nmodels hinder the generalization ability. In this paper, we propose a fully\ndifferentiable, interpretable, and lightweight monocular VIO model that\ncontains only 4 trainable parameters. Specifically, we first adopt Unscented\nKalman Filter as a differentiable layer to predict the pitch and roll, where\nthe covariance matrices of noise are learned to filter out the noise of the IMU\nraw data. Second, the refined pitch and roll are adopted to retrieve a\ngravity-aligned BEV image of each frame using differentiable camera projection.\nFinally, a differentiable pose estimator is utilized to estimate the remaining\n4 DoF poses between the BEV frames. Our method allows for learning the\ncovariance matrices end-to-end supervised by the pose estimation loss,\ndemonstrating superior performance to empirical baselines. Experimental results\non synthetic and real-world datasets demonstrate that our simple approach is\ncompetitive with state-of-the-art methods and generalizes well on unseen\nscenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haozhe Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Patch Convolutional Neural Network for View-based 3D Model Retrieval. (arXiv:2109.12299v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12299","description":"<p>Recently, many view-based 3D model retrieval methods have been proposed and\nhave achieved state-of-the-art performance. Most of these methods focus on\nextracting more discriminative view-level features and effectively aggregating\nthe multi-view images of a 3D model, but the latent relationship among these\nmulti-view images is not fully explored. Thus, we tackle this problem from the\nperspective of exploiting the relationships between patch features to capture\nlong-range associations among multi-view images. To capture associations among\nviews, in this work, we propose a novel patch convolutional neural network\n(PCNN) for view-based 3D model retrieval. Specifically, we first employ a CNN\nto extract patch features of each view image separately. Secondly, a novel\nneural network module named PatchConv is designed to exploit intrinsic\nrelationships between neighboring patches in the feature space to capture\nlong-range associations among multi-view images. Then, an adaptive weighted\nview layer is further embedded into PCNN to automatically assign a weight to\neach view according to the similarity between each view feature and the\nview-pooling feature. Finally, a discrimination loss function is employed to\nextract the discriminative 3D model feature, which consists of softmax loss\nvalues generated by the fusion lassifier and the specific classifier. Extensive\nexperimental results on two public 3D model retrieval benchmarks, namely, the\nModelNet40, and ModelNet10, demonstrate that our proposed PCNN can outperform\nstate-of-the-art approaches, with mAP alues of 93.67%, and 96.23%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuxiang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengyong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Multi-Instance Learning for Retinal Disease Recognition. (arXiv:2109.12307v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12307","description":"<p>This paper attacks an emerging challenge of multi-modal retinal disease\nrecognition. Given a multi-modal case consisting of a color fundus photo (CFP)\nand an array of OCT B-scan images acquired during an eye examination, we aim to\nbuild a deep neural network that recognizes multiple vision-threatening\ndiseases for the given case. As the diagnostic efficacy of CFP and OCT is\ndisease-dependent, the network's ability of being both selective and\ninterpretable is important. Moreover, as both data acquisition and manual\nlabeling are extremely expensive in the medical domain, the network has to be\nrelatively lightweight for learning from a limited set of labeled multi-modal\nsamples. Prior art on retinal disease recognition focuses either on a single\ndisease or on a single modality, leaving multi-modal fusion largely\nunderexplored. We propose in this paper Multi-Modal Multi-Instance Learning\n(MM-MIL) for selectively fusing CFP and OCT modalities. Its lightweight\narchitecture (as compared to current multi-head attention modules) makes it\nsuited for learning from relatively small-sized datasets. For an effective use\nof MM-MIL, we propose to generate a pseudo sequence of CFPs by over sampling a\ngiven CFP. The benefits of this tactic include well balancing instances across\nmodalities, increasing the resolution of the CFP input, and finding out regions\nof the CFP most relevant with respect to the final diagnosis. Extensive\nexperiments on a real-world dataset consisting of 1,206 multi-modal cases from\n1,193 eyes of 836 subjects demonstrate the viability of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hailan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianchun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dayong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Youxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification. (arXiv:2109.12333v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12333","description":"<p>Unsupervised person re-identification (Re-ID) is a promising and very\nchallenging research problem in computer vision. Learning robust and\ndiscriminative features with unlabeled data is of central importance to Re-ID.\nRecently, more attention has been paid to unsupervised Re-ID algorithms based\non clustered pseudo-label. However, the previous approaches did not fully\nexploit information of hard samples, simply using cluster centroid or all\ninstances for contrastive learning. In this paper, we propose a Hard-sample\nGuided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss\nwith instance-level loss for unsupervised person Re-ID. Our approach applies\ncluster centroid contrastive loss to ensure that the network is updated in a\nmore stable way. Meanwhile, introduction of a hard instance contrastive loss\nfurther mines the discriminative information. Extensive experiments on two\npopular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms\nprevious state-of-the-art methods and significantly improves the performance of\nunsupervised person Re-ID. The code of our work is available soon at\nhttps://github.com/bupt-ai-cz/HHCL-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting survival of glioblastoma from automatic whole-brain and tumor segmentation of MR images. (arXiv:2109.12334v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12334","description":"<p>Survival prediction models can potentially be used to guide treatment of\nglioblastoma patients. However, currently available MR imaging biomarkers\nholding prognostic information are often challenging to interpret, have\ndifficulties generalizing across data acquisitions, or are only applicable to\npre-operative MR data. In this paper we aim to address these issues by\nintroducing novel imaging features that can be automatically computed from MR\nimages and fed into machine learning models to predict patient survival. The\nfeatures we propose have a direct biological interpretation: They measure the\ndeformation caused by the tumor on the surrounding brain structures, comparing\nthe shape of various structures in the patient's brain to their expected shape\nin healthy individuals. To obtain the required segmentations, we use an\nautomatic method that is contrast-adaptive and robust to missing modalities,\nmaking the features generalizable across scanners and imaging protocols. Since\nthe features we propose do not depend on characteristics of the tumor region\nitself, they are also applicable to post-operative images, which have been much\nless studied in the context of survival prediction. Using experiments involving\nboth pre- and post-operative data, we show that the proposed features carry\nprognostic value in terms of overall- and progression-free survival, over and\nabove that of conventional non-imaging features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Palsson_S/0/1/0/all/0/1\">Sveinn P&#xe1;lsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cerri_S/0/1/0/all/0/1\">Stefano Cerri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poulsen_H/0/1/0/all/0/1\">Hans Skovgaard Poulsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Urup_T/0/1/0/all/0/1\">Thomas Urup</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Law_I/0/1/0/all/0/1\">Ian Law</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-sensitive Information Retention for Accurate Binary Neural Network. (arXiv:2109.12338v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12338","description":"<p>Model binarization is an effective method of compressing neural networks and\naccelerating their inference process, which enables state-of-the-art models to\nrun on resource-limited devices. However, a significant performance gap still\nexists between the 1-bit model and the 32-bit one. The empirical study shows\nthat binarization causes a great loss of information in the forward and\nbackward propagation which harms the performance of binary neural networks\n(BNNs), and the limited information representation ability of binarized\nparameter is one of the bottlenecks of BNN performance. We present a novel\nDistribution-sensitive Information Retention Network (DIR-Net) to retain the\ninformation of the forward activations and backward gradients, which improves\nBNNs by distribution-sensitive optimization without increasing the overhead in\nthe inference process. The DIR-Net mainly relies on two technical\ncontributions: (1) Information Maximized Binarization (IMB): minimizing the\ninformation loss and the quantization error of weights/activations\nsimultaneously by balancing and standardizing the weight distribution in the\nforward propagation; (2) Distribution-sensitive Two-stage Estimator (DTE):\nminimizing the information loss of gradients by gradual distribution-sensitive\napproximation of the sign function in the backward propagation, jointly\nconsidering the updating capability and accurate gradient. The DIR-Net\ninvestigates both forward and backward processes of BNNs from the unified\ninformation perspective, thereby provides new insight into the mechanism of\nnetwork binarization. Comprehensive experiments on CIFAR-10 and ImageNet\ndatasets show our DIR-Net consistently outperforms the SOTA binarization\napproaches under mainstream and compact architectures. Additionally, we conduct\nour DIR-Net on real-world resource-limited devices which achieves 11.1 times\nstorage saving and 5.4 times speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XianglongLiu/0/1/0/all/0/1\">XianglongLiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of MGMT Methylation Status of Glioblastoma using Radiomics and Latent Space Shape Features. (arXiv:2109.12339v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12339","description":"<p>In this paper we propose a method for predicting the status of MGMT promoter\nmethylation in high-grade gliomas. From the available MR images, we segment the\ntumor using deep convolutional neural networks and extract both radiomic\nfeatures and shape features learned by a variational autoencoder. We\nimplemented a standard machine learning workflow to obtain predictions,\nconsisting of feature selection followed by training of a random forest\nclassification model. We trained and evaluated our method on the\nRSNA-ASNR-MICCAI BraTS 2021 challenge dataset and submitted our predictions to\nthe challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Palsson_S/0/1/0/all/0/1\">Sveinn P&#xe1;lsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cerri_S/0/1/0/all/0/1\">Stefano Cerri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeNet: A lightweight One-Shot Aggregation Convolutional Network. (arXiv:2109.12342v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12342","description":"<p>The architecture of deep convolutional networks (CNNs) has evolved for years,\nbecoming more accurate and faster. However, it is still challenging to design\nreasonable network structures that aim at obtaining the best accuracy under a\nlimited computational budget. In this paper, we propose a Tree block, named\nafter its appearance, which extends the One-Shot Aggregation (OSA) module while\nbeing more lightweight and flexible. Specifically, the Tree block replaces each\nof the $3\\times3$ Conv layers in OSA into a stack of shallow residual block\n(SRB) and $1\\times1$ Conv layer. The $1\\times1$ Conv layer is responsible for\ndimension increasing and the SRB is fed into the next step. By doing this, when\naggregating the same number of subsequent feature maps, the Tree block has a\ndeeper network structure while having less model complexity. In addition,\nresidual connection and efficient channel attention(ECA) is added to the Tree\nblock to further improve the performance of the network. Based on the Tree\nblock, we build efficient backbone models calling TreeNets. TreeNet has a\nsimilar network architecture to ResNet, making it flexible to replace ResNet in\nvarious computer vision frameworks. We comprehensively evaluate TreeNet on\ncommon-used benchmarks, including ImageNet-1k for classification, MS COCO for\nobject detection, and instance segmentation. Experimental results demonstrate\nthat TreeNet is more efficient and performs favorably against the current\nstate-of-the-art backbone methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1\">Lu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yubin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging. (arXiv:2109.12347v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12347","description":"<p>Machine learning models commonly exhibit unexpected failures post-deployment\ndue to either data shifts or uncommon situations in the training environment.\nDomain experts typically go through the tedious process of inspecting the\nfailure cases manually, identifying failure modes and then attempting to fix\nthe model. In this work, we aim to standardise and bring principles to this\nprocess through answering two critical questions: (i) how do we know that we\nhave identified meaningful and distinct failure types?; (ii) how can we\nvalidate that a model has, indeed, been repaired? We suggest that the quality\nof the identified failure types can be validated through measuring the intra-\nand inter-type generalisation after fine-tuning and introduce metrics to\ncompare different subtyping methods. Furthermore, we argue that a model can be\nconsidered repaired if it achieves high accuracy on the failure types while\nretaining performance on the previously correct data. We combine these two\nideas into a principled framework for evaluating the quality of both the\nidentified failure subtypes and model repairment. We evaluate its utility on a\nclassification and an object detection tasks. Our code is available at\nhttps://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henn_T/0/1/0/all/0/1\">Thomas Henn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_Y/0/1/0/all/0/1\">Yasukazu Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquet_C/0/1/0/all/0/1\">Cl&#xe9;ment Jacquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshizawa_S/0/1/0/all/0/1\">Shunsuke Yoshizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andou_M/0/1/0/all/0/1\">Masamichi Andou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchen_S/0/1/0/all/0/1\">Stephen Tchen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saga_R/0/1/0/all/0/1\">Ryosuke Saga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishihara_H/0/1/0/all/0/1\">Hiroyuki Ishihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_K/0/1/0/all/0/1\">Katsuhiko Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Mitochondria Segmentation. (arXiv:2109.12363v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12363","description":"<p>Mitochondria segmentation in electron microscopy images is essential in\nneuroscience. However, due to the image degradation during the imaging process,\nthe large variety of mitochondrial structures, as well as the presence of\nnoise, artifacts and other sub-cellular structures, mitochondria segmentation\nis very challenging. In this paper, we propose a novel and effective\ncontrastive learning framework to learn a better feature representation from\nhard examples to improve segmentation. Specifically, we adopt a point sampling\nstrategy to pick out representative pixels from hard examples in the training\nphase. Based on these sampled pixels, we introduce a pixel-wise label-based\ncontrastive loss which consists of a similarity loss term and a consistency\nloss term. The similarity term can increase the similarity of pixels from the\nsame class and the separability of pixels from different classes in feature\nspace, while the consistency term is able to enhance the sensitivity of the 3D\nmodel to changes in image content from frame to frame. We demonstrate the\neffectiveness of our method on MitoEM dataset as well as FIB-SEM dataset and\nshow better or on par with state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhili Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compositional Feature Embedding and Similarity Metric for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.12380v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12380","description":"<p>Fine-grained visual categorization (FGVC), which aims at classifying objects\nwith small inter-class variances, has been significantly advanced in recent\nyears. However, ultra-fine-grained visual categorization (ultra-FGVC), which\ntargets at identifying subclasses with extremely similar patterns, has not\nreceived much attention. In ultra-FGVC datasets, the samples per category are\nalways scarce as the granularity moves down, which will lead to overfitting\nproblems. Moreover, the difference among different categories is too subtle to\ndistinguish even for professional experts. Motivated by these issues, this\npaper proposes a novel compositional feature embedding and similarity metric\n(CECS). Specifically, in the compositional feature embedding module, we\nrandomly select patches in the original input image, and these patches are then\nreplaced by patches from the images of different categories or masked out. Then\nthe replaced and masked images are used to augment the original input images,\nwhich can provide more diverse samples and thus largely alleviate overfitting\nproblem resulted from limited training samples. Besides, learning with diverse\nsamples forces the model to learn not only the most discriminative features but\nalso other informative features in remaining regions, enhancing the\ngeneralization and robustness of the model. In the compositional similarity\nmetric module, a new similarity metric is developed to improve the\nclassification performance by narrowing the intra-category distance and\nenlarging the inter-category distance. Experimental results on two ultra-FGVC\ndatasets and one FGVC dataset with recent benchmark methods consistently\ndemonstrate that the proposed CECS method achieves the state of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaohua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Progressive and Coarse-to-fine Registration of Brain MRI via Deformation Field Integration and Non-Rigid Feature Fusion. (arXiv:2109.12384v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12384","description":"<p>Registration of brain MRI images requires to solve a deformation field, which\nis extremely difficult in aligning intricate brain tissues, e.g., subcortical\nnuclei, etc. Existing efforts resort to decomposing the target deformation\nfield into intermediate sub-fields with either tiny motions, i.e., progressive\nregistration stage by stage, or lower resolutions, i.e., coarse-to-fine\nestimation of the full-size deformation field. In this paper, we argue that\nthose efforts are not mutually exclusive, and propose a unified framework for\nrobust brain MRI registration in both progressive and coarse-to-fine manners\nsimultaneously. Specifically, building on a dual-encoder U-Net, the\nfixed-moving MRI pair is encoded and decoded into multi-scale deformation\nsub-fields from coarse to fine. Each decoding block contains two proposed novel\nmodules: i) in Deformation Field Integration (DFI), a single integrated\nsub-field is calculated, warping by which is equivalent to warping\nprogressively by sub-fields from all previous decoding blocks, and ii) in\nNon-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned\nby DFI-integrated sub-field, and then fused to predict a finer sub-field.\nLeveraging both DFI and NFF, the target deformation field is factorized into\nmulti-scale sub-fields, where the coarser fields alleviate the estimate of a\nfiner one and the finer field learns to make up those misalignments insolvable\nby previous coarser ones. The extensive and comprehensive experimental results\non both private and public datasets demonstrate a superior registration\nperformance of brain MRI images over progressive registration only and\ncoarse-to-fine estimation only, with an increase by at most 10% in the average\nDice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jinxin Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_H/0/1/0/all/0/1\">Hongkuan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haobo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-source Few-shot Domain Adaptation. (arXiv:2109.12391v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12391","description":"<p>Multi-source Domain Adaptation (MDA) aims to transfer predictive models from\nmultiple, fully-labeled source domains to an unlabeled target domain. However,\nin many applications, relevant labeled source datasets may not be available,\nand collecting source labels can be as expensive as labeling the target data\nitself. In this paper, we investigate Multi-source Few-shot Domain Adaptation\n(MFDA): a new domain adaptation scenario with limited multi-source labels and\nunlabeled target data. As we show, existing methods often fail to learn\ndiscriminative features for both source and target domains in the MFDA setting.\nTherefore, we propose a novel framework, termed Multi-Source Few-shot\nAdaptation Network (MSFAN), which can be trained end-to-end in a\nnon-adversarial manner. MSFAN operates by first using a type of prototypical,\nmulti-domain, self-supervised learning to learn features that are not only\ndomain-invariant but also class-discriminative. Second, MSFAN uses a small,\nlabeled support set to enforce feature consistency and domain invariance across\ndomains. Finally, prototypes from multiple sources are leveraged to learn\nbetter classifiers. Compared with state-of-the-art MDA methods, MSFAN improves\nthe mean classification accuracy over different domain pairs on MFDA by 20.2%,\n9.4%, and 16.2% on Office, Office-Home, and DomainNet, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_H/0/1/0/all/0/1\">Hari Prasanna Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentelli_A/0/1/0/all/0/1\">Alberto Sangiovanni Vincentelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vehicle Detection and Tracking From Surveillance Cameras in Urban Scenes. (arXiv:2109.12414v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12414","description":"<p>Detecting and tracking vehicles in urban scenes is a crucial step in many\ntraffic-related applications as it helps to improve road user safety among\nother benefits. Various challenges remain unresolved in multi-object tracking\n(MOT) including target information description, long-term occlusions and fast\nmotion. We propose a multi-vehicle detection and tracking system following the\ntracking-by-detection paradigm that tackles the previously mentioned\nchallenges. Our MOT method extends an Intersection-over-Union (IOU)-based\ntracker with vehicle re-identification features. This allows us to utilize\nappearance information to better match objects after long occlusion phases\nand/or when object location is significantly shifted due to fast motion. We\noutperform our baseline MOT method on the UA-DETRAC benchmark while maintaining\na total processing speed suitable for online use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messoussi_O/0/1/0/all/0/1\">Oumayma Messoussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_F/0/1/0/all/0/1\">Felipe Gohring de Magalhaes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarre_F/0/1/0/all/0/1\">Francois Lamarre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perreault_F/0/1/0/all/0/1\">Francis Perreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogoba_I/0/1/0/all/0/1\">Ibrahima Sogoba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolescu_G/0/1/0/all/0/1\">Gabriela Nicolescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L$^{2}$NAS: Learning to Optimize Neural Architectures via Continuous-Action Reinforcement Learning. (arXiv:2109.12425v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12425","description":"<p>Neural architecture search (NAS) has achieved remarkable results in deep\nneural network design. Differentiable architecture search converts the search\nover discrete architectures into a hyperparameter optimization problem which\ncan be solved by gradient descent. However, questions have been raised\nregarding the effectiveness and generalizability of gradient methods for\nsolving non-convex architecture hyperparameter optimization problems. In this\npaper, we propose L$^{2}$NAS, which learns to intelligently optimize and update\narchitecture hyperparameters via an actor neural network based on the\ndistribution of high-performing architectures in the search history. We\nintroduce a quantile-driven training procedure which efficiently trains\nL$^{2}$NAS in an actor-critic framework via continuous-action reinforcement\nlearning. Experiments show that L$^{2}$NAS achieves state-of-the-art results on\nNAS-Bench-201 benchmark as well as DARTS search space and Once-for-All\nMobileNetV3 search space. We also show that search policies generated by\nL$^{2}$NAS are generalizable and transferable across different training\ndatasets with minimal fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith G. Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Linglong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search. (arXiv:2109.12426v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12426","description":"<p>Neural architecture search automates neural network design and has achieved\nstate-of-the-art results in many deep learning applications. While recent\nliterature has focused on designing networks to maximize accuracy, little work\nhas been conducted to understand the compatibility of architecture design\nspaces to varying hardware. In this paper, we analyze the neural blocks used to\nbuild Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to\nunderstand their predictive power and inference latency on various devices,\nincluding Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and\nSamsung Note10. We introduce a methodology to quantify the friendliness of\nneural blocks to hardware and the impact of their placement in a macro network\non overall network performance via only end-to-end measurements. Based on\nextensive profiling results, we derive design insights and apply them to\nhardware-specific search space reduction. We show that searching in the reduced\nsearch space generates better accuracy-latency Pareto frontiers than searching\nin the original search spaces, customizing architecture search according to the\nhardware. Moreover, insights derived from measurements lead to notably higher\nImageNet top-1 scores on all search spaces investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith G. Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chudak_F/0/1/0/all/0/1\">Fabian Chudak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Unpaired Translation using Focal Loss for Patch Classification. (arXiv:2109.12431v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12431","description":"<p>Image-to-image translation models transfer images from input domain to output\ndomain in an endeavor to retain the original content of the image. Contrastive\nUnpaired Translation is one of the existing methods for solving such problems.\nSignificant advantage of this method, compared to competitors, is the ability\nto train and perform well in cases where both input and output domains are only\na single image. Another key thing that differentiates this method from its\npredecessors is the usage of image patches rather than the whole images. It\nalso turns out that sampling negatives (patches required to calculate the loss)\nfrom the same image achieves better results than a scenario where the negatives\nare sampled from other images in the dataset. This type of approach encourages\nmapping of corresponding patches to the same location in relation to other\npatches (negatives) while at the same time improves the output image quality\nand significantly decreases memory usage as well as the time required to train\nthe model compared to CycleGAN method used as a baseline. Through a series of\nexperiments we show that using focal loss in place of cross-entropy loss within\nthe PatchNCE loss can improve on the model's performance and even surpass the\ncurrent state-of-the-art model for image-to-image translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spiegl_B/0/1/0/all/0/1\">Bernard Spiegl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCal-Net: Joint Region-Channel-Wise Calibrated Network for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.12448v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12448","description":"<p>Semantic segmentation in surgical videos is a prerequisite for a broad range\nof applications towards improving surgical outcomes and surgical video\nanalysis. However, semantic segmentation in surgical videos involves many\nchallenges. In particular, in cataract surgery, various features of the\nrelevant objects such as blunt edges, color and context variation, reflection,\ntransparency, and motion blur pose a challenge for semantic segmentation. In\nthis paper, we propose a novel convolutional module termed as \\textit{ReCal}\nmodule, which can calibrate the feature maps by employing region\nintra-and-inter-dependencies and channel-region cross-dependencies. This\ncalibration strategy can effectively enhance semantic representation by\ncorrelating different representations of the same semantic label, considering a\nmulti-angle local view centering around each pixel. Thus the proposed module\ncan deal with distant visual characteristics of unique objects as well as\ncross-similarities in the visual characteristics of different objects.\nMoreover, we propose a novel network architecture based on the proposed module\ntermed as ReCal-Net. Experimental results confirm the superiority of ReCal-Net\ncompared to rival state-of-the-art approaches for all relevant objects in\ncataract surgery. Moreover, ablation studies reveal the effectiveness of the\nReCal module in boosting semantic segmentation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putzgruber_Adamitsch_D/0/1/0/all/0/1\">Doris Putzgruber-Adamitsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarny_S/0/1/0/all/0/1\">Stephanie Sarny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+El_Shabrawi_Y/0/1/0/all/0/1\">Yosuf El-Shabrawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoeffmann_K/0/1/0/all/0/1\">Klaus Schoeffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 from CXR Images in a 15-class Scenario: an Attempt to Avoid Bias in the System. (arXiv:2109.12453v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12453","description":"<p>As of June 2021, the World Health Organization (WHO) has reported 171.7\nmillion confirmed cases including 3,698,621 deaths from COVID-19. Detecting\nCOVID-19 and other lung diseases from Chest X-Ray (CXR) images can be very\neffective for emergency diagnosis and treatment as CXR is fast and cheap. The\nobjective of this study is to develop a system capable of detecting COVID-19\nalong with 14 other lung diseases from CXRs in a fair and unbiased manner. The\nproposed system consists of a CXR image selection technique and a deep learning\nbased model to classify 15 diseases including COVID-19. The proposed CXR\nselection technique aims to retain the maximum variation uniformly and\neliminate poor quality CXRs with the goal of reducing the training dataset size\nwithout compromising classifier accuracy. More importantly, it reduces the\noften hidden bias and unfairness in decision making. The proposed solution\nexhibits a promising COVID-19 detection scheme in a more realistic situation\nthan most existing studies as it deals with 15 lung diseases together. We hope\nthe proposed method will have wider adoption in medical image classification\nand other related fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bose_C/0/1/0/all/0/1\">Chinmoy Bose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basu_A/0/1/0/all/0/1\">Anirvan Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing AI models for Verified Deployment under Semantic Specifications. (arXiv:2109.12456v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12456","description":"<p>Auditing trained deep learning (DL) models prior to deployment is vital in\npreventing unintended consequences. One of the biggest challenges in auditing\nis in understanding how we can obtain human-interpretable specifications that\nare directly useful to the end-user. We address this challenge through a\nsequence of semantically-aligned unit tests, where each unit test verifies\nwhether a predefined specification (e.g., accuracy over 95%) is satisfied with\nrespect to controlled and semantically aligned variations in the input space\n(e.g., in face recognition, the angle relative to the camera). We perform these\nunit tests by directly verifying the semantically aligned variations in an\ninterpretable latent space of a generative model. Our framework, AuditAI,\nbridges the gap between interpretable formal verification and scalability. With\nevaluations on four different datasets, covering images of towers, chest\nX-rays, human faces, and ImageNet classes, we show how AuditAI allows us to\nobtain controlled variations for verification and certified training while\naddressing the limitations of verifying using only pixel-space perturbations. A\nblog post accompanying the paper is at this link\nhttps://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bharadhwaj_H/0/1/0/all/0/1\">Homanga Bharadhwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Souls in an Adversarial Image: Towards Universal Adversarial Example Detection using Multi-view Inconsistency. (arXiv:2109.12459v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12459","description":"<p>In the evasion attacks against deep neural networks (DNN), the attacker\ngenerates adversarial instances that are visually indistinguishable from benign\nsamples and sends them to the target DNN to trigger misclassifications. In this\npaper, we propose a novel multi-view adversarial image detector, namely Argos,\nbased on a novel observation. That is, there exist two \"souls\" in an\nadversarial instance, i.e., the visually unchanged content, which corresponds\nto the true label, and the added invisible perturbation, which corresponds to\nthe misclassified label. Such inconsistencies could be further amplified\nthrough an autoregressive generative approach that generates images with seed\npixels selected from the original image, a selected label, and pixel\ndistributions learned from the training data. The generated images (i.e., the\n\"views\") will deviate significantly from the original one if the label is\nadversarial, demonstrating inconsistencies that Argos expects to detect. To\nthis end, Argos first amplifies the discrepancies between the visual content of\nan image and its misclassified label induced by the attack using a set of\nregeneration mechanisms and then identifies an image as adversarial if the\nreproduced views deviate to a preset degree. Our experimental results show that\nArgos significantly outperforms two representative adversarial detectors in\nboth detection accuracy and robustness against six well-known adversarial\nattacks. Code is available at:\nhttps://github.com/sohaib730/Argos-Adversarial_Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiani_S/0/1/0/all/0/1\">Sohaib Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_S/0/1/0/all/0/1\">Sana Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chao Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EllipseNet: Anchor-Free Ellipse Detection for Automatic Cardiac Biometrics in Fetal Echocardiography. (arXiv:2109.12474v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12474","description":"<p>As an important scan plane, four chamber view is routinely performed in both\nsecond trimester perinatal screening and fetal echocardiographic examinations.\nThe biometrics in this plane including cardio-thoracic ratio (CTR) and cardiac\naxis are usually measured by sonographers for diagnosing congenital heart\ndisease. However, due to the commonly existing artifacts like acoustic\nshadowing, the traditional manual measurements not only suffer from the low\nefficiency, but also with the inconsistent results depending on the operators'\nskills. In this paper, we present an anchor-free ellipse detection network,\nnamely EllipseNet, which detects the cardiac and thoracic regions in ellipse\nand automatically calculates the CTR and cardiac axis for fetal cardiac\nbiometrics in 4-chamber view. In particular, we formulate the network that\ndetects the center of each object as points and regresses the ellipses'\nparameters simultaneously. We define an intersection-over-union loss to further\nregulate the regression procedure. We evaluate EllipseNet on clinical\nechocardiogram dataset with more than 2000 subjects. Experimental results show\nthat the proposed framework outperforms several state-of-the-art methods.\nSource code will be available at https://git.openi.org.cn/capepoint/EllipseNet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiancong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoxue Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yihua He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation. (arXiv:2109.12484v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12484","description":"<p>Self-supervised methods play an increasingly important role in monocular\ndepth estimation due to their great potential and low annotation cost. To close\nthe gap with supervised methods, recent works take advantage of extra\nconstraints, e.g., semantic segmentation. However, these methods will\ninevitably increase the burden on the model. In this paper, we show theoretical\nand empirical evidence that the potential capacity of self-supervised monocular\ndepth estimation can be excavated without increasing this cost. In particular,\nwe propose (1) a novel data augmentation approach called data grafting, which\nforces the model to explore more cues to infer depth besides the vertical image\nposition, (2) an exploratory self-distillation loss, which is supervised by the\nself-distillation label generated by our new post-processing method - selective\npost-processing, and (3) the full-scale network, designed to endow the encoder\nwith the specialization of depth estimation task and enhance the\nrepresentational power of the model. Extensive experiments show that our\ncontributions can bring significant performance improvement to the baseline\nwith even less computational overhead, and our model, named EPCDepth, surpasses\nthe previous state-of-the-art methods even those supervised by additional\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Rui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yawen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Luyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yangang Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation. (arXiv:2109.12492v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12492","description":"<p>Recently, there has been an increasing interest in image editing methods that\nemploy pre-trained unconditional image generators (e.g., StyleGAN). However,\napplying these methods to translate images to multiple visual domains remains\nchallenging. Existing works do not often preserve the domain-invariant part of\nthe image (e.g., the identity in human face translations), they do not usually\nhandle multiple domains, or do not allow for multi-modal translations. This\nwork proposes an implicit style function (ISF) to straightforwardly achieve\nmulti-modal and multi-domain image-to-image translation from pre-trained\nunconditional generators. The ISF manipulates the semantics of an input latent\ncode to make the image generated from it lying in the desired visual domain.\nOur results in human face and animal manipulations show significantly improved\nresults over the baselines. Our model enables cost-effective multi-modal\nunsupervised image-to-image translations at high resolution using pre-trained\nunconditional GANs. The code and data are available at:\n\\url{https://github.com/yhlleo/stylegan-mmuit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Video Representation Learning by Video Incoherence Detection. (arXiv:2109.12493v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12493","description":"<p>This paper introduces a novel self-supervised method that leverages\nincoherence detection for video representation learning. It roots from the\nobservation that visual systems of human beings can easily identify video\nincoherence based on their comprehensive understanding of videos. Specifically,\nthe training sample, denoted as the incoherent clip, is constructed by multiple\nsub-clips hierarchically sampled from the same raw video with various lengths\nof incoherence between each other. The network is trained to learn high-level\nrepresentation by predicting the location and length of incoherence given the\nincoherent clip as input. Additionally, intra-video contrastive learning is\nintroduced to maximize the mutual information between incoherent clips from the\nsame raw video. We evaluate our proposed method through extensive experiments\non action recognition and video retrieval utilizing various backbone networks.\nExperiments show that our proposed method achieves state-of-the-art performance\nacross different backbone networks and different datasets compared with\nprevious coherence-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lihua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short-Term Load Forecasting Using Time Pooling Deep Recurrent Neural Network. (arXiv:2109.12498v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12498","description":"<p>Integration of renewable energy sources and emerging loads like electric\nvehicles to smart grids brings more uncertainty to the distribution system\nmanagement. Demand Side Management (DSM) is one of the approaches to reduce the\nuncertainty. Some applications like Nonintrusive Load Monitoring (NILM) can\nsupport DSM, however they require accurate forecasting on high resolution data.\nThis is challenging when it comes to single loads like one residential\nhousehold due to its high volatility. In this paper, we review some of the\nexisting Deep Learning-based methods and present our solution using Time\nPooling Deep Recurrent Neural Network. The proposed method augments data using\ntime pooling strategy and can overcome overfitting problems and model\nuncertainties of data more efficiently. Simulation and implementation results\nshow that our method outperforms the existing algorithms in terms of RMSE and\nMAE metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaygan_E/0/1/0/all/0/1\">Elahe Khoshbakhti Vaygan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_R/0/1/0/all/0/1\">Roozbeh Rajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estebsari_A/0/1/0/all/0/1\">Abouzar Estebsari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETA: Photo Albums Event Recognition using Transformers Attention. (arXiv:2109.12499v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12499","description":"<p>In recent years the amounts of personal photos captured increased\nsignificantly, giving rise to new challenges in multi-image understanding and\nhigh-level image understanding. Event recognition in personal photo albums\npresents one challenging scenario where life events are recognized from a\ndisordered collection of images, including both relevant and irrelevant images.\nEvent recognition in images also presents the challenge of high-level image\nunderstanding, as opposed to low-level image object classification. In absence\nof methods to analyze multiple inputs, previous methods adopted temporal\nmechanisms, including various forms of recurrent neural networks. However,\ntheir effective temporal window is local. In addition, they are not a natural\nchoice given the disordered characteristic of photo albums. We address this gap\nwith a tailor-made solution, combining the power of CNNs for image\nrepresentation and transformers for album representation to perform global\nreasoning on image collection, offering a practical and efficient solution for\nphoto albums event recognition. Our solution reaches state-of-the-art results\non 3 prominent benchmarks, achieving above 90\\% mAP on all datasets. We further\nexplore the related image-importance task in event recognition, demonstrating\nhow the learned attentions correlate with the human-annotated importance for\nthis subjective task, thus opening the door for new applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_T/0/1/0/all/0/1\">Tamar Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for MRI Reconstruction with a Parallel Network Training Framework. (arXiv:2109.12502v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12502","description":"<p>Image reconstruction from undersampled k-space data plays an important role\nin accelerating the acquisition of MR data, and a lot of deep learning-based\nmethods have been exploited recently. Despite the achieved inspiring results,\nthe optimization of these methods commonly relies on the fully-sampled\nreference data, which are time-consuming and difficult to collect. To address\nthis issue, we propose a novel self-supervised learning method. Specifically,\nduring model optimization, two subsets are constructed by randomly selecting\npart of k-space data from the undersampled data and then fed into two parallel\nreconstruction networks to perform information recovery. Two reconstruction\nlosses are defined on all the scanned data points to enhance the network's\ncapability of recovering the frequency information. Meanwhile, to constrain the\nlearned unscanned data points of the network, a difference loss is designed to\nenforce consistency between the two parallel networks. In this way, the\nreconstruction model can be properly trained with only the undersampled data.\nDuring the model evaluation, the undersampled data are treated as the inputs\nand either of the two trained networks is expected to reconstruct the\nhigh-quality results. The proposed method is flexible and can be employed in\nany existing deep learning-based method. The effectiveness of the method is\nevaluated on an open brain MRI dataset. Experimental results demonstrate that\nthe proposed self-supervised method can achieve competitive reconstruction\nperformance compared to the corresponding supervised learning method at high\nacceleration rates (4 and 8). The code is publicly available at\n\\url{https://github.com/chenhu96/Self-Supervised-MRI-Reconstruction}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Self-calibration Method for The Internal Time Synchronization of MEMS LiDAR. (arXiv:2109.12506v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12506","description":"<p>This paper proposes a simple self-calibration method for the internal time\nsynchronization of MEMS(Micro-electromechanical systems) LiDAR during research\nand development. Firstly, we introduced the problem of internal time\nmisalignment in MEMS lidar. Then, a robust Minimum Vertical Gradient(MVG) prior\nis proposed to calibrate the time difference between the laser and MEMS mirror,\nwhich can be calculated automatically without any artificial participation or\nspecially designed cooperation target. Finally, actual experiments on MEMS\nLiDARs are implemented to demonstrate the effectiveness of the proposed method.\nIt should be noted that the calibration can be implemented in a simple\nlaboratory environment without any ranging equipment and artificial\nparticipation, which greatly accelerate the progress of research and\ndevelopment in practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xiaoguang Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shiyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1\">Baoling Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunhui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial to Whole Knowledge Distillation: Progressive Distilling Decomposed Knowledge Boosts Student Better. (arXiv:2109.12507v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12507","description":"<p>Knowledge distillation field delicately designs various types of knowledge to\nshrink the performance gap between compact student and large-scale teacher.\nThese existing distillation approaches simply focus on the improvement of\n\\textit{knowledge quality}, but ignore the significant influence of\n\\textit{knowledge quantity} on the distillation procedure. Opposed to the\nconventional distillation approaches, which extract knowledge from a fixed\nteacher computation graph, this paper explores a non-negligible research\ndirection from a novel perspective of \\textit{knowledge quantity} to further\nimprove the efficacy of knowledge distillation. We introduce a new concept of\nknowledge decomposition, and further put forward the \\textbf{P}artial to\n\\textbf{W}hole \\textbf{K}nowledge \\textbf{D}istillation~(\\textbf{PWKD})\nparadigm. Specifically, we reconstruct teacher into weight-sharing sub-networks\nwith same depth but increasing channel width, and train sub-networks jointly to\nobtain decomposed knowledge~(sub-networks with more channels represent more\nknowledge). Then, student extract partial to whole knowledge from the\npre-trained teacher within multiple training stages where cyclic learning rate\nis leveraged to accelerate convergence. Generally, \\textbf{PWKD} can be\nregarded as a plugin to be compatible with existing offline knowledge\ndistillation approaches. To verify the effectiveness of \\textbf{PWKD}, we\nconduct experiments on two benchmark datasets:~CIFAR-100 and ImageNet, and\ncomprehensive evaluation results reveal that \\textbf{PWKD} consistently improve\nexisting knowledge distillation approaches without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized multiscale feature extraction for remaining useful life prediction of bearings with generative adversarial networks. (arXiv:2109.12513v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12513","description":"<p>Bearing is a key component in industrial machinery and its failure may lead\nto unwanted downtime and economic loss. Hence, it is necessary to predict the\nremaining useful life (RUL) of bearings. Conventional data-driven approaches of\nRUL prediction require expert domain knowledge for manual feature extraction\nand may suffer from data distribution discrepancy between training and test\ndata. In this study, we propose a novel generalized multiscale feature\nextraction method with generative adversarial networks. The adversarial\ntraining learns the distribution of training data from different bearings and\nis introduced for health stage division and RUL prediction. To capture the\nsequence feature from a one-dimensional vibration signal, we adapt a U-Net\narchitecture that reconstructs features to process them with multiscale layers\nin the generator of the adversarial network. To validate the proposed method,\ncomprehensive experiments on two rotating machinery datasets have been\nconducted to predict the RUL. The experimental results show that the proposed\nfeature extraction method can effectively predict the RUL and outperforms the\nconventional RUL prediction approaches based on deep neural networks. The\nimplementation code is available at https://github.com/opensuh/GMFE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1\">Sungho Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1\">Paul Lukowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Oh Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Preserving Image Super-Resolution. (arXiv:2109.12530v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12530","description":"<p>Structures matter in single image super-resolution (SISR). Benefiting from\ngenerative adversarial networks (GANs), recent studies have promoted the\ndevelopment of SISR by recovering photo-realistic images. However, there are\nstill undesired structural distortions in the recovered images. In this paper,\nwe propose a structure-preserving super-resolution (SPSR) method to alleviate\nthe above issue while maintaining the merits of GAN-based methods to generate\nperceptual-pleasant details. Firstly, we propose SPSR with gradient guidance\n(SPSR-G) by exploiting gradient maps of images to guide the recovery in two\naspects. On the one hand, we restore high-resolution gradient maps by a\ngradient branch to provide additional structure priors for the SR process. On\nthe other hand, we propose a gradient loss to impose a second-order restriction\non the super-resolved images, which helps generative networks concentrate more\non geometric structures. Secondly, since the gradient maps are handcrafted and\nmay only be able to capture limited aspects of structural information, we\nfurther extend SPSR-G by introducing a learnable neural structure extractor\n(NSE) to unearth richer local structures and provide stronger supervision for\nSR. We propose two self-supervised structure learning methods, contrastive\nprediction and solving jigsaw puzzles, to train the NSEs. Our methods are\nmodel-agnostic, which can be potentially used for off-the-shelf SR networks.\nExperimental results on five benchmark datasets show that the proposed methods\noutperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and\nSSIM metrics. Visual results demonstrate the superiority of our methods in\nrestoring structures while generating natural SR images. Code is available at\nhttps://github.com/Maclory/SPSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMix: Density-Aware Data Augmentation for Unsupervised Domain Adaptation on Single Image Dehazing. (arXiv:2109.12544v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12544","description":"<p>Learning-based methods have achieved great success on single image dehazing\nin recent years. However, these methods are often subject to performance\ndegradation when domain shifts are confronted. Specifically, haze density gaps\nexist among the existing datasets, often resulting in poor performance when\nthese methods are tested across datasets. To address this issue, we propose a\ndensity-aware data augmentation method (DAMix) that generates synthetic hazy\nsamples according to the haze density level of the target domain. These samples\nare generated by combining a hazy image with its corresponding ground truth by\na combination ratio sampled from a density-aware distribution. They not only\ncomply with the atmospheric scattering model but also bridge the haze density\ngap between the source and target domains. DAMix ensures that the model learns\nfrom examples featuring diverse haze densities. To better utilize the various\nhazy samples generated by DAMix, we develop a dual-branch dehazing network\ninvolving two branches that can adaptively remove haze according to the haze\ndensity of the region. In addition, the dual-branch design enlarges the\nlearning capacity of the entire network; hence, our network can fully utilize\nthe DAMix-ed samples. We evaluate the effectiveness of DAMix by applying it to\nthe existing open-source dehazing methods. The experimental results demonstrate\nthat all methods show significant improvements after DAMix is applied.\nFurthermore, by combining DAMix with our model, we can achieve state-of-the-art\n(SOTA) performance in terms of domain adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chia-Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_C/0/1/0/all/0/1\">Chang-Sung Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Nan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Feature Representation for Few-shot Image Classification. (arXiv:2109.12548v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12548","description":"<p>Learning the generalizable feature representation is critical for few-shot\nimage classification. While recent works exploited task-specific feature\nembedding using meta-tasks for few-shot learning, they are limited in many\nchallenging tasks as being distracted by the excursive features such as the\nbackground, domain and style of the image samples. In this work, we propose a\nnovel Disentangled Feature Representation framework, dubbed DFR, for few-shot\nlearning applications. DFR can adaptively decouple the discriminative features\nthat are modeled by the classification branch, from the class-irrelevant\ncomponent of the variation branch. In general, most of the popular deep\nfew-shot learning methods can be plugged in as the classification branch, thus\nDFR can boost their performance on various few-shot tasks. Furthermore, we\npropose a novel FS-DomainNet dataset based on DomainNet, for benchmarking the\nfew-shot domain generalization tasks. We conducted extensive experiments to\nevaluate the proposed DFR on general and fine-grained few-shot classification,\nas well as few-shot domain generalization, using the corresponding four\nbenchmarks, i.e., mini-ImageNet, tiered-ImageNet, CUB, as well as the proposed\nFS-DomainNet. Thanks to the effective feature disentangling, the DFR-based\nfew-shot classifiers achieved the state-of-the-art results on all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Disentangled Residual Network. (arXiv:2109.12556v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12556","description":"<p>Residual networks (ResNets) have been utilized for various computer vision\nand image processing applications. The residual connection improves the\ntraining of the network with better gradient flow. A residual block consists of\nfew convolutional layers having trainable parameters, which leads to\noverfitting. Moreover, the present residual networks are not able to utilize\nthe high and low frequency information suitably, which also challenges the\ngeneralization capability of the network. In this paper, a frequency\ndisentangled residual network (FDResNet) is proposed to tackle these issues.\nSpecifically, FDResNet includes separate connections in the residual block for\nlow and high frequency components, respectively. Basically, the proposed model\ndisentangles the low and high frequency components to increase the\ngeneralization ability. Moreover, the computation of low and high frequency\ncomponents using fixed filters further avoids the overfitting. The proposed\nmodel is tested on benchmark CIFAR10/100, Caltech and TinyImageNet datasets for\nimage classification. The performance of the proposed model is also tested in\nimage retrieval framework. It is noticed that the proposed model outperforms\nits counterpart residual model. The effect of kernel size and standard\ndeviation is also evaluated. The impact of the frequency disentangling is also\nanalyzed using saliency map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satya Rajendra Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yedla_R/0/1/0/all/0/1\">Roshan Reddy Yedla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanodiya_R/0/1/0/all/0/1\">Rakesh Sanodiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei-Ta Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Hashing for Image Retrieval. (arXiv:2109.12564v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12564","description":"<p>Deep learning has shown a tremendous growth in hashing techniques for image\nretrieval. Recently, Transformer has emerged as a new architecture by utilizing\nself-attention without convolution. Transformer is also extended to Vision\nTransformer (ViT) for the visual recognition with a promising performance on\nImageNet. In this paper, we propose a Vision Transformer based Hashing (VTS)\nfor image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone\nnetwork and add the hashing head. The proposed VTS model is fine tuned for\nhashing under six different image retrieval frameworks, including Deep\nSupervised Hashing (DSH), HashNet, GreedyHash, Improved Deep Hashing Network\n(IDHN), Deep Polarized Network (DPN) and Central Similarity Quantization (CSQ)\nwith their objective functions. We perform the extensive experiments on\nCIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image\nretrieval outperforms the recent state-of-the-art hashing techniques with a\ngreat margin. We also find the proposed VTS model as the backbone network is\nbetter than the existing networks, such as AlexNet and ResNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei-Ta Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Video Summarization Method Using Temporal Interest Detection and Key Frame Prediction. (arXiv:2109.12581v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12581","description":"<p>In this paper, a Video Summarization Method using Temporal Interest Detection\nand Key Frame Prediction is proposed for supervised video summarization, where\nvideo summarization is formulated as a combination of sequence labeling and\ntemporal interest detection problem. In our method, we firstly built a flexible\nuniversal network frame to simultaneously predicts frame-level importance\nscores and temporal interest segments, and then combine the two components with\ndifferent weights to achieve a more detailed video summarization. Extensive\nexperiments and analysis on two benchmark datasets prove the effectiveness of\nour method. Specifically, compared with other state-of-the-art methods, its\nperformance is increased by at least 2.6% and 4.2% on TVSum and SumMe\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yubo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shenghui Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-aware scale-adaptive networks for cancer segmentation in whole-slide images. (arXiv:2109.12617v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12617","description":"<p>Cancer segmentation in whole-slide images is a fundamental step for viable\ntumour burden estimation, which is of great value for cancer assessment.\nHowever, factors like vague boundaries or small regions dissociated from viable\ntumour areas make it a challenging task. Considering the usefulness of\nmulti-scale features in various vision-related tasks, we present a\nstructure-aware scale-adaptive feature selection method for efficient and\naccurate cancer segmentation. Based on a segmentation network with a popular\nencoder-decoder architecture, a scale-adaptive module is proposed for selecting\nmore robust features to represent the vague, non-rigid boundaries. Furthermore,\na structural similarity metric is proposed for better tissue structure\nawareness to deal with small region segmentation. In addition, advanced designs\nincluding several attention mechanisms and the selective-kernel convolutions\nare applied to the baseline network for comparative study purposes. Extensive\nexperimental results show that the proposed structure-aware scale-adaptive\nnetworks achieve outstanding performance on liver cancer segmentation when\ncompared to top ten submitted results in the challenge of PAIP 2019. Further\nevaluation on colorectal cancer segmentation shows that the scale-adaptive\nmodule improves the baseline network or outperforms the other excellent designs\nof attention mechanisms when considering the tradeoff between efficiency and\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lopez_G/0/1/0/all/0/1\">Giussepi Lopez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Soft Labels to Model Uncertainty in Medical Image Segmentation. (arXiv:2109.12622v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12622","description":"<p>Medical image segmentation is inherently uncertain. For a given image, there\nmay be multiple plausible segmentation hypotheses, and physicians will often\ndisagree on lesion and organ boundaries. To be suited to real-world\napplication, automatic segmentation systems must be able to capture this\nuncertainty and variability. Thus far, this has been addressed by building deep\nlearning models that, through dropout, multiple heads, or variational\ninference, can produce a set - infinite, in some cases - of plausible\nsegmentation hypotheses for any given image. However, in clinical practice, it\nmay not be practical to browse all hypotheses. Furthermore, recent work shows\nthat segmentation variability plateaus after a certain number of independent\nannotations, suggesting that a large enough group of physicians may be able to\nrepresent the whole space of possible segmentations. Inspired by this, we\npropose a simple method to obtain soft labels from the annotations of multiple\nphysicians and train models that, for each image, produce a single\nwell-calibrated output that can be thresholded at multiple confidence levels,\naccording to each application's precision-recall requirements. We evaluated our\nmethod on the MICCAI 2021 QUBIQ challenge, showing that it performs well across\nmultiple medical image segmentation tasks, produces well-calibrated\npredictions, and, on average, performs better at matching physicians'\npredictions than other physicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Louren&#xe7;o Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logo Generation Using Regional Features: A Faster R-CNN Approach to Generative Adversarial Networks. (arXiv:2109.12628v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12628","description":"<p>In this paper we introduce the Local Logo Generative Adversarial Network\n(LL-GAN) that uses regional features extracted from the Faster Regional\nConvolutional Neural Network (Faster R-CNN) to generate logos. We demonstrate\nthe strength of this approach by training the framework on a small style-rich\ndataset collected online to generate large impressive logos. Our approach beats\nthe state-of-the-art models (StyleGAN2, Self-Attention GANs) that suffer from\nmode collapse due to the size of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ter_Sarkisov_A/0/1/0/all/0/1\">Aram Ter-Sarkisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_E/0/1/0/all/0/1\">Eduardo Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation. (arXiv:2109.12629v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12629","description":"<p>Recent studies have witnessed the effectiveness of 3D convolutions on\nsegmenting volumetric medical images. Compared with the 2D counterparts, 3D\nconvolutions can capture the spatial context in three dimensions. Nevertheless,\nmodels employing 3D convolutions introduce more trainable parameters and are\nmore computationally complex, which may lead easily to model overfitting\nespecially for medical applications with limited available training data. This\npaper aims to improve the effectiveness and efficiency of 3D convolutions by\nintroducing a novel Group Shift Pointwise Convolution (GSP-Conv). GSP-Conv\nsimplifies 3D convolutions into pointwise ones with 1x1x1 kernels, which\ndramatically reduces the number of model parameters and FLOPs (e.g. 27x fewer\nthan 3D convolutions with 3x3x3 kernels). Na\\\"ive pointwise convolutions with\nlimited receptive fields cannot make full use of the spatial image context. To\naddress this problem, we propose a parameter-free operation, Group Shift (GS),\nwhich shifts the feature maps along with different spatial directions in an\nelegant way. With GS, pointwise convolutions can access features from different\nspatial locations, and the limited receptive fields of pointwise convolutions\ncan be compensated. We evaluate the proposed methods on two datasets, PROMISE12\nand BraTS18. Results show that our method, with substantially decreased model\ncomplexity, achieves comparable or even better performance than models\nemploying 3D convolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_D/0/1/0/all/0/1\">Diping Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Wanli Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lixu Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Hybrid Convolutional Neural Network for Accurate Organ Segmentation in 3D Head and Neck CT Images. (arXiv:2109.12634v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12634","description":"<p>Radiation therapy (RT) is widely employed in the clinic for the treatment of\nhead and neck (HaN) cancers. An essential step of RT planning is the accurate\nsegmentation of various organs-at-risks (OARs) in HaN CT images. Nevertheless,\nsegmenting OARs manually is time-consuming, tedious, and error-prone\nconsidering that typical HaN CT images contain tens to hundreds of slices.\nAutomated segmentation algorithms are urgently required. Recently,\nconvolutional neural networks (CNNs) have been extensively investigated on this\ntask. Particularly, 3D CNNs are frequently adopted to process 3D HaN CT images.\nThere are two issues with na\\\"ive 3D CNNs. First, the depth resolution of 3D CT\nimages is usually several times lower than the in-plane resolution. Direct\nemployment of 3D CNNs without distinguishing this difference can lead to the\nextraction of distorted image features and influence the final segmentation\nperformance. Second, a severe class imbalance problem exists, and large organs\ncan be orders of times larger than small organs. It is difficult to\nsimultaneously achieve accurate segmentation for all the organs. To address\nthese issues, we propose a novel hybrid CNN that fuses 2D and 3D convolutions\nto combat the different spatial resolutions and extract effective edge and\nsemantic features from 3D HaN CT images. To accommodate large and small organs,\nour final model, named OrganNet2.5D, consists of only two instead of the\nclassic four downsampling operations, and hybrid dilated convolutions are\nintroduced to maintain the respective field. Experiments on the MICCAI 2015\nchallenge dataset demonstrate that OrganNet2.5D achieves promising performance\ncompared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zijie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_D/0/1/0/all/0/1\">Diping Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lixu Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nesterov Accelerated ADMM for Fast Diffeomorphic Image Registration. (arXiv:2109.12688v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12688","description":"<p>Deterministic approaches using iterative optimisation have been historically\nsuccessful in diffeomorphic image registration (DiffIR). Although these\napproaches are highly accurate, they typically carry a significant\ncomputational burden. Recent developments in stochastic approaches based on\ndeep learning have achieved sub-second runtimes for DiffIR with competitive\nregistration accuracy, offering a fast alternative to conventional iterative\nmethods. In this paper, we attempt to reduce this difference in speed whilst\nretaining the performance advantage of iterative approaches in DiffIR. We first\npropose a simple iterative scheme that functionally composes intermediate\nnon-stationary velocity fields to handle large deformations in images whilst\nguaranteeing diffeomorphisms in the resultant deformation. We then propose a\nconvex optimisation model that uses a regularisation term of arbitrary order to\nimpose smoothness on these velocity fields and solve this model with a fast\nalgorithm that combines Nesterov gradient descent and the alternating direction\nmethod of multipliers (ADMM). Finally, we leverage the computational power of\nGPU to implement this accelerated ADMM solver on a 3D cardiac MRI dataset,\nfurther reducing runtime to less than 2 seconds. In addition to producing\nstrictly diffeomorphic deformations, our methods outperform both\nstate-of-the-art deep learning-based and iterative DiffIR approaches in terms\nof dice and Hausdorff scores, with speed approaching the inference time of deep\nlearning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thorley_A/0/1/0/all/0/1\">Alexander Thorley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunting_K/0/1/0/all/0/1\">Karina Bunting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_V/0/1/0/all/0/1\">Victoria Stoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marvao_A/0/1/0/all/0/1\">Antonio de Marvao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1\">Declan P. O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkoutos_G/0/1/0/all/0/1\">Georgios Gkoutos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_D/0/1/0/all/0/1\">Dipak Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinming Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Multi-Process CTC Detection using Deep Learning. (arXiv:2109.12709v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12709","description":"<p>Circulating Tumor Cells (CTCs) bear great promise as biomarkers in tumor\nprognosis. However, the process of identification and later enumeration of CTCs\nrequire manual labor, which is error-prone and time-consuming. The recent\ndevelopments in object detection via Deep Learning using Mask-RCNNs and wider\navailability of pre-trained models have enabled sensitive tasks with limited\ndata of such to be tackled with unprecedented accuracy. In this report, we\npresent a novel 3-stage detection model for automated identification of\nCirculating Tumor Cells in multi-channel darkfield microscopic images comprised\nof: RetinaNet based identification of Cytokeratin (CK) stains, Mask-RCNN based\ncell detection of DAPI cell nuclei and Otsu thresholding to detect CD-45s. The\ntraining dataset is composed of 46 high variance data points, with 10 Negative\nand 36 Positive data points. The test set is composed of 420 negative data\npoints. The final accuracy of the pipeline is 98.81%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_E/0/1/0/all/0/1\">Elena Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_K/0/1/0/all/0/1\">Kam W. Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_A/0/1/0/all/0/1\">Andrew F. Laine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster Analysis with Deep Embeddings and Contrastive Learning. (arXiv:2109.12714v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12714","description":"<p>Unsupervised disentangled representation learning is a long-standing problem\nin computer vision. This work proposes a novel framework for performing image\nclustering from deep embeddings by combining instance-level contrastive\nlearning with a deep embedding based cluster center predictor. Our approach\njointly learns representations and predicts cluster centers in an end-to-end\nmanner. This is accomplished via a three-pronged approach that combines a\nclustering loss, an instance-wise contrastive loss, and an anchor loss. Our\nfundamental intuition is that using an ensemble loss that incorporates\ninstance-level features and a clustering procedure focusing on semantic\nsimilarity reinforces learning better representations in the latent space. We\nobserve that our method performs exceptionally well on popular vision datasets\nwhen evaluated using standard clustering metrics such as Normalized Mutual\nInformation (NMI), in addition to producing geometrically well-separated\ncluster embeddings as defined by the Euclidean distance. Our framework performs\non par with widely accepted clustering methods and outperforms the\nstate-of-the-art contrastive learning method on the CIFAR-10 dataset with an\nNMI score of 0.772, a 7-8% improvement on the strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundareswaran_R/0/1/0/all/0/1\">Ramakrishnan Sundareswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_Gerena_J/0/1/0/all/0/1\">Jansel Herrera-Gerena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Just_J/0/1/0/all/0/1\">John Just</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janessari_A/0/1/0/all/0/1\">Ali Janessari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Markerless Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for Autonomous Minimally Invasive Robotic Surgery. (arXiv:2109.12722v1 [cs.RO])","link":"http://arxiv.org/abs/2109.12722","description":"<p>Suture needle localization plays a crucial role towards autonomous suturing.\nTo track the 6D pose of a suture needle robustly, previous approaches usually\nadd markers on the needle or perform complex operations for feature extraction,\nmaking these methods difficult to be applicable to real-world environments.\nTherefore in this work, we present a novel approach for markerless suture\nneedle pose tracking using Bayesian filters. A data-efficient feature point\ndetector is trained to extract the feature points on the needle. Then based on\nthese detections, we propose a novel observation model that measures the\noverlap between the detections and the expected projection of the needle, which\ncan be calculated efficiently. In addition, for the proposed method, we derive\nthe approximation for the covariance of the observation noise, making this\nmodel more robust to the uncertainty in the detections. The experimental\nresults in simulation show that the proposed observation model achieves low\ntracking errors of approximately 1.5mm in position in space and 1 degree in\norientation. We also demonstrate the qualitative results of our trained\nmarkerless feature detector combined with the proposed observation model in\nreal-world environments. The results show high consistency between the\nprojection of the tracked pose and that of the real pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1\">Zih-Yun Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_A/0/1/0/all/0/1\">Albert Z Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1\">Florian Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_B/0/1/0/all/0/1\">Bjorn Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael C. Yip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research on facial expression recognition based on Multimodal data fusion and neural network. (arXiv:2109.12724v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12724","description":"<p>Facial expression recognition is a challenging task when neural network is\napplied to pattern recognition. Most of the current recognition research is\nbased on single source facial data, which generally has the disadvantages of\nlow accuracy and low robustness. In this paper, a neural network algorithm of\nfacial expression recognition based on multimodal data fusion is proposed. The\nalgorithm is based on the multimodal data, and it takes the facial image, the\nhistogram of oriented gradient of the image and the facial landmarks as the\ninput, and establishes CNN, LNN and HNN three sub neural networks to extract\ndata features, using multimodal data feature fusion mechanism to improve the\naccuracy of facial expression recognition. Experimental results show that,\nbenefiting by the complementarity of multimodal data, the algorithm has a great\nimprovement in accuracy, robustness and detection speed compared with the\ntraditional facial expression recognition algorithm. Especially in the case of\npartial occlusion, illumination and head posture transformation, the algorithm\nalso shows a high confidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xubin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengyu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel network training approach for open set image recognition. (arXiv:2109.12756v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12756","description":"<p>Convolutional Neural Networks (CNNs) are commonly designed for closed set\narrangements, where test instances only belong to some \"Known Known\" (KK)\nclasses used in training. As such, they predict a class label for a test sample\nbased on the distribution of the KK classes. However, when used under the Open\nSet Recognition (OSR) setup (where an input may belong to an \"Unknown Unknown\"\nor UU class), such a network will always classify a test instance as one of the\nKK classes even if it is from a UU class. As a solution, recently, data\naugmentation based on Generative Adversarial Networks(GAN) has been used. In\nthis work, we propose a novel approach for mining a \"Known UnknownTrainer\" or\nKUT set and design a deep OSR Network (OSRNet) to harness this dataset. The\ngoal isto teach OSRNet the essence of the UUs through KUT set, which is\neffectively a collection of mined \"hard Known Unknown negatives\". Once trained,\nOSRNet can detect the UUs while maintaining high classification accuracy on\nKKs. We evaluate OSRNet on six benchmark datasets and demonstrate it\noutperforms contemporary OSR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossaina_M/0/1/0/all/0/1\">Md Tahmid Hossaina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Shyh Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guojun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Thermal Infrared Monitoring of Volcanoes: A Deep Learning Approach for Intermittent Image Series. (arXiv:2109.12767v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12767","description":"<p>Active volcanoes are globally distributed and pose societal risks at multiple\ngeographic scales, ranging from local hazards to regional/international\ndisruptions. Many volcanoes do not have continuous ground monitoring networks;\nmeaning that satellite observations provide the only record of volcanic\nbehavior and unrest. Among these remote sensing observations, thermal imagery\nis inspected daily by volcanic observatories for examining the early signs,\nonset, and evolution of eruptive activity. However, thermal scenes are often\nobstructed by clouds, meaning that forecasts must be made off image sequences\nwhose scenes are only usable intermittently through time. Here, we explore\nforecasting this thermal data stream from a deep learning perspective using\nexisting architectures that model sequences with varying spatiotemporal\nconsiderations. Additionally, we propose and evaluate new architectures that\nexplicitly model intermittent image sequences. Using ASTER Kinetic Surface\nTemperature data for $9$ volcanoes between $1999$ and $2020$, we found that a\nproposed architecture (ConvLSTM + Time-LSTM + U-Net) forecasts volcanic\ntemperature imagery with the lowest RMSE ($4.164^{\\circ}$C, other methods:\n$4.217-5.291^{\\circ}$C). Additionally, we examined performance on multiple time\nseries derived from the thermal imagery and the effect of training with data\nfrom singular volcanoes. Ultimately, we found that models with the lowest RMSE\non forecasting imagery did not possess the lowest RMSE on recreating time\nseries derived from that imagery and that training with individual volcanoes\ngenerally worsened performance relative to a multi-volcano data set. This work\nhighlights the potential of data-driven deep learning models for volcanic\nunrest forecasting while revealing the need for carefully constructed\noptimization targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jeremy Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_G/0/1/0/all/0/1\">Guido Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wauthier_C/0/1/0/all/0/1\">Christelle Wauthier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multimedia Event Extraction from Video and Article. (arXiv:2109.12776v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12776","description":"<p>Visual and textual modalities contribute complementary information about\nevents described in multimedia documents. Videos contain rich dynamics and\ndetailed unfoldings of events, while text describes more high-level and\nabstract concepts. However, existing event extraction methods either do not\nhandle video or solely target video while ignoring other modalities. In\ncontrast, we propose the first approach to jointly extract events from video\nand text articles. We introduce the new task of Video MultiMedia Event\nExtraction (Video M2E2) and propose two novel components to build the first\nsystem towards this task. First, we propose the first self-supervised\nmultimodal event coreference model that can determine coreference between video\nevents and text events without any manually annotated pairs. Second, we\nintroduce the first multimodal transformer which extracts structured event\ninformation jointly from both videos and text documents. We also construct and\nwill publicly release a new benchmark of video-article pairs, consisting of 860\nvideo-article pairs with extensive annotations for evaluating methods on this\ntask. Our experimental results demonstrate the effectiveness of our proposed\nmethod on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score\ngain on multimodal event coreference resolution and multimedia event\nextraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_S/0/1/0/all/0/1\">Shoya Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1\">Lovish Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Multiple CNNs for Triaging Medical Workflow. (arXiv:2109.12783v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12783","description":"<p>High hospitalization rates due to the global spread of Covid-19 bring about a\nneed for improvements to classical triaging workflows. To this end,\nconvolutional neural networks (CNNs) can effectively differentiate critical\nfrom non-critical images so that critical cases may be addressed quickly, so\nlong as there exists some representative image for the illness. Presented is a\nconglomerate neural network system consisting of multiple VGG16 CNNs; the\nsystem trains on weighted skin disease images re-labelled as critical or\nnon-critical, to then attach to input images a critical index between 0 and 10.\nA critical index offers a more comprehensive rating system compared to binary\ncritical/non-critical labels. Results for batches of input images run through\nthe trained network are promising. A batch is shown being re-ordered by the\nproposed architecture from most critical to least critical roughly accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghantasala_L/0/1/0/all/0/1\">Lakshmi A. Ghantasala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Frame Rate Video Quality Assessment using VMAF and Entropic Differences. (arXiv:2109.12785v1 [cs.MM])","link":"http://arxiv.org/abs/2109.12785","description":"<p>The popularity of streaming videos with live, high-action content has led to\nan increased interest in High Frame Rate (HFR) videos. In this work we address\nthe problem of frame rate dependent Video Quality Assessment (VQA) when the\nvideos to be compared have different frame rate and compression factor. The\ncurrent VQA models such as VMAF have superior correlation with perceptual\njudgments when videos to be compared have same frame rates and contain\nconventional distortions such as compression, scaling etc. However this\nframework requires additional pre-processing step when videos with different\nframe rates need to be compared, which can potentially limit its overall\nperformance. Recently, Generalized Entropic Difference (GREED) VQA model was\nproposed to account for artifacts that arise due to changes in frame rate, and\nshowed superior performance on the LIVE-YT-HFR database which contains frame\nrate dependent artifacts such as judder, strobing etc. In this paper we propose\na simple extension, where the features from VMAF and GREED are fused in order\nto exploit the advantages of both models. We show through various experiments\nthat the proposed fusion framework results in more efficient features for\npredicting frame rate dependent video quality. We also evaluate the fused\nfeature set on standard non-HFR VQA databases and obtain superior performance\nthan both GREED and VMAF, indicating the combined feature set captures\ncomplimentary perceptual quality information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C Madhusudana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12800","description":"<p>Deep generative networks in recent years have reinforced the need for caution\nwhile consuming various modalities of digital information. One avenue of\ndeepfake creation is aligned with injection and removal of tumors from medical\nscans. Failure to detect medical deepfakes can lead to large setbacks on\nhospital resources or even loss of life. This paper attempts to address the\ndetection of such attacks with a structured case study. We evaluate different\nmachine learning algorithms and pretrained convolutional neural networks on\ndistinguishing between tampered and untampered data. The findings of this work\nshow near perfect accuracy in detecting instances of tumor injections and\nremovals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solaiyappan_S/0/1/0/all/0/1\">Siddharth Solaiyappan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect Of Personalized Calibration On Gaze Estimation Using Deep-Learning. (arXiv:2109.12801v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12801","description":"<p>With the increase in computation power and the development of new\nstate-of-the-art deep learning algorithms, appearance-based gaze estimation is\nbecoming more and more popular. It is believed to work well with curated\nlaboratory data sets, however it faces several challenges when deployed in real\nworld scenario. One such challenge is to estimate the gaze of a person about\nwhich the Deep Learning model trained for gaze estimation has no knowledge\nabout. To analyse the performance in such scenarios we have tried to simulate a\ncalibration mechanism. In this work we use the MPIIGaze data set. We trained a\nmulti modal convolutional neural network and analysed its performance with and\nwithout calibration and this evaluation provides clear insights on how\ncalibration improved the performance of the Deep Learning model in estimating\ngaze in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_N/0/1/0/all/0/1\">Nairit Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riou_S/0/1/0/all/0/1\">S&#xe9;bastien Riou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-shot Palm Vein Verification Using Siamese Networks. (arXiv:2109.12808v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12808","description":"<p>The use of deep learning methods to extract vascular biometric patterns from\nthe palm surface has been of interest among researchers in recent years. In\nmany biometric recognition tasks, there is a limit in the number of training\nsamples. This is because of limited vein biometric databases being available\nfor research. This restricts the application of deep learning methods to design\nalgorithms that can effectively identify or authenticate people for vein\nrecognition. This paper proposes an architecture using Siamese neural network\nstructure for few shot palm vein identification. The proposed network uses\nimages from both the palms and consists of two sub-nets that share weights to\nidentify a person. The architecture performance was tested on the HK PolyU\nmulti spectral palm vein database with limited samples. The results suggest\nthat the method is effective since it has 91.9% precision, 91.1% recall, 92.2%\nspecificity, 91.5%, F1-Score, and 90.5% accuracy values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marattukalam_F/0/1/0/all/0/1\">Felix Marattukalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulla_W/0/1/0/all/0/1\">Waleed H. Abdulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swain_A/0/1/0/all/0/1\">Akshya Swain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v1 [cs.NE])","link":"http://arxiv.org/abs/2109.12813","description":"<p>We present an end-to-end trainable modular event-driven neural architecture\nthat uses local synaptic and threshold adaptation rules to perform\ntransformations between arbitrary spatio-temporal spike patterns. The\narchitecture represents a highly abstracted model of existing Spiking Neural\nNetwork (SNN) architectures. The proposed Optimized Deep Event-driven Spiking\nneural network Architecture (ODESA) can simultaneously learn hierarchical\nspatio-temporal features at multiple arbitrary time scales. ODESA performs\nonline learning without the use of error back-propagation or the calculation of\ngradients. Through the use of simple local adaptive selection thresholds at\neach node, the network rapidly learns to appropriately allocate its neuronal\nresources at each layer for any given problem without using a real-valued error\nmeasure. These adaptive selection thresholds are the central feature of ODESA,\nensuring network stability and remarkable robustness to noise as well as to the\nselection of initial system parameters. Network activations are inherently\nsparse due to a hard Winner-Take-All (WTA) constraint at each layer. We\nevaluate the architecture on existing spatio-temporal datasets, including the\nspike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based\non International Morse Code that we created. These tests demonstrate the\nhierarchical spatio-temporal learning capabilities of ODESA. Through these\ntests, we demonstrate ODESA can optimally solve practical and highly\nchallenging hierarchical spatio-temporal learning tasks with the minimum\npossible number of computing nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bethi_Y/0/1/0/all/0/1\">Yeshwanth Bethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gregory Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaik_A/0/1/0/all/0/1\">Andre van Schaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1\">Saeed Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining. (arXiv:1912.05759v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.05759","description":"<p>A DNN architecture referred to as GPRInvNet was proposed to tackle the\nchallenges of mapping the ground-penetrating radar (GPR) B-Scan data to complex\npermittivity maps of subsurface structures. The GPRInvNet consisted of a\ntrace-to-trace encoder and a decoder. It was specially designed to take into\naccount the characteristics of GPR inversion when faced with complex GPR B-Scan\ndata, as well as addressing the spatial alignment issues between time-series\nB-Scan data and spatial permittivity maps. It displayed the ability to fuse\nfeatures from several adjacent traces on the B-Scan data to enhance each trace,\nand then further condense the features of each trace separately. As a result,\nthe sensitive zones on the permittivity maps spatially aligned to the enhanced\ntrace could be reconstructed accurately. The GPRInvNet has been utilized to\nreconstruct the permittivity map of tunnel linings. A diverse range of\ndielectric models of tunnel linings containing complex defects has been\nreconstructed using GPRInvNet. The results have demonstrated that the GPRInvNet\nis capable of effectively reconstructing complex tunnel lining defects with\nclear boundaries. Comparative results with existing baseline methods also\ndemonstrated the superiority of the GPRInvNet. For the purpose of generalizing\nthe GPRInvNet to real GPR data, some background noise patches recorded from\npractical model testing were integrated into the synthetic GPR data to retrain\nthe GPRInvNet. The model testing has been conducted for validation, and\nexperimental results revealed that the GPRInvNet had also achieved satisfactory\nresults with regard to the real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxiao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengfang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1\">Anthony G. Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.00436","description":"<p>Several approaches have been proposed in recent literature to alleviate the\nlong-tail problem, mainly in object classification tasks. In this paper, we\nmake the first large-scale study concerning the task of Long-Tail Visual\nRelationship Recognition (LTVRR). LTVRR aims at improving the learning of\nstructured visual relationships that come from the long-tail (e.g., \"rabbit\ngrazing on grass\"). In this setup, the subject, relation, and object classes\neach follow a long-tail distribution. To begin our study and make a future\nbenchmark for the community, we introduce two LTVRR-related benchmarks, dubbed\nVG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.\nWe use these benchmarks to study the performance of several state-of-the-art\nlong-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic\nhubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR\nsetup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top\nof existing models and despite being simple, our results show that they can\nremarkably improve the performance, especially on tail classes. Benchmarks,\ncode, and models have been made available at:\nhttps://github.com/Vision-CAIR/LTVRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelkarim_S/0/1/0/all/0/1\">Sherif Abdelkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aniket Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Convolutional Online Tracking. (arXiv:2004.07109v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.07109","description":"<p>Online learning has turned out to be effective for improving tracking\nperformance. However, it could be simply applied for classification branch, but\nstill remains challenging to adapt to regression branch due to its complex\ndesign and intrinsic requirement for high-quality online samples. To tackle\nthis issue, we present the fully convolutional online tracking framework,\ncoined as FCOT, and focus on enabling online learning for both classification\nand regression branches by using a target filter based tracking paradigm. Our\nkey contribution is to introduce an online regression model generator (RMG) for\ninitializing weights of the target filter with online samples and then\noptimizing this target filter weights based on the groundtruth samples at the\nfirst frame. Based on the online RGM, we devise a simple anchor-free tracker\n(FCOT), composed of a feature backbone, an up-sampling decoder, a multi-scale\nclassification branch, and a multi-scale regression branch. Thanks to the\nunique design of RMG, our FCOT can not only be more effective in handling\ntarget variation along temporal dimension thus generating more precise results,\nbut also overcome the issue of error accumulation during the tracking\nprocedure. In addition, due to its simplicity in design, our FCOT could be\ntrained and deployed in a fully convolutional manner with a real-time running\nspeed. The proposed FCOT achieves the state-of-the-art performance on seven\nbenchmarks, including VOT2018, LaSOT, TrackingNet, GOT-10k, OTB100, UAV123, and\nNFS. Code and models of our FCOT have been released at:\n\\url{https://github.com/MCG-NJU/FCOT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yutao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multi-Dimension Pruning via Numerical Gradient Update. (arXiv:2005.08931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.08931","description":"<p>We present joint multi-dimension pruning (abbreviated as JointPruning), an\neffective method of pruning a network on three crucial aspects: spatial, depth\nand channel simultaneously. To tackle these three naturally different\ndimensions, we proposed a general framework by defining pruning as seeking the\nbest pruning vector (i.e., the numerical value of layer-wise channel number,\nspacial size, depth) and construct a unique mapping from the pruning vector to\nthe pruned network structures. Then we optimize the pruning vector with\ngradient update and model joint pruning as a numerical gradient optimization\nprocess. To overcome the challenge that there is no explicit function between\nthe loss and the pruning vectors, we proposed self-adapted stochastic gradient\nestimation to construct a gradient path through network loss to pruning vectors\nand enable efficient gradient update. We show that the joint strategy discovers\na better status than previous studies that focused on individual dimensions\nsolely, as our method is optimized collaboratively across the three dimensions\nin a single end-to-end training and it is more efficient than the previous\nexhaustive methods. Extensive experiments on large-scale ImageNet dataset\nacross a variety of network architectures MobileNet V1&amp;V2&amp;V3 and ResNet\ndemonstrate the effectiveness of our proposed method. For instance, we achieve\nsignificant margins of 2.5% and 2.6% improvement over the state-of-the-art\napproach on the already compact MobileNet V1&amp;V2 under an extremely large\ncompression ratio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yichen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06666","description":"<p>The de-facto approach to many vision tasks is to start from pretrained visual\nrepresentations, typically learned via supervised training on ImageNet. Recent\nmethods have explored unsupervised pretraining to scale to vast quantities of\nunlabeled images. In contrast, we aim to learn high-quality visual\nrepresentations from fewer images. To this end, we revisit supervised\npretraining, and seek data-efficient alternatives to classification-based\npretraining. We propose VirTex -- a pretraining approach using semantically\ndense captions to learn visual representations. We train convolutional networks\nfrom scratch on COCO Captions, and transfer them to downstream recognition\ntasks including image classification, object detection, and instance\nsegmentation. On all tasks, VirTex yields features that match or exceed those\nlearned on ImageNet -- supervised or unsupervised -- despite using up to ten\ntimes fewer images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Karan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subjective and Objective Quality Assessment of High Frame Rate Videos. (arXiv:2007.11634v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2007.11634","description":"<p>High frame rate (HFR) videos are becoming increasingly common with the\ntremendous popularity of live, high-action streaming content such as sports.\nAlthough HFR contents are generally of very high quality, high bandwidth\nrequirements make them challenging to deliver efficiently, while simultaneously\nmaintaining their quality. To optimize trade-offs between bandwidth\nrequirements and video quality, in terms of frame rate adaptation, it is\nimperative to understand the intricate relationship between frame rate and\nperceptual video quality. Towards advancing progression in this direction we\ndesigned a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)\ndataset, which is comprised of 480 videos having 6 different frame rates,\nobtained from 16 diverse contents. In order to understand the combined effects\nof compression and frame rate adjustment, we also processed videos at 5\ncompression levels at each frame rate. To obtain subjective labels on the\nvideos, we conducted a human study yielding 19,000 human quality ratings\nobtained from a pool of 85 human subjects. We also conducted a holistic\nevaluation of existing state-of-the-art Full and No-Reference video quality\nalgorithms, and statistically benchmarked their performance on the new\ndatabase. The LIVE-YT-HFR database has been made available online for public\nuse and evaluation purposes, with hopes that it will help advance research in\nthis exciting video technology direction. It may be obtained at\n\\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangxu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene Datasets. (arXiv:2007.12868v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.12868","description":"<p>We propose a novel framework for creating large-scale photorealistic datasets\nof indoor scenes, with ground truth geometry, material, lighting and semantics.\nOur goal is to make the dataset creation process widely accessible,\ntransforming scans into photorealistic datasets with high-quality ground truth\nfor appearance, layout, semantic labels, high quality spatially-varying BRDF\nand complex lighting, including direct, indirect and visibility components.\nThis enables important applications in inverse rendering, scene understanding\nand robotics. We show that deep networks trained on the proposed dataset\nachieve competitive performance for shape, material and lighting estimation on\nreal images, enabling photorealistic augmented reality applications, such as\nobject insertion and material editing. We also show our semantic labels may be\nused for segmentation and multi-task learning. Finally, we demonstrate that our\nframework may also be integrated with physics engines, to create virtual\nrobotics environments with unique ground truth such as friction coefficients\nand correspondence to real scenes. The dataset and all the tools to create such\ndatasets will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Ting-Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_S/0/1/0/all/0/1\">Shen Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sarah Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Meng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yu-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundavarapu_N/0/1/0/all/0/1\">Nitesh Gundavarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonnegative Low Rank Tensor Approximation and its Application to Multi-dimensional Images. (arXiv:2007.14137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14137","description":"<p>The main aim of this paper is to develop a new algorithm for computing\nnonnegative low rank tensor approximation for nonnegative tensors that arise in\nmany multi-dimensional imaging applications. Nonnegativity is one of the\nimportant property as each pixel value refers to nonzero light intensity in\nimage data acquisition. Our approach is different from classical nonnegative\ntensor factorization (NTF) which requires each factorized matrix and/or tensor\nto be nonnegative. In this paper, we determine a nonnegative low Tucker rank\ntensor to approximate a given nonnegative tensor. We propose an alternating\nprojections algorithm for computing such nonnegative low rank tensor\napproximation, which is referred to as NLRT. The convergence of the proposed\nmanifold projection method is established. Experimental results for synthetic\ndata and multi-dimensional images are presented to demonstrate the performance\nof NLRT is better than state-of-the-art NTF methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tai-Xiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junjun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guangjing Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmark for Anonymous Video Analytics. (arXiv:2009.14684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.14684","description":"<p>Out-of-home audience measurement aims to count and characterize the people\nexposed to advertising content in the physical world. While audience\nmeasurement solutions based on computer vision are of increasing interest, no\ncommonly accepted benchmark exists to evaluate and compare their performance.\nIn this paper, we propose the first benchmark for digital out-of-home audience\nmeasurement that evaluates the vision-based tasks of audience localization and\ncounting, and audience demographics. The benchmark is composed of a novel,\ndataset captured at multiple locations and a set of performance measures. Using\nthe benchmark, we present an in-depth comparison of eight open-source\nalgorithms on four hardware platforms with GPU and CPU-optimized inferences and\nof two commercial off-the-shelf solutions for localization, count, age, and\ngender estimation. This benchmark and related open-source codes are available\nat <a href=\"http://ava.eecs.qmul.ac.uk.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1\">Ricardo Sanchez-Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search. (arXiv:2010.04354v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.04354","description":"<p>Quantization Neural Networks (QNN) have attracted a lot of attention due to\ntheir high efficiency. To enhance the quantization accuracy, prior works mainly\nfocus on designing advanced quantization algorithms but still fail to achieve\nsatisfactory results under the extremely low-bit case. In this work, we take an\narchitecture perspective to investigate the potential of high-performance QNN.\nTherefore, we propose to combine Network Architecture Search methods with\nquantization to enjoy the merits of the two sides. However, a naive combination\ninevitably faces unacceptable time consumption or unstable training problem. To\nalleviate these problems, we first propose the joint training of architecture\nand quantization with a shared step size to acquire a large number of quantized\nmodels. Then a bit-inheritance scheme is introduced to transfer the quantized\nmodels to the lower bit, which further reduces the time cost and meanwhile\nimproves the quantization accuracy. Equipped with this overall framework,\ndubbed as Once Quantization-Aware Training~(OQAT), our searched model family,\nOQATNets, achieves a new state-of-the-art compared with various architectures\nunder different bit-widths. In particular, OQAT-2bit-M achieves 61.6% ImageNet\nTop-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin\nof 9% with 10% less computation cost. A series of quantization-friendly\narchitectures are identified easily and extensive analysis can be made to\nsummarize the interaction between quantization and neural architectures. Codes\nand models are released at https://github.com/LaVieEnRoseSMZ/OQA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts. (arXiv:2010.07047v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.07047","description":"<p>Diffusion tensor imaging (DTI) has been used to study the effects of\nneurodegenerative diseases on neural pathways, which may lead to more reliable\nand early diagnosis of these diseases as well as a better understanding of how\nthey affect the brain. We introduce an intelligent visual analytics system for\nstudying patient groups based on their labeled DTI fiber tract data and\ncorresponding statistics. The system's AI-augmented interface guides the user\nthrough an organized and holistic analysis space, including the statistical\nfeature space, the physical space, and the space of patients over different\ngroups. We use a custom machine learning pipeline to help narrow down this\nlarge analysis space, and then explore it pragmatically through a range of\nlinked visualizations. We conduct several case studies using real data from the\nresearch database of Parkinson's Progression Markers Initiative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuroth_T/0/1/0/all/0/1\">Tyson Neuroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction. (arXiv:2010.13715v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2010.13715","description":"<p>We consider the problem of conducting frame rate dependent video quality\nassessment (VQA) on videos of diverse frame rates, including high frame rate\n(HFR) videos. More generally, we study how perceptual quality is affected by\nframe rate, and how frame rate and compression combine to affect perceived\nquality. We devise an objective VQA model called Space-Time GeneRalized\nEntropic Difference (GREED) which analyzes the statistics of spatial and\ntemporal band-pass video coefficients. A generalized Gaussian distribution\n(GGD) is used to model band-pass responses, while entropy variations between\nreference and distorted videos under the GGD model are used to capture video\nquality variations arising from frame rate changes. The entropic differences\nare calculated across multiple temporal and spatial subbands, and merged using\na learned regressor. We show through extensive experiments that GREED achieves\nstate-of-the-art performance on the LIVE-YT-HFR Database when compared with\nexisting VQA models. The features used in GREED are highly generalizable and\nobtain competitive performance even on standard, non-HFR VQA databases. The\nimplementation of GREED has been made available online:\nhttps://github.com/pavancm/GREED\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v10 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14759","description":"<p>This paper presents a hybrid approach between scale-space theory and deep\nlearning, where a deep learning architecture is constructed by coupling\nparameterized scale-space operations in cascade. By sharing the learnt\nparameters between multiple scale channels, and by using the transformation\nproperties of the scale-space primitives under scaling transformations, the\nresulting network becomes provably scale covariant. By in addition performing\nmax pooling over the multiple scale channels, a resulting network architecture\nfor image classification also becomes provably scale invariant. We investigate\nthe performance of such networks on the MNISTLargeScale dataset, which contains\nrescaled images from original MNIST over a factor of 4 concerning training data\nand over a factor of 16 concerning testing data. It is demonstrated that the\nresulting approach allows for scale generalization, enabling good performance\nfor classifying patterns at scales not present in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1\">Tony Lindeberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Transformer. (arXiv:2012.09164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09164","description":"<p>Self-attention networks have revolutionized natural language processing and\nare making impressive strides in image analysis tasks such as image\nclassification and object detection. Inspired by this success, we investigate\nthe application of self-attention networks to 3D point cloud processing. We\ndesign self-attention layers for point clouds and use these to construct\nself-attention networks for tasks such as semantic scene segmentation, object\npart segmentation, and object classification. Our Point Transformer design\nimproves upon prior work across domains and tasks. For example, on the\nchallenging S3DIS dataset for large-scale semantic scene segmentation, the\nPoint Transformer attains an mIoU of 70.4% on Area 5, outperforming the\nstrongest prior model by 3.3 absolute percentage points and crossing the 70%\nmIoU threshold for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers. (arXiv:2012.12102v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.12102","description":"<p>Tumor shape is a key factor that affects tumor growth and metastasis. This\npaper proposes a topological feature computed by persistent homology to\ncharacterize tumor progression from digital pathology and radiology images and\nexamines its effect on the time-to-event data. The proposed topological\nfeatures are invariant to scale-preserving transformation and can summarize\nvarious tumor shape patterns. The topological features are represented in\nfunctional space and used as functional predictors in a functional Cox\nproportional hazards model. The proposed model enables interpretable inference\nabout the association between topological shape features and survival risks.\nTwo case studies are conducted using consecutive 143 lung cancer and 77 brain\ntumor patients. The results of both studies show that the topological features\npredict survival prognosis after adjusting clinical variables, and the\npredicted high-risk groups have significantly (at the level of 0.001) worse\nsurvival outcomes than the low-risk groups. Also, the topological shape\nfeatures found to be positively associated with survival hazards are irregular\nand heterogeneous shape patterns, which are known to be related to tumor\nprogression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_C/0/1/0/all/0/1\">Chul Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Memory Attention for Fast Video Semantic Segmentation. (arXiv:2101.01715v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01715","description":"<p>We propose a novel neural network module that transforms an existing\nsingle-frame semantic segmentation model into a video semantic segmentation\npipeline. In contrast to prior works, we strive towards a simple, fast, and\ngeneral module that can be integrated into virtually any single-frame\narchitecture. Our approach aggregates a rich representation of the semantic\ninformation in past frames into a memory module. Information stored in the\nmemory is then accessed through an attention mechanism. In contrast to previous\nmemory-based approaches, we propose a fast local attention layer, providing\ntemporal appearance cues in the local region of prior frames. We further fuse\nthese cues with an encoding of the current frame through a second\nattention-based module. The segmentation decoder processes the fused\nrepresentation to predict the final semantic segmentation. We integrate our\napproach into two popular semantic segmentation networks: ERFNet and PSPNet. We\nobserve an improvement in segmentation performance on Cityscapes by 1.7% and\n2.1% in mIoU respectively, while increasing inference time of ERFNet by only\n1.5ms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Matthieu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Summarization Using Deep Neural Networks: A Survey. (arXiv:2101.06072v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06072","description":"<p>Video summarization technologies aim to create a concise and complete\nsynopsis by selecting the most informative parts of the video content. Several\napproaches have been developed over the last couple of decades and the current\nstate of the art is represented by methods that rely on modern deep neural\nnetwork architectures. This work focuses on the recent advances in the area and\nprovides a comprehensive survey of the existing deep-learning-based methods for\ngeneric video summarization. After presenting the motivation behind the\ndevelopment of technologies for video summarization, we formulate the video\nsummarization task and discuss the main characteristics of a typical\ndeep-learning-based analysis pipeline. Then, we suggest a taxonomy of the\nexisting algorithms and provide a systematic review of the relevant literature\nthat shows the evolution of the deep-learning-based video summarization\ntechnologies and leads to suggestions for future developments. We then report\non protocols for the objective evaluation of video summarization algorithms and\nwe compare the performance of several deep-learning-based approaches. Based on\nthe outcomes of these comparisons, as well as some documented considerations\nabout the amount of annotated data and the suitability of evaluation protocols,\nwe indicate potential future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apostolidis_E/0/1/0/all/0/1\">Evlampios Apostolidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamantidou_E/0/1/0/all/0/1\">Eleni Adamantidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metsai_A/0/1/0/all/0/1\">Alexandros I. Metsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1\">Vasileios Mezaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Variable Models for Visual Question Answering. (arXiv:2101.06399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06399","description":"<p>Current work on Visual Question Answering (VQA) explore deterministic\napproaches conditioned on various types of image and question features. We\nposit that, in addition to image and question pairs, other modalities are\nuseful for teaching machine to carry out question answering. Hence in this\npaper, we propose latent variable models for VQA where extra information (e.g.\ncaptions and answer categories) are incorporated as latent variables, which are\nobserved during training but in turn benefit question-answering performance at\ntest time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the\neffectiveness of our proposed models: they improve over strong baselines,\nespecially those that do not rely on extensive language-vision pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.05228","description":"<p>Increasing demands for understanding the internal behavior of convolutional\nneural networks (CNNs) have led to remarkable improvements in explanation\nmethods. Particularly, several class activation mapping (CAM) based methods,\nwhich generate visual explanation maps by a linear combination of activation\nmaps from CNNs, have been proposed. However, the majority of the methods lack a\nclear theoretical basis on how they assign the coefficients of the linear\ncombination. In this paper, we revisit the intrinsic linearity of CAM with\nrespect to the activation maps; we construct an explanation model of CNN as a\nlinear function of binary variables that denote the existence of the\ncorresponding activation maps. With this approach, the explanation model can be\ndetermined by additive feature attribution methods in an analytic manner. We\nthen demonstrate the adequacy of SHAP values, which is a unique solution for\nthe explanation model with a set of desirable properties, as the coefficients\nof CAM. Since the exact SHAP values are unattainable, we introduce an efficient\napproximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can\nestimate the SHAP values of the activation maps with high speed and accuracy.\nFurthermore, it greatly outperforms other previous CAM-based methods in both\nqualitative and quantitative aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyungsik Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngrock Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.05509","description":"<p>Model compression techniques allow to significantly reduce the computational\ncost associated with data processing by deep neural networks with only a minor\ndecrease in average accuracy. Simultaneously, reducing the model size may have\na large effect on noisy cases or objects belonging to less frequent classes. It\nis a crucial problem from the perspective of the models' safety, especially for\nobject detection in the autonomous driving setting, which is considered in this\nwork. It was shown in the paper that the sensitivity of compressed models to\ndifferent distortion types is nuanced, and some of the corruptions are heavily\nimpacted by the compression methods (i.e., additive noise), while others (blur\neffect) are only slightly affected. A common way to improve the robustness of\nmodels is to use data augmentation, which was confirmed to positively affect\nmodels' robustness, also for highly compressed models. It was further shown\nthat while data imbalance methods brought only a slight increase in accuracy\nfor the baseline model (without compression), the impact was more striking at\nhigher compression rates for the structured pruning. Finally, methods for\nhandling data imbalance brought a significant improvement of the pruned models'\nworst-detected class accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1\">Sebastian Cygert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1\">Andrzej Czy&#x17c;ewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReRankMatch: Semi-Supervised Learning with Semantics-Oriented Similarity Representation. (arXiv:2102.06328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06328","description":"<p>This paper proposes integrating semantics-oriented similarity representation\ninto RankingMatch, a recently proposed semi-supervised learning method. Our\nmethod, dubbed ReRankMatch, aims to deal with the case in which labeled and\nunlabeled data share non-overlapping categories. ReRankMatch encourages the\nmodel to produce the similar image representations for the samples likely\nbelonging to the same class. We evaluate our method on various datasets such as\nCIFAR-10, CIFAR-100, SVHN, STL-10, and Tiny ImageNet. We obtain promising\nresults (4.21% error rate on CIFAR-10 with 4000 labels, 22.32% error rate on\nCIFAR-100 with 10000 labels, and 2.19% error rate on SVHN with 1000 labels)\nwhen the amount of labeled data is sufficient to learn semantics-oriented\nsimilarity representation. The code is made publicly available at\nhttps://github.com/tqtrunghnvn/ReRankMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung Quang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mingu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariant Filters for Efficient Tracking in 3D Imaging. (arXiv:2103.10255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10255","description":"<p>We demonstrate an object tracking method for 3D images with fixed\ncomputational cost and state-of-the-art performance. Previous methods predicted\ntransformation parameters from convolutional layers. We instead propose an\narchitecture that does not include either flattening of convolutional features\nor fully connected layers, but instead relies on equivariant filters to\npreserve transformations between inputs and outputs (e.g. rot./trans. of inputs\nrotate/translate outputs). The transformation is then derived in closed form\nfrom the outputs of the filters. This method is useful for applications\nrequiring low latency, such as real-time tracking. We demonstrate our model on\nsynthetically augmented adult brain MRI, as well as fetal brain MRI, which is\nthe intended use-case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1\">P Ellen Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Latent Classes for Few-shot Segmentation. (arXiv:2103.15402v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15402","description":"<p>Few-shot segmentation (FSS) aims to segment unseen classes given only a few\nannotated samples. Existing methods suffer the problem of feature undermining,\ni.e. potential novel classes are treated as background during training phase.\nOur method aims to alleviate this problem and enhance the feature embedding on\nlatent novel classes. In our work, we propose a novel joint-training framework.\nBased on conventional episodic training on support-query pairs, we add an\nadditional mining branch that exploits latent novel classes via transferable\nsub-clusters, and a new rectification technique on both background and\nforeground categories to enforce more stable prototypes. Over and above that,\nour transferable sub-cluster has the ability to leverage extra unlabeled data\nfor further feature enhancement. Extensive experiments on two FSS benchmarks\ndemonstrate that our method outperforms previous state-of-the-art by a large\nmargin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%\nfewer parameters and 2.5x faster inference speed. The source code is available\nat https://github.com/LiheYoung/MiningFSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lihe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIMstack: A Generative Shape and Instance Model for Unordered Object Stacks. (arXiv:2103.16442v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16442","description":"<p>By estimating 3D shape and instances from a single view, we can capture\ninformation about an environment quickly, without the need for comprehensive\nscanning and multi-view fusion. Solving this task for composite scenes (such as\nobject stacks) is challenging: occluded areas are not only ambiguous in shape\nbut also in instance segmentation; multiple decompositions could be valid. We\nobserve that physics constrains decomposition as well as shape in occluded\nregions and hypothesise that a latent space learned from scenes built under\nphysics simulation can serve as a prior to better predict shape and instances\nin occluded regions. To this end we propose SIMstack, a depth-conditioned\nVariational Auto-Encoder (VAE), trained on a dataset of objects stacked under\nphysics simulation. We formulate instance segmentation as a centre voting task\nwhich allows for class-agnostic detection and doesn't require setting the\nmaximum number of objects in the scene. At test time, our model can generate 3D\nshape and instance segmentation from a single depth view, probabilistically\nsampling proposals for the occluded region from the learned latent space. Our\nmethod has practical applications in providing robots some of the ability\nhumans have to make rapid intuitive inferences of partially observed scenes. We\ndemonstrate an application for precise (non-disruptive) object grasping of\nunknown objects from a single depth view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landgraf_Z/0/1/0/all/0/1\">Zoe Landgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scona_R/0/1/0/all/0/1\">Raluca Scona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misclassification-Aware Gaussian Smoothing and Mixed Augmentations improves Robustness against Domain Shifts. (arXiv:2104.01231v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.01231","description":"<p>Deep neural networks achieve high prediction accuracy when the train and test\ndistributions coincide. In practice though, various types of corruptions can\ndeviate from this setup and cause severe performance degradations. Few methods\nhave been proposed to address generalization in the presence of unforeseen\ndomain shifts. In this paper, we propose a misclassification-aware Gaussian\nsmoothing approach, coupled with mixed data augmentations, for improving\nrobustness of image classifiers against a variety of corruptions while still\nmaintaining high clean accuracy. We show that our method improves upon the\nstate-of-the-art in robustness and uncertainty calibration on several image\nclassification benchmarks and network architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_A/0/1/0/all/0/1\">Athanasios Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypothesis-driven Stream Learning with Augmented Memory. (arXiv:2104.02206v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02206","description":"<p>Stream learning refers to the ability to acquire and transfer knowledge\nacross a continuous stream of data without forgetting and without repeated\npasses over the data. A common way to avoid catastrophic forgetting is to\nintersperse new examples with replays of old examples stored as image pixels or\nreproduced by generative models. Here, we consider stream learning in image\nclassification tasks and propose a novel hypotheses-driven Augmented Memory\nNetwork, which efficiently consolidates previous knowledge with a limited\nnumber of hypotheses in the augmented memory and replays relevant hypotheses to\navoid catastrophic forgetting. The advantages of hypothesis-driven replay over\npixel-level replay and generative replay are two-fold. First, hypothesis-based\nknowledge consolidation avoids redundant information in the image pixel space\nand makes memory usage more efficient. Second, hypotheses in the augmented\nmemory can be re-used for learning new tasks, improving generalization and\ntransfer learning ability. We evaluated our method on three stream learning\nobject recognition datasets. Our method performs comparably well or better than\nstate-of-the-art methods, while offering more efficient memory usage. All\nsource code and data are publicly available\nhttps://github.com/kreimanlab/AugMem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badkundri_R/0/1/0/all/0/1\">Rohil Badkundri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbot_M/0/1/0/all/0/1\">Morgan B. Talbot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zawar_R/0/1/0/all/0/1\">Rushikesh Zawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval. (arXiv:2104.08271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08271","description":"<p>In recent years, considerable progress on the task of text-video retrieval\nhas been achieved by leveraging large-scale pretraining on visual and audio\ndatasets to construct powerful video encoders. By contrast, despite the natural\nsymmetry, the design of effective algorithms for exploiting large-scale\nlanguage pretraining remains under-explored. In this work, we are the first to\ninvestigate the design of such algorithms and propose a novel generalized\ndistillation method, TeachText, which leverages complementary cues from\nmultiple text encoders to provide an enhanced supervisory signal to the\nretrieval model. Moreover, we extend our method to video side modalities and\nshow that we can effectively reduce the number of used modalities at test time\nwithout compromising performance. Our approach advances the state of the art on\nseveral video retrieval benchmarks by a significant margin and adds no\ncomputational overhead at test time. Last but not least, we show an effective\napplication of our method for eliminating noise from retrieval datasets. Code\nand data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_I/0/1/0/all/0/1\">Ioana Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogolin_S/0/1/0/all/0/1\">Simion-Vlad Bogolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leordeanu_M/0/1/0/all/0/1\">Marius Leordeanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?. (arXiv:2104.09425v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.09425","description":"<p>While additional training data improves the robustness of deep neural\nnetworks against adversarial examples, it presents the challenge of curating a\nlarge number of specific real-world samples. We circumvent this challenge by\nusing additional data from proxy distributions learned by state-of-the-art\ngenerative models. We first seek to formally understand the transfer of\nrobustness from classifiers trained on proxy distributions to the real data\ndistribution. We prove that the difference between the robustness of a\nclassifier on the two distributions is upper bounded by the conditional\nWasserstein distance between them. Motivated by our result, we next ask how to\nempirically select an appropriate generative model? We find that existing\ndistance metrics, such as FID, fail to correctly determine the robustness\ntransfer from proxy distributions. We propose a robust discrimination approach,\nwhich measures the distinguishability of synthetic and real samples under\nadversarial perturbations. Our approach accurately predicts the robustness\ntransfer from different proxy distributions. After choosing a proxy\ndistribution, the next question is which samples are most beneficial? We\nsuccessfully optimize this selection by estimating the importance of each\nsample in robustness transfer. Finally, using our selection criterion for proxy\ndistribution and individual samples, we curate a set of ten million most\nbeneficial synthetic samples for robust training on the CIFAR-10 dataset. Using\nthis set we improve robust accuracy by up to 7.5% and 6.7% in $\\ell_{\\infty}$\nand $\\ell_2$ threat model, and certified robust accuracy by 7.6% in $\\ell_2$\nthreat model over baselines not using proxy distributions on the CIFAR-10\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handina_T/0/1/0/all/0/1\">Tinashe Handina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Sihui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor-based Plain Net for Mobile Image Super-Resolution. (arXiv:2105.09750v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.09750","description":"<p>Along with the rapid development of real-world applications, higher\nrequirements on the accuracy and efficiency of image super-resolution (SR) are\nbrought forward. Though existing methods have achieved remarkable success, the\nmajority of them demand plenty of computational resources and large amount of\nRAM, and thus they can not be well applied to mobile device. In this paper, we\naim at designing efficient architecture for 8-bit quantization and deploy it on\nmobile device. First, we conduct an experiment about meta-node latency by\ndecomposing lightweight SR architectures, which determines the portable\noperations we can utilize. Then, we dig deeper into what kind of architecture\nis beneficial to 8-bit quantization and propose anchor-based plain net (ABPN).\nFinally, we adopt quantization-aware training strategy to further boost the\nperformance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in\nterms of PSNR, while satisfying realistic needs at the same time. Code is\navaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1\">Zongcai Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety Metrics for Semantic Segmentation in Autonomous Driving. (arXiv:2105.10142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10142","description":"<p>Within the context of autonomous driving, safety-related metrics for deep\nneural networks have been widely studied for image classification and object\ndetection. In this paper, we further consider safety-aware correctness and\nrobustness metrics specialized for semantic segmentation. The novelty of our\nproposal is to move beyond pixel-level metrics: Given two images with each\nhaving N pixels being class-flipped, the designed metrics should, depending on\nthe clustering of pixels being class-flipped or the location of occurrence,\nreflect a different level of safety criticality. The result evaluated on an\nautonomous driving dataset demonstrates the validity and practicality of our\nproposed methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hsuan-Cheng Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDDA: Explanation-driven Data Augmentation to Improve Explanation Faithfulness. (arXiv:2105.14162v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14162","description":"<p>Recent years have seen the introduction of a range of methods for post-hoc\nexplainability of image classifier predictions. However, these post-hoc\nexplanations may not always be faithful to classifier predictions, which poses\na significant challenge when attempting to debug models based on such\nexplanations. To this end, we seek a methodology that can improve the\nfaithfulness of an explanation method with respect to model predictions which\ndoes not require ground truth explanations. We achieve this through a novel\nexplanation-driven data augmentation (EDDA) technique that augments the\ntraining data with occlusions inferred from model explanations; this is based\non the simple motivating principle that \\emph{if} the explainer is faithful to\nthe model \\emph{then} occluding salient regions for the model prediction should\ndecrease the model confidence in the prediction, while occluding non-salient\nregions should not change the prediction. To verify that the proposed\naugmentation method has the potential to improve faithfulness, we evaluate EDDA\nusing a variety of datasets and classification models. We demonstrate\nempirically that our approach leads to a significant increase of faithfulness,\nwhich can facilitate better debugging and successful deployment of image\nclassification models in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a> (co-first author), <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiani Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_C/0/1/0/all/0/1\">Chiheb Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jongseong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonjeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closer Look at the Uncertainty Estimation in Semantic Segmentation under Distributional Shift. (arXiv:2106.00076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00076","description":"<p>While recent computer vision algorithms achieve impressive performance on\nmany benchmarks, they lack robustness - presented with an image from a\ndifferent distribution, (e.g. weather or lighting conditions not considered\nduring training), they may produce an erroneous prediction. Therefore, it is\ndesired that such a model will be able to reliably predict its confidence\nmeasure. In this work, uncertainty estimation for the task of semantic\nsegmentation is evaluated under a varying level of domain shift: in a\ncross-dataset setting and when adapting a model trained on data from the\nsimulation. It was shown that simple color transformations already provide a\nstrong baseline, comparable to using more sophisticated style-transfer data\naugmentation. Further, by constructing an ensemble consisting of models using\ndifferent backbones and/or augmentation methods, it was possible to improve\nsignificantly model performance in terms of overall accuracy and uncertainty\nestimation under the domain shift setting. The Expected Calibration Error (ECE)\non challenging GTA to Cityscapes adaptation was reduced from 4.05 to the\ncompetitive value of 1.1. Further, an ensemble of models was utilized in the\nself-training setting to improve the pseudo-labels generation, which resulted\nin a significant gain in the final model accuracy, compared to the standard\nfine-tuning (without ensemble).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1\">Sebastian Cygert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewski_B/0/1/0/all/0/1\">Bart&#x142;omiej Wr&#xf3;blewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_K/0/1/0/all/0/1\">Karol Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slowinski_R/0/1/0/all/0/1\">Rados&#x142;aw S&#x142;owi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1\">Andrzej Czy&#x17c;ewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR Image Change Detection Based on Multiscale Capsule Network. (arXiv:2106.06896v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06896","description":"<p>Traditional change detection methods based on convolutional neural networks\n(CNNs) face the challenges of speckle noise and deformation sensitivity for\nsynthetic aperture radar images. To mitigate these issues, we proposed a\nMultiscale Capsule Network (Ms-CapsNet) to extract the discriminative\ninformation between the changed and unchanged pixels. On the one hand, the\ncapsule module is employed to exploit the spatial relationship of features.\nTherefore, equivariant properties can be achieved by aggregating the features\nfrom different positions. On the other hand, an adaptive fusion convolution\n(AFC) module is designed for the proposed Ms-CapsNet. Higher semantic features\ncan be captured for the primary capsules. Feature extracted by the AFC module\nsignificantly improves the robustness to speckle noise. The effectiveness of\nthe proposed Ms-CapsNet is verified on three real SAR datasets. The comparison\nexperiments with four state-of-the-art methods demonstrated the efficiency of\nthe proposed method. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_MS_CapsNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Network Training Using Artificial Triplets By Sampling and Image Transformation. (arXiv:2106.07015v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07015","description":"<p>The device used in this work detects the objects over the surface of the\nwater using two thermal cameras which aid the users to detect and avoid the\nobjects in scenarios where the human eyes cannot (night, fog, etc.). To avoid\nthe obstacle collision autonomously, it is required to track the objects in\nreal-time and assign a specific identity to each object to determine its\ndynamics (trajectory, velocity, etc.) for making estimated collision\npredictions. In the following work, a Machine Learning (ML) approach for\nComputer Vision (CV) called Convolutional Neural Network (CNN) was used using\nTensorFlow as the high-level programming environment in Python. To validate the\nalgorithm a test set was generated using an annotation tool that was created\nduring the work for proper evaluation. Once validated, the algorithm was\ndeployed on the platform and tested with the sequence generated by the test\nboat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar N. Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moser_D/0/1/0/all/0/1\">David Moser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08613","description":"<p>Video anomaly detection has gained significant attention due to the\nincreasing requirements of automatic monitoring for surveillance videos.\nEspecially, the prediction based approach is one of the most studied methods to\ndetect anomalies by predicting frames that include abnormal events in the test\nset after learning with the normal frames of the training set. However, a lot\nof prediction networks are computationally expensive owing to the use of\npre-trained optical flow networks, or fail to detect abnormal situations\nbecause of their strong generative ability to predict even the anomalies. To\naddress these shortcomings, we propose spatial rotation transformation (SRT)\nand temporal mixing transformation (TMT) to generate irregular patch cuboids\nwithin normal frame cuboids in order to enhance the learning of normal\nfeatures. Additionally, the proposed patch transformation is used only during\nthe training phase, allowing our model to detect abnormal frames at fast speed\nduring inference. Our model is evaluated on three anomaly detection benchmarks,\nachieving competitive accuracy and surpassing all the previous works in terms\nof speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">MyeongAh Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Visual Context for Weakly Supervised Person Search. (arXiv:2106.10506v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10506","description":"<p>Person search has recently emerged as a challenging task that jointly\naddresses pedestrian detection and person re-identification. Existing\napproaches follow a fully supervised setting where both bounding box and\nidentity annotations are available. However, annotating identities is\nlabor-intensive, limiting the practicability and scalability of current\nframeworks. This paper inventively considers weakly supervised person search\nwith only bounding box annotations. We proposed to address this novel task by\ninvestigating three levels of context clues (i.e., detection, memory and scene)\nin unconstrained natural images. The first two are employed to promote local\nand global discriminative capabilities, while the latter enhances clustering\naccuracy. Despite its simple design, our CGPS achieves 80.0% in mAP on\nCUHK-SYSU, boosting the baseline model by 8.8%. Surprisingly, it even achieves\ncomparable performance with several supervised person search models. Our code\nis available at https://github.com/ljpadam/CGPS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13301","description":"<p>Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14160","description":"<p>In autonomous driving, goal-based multi-trajectory prediction methods are\nproved to be effective recently, where they first score goal candidates, then\nselect a final set of goals, and finally complete trajectories based on the\nselected goals. However, these methods usually involve goal predictions based\non sparse predefined anchors. In this work, we propose an anchor-free model,\nnamed DenseTNT, which performs dense goal probability estimation for trajectory\nprediction. Our model achieves state-of-the-art performance, and ranks 1st on\nthe Waymo Open Dataset Motion Prediction Challenge. Project page is at\nhttps://github.com/Tsinghua-MARS-Lab/DenseTNT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15475","description":"<p>Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bidur Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.16093","description":"<p>Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn\na model from several labeled source domains while performing well on a\ndifferent target domain where only unlabeled data are available at training\ntime. To align source and target features distributions, several recent works\nuse source and target explicit statistics matching such as features moments or\nclass centroids. Yet, these approaches do not guarantee class conditional\ndistributions alignment across domains. In this work, we propose a new\nframework called Contrastive Multi-Source Domain Adaptation (CMSDA) for\nmulti-source UDA that addresses this limitation. Discriminative features are\nlearned from interpolated source examples via cross entropy minimization and\nfrom target examples via consistency regularization and hard pseudo-labeling.\nSimultaneously, interpolated source examples are leveraged to align source\nclass conditional distributions through an interpolated version of the\nsupervised contrastive loss. This alignment leads to more general and\ntransferable features which further improves the generalization on the target\ndomain. Extensive experiments have been carried out on three standard\nmulti-source UDA datasets where our method reports state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00272","description":"<p>With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, learning over patch-wise features using convolutional neural\nnetworks limits the ability of the model to capture global contextual\ninformation and comprehensively model tissue composition. The phenotypical and\ntopological distribution of constituent histological entities play a critical\nrole in tissue diagnosis. As such, graph data representations and deep learning\nhave attracted significant attention for encoding tissue representations, and\ncapturing intra- and inter- entity level interactions. In this review, we\nprovide a conceptual grounding for graph analytics in digital pathology,\nincluding entity-graph construction and graph architectures, and present their\ncurrent success for tumor localization and classification, tumor invasion and\nstaging, image retrieval, and survival prediction. We provide an overview of\nthese methods in a systematic manner organized by the graph representation of\nthe input image, scale, and organ on which they operate. We also outline the\nlimitations of existing techniques, and suggest potential future research\ndirections in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v2 [cs.NI] UPDATED)","link":"http://arxiv.org/abs/2107.01002","description":"<p>We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is precise, requires weaker label-information that can be\neasily collected, and works well in non-line-of-sight conditions. Our first\ncontribution is a novel dimensionality reduction method for charting. It\ncombines a triplet-loss with a multi-scale clustering-loss to map the\nhigh-dimensional CSI representation to a 2D/3D latent space. Our second\ncontribution is two weakly supervised losses that map this latent space into a\nCartesian map, resulting in meter-accuracy position results. These losses only\nrequire simple to acquire priors: a sketch of the floorplan, approximate\naccess-point locations and a few CSI packets that are labelled with the\ncorresponding zone in the floorplan. Thirdly, we report results and a\nrobustness study for 2D positioning in two single-floor office buildings and 3D\npositioning in a two-story home.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.03035","description":"<p>Coronary artery disease (CAD) has posed a leading threat to the lives of\ncardiovascular disease patients worldwide for a long time. Therefore, automated\ndiagnosis of CAD has indispensable significance in clinical medicine. However,\nthe complexity of coronary artery plaques that cause CAD makes the automatic\ndetection of coronary artery stenosis in Coronary CT angiography (CCTA) a\ndifficult task. In this paper, we propose a Transformer network (TR-Net) for\nthe automatic detection of significant stenosis (i.e. luminal narrowing &gt; 50%)\nwhile practically completing the computer-assisted diagnosis of CAD. The\nproposed TR-Net introduces a novel Transformer, and tightly combines\nconvolutional layers and Transformer encoders, allowing their advantages to be\ndemonstrated in the task. By analyzing semantic information sequences, TR-Net\ncan fully understand the relationship between image information in each\nposition of a multiplanar reformatted (MPR) image, and accurately detect\nsignificant stenosis based on both local and global information. We evaluate\nour TR-Net on a dataset of 76 patients from different patients annotated by\nexperienced radiologists. Experimental results illustrate that our TR-Net has\nachieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and\nMCC (0.74) indicators compared with the state-of-the-art methods. The source\ncode is publicly available from the link (https://github.com/XinghuaMa/TR-Net).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xinghua Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuanquan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v2 [physics.ins-det] UPDATED)","link":"http://arxiv.org/abs/2107.07714","description":"<p>Optical inspection of 1191 silicon micro-strip sensors was performed using a\ncustom made optical inspection setup, employing a machine-learning based\napproach for the defect analysis and subsequent quality assurance. Furthermore,\nmetrological control of the sensor's surface was performed. In this manuscript,\nwe present the analysis of various sensor surface defects. Among these are\nimplant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,\nsurface scratches, double metallization layer defects, passivation layer\ndefects, bias resistor defects as well as dust particle identification. The\ndefect detection was done using the application of Convolutional Deep Neural\nNetworks (CDNNs). From this, defective strips and defect clusters were\nidentified, as well as a 2D map of the defects using their geometrical\npositions on the sensor was performed. Based on the total number of defects\nfound on the sensor's surface, a method for the estimation of sensor's overall\nquality grade and quality score was proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Lavrik_E/0/1/0/all/0/1\">E. Lavrik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shiroya_M/0/1/0/all/0/1\">M. Shiroya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schmidt_H/0/1/0/all/0/1\">H.R. Schmidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Toia_A/0/1/0/all/0/1\">A. Toia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heuser_J/0/1/0/all/0/1\">J.M. Heuser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09244","description":"<p>Building change detection underpins many important applications, especially\nin the military and crisis management domains. Recent methods used for change\ndetection have shifted towards deep-learning, which depends on the quality of\nits training data. The assembly of large-scale annotated satellite imagery\ndatasets is therefore essential for global building change surveillance.\nExisting datasets almost exclusively offer near-nadir viewing angles. This\nlimits the range of changes that can be detected. By offering larger\nobservation ranges, the scroll imaging mode of optical satellites presents an\nopportunity to overcome this restriction. This paper therefore introduces\nS2Looking, a building change detection dataset that contains large-scale\nside-looking satellite images captured at various off-nadir angles. The dataset\nconsists of 5000 bitemporal image pairs of rural areas and more than 65,920\nannotated instances of changes throughout the world. The dataset can be used to\ntrain deep-learning-based change detection algorithms. It expands upon existing\ndatasets by providing: 1) larger viewing angles; 2) large illumination\nvariances; and 3) the added complexity of rural images. To facilitate use of\nthe dataset, a benchmark task has been established and preliminary tests\nsuggest deep-learning algorithms find the dataset significantly more\nchallenging than the closest competing near-nadir dataset, LEVIR-CD+. S2Looking\nmay therefore promote important advances in existing building change detection\nalgorithms. The dataset is available at https://github.com/S2Looking/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donghai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiabao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shouye Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bitao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11975","description":"<p>Few-shot learning aims to train a classifier that can generalize well when\njust a small number of labeled examples per class are given. We introduce a\ntransductive maximum margin classifier for few-shot learning (FS-TMMC). The\nbasic idea of the classical maximum margin classifier is to solve an optimal\nprediction function so that the training data can be correctly classified by\nthe resulting classifer with the largest geometric margin. In few-shot\nlearning, it is challenging to find such classifiers with good generalization\nability due to the insufficiency of training data in the support set. FS-TMMC\nleverages the unlabeled query examples to adjust the separating hyperplane of\nthe maximum margin classifier such that the prediction function is optimal on\nboth the support and query sets. Furthermore, we use an efficient and effective\nquasi-Newton algorithm, the L-BFGS method for optimization. Experimental\nresults on three standard few-shot learning benchmarks including miniImagenet,\ntieredImagenet and CUB show that our method achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12674","description":"<p>Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.02425","description":"<p>Grasping in cluttered scenes has always been a great challenge for robots,\ndue to the requirement of the ability to well understand the scene and object\ninformation. Previous works usually assume that the geometry information of the\nobjects is available, or utilize a step-wise, multi-stage strategy to predict\nthe feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF\ngrasp pose estimation as a simultaneous multi-task learning problem. In a\nunified framework, we jointly predict the feasible 6-DoF grasp poses, instance\nsemantic segmentation, and collision information. The whole framework is\njointly optimized and end-to-end differentiable. Our model is evaluated on\nlarge-scale benchmarks as well as the real robot system. On the public dataset,\nour method outperforms prior state-of-the-art methods by a large margin (+4.08\nAP). We also demonstrate the implementation of our model on a real robotic\nplatform and show that the robot can accurately grasp target objects in\ncluttered scenarios with a high success rate. Project link:\nhttps://openbyterobotics.github.io/sscl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1\">Ruihang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRAEM -- A discriminatively trained reconstruction embedding for surface anomaly detection. (arXiv:2108.07610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07610","description":"<p>Visual surface anomaly detection aims to detect local image regions that\nsignificantly deviate from normal appearance. Recent surface anomaly detection\nmethods rely on generative models to accurately reconstruct the normal areas\nand to fail on anomalies. These methods are trained only on anomaly-free\nimages, and often require hand-crafted post-processing steps to localize the\nanomalies, which prohibits optimizing the feature extraction for maximal\ndetection capability. In addition to reconstructive approach, we cast surface\nanomaly detection primarily as a discriminative problem and propose a\ndiscriminatively trained reconstruction anomaly embedding model (DRAEM). The\nproposed method learns a joint representation of an anomalous image and its\nanomaly-free reconstruction, while simultaneously learning a decision boundary\nbetween normal and anomalous examples. The method enables direct anomaly\nlocalization without the need for additional complicated post-processing of the\nnetwork output and can be trained using simple and general anomaly simulations.\nOn the challenging MVTec anomaly detection dataset, DRAEM outperforms the\ncurrent state-of-the-art unsupervised methods by a large margin and even\ndelivers detection performance close to the fully-supervised methods on the\nwidely used DAGM surface-defect detection dataset, while substantially\noutperforming them in localization accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1\">Vitjan Zavrtanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1\">Danijel Sko&#x10d;aj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Journey from SDRTV to HDRTV. (arXiv:2108.07978v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07978","description":"<p>Nowadays modern displays are capable to render video content with high\ndynamic range (HDR) and wide color gamut (WCG). However, most available\nresources are still in standard dynamic range (SDR). Therefore, there is an\nurgent demand to transform existing SDR-TV contents into their HDR-TV versions.\nIn this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the\nformation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step\nsolution pipeline including adaptive global color mapping, local enhancement\nand highlight generation. Moreover, the above analysis inspires us to present a\nlightweight network that utilizes global statistics as guidance to conduct\nimage-adaptive color mapping. In addition, we construct a dataset using HDR\nvideos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate\nthe results of SDRTV-to-HDRTV algorithms. Furthermore, our final results\nachieve state-of-the-art performance in quantitative comparisons and visual\nquality. The code and dataset are available at\nhttps://github.com/chxy95/HDRTVNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lynhoo Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PW-MAD: Pixel-wise Supervision for Generalized Face Morphing Attack Detection. (arXiv:2108.10291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10291","description":"<p>A face morphing attack image can be verified to multiple identities, making\nthis attack a major vulnerability to processes based on identity verification,\nsuch as border checks. Various methods have been proposed to detect face\nmorphing attacks, however, with low generalizability to unexpected\npost-morphing processes. A major post-morphing process is the print and scan\noperation performed in many countries when issuing a passport or identity\ndocument. In this work, we address this generalization problem by adapting a\npixel-wise supervision approach where we train a network to classify each pixel\nof the image into an attack or not, rather than only having one label for the\nwhole image. Our pixel-wise morphing attack detection (PW-MAD) solution proved\nto perform more accurately than a set of established baselines. More\nimportantly, PW-MAD shows high generalizability in comparison to related works,\nwhen evaluated on unknown re-digitized attacks. Additionally to our PW-MAD\napproach, we create a new face morphing attack dataset with digital and\nre-digitized samples, namely the LMA-DRD dataset that is publicly available for\nresearch purposes upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_N/0/1/0/all/0/1\">Noemie Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the OR. (arXiv:2108.11801v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11801","description":"<p>The fine-grained localization of clinicians in the operating room (OR) is a\nkey component to design the new generation of OR support systems. Computer\nvision models for person pixel-based segmentation and body-keypoints detection\nare needed to better understand the clinical activities and the spatial layout\nof the OR. This is challenging, not only because OR images are very different\nfrom traditional vision datasets, but also because data and annotations are\nhard to collect and generate in the OR due to privacy concerns. To address\nthese concerns, we first study how joint person pose estimation and instance\nsegmentation can be performed on low resolutions images from 1x to 12x. Second,\nto address the domain shift and the lack of annotations, we propose a novel\nunsupervised domain adaptation method, called \\emph{AdaptOR}, to adapt a model\nfrom an \\emph{in-the-wild} labeled source domain to a statistically different\nunlabeled target domain. We propose to exploit explicit geometric constraints\non the different augmentations of the unlabeled target domain image to generate\naccurate pseudo labels, and using these pseudo labels to train the model on\nhigh- and low-resolution OR images in a \\emph{self-training} framework.\nFurthermore, we propose \\emph{disentangled feature normalization} to handle the\nstatistically different source and target domain data. Extensive experimental\nresults with detailed ablation studies on the two OR datasets \\emph{MVOR+} and\n\\emph{TUM-OR-test} show the effectiveness of our approach against strongly\nconstructed baselines, especially on the low-resolution privacy-preserving OR\nimages. Finally, we show the generality of our method as a semi-supervised\nlearning (SSL) method on the large-scale \\emph{COCO} dataset, where we achieve\ncomparable results with as few as \\textbf{1\\%} of labeled supervision against a\nmodel trained with 100\\% labeled supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation Invariance and Extensive Data Augmentation: a strategy for the Mitosis Domain Generalization (MIDOG) Challenge. (arXiv:2109.00823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00823","description":"<p>Automated detection of mitotic figures in histopathology images is a\nchallenging task: here, we present the different steps that describe the\nstrategy we applied to participate in the MIDOG 2021 competition. The purpose\nof the competition was to evaluate the generalization of solutions to images\nacquired with unseen target scanners (hidden for the participants) under the\nconstraint of using training data from a limited set of four independent source\nscanners. Given this goal and constraints, we joined the challenge by proposing\na straight-forward solution based on a combination of state-of-the-art deep\nlearning methods with the aim of yielding robustness to possible\nscanner-related distributional shifts at inference time. Our solution combines\nmethods that were previously shown to be efficient for mitosis detection: hard\nnegative mining, extensive data augmentation, rotation-invariant convolutional\nnetworks.\n</p>\n<p>We trained five models with different splits of the provided dataset. The\nsubsequent classifiers produced F1-scores with a mean and standard deviation of\n0.747+/-0.032 on the test splits. The resulting ensemble constitutes our\ncandidate algorithm: its automated evaluation on the preliminary test set of\nthe challenge returned a F1-score of 0.6828.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lafarge_M/0/1/0/all/0/1\">Maxime W. Lafarge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koelzer_V/0/1/0/all/0/1\">Viktor H. Koelzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images. (arXiv:2109.00869v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00869","description":"<p>Breast cancer is the most prevalent cancer worldwide and is increasing in\nincidence, with over two million new cases now diagnosed each year. As part of\ndiagnostic tumour grading, histopathologists manually count the number of\ndividing cells (mitotic figures) in a sample. Since the process is subjective\nand time-consuming, artificial intelligence (AI) methods have been developed to\nautomate the process, however these methods often perform poorly when applied\nto data from outside of the original (training) domain, i.e. they do not\ngeneralise well to variations in histological background, staining protocols,\nor scanner types. Style transfer, a form of domain adaptation, provides the\nmeans to transform images from different domains to a shared visual appearance\nand have been adopted in various applications to mitigate the issue of domain\nshift. In this paper we train two mitosis detection models and two style\ntransfer methods and evaluate the usefulness of the latter for improving\nmitosis detection performance in images digitised using different scanners. We\nfound that the best of these models, U-Net without style transfer, achieved an\nF1-score of 0.693 on the MIDOG 2021 preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breen_J/0/1/0/all/0/1\">Jack Breen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zucker_K/0/1/0/all/0/1\">Kieran Zucker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orsi_N/0/1/0/all/0/1\">Nicolas Orsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hall_G/0/1/0/all/0/1\">Geoff Hall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascade RCNN for MIDOG Challenge. (arXiv:2109.01085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01085","description":"<p>Mitotic counts are one of the key indicators of breast cancer prognosis.\nHowever, accurate mitotic cell counting is still a difficult problem and is\nlabourious. Automated methods have been proposed for this task, but are usually\ndependent on the training images and show poor performance on unseen domains.\nIn this work, we present a multi-stage mitosis detection method based on a\nCascade RCNN developed to be sequentially more selective against false\npositives. On the preliminary test set, the algorithm scores an F1-score of\n0.7492.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Salar Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambandkhameneh_F/0/1/0/all/0/1\">Fariba Dambandkhameneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsos_D/0/1/0/all/0/1\">Dimitri Androutsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Done_S/0/1/0/all/0/1\">Susan Done</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Solutions in DeepFakes. (arXiv:2109.05397v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05397","description":"<p>Deep learning has been successfully appertained to solve various complex\nproblems in the area of big data analytics to computer vision. A deep\nlearning-powered application recently emerged is Deep Fake. It helps to create\nfake images and videos that human cannot distinguish them from the real ones\nand are recent off-shelf manipulation technique that allows swapping two\nidentities in a single video. Technology is a controversial technology with\nmany wide-reaching issues impacting society. So, to counter this emerging\nproblem, we introduce a dataset of 140k real and fake faces which contain 70k\nreal faces from the Flickr dataset collected by Nvidia, as well as 70k fake\nfaces sampled from 1 million fake faces generated by style GAN. We will train\nour model in the dataset so that our model can identify real or fake faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jatin Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Joint Source-Channel Coding for Multi-Task Network. (arXiv:2109.05779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05779","description":"<p>Multi-task learning (MTL) is an efficient way to improve the performance of\nrelated tasks by sharing knowledge. However, most existing MTL networks run on\na single end and are not suitable for collaborative intelligence (CI)\nscenarios. In this work, we propose an MTL network with a deep joint\nsource-channel coding (JSCC) framework, which allows operating under CI\nscenarios. We first propose a feature fusion based MTL network (FFMNet) for\njoint object detection and semantic segmentation. Compared with other MTL\nnetworks, FFMNet gets higher performance with fewer parameters. Then FFMNet is\nsplit into two parts, which run on a mobile device and an edge server\nrespectively. The feature generated by the mobile device is transmitted through\nthe wireless channel to the edge server. To reduce the transmission overhead of\nthe intermediate feature, a deep JSCC network is designed. By combining two\nnetworks together, the whole model achieves 512x compression for the\nintermediate feature and a performance loss within 2% on both tasks. At last,\nby training with noise, the FFMNet with JSCC is robust to various channel\nconditions and outperforms the separate source and channel coding scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhicong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaopeng Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation. (arXiv:2109.07702v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.07702","description":"<p>Medical image segmentation has significantly benefitted thanks to deep\nlearning architectures. Furthermore, semi-supervised learning (SSL) has\nrecently been a growing trend for improving a model's overall performance by\nleveraging abundant unlabeled data. Moreover, learning multiple tasks within\nthe same model further improves model generalizability. To generate smoother\nand accurate segmentation masks from 3D cardiac MR images, we present a\nMulti-task Cross-task learning consistency approach to enforce the correlation\nbetween the pixel-level (segmentation) and the geometric-level (distance map)\ntasks. Our extensive experimentation with varied quantities of labeled data in\nthe training sets justifies the effectiveness of our model for the segmentation\nof the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR)\nimages. With the incorporation of uncertainty estimates to detect failures in\nthe segmentation masks generated by CNNs, our study further showcases the\npotential of our model to flag low-quality segmentation from a given model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hasan_S/0/1/0/all/0/1\">S. M. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linte_C/0/1/0/all/0/1\">Cristian A. Linte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08685","description":"<p>The availability of high quality annotated medical imaging datasets is a\nmajor problem that collides with machine learning applications in the field of\nmedical imaging analysis and impedes its advancement. Self-supervised learning\nis a recent training paradigm that enables learning robust representations\nwithout the need for human annotation which can be considered as an effective\nsolution for the scarcity in annotated medical data. This article reviews the\nstate-of-the-art research directions in self-supervised learning approaches for\nimage data with concentration on their applications in the field of medical\nimaging analysis. The article covers a set of the most recent self-supervised\nlearning methods from the computer vision field as they are applicable to the\nmedical imaging analysis and categorize them as predictive, generative and\ncontrastive approaches. Moreover, the article covers (40) of the most recent\nresearches in the field of self-supervised learning in medical imaging analysis\naiming at shedding the light on the recent innovation in the field. Ultimately,\nthe article concludes with possible future research directions in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shurrab_S/0/1/0/all/0/1\">Saeed Shurrab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duwairi_R/0/1/0/all/0/1\">Rehab Duwairi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPTQ: Hardware-Friendly Post Training Quantization. (arXiv:2109.09113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09113","description":"<p>Neural network quantization enables the deployment of models on edge devices.\nAn essential requirement for their hardware efficiency is that the quantizers\nare hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To\nthe best of our knowledge, current post-training quantization methods do not\nsupport all of these constraints simultaneously. In this work, we introduce a\nhardware-friendly post training quantization (HPTQ) framework, which addresses\nthis problem by synergistically combining several known quantization methods.\nWe perform a large-scale study on four tasks: classification, object detection,\nsemantic segmentation and pose estimation over a wide variety of network\narchitectures. Our extensive experiments show that competitive results can be\nobtained under hardware-friendly constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habi_H/0/1/0/all/0/1\">Hai Victor Habi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peretz_R/0/1/0/all/0/1\">Reuven Peretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1\">Elad Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikstein_L/0/1/0/all/0/1\">Lior Dikstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_O/0/1/0/all/0/1\">Oranit Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diamant_I/0/1/0/all/0/1\">Idit Diamant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jennings_R/0/1/0/all/0/1\">Roy H. Jennings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netzer_A/0/1/0/all/0/1\">Arnon Netzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09129","description":"<p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that\naffect patients' social abilities. In recent years, many studies have employed\ndeep learning to diagnose this brain dysfunction through functional MRI (fMRI).\nHowever, existing approaches solely focused on the abnormal brain functional\nconnections but ignored the impact of regional activities. Due to this biased\nprior knowledge, previous diagnosis models suffered from inter-site\nheterogeneity and inter-individual phenotypic differences. To address this\nissue, we propose a novel feature extraction method for fMRI that can learn a\npersonalized lower-resolution representation of the entire brain networking\nregarding both the functional connections and regional activities.\nSpecifically, we abstract the brain imaging as a graph structure and\nstraightforwardly downsample it to sparse substructures by hierarchical graph\npooling. The down-scaled feature vectors are embedded into a population graph\nwhere the hidden inter-subject heterogeneity and homogeneity are explicitly\nexpressed as inter- and intra-community connectivity differences. Subsequently,\nwe fuse the imaging and non-imaging information by graph convolutional networks\n(GCN), which recalibrates features to node embeddings under phenotypic\nstatistics. By these means, our framework can extract features directly and\nefficiently from the entire fMRI and be aware of implicit inter-individual\nvariance. We have evaluated our framework on the ABIDE-I dataset with 10-fold\ncross-validation. The present model has achieved a mean classification accuracy\nof 85.95\\% and a mean AUC of 0.92, better than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Li Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jundong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_M/0/1/0/all/0/1\">Mingqin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_C/0/1/0/all/0/1\">Chi Wah Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_K/0/1/0/all/0/1\">Kei Hang Katie Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Automated Framework for COVID-19 Disease Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09241","description":"<p>The objective of this study is to develop a robust deep learning-based\nframework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and\nNormal cases based on chest CT scans acquired in different imaging centers\nusing various protocols, and radiation doses. We showed that while our proposed\nmodel is trained on a relatively small dataset acquired from only one imaging\ncenter using a specific scanning protocol, the model performs well on\nheterogeneous test sets obtained by multiple scanners using different technical\nparameters. We also showed that the model can be updated via an unsupervised\napproach to cope with the data shift between the train and test sets and\nenhance the robustness of the model upon receiving a new external dataset from\na different center. We adopted an ensemble architecture to aggregate the\npredictions from multiple versions of the model. For initial training and\ndevelopment purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76\nNormal cases was used, which contained volumetric CT scans acquired from one\nimaging center using a constant standard radiation dose scanning protocol. To\nevaluate the model, we collected four different test sets retrospectively to\ninvestigate the effects of the shifts in the data characteristics on the\nmodel's performance. Among the test cases, there were CT scans with similar\ncharacteristics as the train set as well as noisy low-dose and ultra-low dose\nCT scans. In addition, some test CT scans were obtained from patients with a\nhistory of cardiovascular diseases or surgeries. The entire test dataset used\nin this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental\nresults indicate that our proposed framework performs well on all test sets\nachieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity\nof 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:\n[76.50-99.19]).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Enshaei_N/0/1/0/all/0/1\">Nastaran Enshaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafiee_M/0/1/0/all/0/1\">Moezedin Javad Rafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafiee_A/0/1/0/all/0/1\">Akbar Shafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fard_F/0/1/0/all/0/1\">Faranak Babaki Fard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Source Video Domain Adaptation with Temporal Attentive Moment Alignment. (arXiv:2109.09964v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09964","description":"<p>Multi-Source Domain Adaptation (MSDA) is a more practical domain adaptation\nscenario in real-world scenarios. It relaxes the assumption in conventional\nUnsupervised Domain Adaptation (UDA) that source data are sampled from a single\ndomain and match a uniform data distribution. MSDA is more difficult due to the\nexistence of different domain shifts between distinct domain pairs. When\nconsidering videos, the negative transfer would be provoked by spatial-temporal\nfeatures and can be formulated into a more challenging Multi-Source Video\nDomain Adaptation (MSVDA) problem. In this paper, we address the MSVDA problem\nby proposing a novel Temporal Attentive Moment Alignment Network (TAMAN) which\naims for effective feature transfer by dynamically aligning both spatial and\ntemporal feature moments. TAMAN further constructs robust global temporal\nfeatures by attending to dominant domain-invariant local temporal features with\nhigh local classification confidence and low disparity between global and local\nfeature discrepancies. To facilitate future research on the MSVDA problem, we\nintroduce comprehensive benchmarks, covering extensive MSVDA scenarios.\nEmpirical results demonstrate a superior performance of the proposed TAMAN\nacross multiple MSVDA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs. (arXiv:2109.10257v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10257","description":"<p>Several applications such as autonomous driving, augmented reality and\nvirtual reality require a precise prediction of the 3D human pose. Recently, a\nnew problem was introduced in the field to predict the 3D human poses from\nobserved 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN\nmodel that predicts the future 3D skeleton poses in a single pass from the 2D\nones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction\nbetween the skeleton joints by exploiting their spatial configuration. This is\nbeing achieved by formulating the problem as a graph structure while learning a\nsuitable graph adjacency kernel. By the design, Skeleton-Graph predicts the\nfuture 3D poses without divergence in the long-term, unlike prior works. We\nalso introduce a new metric that measures the divergence of predictions in the\nlong term. Our results show an FDE improvement of at least 27% and an ADE of 4%\non both the GTA-IM and PROX datasets respectively in comparison with prior\nworks. Also, we are 88% and 93% less divergence on the long-term motion\nprediction in comparison with prior works on both GTA-IM and PROX datasets.\nCode is available at https://github.com/abduallahmohamed/Skeleton-Graph.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abduallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huancheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1\">Christian Claudel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Transfer Attacks With Unknown Data and Class Overlap. (arXiv:2109.11125v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11125","description":"<p>The ability to transfer adversarial attacks from one model (the surrogate) to\nanother model (the victim) has been an issue of concern within the machine\nlearning (ML) community. The ability to successfully evade unseen models\nrepresents an uncomfortable level of ease toward implementing attacks. In this\nwork we note that as studied, current transfer attack research has an\nunrealistic advantage for the attacker: the attacker has the exact same\ntraining data as the victim. We present the first study of transferring\nadversarial attacks focusing on the data available to attacker and victim under\nimperfect settings without querying the victim, where there is some variable\nlevel of overlap in the exact data used or in the classes learned by each\nmodel. This threat model is relevant to applications in medicine, malware, and\nothers. Under this new threat model attack success rate is not correlated with\ndata or class overlap in the way one would expect, and varies with dataset.\nThis makes it difficult for attacker and defender to reason about each other\nand contributes to the broader study of model robustness and security. We\nremedy this by developing a masked version of Projected Gradient Descent that\nsimulates class disparity, which enables the attacker to reliably estimate a\nlower-bound on their attack's success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richards_L/0/1/0/all/0/1\">Luke E. Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andr&#xe9; Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capps_R/0/1/0/all/0/1\">Ryan Capps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsythe_S/0/1/0/all/0/1\">Steven Forsythe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much \"human-like\" visual experience do current self-supervised learning algorithms need to achieve human-level object recognition?. (arXiv:2109.11523v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11523","description":"<p>This paper addresses a fundamental question: how good are our current\nself-supervised visual representation learning algorithms relative to humans?\nMore concretely, how much \"human-like\", natural visual experience would these\nalgorithms need in order to reach human-level performance in a complex,\nrealistic visual object recognition task such as ImageNet? Using a scaling\nexperiment, here we estimate that the answer is on the order of a million years\nof natural visual experience, in other words several orders of magnitude longer\nthan a human lifetime. However, this estimate is quite sensitive to some\nunderlying assumptions, underscoring the need to run carefully controlled human\nexperiments. We discuss the main caveats surrounding our estimate and the\nimplications of this rather surprising result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From images in the wild to video-informed image classification. (arXiv:2109.12040v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2109.12040","description":"<p>Image classifiers work effectively when applied on structured images, yet\nthey often fail when applied on images with very high visual complexity. This\npaper describes experiments applying state-of-the-art object classifiers toward\na unique set of images in the wild with high visual complexity collected on the\nisland of Bali. The text describes differences between actual images in the\nwild and images from Imagenet, and then discusses a novel approach combining\ninformational cues particular to video with an ensemble of imperfect\nclassifiers in order to improve classification results on video sourced images\nof plants in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohlen_M/0/1/0/all/0/1\">Marc B&#xf6;hlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandola_V/0/1/0/all/0/1\">Varun Chandola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujarwo_W/0/1/0/all/0/1\">Wawan Sujarwo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raunaq Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}