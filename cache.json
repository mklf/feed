{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection. (arXiv:2109.09777v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09777","description":"<p>This paper describes our submission to the DISRPT2021 Shared Task on\nDiscourse Unit Segmentation, Connective Detection, and Relation Classification.\nOur system, called DisCoDisCo, is a Transformer-based neural classifier which\nenhances contextualized word embeddings (CWEs) with hand-crafted features,\nrelying on tokenwise sequence tagging for discourse segmentation and connective\ndetection, and a feature-rich, encoder-less sentence pair classifier for\nrelation classification. Our results for the first two tasks outperform SOTA\nscores from the previous 2019 shared task, and results on relation\nclassification suggest strong performance on the new 2021 benchmark. Ablation\ntests show that including features beyond CWEs are helpful for both tasks, and\na partial evaluation of multiple pre-trained Transformer-based language models\nindicates that models pre-trained on the Next Sentence Prediction (NSP) task\nare optimal for relation classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzad_S/0/1/0/all/0/1\">Shabnam Behzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Janet Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology. (arXiv:2109.09780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09780","description":"<p>An important question concerning contextualized word embedding (CWE) models\nlike BERT is how well they can represent different word senses, especially\nthose in the long tail of uncommon senses. Rather than build a WSD system as in\nprevious work, we investigate contextualized embedding neighborhoods directly,\nformulating a query-by-example nearest neighbor retrieval task and examining\nranking performance for words and senses in different frequency bands. In an\nevaluation on two English sense-annotated corpora, we find that several popular\nCWE models all outperform a random baseline even for proportionally rare\nsenses, without explicit sense supervision. However, performance varies\nconsiderably even among models with similar architectures and pretraining\nregimes, with especially large differences for rare word senses, revealing that\nCWE models are not all created equal when it comes to approximating word senses\nin their native representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. (arXiv:2109.09784v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09784","description":"<p>State-of-the-art abstractive summarization systems often generate\n\\emph{hallucinations}; i.e., content that is not directly inferable from the\nsource text. Despite being assumed incorrect, many of the hallucinated contents\nare consistent with world knowledge (factual hallucinations). Including these\nfactual hallucinations into a summary can be beneficial in providing additional\nbackground information. In this work, we propose a novel detection approach\nthat separates factual from non-factual hallucinations of entities. Our method\nis based on an entity's prior and posterior probabilities according to\npre-trained and finetuned masked language models, respectively. Empirical\nresults suggest that our method vastly outperforms three strong baselines in\nboth accuracy and F1 scores and has a strong correlation with human judgments\non factuality classification tasks. Furthermore, our approach can provide\ninsight into whether a particular hallucination is caused by the summarizer's\npre-training or fine-tuning step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Induction Through the Lens of Visual Perception. (arXiv:2109.09790v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09790","description":"<p>Most previous work on grammar induction focuses on learning phrasal or\ndependency structure purely from text. However, because the signal provided by\ntext alone is limited, recently introduced visually grounded syntax models make\nuse of multimodal information leading to improved performance in constituency\ngrammar induction. However, as compared to dependency grammars, constituency\ngrammars do not provide a straightforward way to incorporate visual information\nwithout enforcing language-specific heuristics. In this paper, we propose an\nunsupervised grammar induction model that leverages word concreteness and a\nstructural vision-based heuristic to jointly learn constituency-structure and\ndependency-structure grammars. Our experiments find that concreteness is a\nstrong indicator for learning dependency grammars, improving the direct\nattachment score (DAS) by over 50\\% as compared to state-of-the-art models\ntrained on pure text. Next, we propose an extension of our model that leverages\nboth word concreteness and visual semantic role labels in constituency and\ndependency parsing. Our experiments show that the proposed extension\noutperforms the current state-of-the-art visually grounded models in\nconstituency parsing even with a smaller grammar size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Fake News: Robust Generalisable News Classification Using Transformers. (arXiv:2109.09796v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09796","description":"<p>As online news has become increasingly popular and fake news increasingly\nprevalent, the ability to audit the veracity of online news content has become\nmore important than ever. Such a task represents a binary classification\nchallenge, for which transformers have achieved state-of-the-art results. Using\nthe publicly available ISOT and Combined Corpus datasets, this study explores\ntransformers' abilities to identify fake news, with particular attention given\nto investigating generalisation to unseen datasets with varying styles, topics\nand class distributions. Moreover, we explore the idea that opinion-based news\narticles cannot be classified as real or fake due to their subjective nature\nand often sensationalised language, and propose a novel two-step classification\npipeline to remove such articles from both model training and the final\ndeployed inference system. Experiments over the ISOT and Combined Corpus\ndatasets show that transformers achieve an increase in F1 scores of up to 4.9%\nfor out of distribution generalisation compared to baseline approaches, with a\nfurther increase of 10.1% following the implementation of our two-step\nclassification pipeline. To the best of our knowledge, this study is the first\nto investigate generalisation of transformers in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blackledge_C/0/1/0/all/0/1\">Ciara Blackledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Span Representation for Domain-adapted Coreference Resolution. (arXiv:2109.09811v1 [cs.LG])","link":"http://arxiv.org/abs/2109.09811","description":"<p>Recent work has shown fine-tuning neural coreference models can produce\nstrong performance when adapting to different domains. However, at the same\ntime, this can require a large amount of annotated target examples. In this\nwork, we focus on supervised domain adaptation for clinical notes, proposing\nthe use of concept knowledge to more efficiently adapt coreference models to a\nnew domain. We develop methods to improve the span representations via (1) a\nretrofitting loss to incentivize span representations to satisfy a\nknowledge-based distance function and (2) a scaffolding loss to guide the\nrecovery of knowledge from the span representation. By integrating these\nlosses, our model is able to improve our baseline precision and F-1 score. In\nparticular, we show that incorporating knowledge with end-to-end coreference\nmodels results in better performance on the most challenging, domain-specific\nspans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Nupoor Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Methods for Anaphoric Zero Pronouns. (arXiv:2109.09825v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09825","description":"<p>In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and\nmany others, unrealized (null) arguments in certain syntactic positions can\nrefer to a previously introduced entity, and are thus called anaphoric zero\npronouns. The existing resources for studying anaphoric zero pronoun\ninterpretation are however still limited. In this paper, we use five data\naugmentation methods to generate and detect anaphoric zero pronouns\nautomatically. We use the augmented data as additional training materials for\ntwo anaphoric zero pronoun systems for Arabic. Our experimental results show\nthat data augmentation improves the performance of the two systems, surpassing\nthe state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aloraini_A/0/1/0/all/0/1\">Abdulrahman Aloraini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreamSide: A Fully-Customizable Open-Source Toolkit for Efficient Annotation of Meaning Representations. (arXiv:2109.09853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09853","description":"<p>This demonstration paper presents StreamSide, an open-source toolkit for\nannotating multiple kinds of meaning representations. StreamSide supports\nframe-based annotation schemes e.g., Abstract Meaning Representation (AMR) and\nframeless annotation schemes e.g., Widely Interpretable Semantic Representation\n(WISeR). Moreover, it supports both sentence-level and document-level\nannotation by allowing annotators to create multi-rooted graphs for input text.\nIt can open and automatically convert between several types of input formats\nincluding plain text, Penman notation, and its own JSON format enabling richer\nannotation. It features reference frames for AMR predicate argument structures,\nand also concept-to-text alignment. StreamSide is released under the Apache 2.0\nlicense, and is completely open-source so that it can be customized to annotate\nenriched meaning representations in different languages (e.g., Uniform Meaning\nRepresentations). All StreamSide resources are publicly distributed through our\nopen source project at: https://github.com/emorynlp/StreamSide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_G/0/1/0/all/0/1\">Gregor Williamson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. (arXiv:2109.09858v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09858","description":"<p>Abstract Meaning Representation (AMR) is a graphical meaning representation\nlanguage designed to represent propositional information about argument\nstructure. However, at present it is unable to satisfyingly represent\nnon-veridical intensional contexts, often licensing inappropriate inferences.\nIn this paper, we show how to resolve the problem of non-veridicality without\nappealing to layered graphs through a mapping from AMRs into Simply-Typed\nLambda Calculus (STLC). At least for some cases, this requires the introduction\nof a new role :content which functions as an intensional operator. The\ntranslation proposed is inspired by the formal linguistics literature on the\nevent semantics of attitude reports. Next, we address the interaction of\nquantifier scope and intensional operators in so-called de re/de dicto\nambiguities. We adopt a scope node from the literature and provide an explicit\nmultidimensional semantics utilizing Cooper storage which allows us to derive\nthe de re and de dicto scope readings as well as intermediate scope readings\nwhich prove difficult for accounts without a scope node.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williamson_G/0/1/0/all/0/1\">Gregor Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_P/0/1/0/all/0/1\">Patrick Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuxin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Identification with a Reciprocal Rank Classifier. (arXiv:2109.09862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09862","description":"<p>Language identification is a critical component of language processing\npipelines (Jauhiainen et al.,2019) and is not a solved problem in real-world\nsettings. We present a lightweight and effective language identifier that is\nrobust to changes of domain and to the absence of copious training data.\n</p>\n<p>The key idea for classification is that the reciprocal of the rank in a\nfrequency table makes an effective additive feature score, hence the term\nReciprocal Rank Classifier (RRC). The key finding for language classification\nis that ranked lists of words and frequencies of characters form a sufficient\nand robust representation of the regularities of key languages and their\northographies.\n</p>\n<p>We test this on two 22-language data sets and demonstrate zero-effort domain\nadaptation from a Wikipedia training set to a Twitter test set. When trained on\nWikipedia but applied to Twitter the macro-averaged F1-score of a\nconventionally trained SVM classifier drops from 90.9% to 77.7%. By contrast,\nthe macro F1-score of RRC drops only from 93.1% to 90.6%. These classifiers are\ncompared with those from fastText and langid. The RRC performs better than\nthese established systems in most experiments, especially on short Wikipedia\ntexts and Twitter.\n</p>\n<p>The RRC classifier can be improved for particular domains and conversational\nsituations by adding words to the ranked lists. Using new terms learned from\nsuch conversations, we demonstrate a further 7.9% increase in accuracy of\nsample message classification, and 1.7% increase for conversation\nclassification. Surprisingly, this made results on Twitter data slightly worse.\n</p>\n<p>The RRC classifier is available as an open source Python package\n(https://github.com/LivePersonInc/lplangid).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_C/0/1/0/all/0/1\">Chris Brew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Short Text Clustering. (arXiv:2109.09894v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09894","description":"<p>Effective representation learning is critical for short text clustering due\nto the sparse, high-dimensional and noise attributes of short text corpus.\nExisting pre-trained models (e.g., Word2vec and BERT) have greatly improved the\nexpressiveness for short text representations with more condensed,\nlow-dimensional and continuous features compared to the traditional\nBag-of-Words (BoW) model. However, these models are trained for general\npurposes and thus are suboptimal for the short text clustering task. In this\npaper, we propose two methods to exploit the unsupervised autoencoder (AE)\nframework to further tune the short text representations based on these\npre-trained text models for optimal clustering performance. In our first method\nStructural Text Network Graph Autoencoder (STN-GAE), we exploit the structural\ntext information among the corpus by constructing a text network, and then\nadopt graph convolutional network as encoder to fuse the structural features\nwith the pre-trained text features for text representation learning. In our\nsecond method Soft Cluster Assignment Autoencoder (SCA-AE), we adopt an extra\nsoft cluster assignment constraint on the latent space of autoencoder to\nencourage the learned text representations to be more clustering-friendly. We\ntested two methods on seven popular short text datasets, and the experimental\nresults show that when only using the pre-trained model for short text\nclustering, BERT performs better than BoW and Word2vec. However, as long as we\nfurther tune the pre-trained representations, the proposed method like SCA-AE\ncan greatly increase the clustering performance, and the accuracy improvement\ncompared to use BERT alone could reach as much as 14\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiangyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuiqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guangyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization in Text-based Games via Hierarchical Reinforcement Learning. (arXiv:2109.09968v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09968","description":"<p>Deep reinforcement learning provides a promising approach for text-based\ngames in studying natural language communication between humans and artificial\nagents. However, the generalization still remains a big challenge as the agents\ndepend critically on the complexity and variety of training tasks. In this\npaper, we address this problem by introducing a hierarchical framework built\nupon the knowledge graph-based RL agent. In the high level, a meta-policy is\nexecuted to decompose the whole game into a set of subtasks specified by\ntextual goals, and select one of them based on the KG. Then a sub-policy in the\nlow level is executed to conduct goal-conditioned reinforcement learning. We\ncarry out experiments on games with various difficulty levels and show that the\nproposed method enjoys favorable generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunqiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Kernel-Smoothed Machine Translation with Retrieved Examples. (arXiv:2109.09991v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09991","description":"<p>How to effectively adapt neural machine translation (NMT) models according to\nemerging cases without retraining? Despite the great success of neural machine\ntranslation, updating the deployed models online remains a challenge. Existing\nnon-parametric approaches that retrieve similar examples from a database to\nguide the translation process are promising but are prone to overfit the\nretrieved examples. However, non-parametric methods are prone to overfit the\nretrieved examples. In this work, we propose to learn Kernel-Smoothed\nTranslation with Example Retrieval (KSTER), an effective approach to adapt\nneural machine translation models online. Experiments on domain adaptation and\nmulti-domain machine translation datasets show that even without expensive\nretraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over\nthe best existing online adaptation methods. The code and trained models are\nreleased at https://github.com/jiangqn/KSTER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingnan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negation-Instance Based Evaluation of End-to-End Negation Resolution. (arXiv:2109.10013v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10013","description":"<p>In this paper, we revisit the task of negation resolution, which includes the\nsubtasks of cue detection (e.g. \"not\", \"never\") and scope resolution. In the\ncontext of previous shared tasks, a variety of evaluation metrics have been\nproposed. Subsequent works usually use different subsets of these, including\nvariations and custom implementations, rendering meaningful comparisons between\nsystems difficult. Examining the problem both from a linguistic perspective and\nfrom a downstream viewpoint, we here argue for a negation-instance based\napproach to evaluating negation resolution. Our proposed metrics correspond to\nexpectations over per-instance scores and hence are intuitively interpretable.\nTo render research comparable and to foster future work, we provide results for\na set of current state-of-the-art systems for negation resolution on three\nEnglish corpora, and make our implementation of the evaluation scripts publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sineva_E/0/1/0/all/0/1\">Elizaveta Sineva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Comments are Equal: Insights into Comment Moderation from a Topic-Aware Model. (arXiv:2109.10033v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10033","description":"<p>Moderation of reader comments is a significant problem for online news\nplatforms. Here, we experiment with models for automatic moderation, using a\ndataset of comments from a popular Croatian newspaper. Our analysis shows that\nwhile comments that violate the moderation rules mostly share common linguistic\nand thematic features, their content varies across the different sections of\nthe newspaper. We therefore make our models topic-aware, incorporating semantic\nfeatures from a topic model into the classification decision. Our results show\nthat topic information improves the performance of the model, increases its\nconfidence in correct outputs, and helps us understand the model's outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zosa_E/0/1/0/all/0/1\">Elaine Zosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_R/0/1/0/all/0/1\">Ravi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karan_M/0/1/0/all/0/1\">Mladen Karan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10044","description":"<p>This report describes the parsing problem for Combinatory Categorial Grammar\n(CCG), showing how a combination of Transformer-based neural models and a\nsymbolic CCG grammar can lead to substantial gains over existing approaches.\nThe report also documents a 20-year research program, showing how NLP methods\nhave evolved over this time. The staggering accuracy improvements provided by\nneural models for CCG parsing can be seen as a reflection of the improvements\nseen in NLP more generally. The report provides a minimal introduction to CCG\nand CCG parsing, with many pointers to the relevant literature. It then\ndescribes the CCG supertagging problem, and some recent work from Tian et al.\n(2020) which applies Transformer-based models to supertagging with great\neffect. I use this existing model to develop a CCG multitagger, which can serve\nas a front-end to an existing CCG parser. Simply using this new multitagger\nprovides substantial gains in parsing accuracy. I then show how a\nTransformer-based model from the parsing literature can be combined with the\ngrammar-based CCG parser, setting a new state-of-the-art for the CCGbank\nparsing task of almost 93% F-score for labelled dependencies, with complete\nsentence accuracies of over 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. (arXiv:2109.10052v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10052","description":"<p>In this paper, we investigate what types of stereotypical information are\ncaptured by pretrained language models. We present the first dataset comprising\nstereotypical attributes of a range of social groups and propose a method to\nelicit stereotypes encoded by pretrained language models in an unsupervised\nfashion. Moreover, we link the emergent stereotypes to their manifestation as\nbasic emotions as a means to study their emotional effects in a more\ngeneralized manner. To demonstrate how our methods can be used to analyze\nemotion and stereotype shifts due to linguistic experience, we use fine-tuning\non news sources as a case study. Our experiments expose how attitudes towards\ndifferent social groups vary across models and how quickly emotions and\nstereotypes can shift at the fine-tuning stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choenni_R/0/1/0/all/0/1\">Rochelle Choenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rooij_R/0/1/0/all/0/1\">Robert van Rooij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10080","description":"<p>Adverse Drug Event (ADE) extraction mod-els can rapidly examine large\ncollections of so-cial media texts, detecting mentions of drug-related adverse\nreactions and trigger medicalinvestigations. However, despite the recent\nad-vances in NLP, it is currently unknown if suchmodels are robust in face\nofnegation, which ispervasive across language varieties.In this paper we\nevaluate three state-of-the-artsystems, showing their fragility against\nnega-tion, and then we introduce two possible strate-gies to increase the\nrobustness of these mod-els: a pipeline approach, relying on a\nspecificcomponent for negation detection; an augmen-tation of an ADE extraction\ndataset to artifi-cially create negated samples and further trainthe models.We\nshow that both strategies bring significantincreases in performance, lowering\nthe num-ber of spurious entities predicted by the mod-els. Our dataset and code\nwill be publicly re-leased to encourage research on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. (arXiv:2109.10086v1 [cs.IR])","link":"http://arxiv.org/abs/2109.10086","description":"<p>In neural Information Retrieval (IR), ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\n\\emph{sparse} representations for documents and queries, that could inherit\nfrom the desirable properties of bag-of-words models such as the exact matching\nof terms and the efficiency of inverted indexes. Introduced recently, the\nSPLADE model provides highly sparse representations and competitive results\nwith respect to state-of-the-art dense and sparse approaches. In this paper, we\nbuild on SPLADE and propose several significant improvements in terms of\neffectiveness and/or efficiency. More specifically, we modify the pooling\nmechanism, benchmark a model solely based on document expansion, and introduce\nmodels trained with distillation. We also report results on the BEIR benchmark.\nOverall, SPLADE is considerably improved with more than $9$\\% gains on NDCG@10\non TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InvBERT: Text Reconstruction from Contextualized Embeddings used for Derived Text Formats of Literary Works. (arXiv:2109.10104v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10104","description":"<p>Digital Humanities and Computational Literary Studies apply text mining\nmethods to investigate literature. Such automated approaches enable\nquantitative studies on large corpora which would not be feasible by manual\ninspection alone. However, due to copyright restrictions, the availability of\nrelevant digitized literary works is limited. Derived Text Formats (DTFs) have\nbeen proposed as a solution. Here, textual materials are transformed in such a\nway that copyright-critical features are removed, but that the use of certain\nanalytical methods remains possible. Contextualized word embeddings produced by\ntransformer-encoders (like BERT) are promising candidates for DTFs because they\nallow for state-of-the-art performance on various analytical tasks and, at\nfirst sight, do not disclose the original text. However, in this paper we\ndemonstrate that under certain conditions the reconstruction of the original\ncopyrighted text becomes feasible and its publication in the form of\ncontextualized word representations is not safe. Our attempts to invert BERT\nsuggest, that publishing parts of the encoder together with the contextualized\nembeddings is critical, since it allows to generate data to train a decoder\nwith a reconstruction accuracy sufficient to violate copyright laws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hohmann_J/0/1/0/all/0/1\">Johannes H&#xf6;hmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugler_K/0/1/0/all/0/1\">Kai Kugler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Difficulty of Segmenting Words with Attention. (arXiv:2109.10107v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10107","description":"<p>Word segmentation, the problem of finding word boundaries in speech, is of\ninterest for a range of tasks. Previous papers have suggested that for\nsequence-to-sequence models trained on tasks such as speech translation or\nspeech recognition, attention can be used to locate and segment the words. We\nshow, however, that even on monolingual data this approach is brittle. In our\nexperiments with different input types, data sizes, and segmentation\nalgorithms, only models trained to predict phones from words succeed in the\ntask. Models trained to predict words from either phones or speech (i.e., the\nopposite direction needed to generalize to new data), yield much worse results,\nsuggesting that attention-based segmentation is only useful in limited\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review on Summarizing Financial News Using Deep Learning. (arXiv:2109.10118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10118","description":"<p>Investors make investment decisions depending on several factors such as\nfundamental analysis, technical analysis, and quantitative analysis. Another\nfactor on which investors can make investment decisions is through sentiment\nanalysis of news headlines, the sole purpose of this study. Natural Language\nProcessing techniques are typically used to deal with such a large amount of\ndata and get valuable information out of it. NLP algorithms convert raw text\ninto numerical representations that machines can easily understand and\ninterpret. This conversion can be done using various embedding techniques. In\nthis research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,\nGloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.\nThis work aims to evaluate these model's performance to choose the robust model\nin identifying the significant factors influencing the prediction. During this\nresearch, it was expected that Deep Leaming would be applied to get the desired\nresults or achieve better accuracy than the state-of-the-art. The models are\ncompared to check their outputs to know which one has performed better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_S/0/1/0/all/0/1\">Saurabh Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvFiT: Conversational Fine-Tuning of Pretrained Language Models. (arXiv:2109.10126v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10126","description":"<p>Transformer-based language models (LMs) pretrained on large text collections\nare proven to store a wealth of semantic knowledge. However, 1) they are not\neffective as sentence encoders when used off-the-shelf, and 2) thus typically\nlag behind conversationally pretrained (e.g., via response selection) encoders\non conversational tasks such as intent detection (ID). In this work, we propose\nConvFiT, a simple and efficient two-stage procedure which turns any pretrained\nLM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and\ntask-specialised sentence encoder (after Stage 2). We demonstrate that 1)\nfull-blown conversational pretraining is not required, and that LMs can be\nquickly transformed into effective conversational encoders with much smaller\namounts of unannotated data; 2) pretrained LMs can be fine-tuned into\ntask-specialised sentence encoders, optimised for the fine-grained semantics of\na particular task. Consequently, such specialised sentence encoders allow for\ntreating ID as a simple semantic similarity task based on interpretable nearest\nneighbours retrieval. We validate the robustness and versatility of the ConvFiT\nframework with such similarity-based inference on the standard ID evaluation\nsets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board,\nwith particular gains in the most challenging, few-shot setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Pei-Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coope_S/0/1/0/all/0/1\">Sam Coope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerz_D/0/1/0/all/0/1\">Daniela Gerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrksic_N/0/1/0/all/0/1\">Nikola Mrk&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1\">Tsung-Hsien Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement. (arXiv:2109.10133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10133","description":"<p>Many recent works have demonstrated that unsupervised sentence\nrepresentations of neural networks encode syntactic information by observing\nthat neural language models are able to predict the agreement between a verb\nand its subject. We take a critical look at this line of research by showing\nthat it is possible to achieve high accuracy on this agreement task with simple\nsurface heuristics, indicating a possible flaw in our assessment of neural\nnetworks' syntactic ability. Our fine-grained analyses of results on the\nlong-range French object-verb agreement show that contrary to LSTMs,\nTransformers are able to capture a non-trivial amount of grammatical structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crabbe_B/0/1/0/all/0/1\">Benoit Crabb&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation with Noisy Labels for Natural Language Understanding. (arXiv:2109.10147v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10147","description":"<p>Knowledge Distillation (KD) is extensively used to compress and deploy large\npre-trained language models on edge devices for real-world applications.\nHowever, one neglected area of research is the impact of noisy (corrupted)\nlabels on KD. We present, to the best of our knowledge, the first study on KD\nwith noisy labels in Natural Language Understanding (NLU). We document the\nscope of the problem and present two methods to mitigate the impact of label\nnoise. Experiments on the GLUE benchmark show that our methods are effective\neven under high noise levels. Nevertheless, our results indicate that more\nresearch is necessary to cope with label noise under the KD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_S/0/1/0/all/0/1\">Shivendra Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10164","description":"<p>Intermediate layer knowledge distillation (KD) can improve the standard KD\ntechnique (which only targets the output of teacher and student models)\nespecially over large pre-trained language models. However, intermediate layer\ndistillation suffers from excessive computational burdens and engineering\nefforts required for setting up a proper layer mapping. To address these\nproblems, we propose a RAndom Intermediate Layer Knowledge Distillation\n(RAIL-KD) approach in which, intermediate layers from the teacher model are\nselected randomly to be distilled into the intermediate layers of the student\nmodel. This randomized selection enforce that: all teacher layers are taken\ninto account in the training process, while reducing the computational cost of\nintermediate layer distillation. Also, we show that it act as a regularizer for\nimproving the generalizability of the student model. We perform extensive\nexperiments on GLUE tasks as well as on out-of-domain test sets. We show that\nour proposed RAIL-KD approach outperforms other state-of-the-art intermediate\nlayer KD methods considerably in both performance and training-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haidar_M/0/1/0/all/0/1\">Md Akmal Haidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anchuri_N/0/1/0/all/0/1\">Nithin Anchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings. (arXiv:2109.10179v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10179","description":"<p>How do neural networks \"perceive\" speech sounds from unknown languages? Does\nthe typological similarity between the model's training language (L1) and an\nunknown language (L2) have an impact on the model representations of L2 speech\nsignals? To answer these questions, we present a novel experimental design\nbased on representational similarity analysis (RSA) to analyze acoustic word\nembeddings (AWEs) -- vector representations of variable-duration spoken-word\nsegments. First, we train monolingual AWE models on seven Indo-European\nlanguages with various degrees of typological similarity. We then employ RSA to\nquantify the cross-lingual similarity by simulating native and non-native\nspoken-word processing using AWEs. Our experiments show that typological\nsimilarity indeed affects the representational similarity of the models in our\nstudy. We further discuss the implications of our work on modeling speech\nprocessing and language similarity with neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1\">Badr M. Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaitova_I/0/1/0/all/0/1\">Iuliia Zaitova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avgustinova_T/0/1/0/all/0/1\">Tania Avgustinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobius_B/0/1/0/all/0/1\">Bernd M&#xf6;bius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranslateLocally: Blazing-fast translation running on the local CPU. (arXiv:2109.10194v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10194","description":"<p>Every day, millions of people sacrifice their privacy and browsing habits in\nexchange for online machine translation. Companies and governments with\nconfidentiality requirements often ban online translation or pay a premium to\ndisable logging. To bring control back to the end user and demonstrate speed,\nwe developed translateLocally. Running locally on a desktop or laptop CPU,\ntranslateLocally delivers cloud-like translation speed and quality even on 10\nyear old hardware. The open-source software is based on Marian and runs on\nLinux, Windows, and macOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linde_J/0/1/0/all/0/1\">Jelmer Van der Linde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Source, Two Targets: Challenges and Rewards of Dual Decoding. (arXiv:2109.10197v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10197","description":"<p>Machine translation is generally understood as generating one target text\nfrom an input source document. In this paper, we consider a stronger\nrequirement: to jointly generate two texts so that each output side effectively\ndepends on the other. As we discuss, such a device serves several practical\npurposes, from multi-target machine translation to the generation of controlled\nvariations of the target text. We present an analysis of possible\nimplementations of dual decoding, and experiment with four applications.\nViewing the problem from multiple angles allows us to better highlight the\nchallenges of dual decoding and to also thoroughly analyze the benefits of\ngenerating matched, rather than independent, translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blindness to Modality Helps Entailment Graph Mining. (arXiv:2109.10227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10227","description":"<p>Understanding linguistic modality is widely seen as important for downstream\ntasks such as Question Answering and Knowledge Graph Population. Entailment\nGraph learning might also be expected to benefit from attention to modality. We\nbuild Entailment Graphs using a news corpus filtered with a modality parser,\nand show that stripping modal modifiers from predicates in fact increases\nperformance. This suggests that for some tasks, the pragmatics of modal\nmodification of predicates allows them to contribute as evidence of entailment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vroe_S/0/1/0/all/0/1\">Sander Bijl de Vroe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets. (arXiv:2109.10234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10234","description":"<p>We introduce BERTweetFR, the first large-scale pre-trained language model for\nFrench tweets. Our model is initialized using the general-domain French\nlanguage model CamemBERT which follows the base architecture of RoBERTa.\nExperiments show that BERTweetFR outperforms all previous general-domain French\nlanguage models on two downstream Twitter NLP tasks of offensiveness\nidentification and named entity recognition. The dataset used in the\noffensiveness detection task is first created and annotated by our team,\nfilling in the gap of such analytic datasets in French. We make our model\npublicly available in the transformers library with the aim of promoting future\nresearch in analytic tasks for French tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanzhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1\">Virgile Rennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xypolopoulos_C/0/1/0/all/0/1\">Christos Xypolopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10246","description":"<p>Linguistic representations derived from text alone have been criticized for\ntheir lack of grounding, i.e., connecting words to their meanings in the\nphysical world. Vision-and-Language (VL) models, trained jointly on text and\nimage or video data, have been offered as a response to such criticisms.\nHowever, while VL pretraining has shown success on multimodal tasks such as\nvisual question answering, it is not yet known how the internal linguistic\nrepresentations themselves compare to their text-only counterparts. This paper\ncompares the semantic representations learned via VL vs. text-only pretraining\nfor two recent VL models using a suite of analyses (clustering, probing, and\nperformance on a commonsense question answering task) in a language-only\nsetting. We find that the multimodal models fail to significantly outperform\nthe text-only variants, suggesting that future work is required if multimodal\npretraining is to be pursued as a means of improving NLP in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1\">Tian Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiomer: A Convolutional Transformer for Keyword Spotting. (arXiv:2109.10252v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10252","description":"<p>Transformers have seen an unprecedented rise in Natural Language Processing\nand Computer Vision tasks. However, in audio tasks, they are either infeasible\nto train due to extremely large sequence length of audio waveforms or reach\ncompetitive performance after feature extraction through Fourier-based methods,\nincurring a loss-floor. In this work, we introduce an architecture, Audiomer,\nwhere we combine 1D Residual Networks with Performer Attention to achieve\nstate-of-the-art performance in Keyword Spotting with raw audio waveforms,\nout-performing all previous methods while also being computationally cheaper,\nmuch more parameter and data-efficient. Audiomer allows for deployment in\ncompute-constrained devices and training on smaller datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamdar_J/0/1/0/all/0/1\">Juhi Kamdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Meet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10274","description":"<p>In this paper, we connect language model adaptation with concepts of machine\nlearning theory. We consider a training setup with a large out-of-domain set\nand a small in-domain set. As a first contribution, we derive how the benefit\nof training a model on either set depends on the size of the sets and the\ndistance between their underlying distribution. As a second contribution, we\npresent how the most popular data selection techniques -- importance sampling,\nintelligent data selection and influence functions -- can be presented in a\ncommon framework which highlights their similarity and also their subtle\ndifferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From English to Signal Temporal Logic. (arXiv:2109.10294v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10294","description":"<p>Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n</p>\n<p>In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickovic_D/0/1/0/all/0/1\">Dejan Ni&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isakovic_H/0/1/0/all/0/1\">Haris Isakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10341","description":"<p>Document-level neural machine translation (DocNMT) delivers coherent\ntranslations by incorporating cross-sentence context. However, for most\nlanguage pairs there's a shortage of parallel documents, although parallel\nsentences are readily available. In this paper, we study whether and how\ncontextual modeling in DocNMT is transferable from sentences to documents in a\nzero-shot fashion (i.e. no parallel documents for student languages) through\nmultilingual modeling. Using simple concatenation-based DocNMT, we explore the\neffect of 3 factors on multilingual transfer: the number of document-supervised\nteacher languages, the data schedule for parallel documents at training, and\nthe data condition of parallel documents (genuine vs. backtranslated). Our\nexperiments on Europarl-7 and IWSLT-10 datasets show the feasibility of\nmultilingual transfer for DocNMT, particularly on document-specific metrics. We\nobserve that more teacher languages and adequate data schedule both contribute\nto better transfer quality. Surprisingly, the transfer is less sensitive to the\ndata condition and multilingual DocNMT achieves comparable performance with\nboth back-translated and genuine document pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabirmoghaddam_A/0/1/0/all/0/1\">Ali Dabirmoghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arivazhagan_N/0/1/0/all/0/1\">Naveen Arivazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Guided Pre-Training for Open-Domain Question Answering. (arXiv:2109.10346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10346","description":"<p>Answering complex open-domain questions requires understanding the latent\nrelations between involving entities. However, we found that the existing QA\ndatasets are extremely imbalanced in some types of relations, which hurts the\ngeneralization performance over questions with long-tail relations. To remedy\nthis problem, in this paper, we propose a Relation-Guided Pre-Training\n(RGPT-QA) framework. We first generate a relational QA dataset covering a wide\nrange of relations from both the Wikidata triplets and Wikipedia hyperlinks. We\nthen pre-train a QA model to infer the latent relations from the question, and\nthen conduct extractive QA to get the target answer entity. We demonstrate that\nby pretraining with propoed RGPT-QA techique, the popular open-domain QA model,\nDense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute\nimprovement in Exact Match accuracy on Natural Questions, TriviaQA, and\nWebQuestions. Particularly, we show that RGPT-QA improves significantly on\nquestions with long-tail relations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informed Sampling for Diversity in Concept-to-Text NLG. (arXiv:2004.14364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14364","description":"<p>Deep-learning models for language generation tasks tend to produce repetitive\noutput. Various methods have been proposed to encourage lexical diversity\nduring decoding, but this often comes at a cost to the perceived fluency and\nadequacy of the output. In this work, we propose to ameliorate this cost by\nusing an Imitation Learning approach to explore the level of diversity that a\nlanguage generation model can reliably produce. Specifically, we augment the\ndecoding process with a meta-classifier trained to distinguish which words at\nany given timestep will lead to high-quality output. We focus our experiments\non concept-to-text generation where models are sensitive to the inclusion of\nirrelevant words due to the strict relation between input and output. Our\nanalysis shows that previous methods for diversity underperform in this\nsetting, while human evaluation suggests that our proposed method achieves a\nhigh level of diversity with minimal effect to the output's fluency and\nadequacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models. (arXiv:2009.13267v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.13267","description":"<p>The discrepancy between maximum likelihood estimation (MLE) and task measures\nsuch as BLEU score has been studied before for autoregressive neural machine\ntranslation (NMT) and resulted in alternative training algorithms (Ranzato et\nal., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,\nMLE training remains the de facto approach for autoregressive NMT because of\nits computational efficiency and stability. Despite this mismatch between the\ntraining objective and task measure, we notice that the samples drawn from an\nMLE-based trained NMT support the desired distribution -- there are samples\nwith much higher BLEU score comparing to the beam decoding output. To benefit\nfrom this observation, we train an energy-based model to mimic the behavior of\nthe task measure (i.e., the energy-based model assigns lower energy to samples\nwith higher BLEU score), which is resulted in a re-ranking algorithm based on\nthe samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal\nenergy models (over target sentence) and joint energy models (over both source\nand target sentences). Our EBR with the joint energy model consistently\nimproves the performance of the Transformer-based NMT: +4 BLEU points on\nIWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on\nWMT'16 English-German tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Sumanta Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rooshenas_A/0/1/0/all/0/1\">Amirmohammad Rooshenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Subhajit Naskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not all parameters are born equal: Attention is mostly what you need. (arXiv:2010.11859v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11859","description":"<p>Transformers are widely used in state-of-the-art machine translation, but the\nkey to their success is still unknown. To gain insight into this, we consider\nthree groups of parameters: embeddings, attention, and feed forward neural\nnetwork (FFN) layers. We examine the relative importance of each by performing\nan ablation study where we initialise them at random and freeze them, so that\ntheir weights do not change over the course of the training. Through this, we\nshow that the attention and FFN are equally important and fulfil the same\nfunctionality in a model. We show that the decision about whether a component\nis frozen or allowed to train is at least as important for the final model\nperformance as its number of parameters. At the same time, the number of\nparameters alone is not indicative of a component's importance. Finally, while\nthe embedding layer is the least essential for machine translation tasks, it is\nthe most important component for language modelling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation. (arXiv:2101.00421v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00421","description":"<p>Machine translation systems are vulnerable to domain mismatch, especially in\na low-resource scenario. Out-of-domain translations are often of poor quality\nand prone to hallucinations, due to exposure bias and the decoder acting as a\nlanguage model. We adopt two approaches to alleviate this problem: lexical\nshortlisting restricted by IBM statistical alignments, and hypothesis\nre-ranking based on similarity. The methods are computationally cheap, widely\nknown, but not extensively experimented on domain adaptation. We demonstrate\nsuccess on low-resource out-of-domain test sets, however, the methods are\nineffective when there is sufficient data or too great domain mismatch. This is\ndue to both the IBM model losing its advantage over the implicitly learned\nneural alignment, and issues with subword segmentation of out-of-domain words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Retrieval Conversational Machine Reading. (arXiv:2102.08633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08633","description":"<p>In conversational machine reading, systems need to interpret natural language\nrules, answer high-level questions such as \"May I qualify for VA health care\nbenefits?\", and ask follow-up clarification questions whose answer is necessary\nto answer the original question. However, existing works assume the rule text\nis provided for each user question, which neglects the essential retrieval step\nin real scenarios. In this work, we propose and investigate an open-retrieval\nsetting of conversational machine reading. In the open-retrieval setting, the\nrelevant rule texts are unknown so that a system needs to retrieve\nquestion-relevant evidence from a collection of rule texts, and answer users'\nhigh-level questions according to multiple retrieved rule texts in a\nconversational manner. We propose MUDERN, a Multi-passage Discourse-aware\nEntailment Reasoning Network which extracts conditions in the rule texts\nthrough discourse segmentation, conducts multi-passage entailment reasoning to\nanswer user questions directly, or asks clarification follow-up questions to\ninquiry more information. On our created OR-ShARC dataset, MUDERN achieves the\nstate-of-the-art performance, outperforming existing single-passage\nconversational machine reading models as well as a new multi-passage\nconversational machine reading baseline by a large margin. In addition, we\nconduct in-depth analyses to provide new insights into this new setting and our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02258","description":"<p>Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders. (arXiv:2104.03630v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03630","description":"<p>Powerful sentence encoders trained for multiple languages are on the rise.\nThese systems are capable of embedding a wide range of linguistic properties\ninto vector representations. While explicit probing tasks can be used to verify\nthe presence of specific linguistic properties, it is unclear whether the\nvector representations can be manipulated to indirectly steer such properties.\nFor efficient learning, we investigate the use of a geometric mapping in\nembedding space to transform linguistic properties, without any tuning of the\npre-trained sentence encoder or decoder. We validate our approach on three\nlinguistic properties using a pre-trained multilingual autoencoder and analyze\nthe results in both monolingual and cross-lingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1\">Maarten De Raedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1\">Fr&#xe9;deric Godin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buteneers_P/0/1/0/all/0/1\">Pieter Buteneers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Condenser: a Pre-training Architecture for Dense Retrieval. (arXiv:2104.08253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08253","description":"<p>Pre-trained Transformer language models (LM) have become go-to text\nrepresentation encoders. Prior research fine-tunes deep LMs to encode text\nsequences such as sentences and passages into single dense vector\nrepresentations for efficient text comparison and retrieval. However, dense\nencoders require a lot of data and sophisticated techniques to effectively\ntrain and suffer in low data situations. This paper finds a key reason is that\nstandard LMs' internal attention structure is not ready-to-use for dense\nencoders, which needs to aggregate text information into the dense\nrepresentation. We propose to pre-train towards dense encoder with a novel\nTransformer architecture, Condenser, where LM prediction CONditions on DENSE\nRepresentation. Our experiments show Condenser improves over standard LM by\nlarge margins on various text retrieval and similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12227","description":"<p>Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12265","description":"<p>This paper provides a new approach for offensive language and hate speech\ndetection on social media. Our approach incorporates an offensive lexicon\ncomposed of implicit and explicit offensive and swearing expressions annotated\nwith binary classes: context-dependent and context-independent offensive. Due\nto the severity of the hate speech and offensive comments in Brazil, and the\nlack of research in Portuguese, Brazilian Portuguese is the language used to\nvalidate the proposed method. Nevertheless, our proposal may be applied to any\nother language or domain. Based on the obtained results, the proposed approach\nshowed high-performance overcoming the current baselines for European and\nBrazilian Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction. (arXiv:2105.15033v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.15033","description":"<p>Knowledge Graph has been proven effective in modeling structured information\nand conceptual knowledge, especially in the medical domain. However, the lack\nof high-quality annotated corpora remains a crucial problem for advancing the\nresearch and applications on this task. In order to accelerate the research for\ndomain-specific knowledge graphs in the medical domain, we introduce DiaKG, a\nhigh-quality Chinese dataset for Diabetes knowledge graph, which contains\n22,050 entities and 6,890 relations in total. We implement recent typical\nmethods for Named Entity Recognition and Relation Extraction as a benchmark to\nevaluate the proposed dataset thoroughly. Empirical results show that the DiaKG\nis challenging for most existing methods and further analysis is conducted to\ndiscuss future research direction for improvements. We hope the release of this\ndataset can assist the construction of diabetes knowledge graphs and facilitate\nAI-based applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaozhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaobin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Ji Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Personalized Dialogue Generation with Contrastive Learning. (arXiv:2106.07857v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07857","description":"<p>Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nBilateral Personalized Dialogue Generation (BPDG) method for dyadic\nconversation, which integrates user and robot personas into dialogue generation\nvia designing a dynamic persona-aware fusion method. To bridge the gap between\nthe learning objective function and evaluation metrics, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted with contrastive learning to\nselect the proper response from the generated candidates. Moreover, a bilateral\npersona accuracy metric is designed to measure the degree of bilateral\npersonalization. Experimental results demonstrate that, compared with several\nstate-of-the-art methods, the final results of the proposed method are more\npersonalized and consistent with bilateral personas in terms of both automatic\nand manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanjun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09700","description":"<p>Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1\">Rahul Nadkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You should evaluate your language model on marginal likelihood over tokenisations. (arXiv:2109.02550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02550","description":"<p>Neural language models typically tokenise input text into sub-word units to\nachieve an open vocabulary. The standard approach is to use a single canonical\ntokenisation at both train and test time. We suggest that this approach is\nunsatisfactory and may bottleneck our evaluation of language model performance.\nUsing only the one-best tokenisation ignores tokeniser uncertainty over\nalternative tokenisations, which may hurt model out-of-domain performance.\n</p>\n<p>In this paper, we argue that instead, language models should be evaluated on\ntheir marginal likelihood over tokenisations. We compare different estimators\nfor the marginal likelihood based on sampling, and show that it is feasible to\nestimate the marginal likelihood with a manageable number of samples. We then\nevaluate pretrained English and German language models on both the\none-best-tokenisation and marginal perplexities, and show that the marginal\nperplexity can be significantly better than the one best, especially on\nout-of-domain data. We link this difference in perplexity to the tokeniser\nuncertainty as measured by tokeniser entropy. We discuss some implications of\nour results for language model training and evaluation, particularly with\nregard to tokenisation robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimell_L/0/1/0/all/0/1\">Laura Rimell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08828","description":"<p>Empathy is a complex cognitive ability based on the reasoning of others'\naffective states. In order to better understand others and express stronger\nempathy in dialogues, we argue that two issues must be tackled at the same\ntime: (i) identifying which word is the cause for the other's emotion from his\nor her utterance and (ii) reflecting those specific words in the response\ngeneration. However, previous approaches for recognizing emotion cause words in\ntext require sub-utterance level annotations, which can be demanding. Taking\ninspiration from social cognition, we leverage a generative estimator to infer\nemotion cause words from utterances with no word-level label. Also, we\nintroduce a novel method based on pragmatics to make dialogue models focus on\ntargeted words in the input during generation. Our method is applicable to any\ndialogue models with no additional training on the fly. We show our approach\nimproves multiple best-performing dialogue agents on generating more focused\nempathetic responses in terms of both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09105","description":"<p>Language Models (LMs) have been ubiquitously leveraged in various tasks\nincluding spoken language understanding (SLU). Spoken language requires careful\nunderstanding of speaker interactions, dialog states and speech induced\nmultimodal behaviors to generate a meaningful representation of the\nconversation. In this work, we propose to dissect SLU into three representative\nproperties:conversational (disfluency, pause, overtalk), channel (speaker-type,\nturn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based\nlanguage models (BERT, RoBERTa) trained on spoken transcripts to investigate\nits ability to understand multifarious properties in absence of any speech\ncues. Empirical results indicate that LM is surprisingly good at capturing\nconversational properties such as pause prediction and overtalk detection from\nlexical tokens. On the downsides, the LM scores low on turn-tasks and ASR\nerrors predictions. Additionally, pre-training the LM on spoken transcripts\nrestrain its linguistic understanding. Finally, we establish the efficacy and\ntransferability of the mentioned properties on two benchmark datasets:\nSwitchboard Dialog Act and Disfluency datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_M/0/1/0/all/0/1\">Mukuntha Narayanan Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepa_J/0/1/0/all/0/1\">Jithendra Vepa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09195","description":"<p>Current pre-trained models applied to summarization are prone to factual\ninconsistencies which either misrepresent the source text or introduce\nextraneous information. Thus, comparing the factual consistency of summaries is\nnecessary as we develop improved models. However, the optimal human evaluation\nsetup for factual consistency has not been standardized. To address this issue,\nwe crowdsourced evaluations for factual consistency using the rating-based\nLikert scale and ranking-based Best-Worst Scaling protocols, on 100 articles\nfrom each of the CNN-Daily Mail and XSum datasets over four state-of-the-art\nmodels, to determine the most reliable evaluation framework. We find that\nranking-based protocols offer a more reliable measure of summary quality across\ndatasets, while the reliability of Likert ratings depends on the target dataset\nand the evaluation design. Our crowdsourcing templates and summary evaluations\nwill be publicly available to facilitate future research on factual consistency\nin summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MetaMedSeg: Volumetric Meta-learning for Few-Shot Organ Segmentation. (arXiv:2109.09734v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09734","description":"<p>The lack of sufficient annotated image data is a common issue in medical\nimage segmentation. For some organs and densities, the annotation may be\nscarce, leading to poor model training convergence, while other organs have\nplenty of annotated data. In this work, we present MetaMedSeg, a gradient-based\nmeta-learning algorithm that redefines the meta-learning task for the\nvolumetric medical data with the goal to capture the variety between the\nslices. We also explore different weighting schemes for gradients aggregation,\narguing that different tasks might have different complexity, and hence,\ncontribute differently to the initialization. We propose an importance-aware\nweighting scheme to train our model. In the experiments, we present an\nevaluation of the medical decathlon dataset by extracting 2D slices from CT and\nMRI volumes of different organs and performing semantic segmentation. The\nresults show that our proposed volumetric task definition leads to up to 30%\nimprovement in terms of IoU compared to related baselines. The proposed update\nrule is also shown to improve the performance for complex scenarios where the\ndata distribution of the target organ is very different from the source organs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Makarevich_A/0/1/0/all/0/1\">Anastasia Makarevich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling. (arXiv:2109.09735v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09735","description":"<p>Domain adaptation typically requires to access source domain data to utilize\ntheir distribution information for domain alignment with the target data.\nHowever, in many real-world scenarios, the source data may not be accessible\nduring the model adaptation in the target domain due to privacy issue. This\npaper studies the practical yet challenging source-free unsupervised domain\nadaptation problem, in which only an existing source model and the unlabeled\ntarget data are available for model adaptation. We present a novel denoised\npseudo-labeling method for this problem, which effectively makes use of the\nsource model and unlabeled target data to promote model self-adaptation from\npseudo labels. Importantly, considering that the pseudo labels generated from\nsource model are inevitably noisy due to domain shift, we further introduce two\ncomplementary pixel-level and class-level denoising schemes with uncertainty\nestimation and prototype estimation to reduce noisy pseudo labels and select\nreliable ones to enhance the pseudo-labeling efficacy. Experimental results on\ncross-domain fundus image segmentation show that without using any source\nimages or altering source training, our approach achieves comparable or even\nhigher performance than state-of-the-art source-dependent unsupervised domain\nadaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quande Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation with Semantic Consistency across Heterogeneous Modalities for MRI Prostate Lesion Segmentation. (arXiv:2109.09736v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09736","description":"<p>Any novel medical imaging modality that differs from previous protocols e.g.\nin the number of imaging channels, introduces a new domain that is\nheterogeneous from previous ones. This common medical imaging scenario is\nrarely considered in the domain adaptation literature, which handles shifts\nacross domains of the same dimensionality. In our work we rely on stochastic\ngenerative modeling to translate across two heterogeneous domains at pixel\nspace and introduce two new loss functions that promote semantic consistency.\nFirstly, we introduce a semantic cycle-consistency loss in the source domain to\nensure that the translation preserves the semantics. Secondly, we introduce a\npseudo-labelling loss, where we translate target data to source, label them by\na source-domain network, and use the generated pseudo-labels to supervise the\ntarget-domain network. Our results show that this allows us to extract\nsystematically better representations for the target domain. In particular, we\naddress the challenge of enhancing performance on VERDICT-MRI, an advanced\ndiffusion-weighted imaging technique, by exploiting labeled mp-MRI data. When\ncompared to several unsupervised domain adaptation approaches, our approach\nyields substantial improvements, that consistently carry over to the\nsemi-supervised and supervised learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chiou_E/0/1/0/all/0/1\">Eleni Chiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giganti_F/0/1/0/all/0/1\">Francesco Giganti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punwani_S/0/1/0/all/0/1\">Shonit Punwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokkinos_I/0/1/0/all/0/1\">Iasonas Kokkinos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panagiotaki_E/0/1/0/all/0/1\">Eleftheria Panagiotaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities. (arXiv:2109.09738v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09738","description":"<p>Goal: This work aims at developing a novel calibration-free fast parallel MRI\n(pMRI) reconstruction method incorporate with discrete-time optimal control\nframework. The reconstruction model is designed to learn a regularization that\ncombines channels and extracts features by leveraging the information sharing\namong channels of multi-coil images. We propose to recover both magnitude and\nphase information by taking advantage of structured multiplayer convolutional\nnetworks in image and Fourier spaces. Methods: We develop a novel variational\nmodel with a learnable objective function that integrates an adaptive\nmulti-coil image combination operator and effective image regularization in the\nimage and Fourier spaces. We cast the reconstruction network as a structured\ndiscrete-time optimal control system, resulting in an optimal control\nformulation of parameter training where the parameters of the objective\nfunction play the role of control variables. We demonstrate that the Lagrangian\nmethod for solving the control problem is equivalent to back-propagation,\nensuring the local convergence of the training algorithm. Results: We conduct a\nlarge number of numerical experiments of the proposed method with comparisons\nto several state-of-the-art pMRI reconstruction networks on real pMRI datasets.\nThe numerical results demonstrate the promising performance of the proposed\nmethod evidently. Conclusion: The proposed method provides a general deep\nnetwork design and training framework for efficient joint-channel pMRI\nreconstruction. Significance: By learning multi-coil image combination operator\nand performing regularizations in both image domain and k-space domain, the\nproposed method achieves a highly efficient image reconstruction network for\npMRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bian_W/0/1/0/all/0/1\">Wanyu Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multifield Cosmology with Artificial Intelligence. (arXiv:2109.09747v1 [astro-ph.CO])","link":"http://arxiv.org/abs/2109.09747","description":"<p>Astrophysical processes such as feedback from supernovae and active galactic\nnuclei modify the properties and spatial distribution of dark matter, gas, and\ngalaxies in a poorly understood way. This uncertainty is one of the main\ntheoretical obstacles to extract information from cosmological surveys. We use\n2,000 state-of-the-art hydrodynamic simulations from the CAMELS project\nspanning a wide variety of cosmological and astrophysical models and generate\nhundreds of thousands of 2-dimensional maps for 13 different fields: from dark\nmatter to gas and stellar properties. We use these maps to train convolutional\nneural networks to extract the maximum amount of cosmological information while\nmarginalizing over astrophysical effects at the field level. Although our maps\nonly cover a small area of $(25~h^{-1}{\\rm Mpc})^2$, and the different fields\nare contaminated by astrophysical effects in very different ways, our networks\ncan infer the values of $\\Omega_{\\rm m}$ and $\\sigma_8$ with a few percent\nlevel precision for most of the fields. We find that the marginalization\nperformed by the network retains a wealth of cosmological information compared\nto a model trained on maps from gravity-only N-body simulations that are not\ncontaminated by astrophysical effects. Finally, we train our networks on\nmultifields -- 2D maps that contain several fields as different colors or\nchannels -- and find that not only they can infer the value of all parameters\nwith higher accuracy than networks trained on individual fields, but they can\nconstrain the value of $\\Omega_{\\rm m}$ with higher accuracy than the maps from\nthe N-body simulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Angles_Alcazar_D/0/1/0/all/0/1\">Daniel Angl&#xe9;s-Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Genel_S/0/1/0/all/0/1\">Shy Genel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Spergel_D/0/1/0/all/0/1\">David N. Spergel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wandelt_B/0/1/0/all/0/1\">Benjamin Wandelt</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nicola_A/0/1/0/all/0/1\">Andrina Nicola</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Thiele_L/0/1/0/all/0/1\">Leander Thiele</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hassan_S/0/1/0/all/0/1\">Sultan Hassan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Matilla_J/0/1/0/all/0/1\">Jose Manuel Zorrilla Matilla</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Narayanan_D/0/1/0/all/0/1\">Desika Narayanan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dave_R/0/1/0/all/0/1\">Romeel Dave</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Vogelsberger_M/0/1/0/all/0/1\">Mark Vogelsberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09808","description":"<p>The structural network of the brain, or structural connectome, can be\nrepresented by fiber bundles generated by a variety of tractography methods.\nWhile such methods give qualitative insights into brain structure, there is\ncontroversy over whether they can provide quantitative information, especially\nat the population level. In order to enable population-level statistical\nanalysis of the structural connectome, we propose representing a connectome as\na Riemannian metric, which is a point on an infinite-dimensional manifold. We\nequip this manifold with the Ebin metric, a natural metric structure for this\nspace, to get a Riemannian manifold along with its associated geometric\nproperties. We then use this Riemannian framework to apply object-oriented\nstatistical analysis to define an atlas as the Fr\\'echet mean of a population\nof Riemannian metrics. This formulation ties into the existing framework for\ndiffeomorphic construction of image atlases, allowing us to construct a\nmultimodal atlas by simultaneously integrating complementary white matter\nstructure details from DWMRI and cortical details from T1-weighted MRI. We\nillustrate our framework with 2D data examples of connectome registration and\natlas formation. Finally, we build an example 3D multimodal atlas using T1\nimages and connectomes derived from diffusion tensors estimated from a subset\nof subjects from the Human Connectome Project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campbell_K/0/1/0/all/0/1\">Kristen M. Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haocheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhe Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Martin Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_P/0/1/0/all/0/1\">P. Thomas Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sarang C. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma and other skin lesions, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images.\nContributions of this work include the application of different debiasing\ntechniques for artefact bias removal and the concept of instrument bias\nunlearning for domain generalisation in melanoma detection. Our experimental\nresults provide evidence that the effects of each of the aforementioned biases\nare notably reduced, with different debiasing techniques excelling at different\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09824","description":"<p>This paper investigates the effectiveness of systematically probing Google\nTrendsagainst textual translations of visual aspects as exogenous knowledge to\npredict the sales of brand-new fashion items, where past sales data is not\navailable, but only an image and few metadata are available. In particular, we\npropose GTM-Transformer, standing for Google Trends Multimodal Transformer,\nwhose encoder works on the representation of the exogenous time series, while\nthe decoder forecasts the sales using the Google Trends encoding, and the\navailable visual and metadata information. Our model works in a\nnon-autoregressive manner, avoiding the compounding effect of the first-step\nerrors. As a second contribution, we present the VISUELLE dataset, which is the\nfirst publicly available dataset for the task of new fashion product sales\nforecasting, containing the sales of 5577 new products sold between 2016-2019,\nderived from genuine historical data ofNunalie, an Italian fast-fashion\ncompany. Our dataset is equipped with images of products, metadata, related\nsales, and associated Google Trends. We use VISUELLE to compare our approach\nagainst state-of-the-art alternatives and numerous baselines, showing that\nGTM-Transformer is the most accurate in terms of both percentage and absolute\nerror. It is worth noting that the addition of exogenous knowledge boosts the\nforecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting\nGoogle Trends. The code and dataset are both available at\nhttps://github.com/HumaticsLAB/GTM-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1\">Matteo Denitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viewpoint Invariant Dense Matching for Visual Geolocalization. (arXiv:2109.09827v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09827","description":"<p>In this paper we propose a novel method for image matching based on dense\nlocal features and tailored for visual geolocalization. Dense local features\nmatching is robust against changes in illumination and occlusions, but not\nagainst viewpoint shifts which are a fundamental aspect of geolocalization. Our\nmethod, called GeoWarp, directly embeds invariance to viewpoint shifts in the\nprocess of extracting dense features. This is achieved via a trainable module\nwhich learns from the data an invariance that is meaningful for the task of\nrecognizing places. We also devise a new self-supervised loss and two new\nweakly supervised losses to train this module using only unlabeled data and\nweak labels. GeoWarp is implemented efficiently as a re-ranking method that can\nbe easily embedded into pre-existing visual geolocalization pipelines.\nExperimental validation on standard geolocalization benchmarks demonstrates\nthat GeoWarp boosts the accuracy of state-of-the-art retrieval architectures.\nThe code and trained models are available at\nhttps://github.com/gmberton/geo_warp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paolicelli_V/0/1/0/all/0/1\">Valerio Paolicelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced-MixUp for Highly Imbalanced Medical Image Classification. (arXiv:2109.09850v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09850","description":"<p>Highly imbalanced datasets are ubiquitous in medical image classification\nproblems. In such problems, it is often the case that rare classes associated\nto less prevalent diseases are severely under-represented in labeled databases,\ntypically resulting in poor performance of machine learning algorithms due to\noverfitting in the learning process. In this paper, we propose a novel\nmechanism for sampling training data based on the popular MixUp regularization\ntechnique, which we refer to as Balanced-MixUp. In short, Balanced-MixUp\nsimultaneously performs regular (i.e., instance-based) and balanced (i.e.,\nclass-based) sampling of the training data. The resulting two sets of samples\nare then mixed-up to create a more balanced training distribution from which a\nneural network can effectively learn without incurring in heavily under-fitting\nthe minority classes. We experiment with a highly imbalanced dataset of retinal\nimages (55K samples, 5 classes) and a long-tail dataset of gastro-intestinal\nvideo frames (10K images, 23 classes), using two CNNs of varying representation\ncapabilities. Experimental results demonstrate that applying Balanced-MixUp\noutperforms other conventional sampling schemes and loss functions specifically\ndesigned to deal with imbalanced data. Code is released at\nhttps://github.com/agaldran/balanced_mixup .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel A. Gonz&#xe1;lez Ballester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS). (arXiv:2109.09854v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09854","description":"<p>Object detection in thermal infrared spectrum provides more reliable data\nsource in low-lighting conditions and different weather conditions, as it is\nuseful both in-cabin and outside for pedestrian, animal, and vehicular\ndetection as well as for detecting street-signs &amp; lighting poles. This paper is\nabout exploring and adapting state-of-the-art object detection and classifier\nframework on thermal vision with seven distinct classes for advanced\ndriver-assistance systems (ADAS). The trained network variants on public\ndatasets are validated on test data with three different test approaches which\ninclude test-time with no augmentation, test-time augmentation, and test-time\nwith model ensembling. Additionally, the efficacy of trained networks is tested\non locally gathered novel test-data captured with an uncooled LWIR prototype\nthermal camera in challenging weather and environmental scenarios. The\nperformance analysis of trained models is investigated by computing precision,\nrecall, and mean average precision scores (mAP). Furthermore, the trained model\narchitecture is optimized using TensorRT inference accelerator and deployed on\nresource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the\ninference time on GPU as well as edge devices for further real-time onboard\ninstallations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Ali Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotariu_C/0/1/0/all/0/1\">Cosmin Rotariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Depth Estimation with Geospatial Context. (arXiv:2109.09879v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09879","description":"<p>Modern cameras are equipped with a wide array of sensors that enable\nrecording the geospatial context of an image. Taking advantage of this, we\nexplore depth estimation under the assumption that the camera is geocalibrated,\na problem we refer to as geo-enabled depth estimation. Our key insight is that\nif capture location is known, the corresponding overhead viewpoint offers a\nvaluable resource for understanding the scale of the scene. We propose an\nend-to-end architecture for depth estimation that uses geospatial context to\ninfer a synthetic ground-level depth map from a co-located overhead image, then\nfuses it inside of an encoder/decoder style segmentation network. To support\nevaluation of our methods, we extend a recently released dataset with overhead\nimagery and corresponding height maps. Results demonstrate that integrating\ngeospatial context significantly reduces error compared to baselines, both at\nclose ranges and when evaluating at much larger distances than existing\nbenchmarks consider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Workman_S/0/1/0/all/0/1\">Scott Workman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanton_H/0/1/0/all/0/1\">Hunter Blanton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation. (arXiv:2109.09881v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09881","description":"<p>Surface normal estimation from a single image is an important task in 3D\nscene understanding. In this paper, we address two limitations shared by the\nexisting methods: the inability to estimate the aleatoric uncertainty and lack\nof detail in the prediction. The proposed network estimates the per-pixel\nsurface normal probability distribution. We introduce a new parameterization\nfor the distribution, such that its negative log-likelihood is the angular loss\nwith learned attenuation. The expected value of the angular error is then used\nas a measure of the aleatoric uncertainty. We also present a novel decoder\nframework where pixel-wise multi-layer perceptrons are trained on a subset of\npixels sampled based on the estimated uncertainty. The proposed\nuncertainty-guided sampling prevents the bias in training towards large planar\nsurfaces and improves the quality of prediction, especially near object\nboundaries and on small structures. Experimental results show that the proposed\nmethod outperforms the state-of-the-art in ScanNet and NYUv2, and that the\nestimated uncertainty correlates well with the prediction error. Code is\navailable at https://github.com/baegwangbin/surface_normal_uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_G/0/1/0/all/0/1\">Gwangbin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Distractors for Few-Shot Classification. (arXiv:2109.09883v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09883","description":"<p>Few-shot classification aims at classifying categories of a novel task by\nlearning from just a few (typically, 1 to 5) labelled examples. An effective\napproach to few-shot classification involves a prior model trained on a\nlarge-sample base domain, which is then finetuned over the novel few-shot task\nto yield generalizable representations. However, task-specific finetuning is\nprone to overfitting due to the lack of enough training examples. To alleviate\nthis issue, we propose a new finetuning approach based on contrastive learning\nthat reuses unlabelled examples from the base domain in the form of\ndistractors. Unlike the nature of unlabelled data used in prior works,\ndistractors belong to classes that do not overlap with the novel categories. We\ndemonstrate for the first time that inclusion of such distractors can\nsignificantly boost few-shot generalization. Our technical novelty includes a\nstochastic pairing of examples sharing the same category in the few-shot task\nand a weighting term that controls the relative influence of task-specific\nnegatives and distractors. An important aspect of our finetuning objective is\nthat it is agnostic to distractor labels and hence applicable to various base\ndomain settings. Compared to state-of-the-art approaches, our method shows\naccuracy gains of up to $12\\%$ in cross-domain and up to $5\\%$ in unsupervised\nprior-learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajshekhar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1\">Jos&#xe9;M.F. Moura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirDOS: Dynamic SLAM benefits from Articulated Objects. (arXiv:2109.09903v1 [cs.RO])","link":"http://arxiv.org/abs/2109.09903","description":"<p>Dynamic Object-aware SLAM (DOS) exploits object-level information to enable\nrobust motion estimation in dynamic environments. It has attracted increasing\nattention with the recent success of learning-based models. Existing methods\nmainly focus on identifying and excluding dynamic objects from the\noptimization. In this paper, we show that feature-based visual SLAM systems can\nalso benefit from the presence of dynamic articulated objects by taking\nadvantage of two observations: (1) The 3D structure of an articulated object\nremains consistent over time; (2) The points on the same object follow the same\nmotion. In particular, we present AirDOS, a dynamic object-aware system that\nintroduces rigidity and motion constraints to model articulated objects. By\njointly optimizing the camera pose, object motion, and the object 3D structure,\nwe can rectify the camera pose estimation, preventing tracking loss, and\ngenerate 4D spatio-temporal maps for both dynamic objects and static scenes.\nExperiments show that our algorithm improves the robustness of visual SLAM\nalgorithms in challenging crowded urban environments. To the best of our\nknowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating\nthat camera pose estimation can be improved by incorporating dynamic\narticulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuheng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henein_M/0/1/0/all/0/1\">Mina Henein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-based Human Motion Estimation and Synthesis from Videos. (arXiv:2109.09913v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09913","description":"<p>Human motion synthesis is an important problem with applications in graphics,\ngaming and simulation environments for robotics. Existing methods require\naccurate motion capture data for training, which is costly to obtain. Instead,\nwe propose a framework for training generative models of physically plausible\nhuman motion directly from monocular RGB videos, which are much more widely\navailable. At the core of our method is a novel optimization formulation that\ncorrects imperfect image-based pose estimations by enforcing physics\nconstraints and reasons about contacts in a differentiable way. This\noptimization yields corrected 3D poses and motions, as well as their\ncorresponding contact forces. Results show that our physically-corrected\nmotions significantly outperform prior work on pose estimation. We can then use\nthese to train a generative model to synthesize future motion. We demonstrate\nboth qualitatively and quantitatively significantly improved motion estimation,\nsynthesis quality and physical plausibility achieved by our method on the large\nscale Human3.6m dataset \\cite{h36m_pami} as compared to prior kinematic and\nphysics-based methods. By enabling learning of motion synthesis from video, our\nmethod paves the way for large-scale, realistic and diverse motion synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kevin Xie</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tingwu Wang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrong Guo</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1\">Florian Shkurti</a> (1) ((1) University of Toronto, (2) Nvidia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey: Transformer based Video-Language Pre-training. (arXiv:2109.09920v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09920","description":"<p>Inspired by the success of transformer-based pre-training methods on natural\nlanguage tasks and further computer vision tasks, researchers have begun to\napply transformer to video processing. This survey aims to give a comprehensive\noverview on transformer-based pre-training methods for Video-Language learning.\nWe first briefly introduce the transformer tructure as the background\nknowledge, including attention mechanism, position encoding etc. We then\ndescribe the typical paradigm of pre-training &amp; fine-tuning on Video-Language\nprocessing in terms of proxy tasks, downstream tasks and commonly used video\ndatasets. Next, we categorize transformer models into Single-Stream and\nMulti-Stream structures, highlight their innovations and compare their\nperformances. Finally, we analyze and discuss the current challenges and\npossible future research directions for Video-Language pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1\">Ludan Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning. (arXiv:2109.09923v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09923","description":"<p>The process of capturing a well-composed photo is difficult and it takes\nyears of experience to master. We propose a novel pipeline for an autonomous\nagent to automatically capture an aesthetic photograph by navigating within a\nlocal region in a scene. Instead of classical optimization over heuristics such\nas the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess\nphoto quality. A reinforcement learning framework is used to optimize the model\nwith respect to the learned aesthetics metric. We train our model in simulation\nwith indoor scenes, and we demonstrate that our system can capture aesthetic\nphotos in both simulation and real world environments on a ground robot. To our\nknowledge, this is the first system that can automatically explore an\nenvironment to capture an aesthetic photo with respect to a learned aesthetic\nestimator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlZayer_H/0/1/0/all/0/1\">Hadi AlZayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Estimation of Reflection Symmetry in Noisy and Partial 3D Point Clouds. (arXiv:2109.09927v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09927","description":"<p>Detecting the reflection symmetry plane of an object represented by a 3D\npoint cloud is a fundamental problem in 3D computer vision and geometry\nprocessing due to its various applications such as compression, object\ndetection, robotic grasping, 3D surface reconstruction, etc. There exist\nseveral efficient approaches for solving this problem for clean 3D point\nclouds. However, this problem becomes difficult to solve in the presence of\noutliers and missing parts due to occlusions while scanning the objects through\n3D scanners. The existing methods try to overcome these challenges mostly by\nvoting-based techniques but fail in challenging settings. In this work, we\npropose a statistical estimator for the plane of reflection symmetry that is\nrobust to outliers and missing parts. We pose the problem of finding the\noptimal estimator as an optimization problem on a 2-sphere that quickly\nconverges to the global solution. We further propose a 3D point descriptor that\nis invariant to 3D reflection symmetry using the spectral properties of the\ngeodesic distance matrix constructed from the neighbors of a point. This helps\nus in decoupling the chicken-and-egg problem of finding optimal symmetry plane\nand correspondences between the reflective symmetric points. We show that the\nproposed approach achieves the state-of-the-art performance on the benchmarks\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1\">Rajendra Nagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MESSFN : a Multi-level and Enhanced Spectral-Spatial Fusion Network for Pan-sharpening. (arXiv:2109.09937v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09937","description":"<p>Dominant pan-sharpening frameworks simply concatenate the MS stream and the\nPAN stream once at a specific level. This way of fusion neglects the\nmulti-level spectral-spatial correlation between the two streams, which is\nvital to improving the fusion performance. In consideration of this, we propose\na Multi-level and Enhanced Spectral-Spatial Fusion Network (MESSFN) with the\nfollowing innovations: First, to fully exploit and strengthen the above\ncorrelation, a Hierarchical Multi-level Fusion Architecture (HMFA) is carefully\ndesigned. A novel Spectral-Spatial (SS) stream is established to hierarchically\nderive and fuse the multi-level prior spectral and spatial expertise from the\nMS stream and the PAN stream. This helps the SS stream master a joint\nspectral-spatial representation in the hierarchical network for better modeling\nthe fusion relationship. Second, to provide superior expertise, consequently,\nbased on the intrinsic characteristics of the MS image and the PAN image, two\nfeature extraction blocks are specially developed. In the MS stream, a Residual\nSpectral Attention Block (RSAB) is proposed to mine the potential spectral\ncorrelations between different spectra of the MS image through adjacent\ncross-spectrum interaction. While in the PAN stream, a Residual Multi-scale\nSpatial Attention Block (RMSAB) is proposed to capture multi-scale information\nand reconstruct precise high-frequency details from the PAN image through an\nimproved spatial attention-based inception structure. The spectral and spatial\nfeature representations are enhanced. Extensive experiments on two datasets\ndemonstrate that the proposed network is competitive with or better than\nstate-of-the-art methods. Our code can be found in github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanlin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IgNet. A Super-precise Convolutional Neural Network. (arXiv:2109.09939v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09939","description":"<p>Convolutional neural networks (CNN) are known to be an effective means to\ndetect and analyze images. Their power is essentially based on the ability to\nextract out images common features. There exist, however, images involving\nunique, irregular features or details. Such is a collection of unusual children\ndrawings reflecting the kids imagination and individuality. These drawings were\nanalyzed by means of a CNN constructed by means of Keras-TensorFlow. The same\nproblem - on a significantly higher level - was solved with newly developed\nfamily of networks called IgNet that is described in this paper. It proved able\nto learn by 100 % all the categorical characteristics of the drawings. In the\ncase of a regression task (learning the young artists ages) IgNet performed\nwith an error of no more than 0.4 %. The principles are discussed of IgNet\ndesign that made it possible to reach such substantial results with rather\nsimple network topology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mackarov_I/0/1/0/all/0/1\">Igor Mackarov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Few-Shot Learning and Dataset for Agricultural Applications. (arXiv:2109.09952v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09952","description":"<p>Automatic classification of pests and plants (both healthy and diseased) is\nof paramount importance in agriculture to improve yield. Conventional deep\nlearning models based on convolutional neural networks require thousands of\nlabeled examples per category. In this work we propose a method to learn from a\nfew samples to automatically classify different pests, plants, and their\ndiseases, using Few-Shot Learning (FSL). We learn a feature extractor to\ngenerate embeddings and then update the embeddings using Transformers. Using\nMahalanobis distance, a class-covariance-based metric, we then calculate the\nsimilarity of the transformed embeddings with the embedding of the image to be\nclassified. Using our proposed architecture, we conduct extensive experiments\non multiple datasets showing the effectiveness of our proposed model. We\nconduct 42 experiments in total to comprehensively analyze the model and it\nachieves up to 14% and 24% performance gains on few-shot image classification\nbenchmarks on two datasets.\n</p>\n<p>We also compile a new FSL dataset containing images of healthy and diseased\nplants taken in real-world settings. Using our proposed architecture which has\nbeen shown to outperform several existing FSL architectures in agriculture, we\nprovide strong baselines on our newly proposed dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nuthalapati_S/0/1/0/all/0/1\">Sai Vidyaranya Nuthalapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunga_A/0/1/0/all/0/1\">Anirudh Tunga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09960","description":"<p>In this paper, we proposed a novel mutual consistency network (MC-Net+) to\neffectively exploit the unlabeled hard regions for semi-supervised medical\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\nmodels trained with limited annotations are prone to output highly uncertain\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\nedges or thin branches) for the image segmentation task. Leveraging these\nregion-level challenging samples can make the semi-supervised segmentation\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\nof two new designs. First, the model contains one shared encoder and multiple\nsightly different decoders (i.e. using different up-sampling strategies). The\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\nmutual consistency constraint is enforced between one decoder's probability\noutput and other decoders' soft pseudo labels. In this way, we minimize the\nmodel's uncertainty during training and force the model to generate invariant\nand low-entropy results in such challenging areas of unlabeled data, in order\nto learn a generalized feature representation. We compared the segmentation\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\nthree public medical datasets. Extension experiments with two common\nsemi-supervised settings demonstrate the superior performance of our model over\nother existing methods, which sets a new state of the art for semi-supervised\nmedical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Source Video Domain Adaptation with Temporal Attentive Moment Alignment. (arXiv:2109.09964v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09964","description":"<p>Multi-Source Domain Adaptation (MSDA) is a more practical domain adaptation\nscenario in real-world scenarios. It relaxes the assumption in conventional\nUnsupervised Domain Adaptation (UDA) that source data are sampled from a single\ndomain and match a uniform data distribution. MSDA is more difficult due to the\nexistence of different domain shifts between distinct domain pairs. When\nconsidering videos, the negative transfer would be provoked by spatial-temporal\nfeatures and can be formulated into a more challenging Multi-Source Video\nDomain Adaptation (MSVDA) problem. In this paper, we address the MSVDA problem\nby proposing a novel Temporal Attentive Moment Alignment Network (TAMAN) which\naims for effective feature transfer by dynamically aligning both spatial and\ntemporal feature moments. TAMAN further constructs robust global temporal\nfeatures by attending to dominant domain-invariant local temporal features with\nhigh local classification confidence and low disparity between global and local\nfeature discrepancies. To facilitate future research on the MSVDA problem, we\nintroduce comprehensive benchmarks, covering extensive MSVDA scenarios.\nEmpirical results demonstrate a superior performance of the proposed TAMAN\nacross multiple MSVDA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated segmentation and extraction of posterior eye segment using OCT scans. (arXiv:2109.10000v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10000","description":"<p>This paper proposes an automated method for the segmentation and extraction\nof the posterior segment of the human eye, including the vitreous, retina,\nchoroid, and sclera compartments, using multi-vendor optical coherence\ntomography (OCT) scans. The proposed method works in two phases. First extracts\nthe retinal pigment epithelium (RPE) layer by applying the adaptive\nthresholding technique to identify the retina-choroid junction. Then, it\nexploits the structure tensor guided approach to extract the inner limiting\nmembrane (ILM) and the choroidal stroma (CS) layers, locating the\nvitreous-retina and choroid-sclera junctions in the candidate OCT scan.\nFurthermore, these three junction boundaries are utilized to conduct posterior\neye compartmentalization effectively for both healthy and disease eye OCT\nscans. The proposed framework is evaluated over 1000 OCT scans, where it\nobtained the mean intersection over union (IoU) and mean Dice similarity\ncoefficient (DSC) scores of 0.874 and 0.930, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_R/0/1/0/all/0/1\">Ramsha Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_S/0/1/0/all/0/1\">Shiyin Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Abstract Reasoning for Raven's Problem Matrices. (arXiv:2109.10011v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10011","description":"<p>Raven's Progressive Matrices (RPM) is highly correlated with human\nintelligence, and it has been widely used to measure the abstract reasoning\nability of humans. In this paper, to study the abstract reasoning capability of\ndeep neural networks, we propose the first unsupervised learning method for\nsolving RPM problems. Since the ground truth labels are not allowed, we design\na pseudo target based on the prior constraints of the RPM formulation to\napproximate the ground truth label, which effectively converts the unsupervised\nlearning strategy into a supervised one. However, the correct answer is wrongly\nlabelled by the pseudo target, and thus the noisy contrast will lead to\ninaccurate model training. To alleviate this issue, we propose to improve the\nmodel performance with negative answers. Moreover, we develop a\ndecentralization method to adapt the feature representation to different RPM\nproblems. Extensive experiments on three datasets demonstrate that our method\neven outperforms some of the supervised approaches. Our code is available at\nhttps://github.com/visiontao/ncd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Tao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Action-Space Prediction for Automated Driving. (arXiv:2109.10024v1 [cs.RO])","link":"http://arxiv.org/abs/2109.10024","description":"<p>Making informed driving decisions requires reliable prediction of other\nvehicles' trajectories. In this paper, we present a novel learned multi-modal\ntrajectory prediction architecture for automated driving. It achieves\nkinematically feasible predictions by casting the learning problem into the\nspace of accelerations and steering angles -- by performing action-space\nprediction, we can leverage valuable model knowledge. Additionally, the\ndimensionality of the action manifold is lower than that of the state manifold,\nwhose intrinsically correlated states are more difficult to capture in a\nlearned manner. For the purpose of action-space prediction, we present the\nsimple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we\nbuild on this notion and introduce the novel Self-Supervised Action-Space\nPrediction (SSP-ASP) architecture that outputs future environment context\nfeatures in addition to trajectories. A key element in the self-supervised\narchitecture is that, based on an observed action history and past context\nfeatures, future context features are predicted prior to future trajectories.\nThe proposed methods are evaluated on real-world datasets containing urban\nintersections and roundabouts, and show accurate predictions, outperforming\nstate-of-the-art for kinematically feasible predictions in several prediction\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janjos_F/0/1/0/all/0/1\">Faris Janjo&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolgov_M/0/1/0/all/0/1\">Maxim Dolgov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPN: Video Provenance Network for Robust Content Attribution. (arXiv:2109.10038v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10038","description":"<p>We present VPN - a content attribution method for recovering provenance\ninformation from videos shared online. Platforms, and users, often transform\nvideo into different quality, codecs, sizes, shapes, etc. or slightly edit its\ncontent such as adding text or emoji, as they are redistributed online. We\nlearn a robust search embedding for matching such video, invariant to these\ntransformations, using full-length or truncated video queries. Once matched\nagainst a trusted database of video clips, associated information on the\nprovenance of the clip is presented to the user. We use an inverted index to\nmatch temporal chunks of video using late-fusion to combine both visual and\naudio features. In both cases, features are extracted via a deep neural network\ntrained using contrastive learning on a dataset of original and augmented video\nclips. We demonstrate high accuracy recall over a corpus of 100,000 videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alexander Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1\">Simon Jenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Person Pose Estimation: A Survey. (arXiv:2109.10056v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10056","description":"<p>Human pose estimation in unconstrained images and videos is a fundamental\ncomputer vision task. To illustrate the evolutionary path in technique, in this\nsurvey we summarize representative human pose methods in a structured taxonomy,\nwith a particular focus on deep learning models and single-person image\nsetting. Specifically, we examine and survey all the components of a typical\nhuman pose estimation pipeline, including data augmentation, model architecture\nand backbone, supervision representation, post-processing, standard datasets,\nevaluation metrics. To envisage the future directions, we finally discuss the\nkey unsolved problems and potential trends for human pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOTR: Face Landmark Localization Using Localization Transformer. (arXiv:2109.10057v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10057","description":"<p>This paper presents a novel Transformer-based facial landmark localization\nnetwork named Localization Transformer (LOTR). The proposed framework is a\ndirect coordinate regression approach leveraging a Transformer network to\nbetter utilize the spatial information in the feature map. An LOTR model\nconsists of three main modules: 1) a visual backbone that converts an input\nimage into a feature map, 2) a Transformer module that improves the feature\nrepresentation from the visual backbone, and 3) a landmark prediction head that\ndirectly predicts the landmark coordinates from the Transformer's\nrepresentation. Given cropped-and-aligned face images, the proposed LOTR can be\ntrained end-to-end without requiring any post-processing steps. This paper also\nintroduces the smooth-Wing loss function, which addresses the gradient\ndiscontinuity of the Wing loss, leading to better convergence than standard\nloss functions such as L1, L2, and Wing loss. Experimental results on the JD\nlandmark dataset provided by the First Grand Challenge of 106-Point Facial\nLandmark Localization indicate the superiority of LOTR over the existing\nmethods on the leaderboard and two recent heatmap-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommanna_B/0/1/0/all/0/1\">Benjaphan Sommanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sanjana Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1\">Pavit Noinongyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samacoits_A/0/1/0/all/0/1\">Aubin Samacoits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritrakool_N/0/1/0/all/0/1\">Nakarin Sritrakool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers. (arXiv:2109.10060v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10060","description":"<p>Dynamic networks have shown their promising capability in reducing\ntheoretical computation complexity by adapting their architectures to the input\nduring inference. However, their practical runtime usually lags behind the\ntheoretical acceleration due to inefficient sparsity. Here, we explore a\nhardware-efficient dynamic inference regime, named dynamic weight slicing,\nwhich adaptively slice a part of network parameters for inputs with diverse\ndifficulty levels, while keeping parameters stored statically and contiguously\nin hardware to prevent the extra burden of sparse computation. Based on this\nscheme, we present dynamic slimmable network (DS-Net) and dynamic slice-able\nnetwork (DS-Net++) by input-dependently adjusting filter numbers of CNNs and\nmultiple dimensions in both CNNs and transformers, respectively. To ensure\nsub-network generality and routing fairness, we propose a disentangled\ntwo-stage optimization scheme with training techniques such as in-place\nbootstrapping (IB), multi-view consistency (MvCo) and sandwich gate\nsparsification (SGS) to train supernet and gate separately. Extensive\nexperiments on 4 datasets and 3 different network architectures demonstrate our\nmethod consistently outperforms state-of-the-art static and dynamic model\ncompression methods by a large margin (up to 6.6%). Typically, DS-Net++\nachieves 2-4x computation reduction and 1.62x real-world acceleration over\nMobileNet, ResNet-50 and Vision Transformer, with minimal accuracy drops\n(0.1-0.3%) on ImageNet. Code release: https://github.com/changlin31/DS-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-aware direct monocular odometry. (arXiv:2109.10077v1 [cs.RO])","link":"http://arxiv.org/abs/2109.10077","description":"<p>We present a framework for direct monocular odometry based on depth\nprediction from a deep neural network. In contrast with existing methods where\ndepth information is only partially exploited, we formulate a novel depth\nprediction residual which allows us to incorporate multi-view depth\ninformation. In addition, we propose to use a truncated robust cost function\nwhich prevents considering inconsistent depth estimations. The photometric and\ndepth-prediction measurements are integrated in a tightly-coupled optimization\nleading to a scale-aware monocular system which does not accumulate scale\ndrift. We demonstrate the validity of our proposal evaluating it on the KITTI\nodometry dataset and comparing it with state-of-the-art monocular and stereo\nSLAM systems. Experiments show that our proposal largely outperforms classic\nmonocular SLAM, being 5 to 9 times more precise, with an accuracy which is\ncloser to that of stereo systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_C/0/1/0/all/0/1\">Carlos Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tard&#xf3;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Interpretable Concept Groups in CNNs. (arXiv:2109.10078v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10078","description":"<p>We propose a novel training methodology -- Concept Group Learning (CGL) --\nthat encourages training of interpretable CNN filters by partitioning filters\nin each layer into concept groups, each of which is trained to learn a single\nvisual concept. We achieve this through a novel regularization strategy that\nforces filters in the same group to be active in similar image regions for a\ngiven layer. We additionally use a regularizer to encourage a sparse weighting\nof the concept groups in each layer so that a few concept groups can have\ngreater importance than others. We quantitatively evaluate CGL's model\ninterpretability using standard interpretability evaluation techniques and find\nthat our method increases interpretability scores in most cases. Qualitatively\nwe compare the image regions that are most active under filters learned using\nCGL versus filters learned without CGL and find that CGL activation regions\nmore strongly concentrate around semantically relevant features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshneya_S/0/1/0/all/0/1\">Saurabh Varshneya</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ledent_A/0/1/0/all/0/1\">Antoine Ledent</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1\">Robert A. Vandermeulen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Enders_M/0/1/0/all/0/1\">Matthias Enders</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a> (1) ((1) Technical University of Kaiserslautern, (2) Technical University of Berlin, (3) University of Birmingham, (4) NPZ Innovation GmbH, (5) University of St.Gallen, Switzerland)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDFNet: Pointwise Dense Flow Network for Urban-Scene Segmentation. (arXiv:2109.10083v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10083","description":"<p>In recent years, using a deep convolutional neural network (CNN) as a feature\nencoder (or backbone) is the most commonly observed architectural pattern in\nseveral computer vision methods, and semantic segmentation is no exception. The\ntwo major drawbacks of this architectural pattern are: (i) the networks often\nfail to capture small classes such as wall, fence, pole, traffic light, traffic\nsign, and bicycle, which are crucial for autonomous vehicles to make accurate\ndecisions. (ii) due to the arbitrarily increasing depth, the networks require\nmassive labeled data and additional regularization techniques to converge and\nto prevent the risk of over-fitting, respectively. While regularization\ntechniques come at minimal cost, the collection of labeled data is an expensive\nand laborious process. In this work, we address these two drawbacks by\nproposing a novel lightweight architecture named point-wise dense flow network\n(PDFNet). In PDFNet, we employ dense, residual, and multiple shortcut\nconnections to allow a smooth gradient flow to all parts of the network. The\nextensive experiments on Cityscapes and CamVid benchmarks demonstrate that our\nmethod significantly outperforms baselines in capturing small classes and in\nfew-data regimes. Moreover, our method achieves considerable performance in\nclassifying out-of-the training distribution samples, evaluated on Cityscapes\nto KITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1\">Venkata Satya Sai Ajay Daliparthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Confidence Calibration for Epistemic Uncertainty Modelling. (arXiv:2109.10092v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10092","description":"<p>Modern neural networks have found to be miscalibrated in terms of confidence\ncalibration, i.e., their predicted confidence scores do not reflect the\nobserved accuracy or precision. Recent work has introduced methods for post-hoc\nconfidence calibration for classification as well as for object detection to\naddress this issue. Especially in safety critical applications, it is crucial\nto obtain a reliable self-assessment of a model. But what if the calibration\nmethod itself is uncertain, e.g., due to an insufficient knowledge base?\n</p>\n<p>We introduce Bayesian confidence calibration - a framework to obtain\ncalibrated confidence estimates in conjunction with an uncertainty of the\ncalibration method. Commonly, Bayesian neural networks (BNN) are used to\nindicate a network's uncertainty about a certain prediction. BNNs are\ninterpreted as neural networks that use distributions instead of weights for\ninference. We transfer this idea of using distributions to confidence\ncalibration. For this purpose, we use stochastic variational inference to build\na calibration mapping that outputs a probability distribution rather than a\nsingle calibrated estimate. Using this approach, we achieve state-of-the-art\ncalibration performance for object detection calibration. Finally, we show that\nthis additional type of uncertainty can be used as a sufficient criterion for\ncovariate shift detection. All code is open source and available at\nhttps://github.com/EFS-OpenSource/calibration-framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuppers_F/0/1/0/all/0/1\">Fabian K&#xfc;ppers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronenberger_J/0/1/0/all/0/1\">Jan Kronenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jonas Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselhoff_A/0/1/0/all/0/1\">Anselm Haselhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10115","description":"<p>We present a large-scale stereo RGB image object pose estimation dataset\nnamed the $\\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address\nchallenging cases such as object transparency, translucency, and specular\nreflection, in addition to the common challenges of occlusion, symmetry, and\nvariations in illumination and environments. In order to collect data of\nsufficient scale for modern deep learning models, we propose a novel method for\nefficiently annotating pose data in a multi-view fashion that allows data\ncapturing in complex and flexible environments. Fully annotated with 6D object\nposes, our dataset contains over 396K frames and over 1.5M annotations of 18\nobjects recorded in 183 scenes constructed in 11 different environments. The 18\nobjects include 8 symmetric objects, 7 transparent objects, and 8 reflective\nobjects. We benchmark two state-of-the-art pose estimation frameworks on\nStereOBJ-1M as baselines for future work. We also propose a novel object-level\npose optimization method for computing 6D pose from keypoint predictions in\nmultiple images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Semantic Stereo Matching / Semantic Depth Estimation. (arXiv:2109.10123v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10123","description":"<p>Stereo matching is one of the widely used techniques for inferring depth from\nstereo images owing to its robustness and speed. It has become one of the major\ntopics of research since it finds its applications in autonomous driving,\nrobotic navigation, 3D reconstruction, and many other fields. Finding pixel\ncorrespondences in non-textured, occluded and reflective areas is the major\nchallenge in stereo matching. Recent developments have shown that semantic cues\nfrom image segmentation can be used to improve the results of stereo matching.\nMany deep neural network architectures have been proposed to leverage the\nadvantages of semantic segmentation in stereo matching. This paper aims to give\na comparison among the state of art networks both in terms of accuracy and in\nterms of speed which are of higher importance in real-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Victor_V/0/1/0/all/0/1\">Viny Saajan Victor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neigel_P/0/1/0/all/0/1\">Peter Neigel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KDFNet: Learning Keypoint Distance Field for 6D Object Pose Estimation. (arXiv:2109.10127v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10127","description":"<p>We present KDFNet, a novel method for 6D object pose estimation from RGB\nimages. To handle occlusion, many recent works have proposed to localize 2D\nkeypoints through pixel-wise voting and solve a Perspective-n-Point (PnP)\nproblem for pose estimation, which achieves leading performance. However, such\nvoting process is direction-based and cannot handle long and thin objects where\nthe direction intersections cannot be robustly found. To address this problem,\nwe propose a novel continuous representation called Keypoint Distance Field\n(KDF) for projected 2D keypoint locations. Formulated as a 2D array, each\nelement of the KDF stores the 2D Euclidean distance between the corresponding\nimage pixel and a specified projected 2D keypoint. We use a fully convolutional\nneural network to regress the KDF for each keypoint. Using this KDF encoding of\nprojected object keypoint locations, we propose to use a distance-based voting\nscheme to localize the keypoints by calculating circle intersections in a\nRANSAC fashion. We validate the design choices of our framework by extensive\nablation experiments. Our proposed method achieves state-of-the-art performance\non Occlusion LINEMOD dataset with an average ADD(-S) accuracy of 50.3% and TOD\ndataset mug subset with an average ADD accuracy of 75.72%. Extensive\nexperiments and visualizations demonstrate that the proposed method is able to\nrobustly estimate the 6D pose in challenging scenarios including occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies. (arXiv:2109.10135v1 [cs.RO])","link":"http://arxiv.org/abs/2109.10135","description":"<p>Data augmentation can be a simple yet powerful tool for autonomous robots to\nfully utilise available data for self-supervised identification of atypical\nscenes or objects. State-of-the-art augmentation methods arbitrarily embed\nstructural peculiarity in focal objects on typical images so that classifying\nthese artefacts can provide guidance for learning representations for the\ndetection of anomalous visual inputs. In this paper, however, we argue that\nlearning such structure-sensitive representations can be a suboptimal approach\nto some classes of anomaly (e.g., unhealthy fruits) which are better recognised\nby a different type of visual element such as \"colour\". We thus propose Channel\nRandomisation as a novel data augmentation method for restricting neural\nnetwork models to learn encoding of \"colour irregularity\" whilst predicting\nchannel-randomised images to ultimately build reliable fruit-monitoring robots\nidentifying atypical fruit qualities. Our experiments show that (1) the\ncolour-based alternative can better learn representations for consistently\naccurate identification of fruit anomalies in various fruit species, and (2)\nvalidation accuracy can be monitored for early stopping of training due to\npositive correlation between the colour-learning task and fruit anomaly\ndetection. Moreover, the proposed approach is evaluated on a new anomaly\ndataset Riseholme-2021, consisting of 3:5K strawberry images collected from a\nmobile robot, which we share with the community to encourage active\nagri-robotics research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Would_O/0/1/0/all/0/1\">Owen Would</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Gomez_A/0/1/0/all/0/1\">Adrian Salazar-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Point Cloud Completion with Geometric-Aware Adversarial Augmentation. (arXiv:2109.10161v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10161","description":"<p>With the popularity of 3D sensors in self-driving and other robotics\napplications, extensive research has focused on designing novel neural network\narchitectures for accurate 3D point cloud completion. However, unlike in point\ncloud classification and reconstruction, the role of adversarial samples in3D\npoint cloud completion has seldom been explored. In this work, we show that\ntraining with adversarial samples can improve the performance of neural\nnetworks on 3D point cloud completion tasks. We propose a novel approach to\ngenerate adversarial samples that benefit both the performance of clean and\nadversarial samples. In contrast to the PGD-k attack, our method generates\nadversarial samples that keep the geometric features in clean samples and\ncontain few outliers. In particular, we use principal directions to constrain\nthe adversarial perturbations for each input point. The gradient components in\nthe mean direction of principal directions are taken as adversarial\nperturbations. In addition, we also investigate the effect of using the minimum\ncurvature direction. Besides, we adopt attack strength accumulation and\nauxiliary Batch Normalization layers method to speed up the training process\nand alleviate the distribution mismatch between clean and adversarial samples.\nExperimental results show that training with the adversarial samples crafted by\nour method effectively enhances the performance of PCN on the ShapeNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10187","description":"<p>Rotated object detection is a challenging task in aerial images as the object\nin aerial images are displayed in arbitrary directions and usually densely\npacked. Although considerable progress has been made, there are still\nchallenges that existing regression-based rotation detectors suffer the problem\nof discontinuous boundaries, which is directly caused by angular periodicity or\ncorner ordering. In this paper, we propose a simple effective framework to\naddress the above challenges. Instead of directly regressing the five\nparameters (coordinates of the central point, width, height, and rotation\nangle) or the four vertices, we use the area ratio of parallelogram (ARP) to\naccurately describe a multi-oriented object. Specifically, we regress\ncoordinates of center point, height and width of minimum circumscribed\nrectangle of oriented object and three area ratios {\\lambda}_1, {\\lambda}_2 and\n{\\lambda}_3. This may facilitate the offset learning and avoid the issue of\nangular periodicity or label points sequence for oriented objects. To further\nremedy the confusion issue nearly horizontal objects, we employ the area ratio\nbetween the object and its horizontal bounding box (minimum circumscribed\nrectangle) to guide the selection of horizontal or oriented detection for each\nobject. We also propose a rotation efficient IoU loss (R-EIoU) to connect the\nhorizontal bounding box with the three area ratios and improve the accurate for\nthe rotating bounding box. Experimental results on three remote sensing\ndatasets including HRSC2016, DOTA and UCAS-AOD and scene text including\nICDAR2015 show that our method achieves superior detection performance compared\nwith many state-of-the-art approaches. The code and model will be coming with\npaper published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10246","description":"<p>Linguistic representations derived from text alone have been criticized for\ntheir lack of grounding, i.e., connecting words to their meanings in the\nphysical world. Vision-and-Language (VL) models, trained jointly on text and\nimage or video data, have been offered as a response to such criticisms.\nHowever, while VL pretraining has shown success on multimodal tasks such as\nvisual question answering, it is not yet known how the internal linguistic\nrepresentations themselves compare to their text-only counterparts. This paper\ncompares the semantic representations learned via VL vs. text-only pretraining\nfor two recent VL models using a suite of analyses (clustering, probing, and\nperformance on a commonsense question answering task) in a language-only\nsetting. We find that the multimodal models fail to significantly outperform\nthe text-only variants, suggesting that future work is required if multimodal\npretraining is to be pursued as a means of improving NLP in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1\">Tian Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs. (arXiv:2109.10257v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10257","description":"<p>Several applications such as autonomous driving, augmented reality and\nvirtual reality requires a precise prediction of the 3D human pose. Recently, a\nnew problem was introduced in the field to predict the 3D human poses from an\nobserved 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN\nmodel that predicts the future 3D skeleton poses in a single pass from the 2D\nones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction\nbetween the skeleton joints by exploiting their spatial configuration. This is\nbeing achieved by formulating the problem as a graph structure while learning a\nsuitable graph adjacency kernel. By the design, Skeleton-Graph predicts the\nfuture 3D poses without divergence on the long-term unlike prior works. We also\nintroduce a new metric that measures the divergence of predictions on the\nlong-term. Our results show an FDE improvement of at least 27% and an ADE of 4%\non both the GTA-IM and PROX datasets respectively in comparison with prior\nworks. Also, we are 88% and 93% less divergence on the long-term motion\nprediction in comparison with prior works on both GTA-IM and PROX datasets.\nhttps://github.com/abduallahmohamed/Skeleton-Graph.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abduallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huancheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1\">Christian Claudel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of single and multitask learning for predicting cognitive decline based on MRI data. (arXiv:2109.10266v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10266","description":"<p>The Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog) is a\nneuropsychological tool that has been designed to assess the severity of\ncognitive symptoms of dementia. Personalized prediction of the changes in\nADAS-Cog scores could help in timing therapeutic interventions in dementia and\nat-risk populations. In the present work, we compared single and multitask\nlearning approaches to predict the changes in ADAS-Cog scores based on\nT1-weighted anatomical magnetic resonance imaging (MRI). In contrast to most\nmachine learning-based prediction methods ADAS-Cog changes, we stratified the\nsubjects based on their baseline diagnoses and evaluated the prediction\nperformances in each group. Our experiments indicated a positive relationship\nbetween the predicted and observed ADAS-Cog score changes in each diagnostic\ngroup, suggesting that T1-weighted MRI has a predictive value for evaluating\ncognitive decline in the entire AD continuum. We further studied whether\ncorrection of the differences in the magnetic field strength of MRI would\nimprove the ADAS-Cog score prediction. The partial least square-based domain\nadaptation slightly improved the prediction performance, but the improvement\nwas marginal. In summary, this study demonstrated that ADAS-Cog change could\nbe, to some extent, predicted based on anatomical MRI. Based on this study, the\nrecommended method for learning the predictive models is a single-task\nregularized linear regression due to its simplicity and good performance. It\nappears important to combine the training data across all subject groups for\nthe most effective predictive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_V/0/1/0/all/0/1\">Vandad Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_M/0/1/0/all/0/1\">Mithilesh Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_M/0/1/0/all/0/1\">Marzieh Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemCal: Semantic LiDAR-Camera Calibration using Neural MutualInformation Estimator. (arXiv:2109.10270v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10270","description":"<p>This paper proposes SemCal: an automatic, targetless, extrinsic calibration\nalgorithm for a LiDAR and camera system using semantic information. We leverage\na neural information estimator to estimate the mutual information (MI) of\nsemantic information extracted from each sensor measurement, facilitating\nsemantic-level data association. By using a matrix exponential formulation of\nthe $se(3)$ transformation and a kernel-based sampling method to sample from\ncamera measurement based on LiDAR projected points, we can formulate the\nLiDAR-Camera calibration problem as a novel differentiable objective function\nthat supports gradient-based optimization methods. We also introduce a\nsemantic-based initial calibration method using 2D MI-based image registration\nand Perspective-n-Point (PnP) solver. To evaluate performance, we demonstrate\nthe robustness of our method and quantitatively analyze the accuracy using a\nsynthetic dataset. We also evaluate our algorithm qualitatively on an urban\ndataset (KITTI360) and an off-road dataset (RELLIS-3D) benchmark datasets using\nboth hand-annotated ground truth labels as well as labels predicted by the\nstate-of-the-art deep learning models, showing improvement over recent\ncomparable calibration approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osteen_P/0/1/0/all/0/1\">Philip Osteen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1\">Srikanth Saripalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning PAC-Bayes Priors for Probabilistic Neural Networks. (arXiv:2109.10304v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10304","description":"<p>Recent works have investigated deep learning models trained by optimising\nPAC-Bayes bounds, with priors that are learnt on subsets of the data. This\ncombination has been shown to lead not only to accurate classifiers, but also\nto remarkably tight risk certificates, bearing promise towards self-certified\nlearning (i.e. use all the data to learn a predictor and certify its quality).\nIn this work, we empirically investigate the role of the prior. We experiment\non 6 datasets with different strategies and amounts of data to learn\ndata-dependent PAC-Bayes priors, and we compare them in terms of their effect\non test performance of the learnt predictors and tightness of their risk\ncertificate. We ask what is the optimal amount of data which should be\nallocated for building the prior and show that the optimum may be dataset\ndependent. We demonstrate that using a small percentage of the prior-building\ndata for validation of the prior leads to promising results. We include a\ncomparison of underparameterised and overparameterised models, along with an\nempirical study of different training objectives and regularisation strategies\nto learn the prior distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1\">Maria Perez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1\">Omar Rivasplata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleeson_M/0/1/0/all/0/1\">Matthew Gleeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CondNet: Conditional Classifier for Scene Segmentation. (arXiv:2109.10322v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10322","description":"<p>The fully convolutional network (FCN) has achieved tremendous success in\ndense visual recognition tasks, such as scene segmentation. The last layer of\nFCN is typically a global classifier (1x1 convolution) to recognize each pixel\nto a semantic label. We empirically show that this global classifier, ignoring\nthe intra-class distinction, may lead to sub-optimal results.\n</p>\n<p>In this work, we present a conditional classifier to replace the traditional\nglobal classifier, where the kernels of the classifier are generated\ndynamically conditioned on the input. The main advantages of the new classifier\nconsist of: (i) it attends on the intra-class distinction, leading to stronger\ndense recognition capability; (ii) the conditional classifier is simple and\nflexible to be integrated into almost arbitrary FCN architectures to improve\nthe prediction. Extensive experiments demonstrate that the proposed classifier\nperforms favourably against the traditional classifier on the FCN architecture.\nThe framework equipped with the conditional classifier (called CondNet)\nachieves new state-of-the-art performances on two datasets. The code and models\nare available at https://git.io/CondNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changqian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven controllers and the need for perception systems in underwater manipulation. (arXiv:2109.10327v1 [cs.RO])","link":"http://arxiv.org/abs/2109.10327","description":"<p>The underwater environment poses a complex problem for developing autonomous\ncapabilities for Underwater Vehicle Manipulator Systems (UVMSs). The modeling\nof UVMSs is a complicated and costly process due to the highly nonlinear\ndynamics and the presence of unknown hydrodynamical effects. This is aggravated\nin tasks where the manipulation of objects is necessary, as this may not only\nintroduce external disturbances that can lead to a fast degradation of the\ncontrol system performance, but also requires the coordinating with a vision\nsystem for the correct grasping and operation of the object. In this article,\nwe introduce a control strategy for UVMSs working with unknown payloads. The\nproposed control strategy is based on a data-driven optimal controller. We\npresent a number of experimental results showing the benefits of the proposed\nstrategy. Furthermore, we include a discussion regarding the visual perception\nrequirements for the UVMS in order to achieve full autonomy in underwater\nmanipulation tasks of unknown payloads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oubre_J/0/1/0/all/0/1\">James P. Oubre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlucho_I/0/1/0/all/0/1\">Ignacio Carlucho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalata_C/0/1/0/all/0/1\">Corina Barbalata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography augumented momentum constrastive learning for SAR image retrieval. (arXiv:2109.10329v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10329","description":"<p>Deep learning-based image retrieval has been emphasized in computer vision.\nRepresentation embedding extracted by deep neural networks (DNNs) not only aims\nat containing semantic information of the image, but also can manage\nlarge-scale image retrieval tasks. In this work, we propose a deep\nlearning-based image retrieval approach using homography transformation\naugmented contrastive learning to perform large-scale synthetic aperture radar\n(SAR) image search tasks. Moreover, we propose a training method for the DNNs\ninduced by contrastive learning that does not require any labeling procedure.\nThis may enable tractability of large-scale datasets with relative ease.\nFinally, we verify the performance of the proposed method by conducting\nexperiments on the polarimetric SAR image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seonho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rysz_M/0/1/0/all/0/1\">Maciej Rysz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dipple_K/0/1/0/all/0/1\">Kathleen M. Dipple</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardalos_P/0/1/0/all/0/1\">Panos M. Pardalos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism. (arXiv:2003.03955v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.03955","description":"<p>Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1\">Doyen Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Ke Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achananuparp_P/0/1/0/all/0/1\">Palakorn Achananuparp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces From Images. (arXiv:2004.14487v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.14487","description":"<p>The connection between visual input and tactile sensing is critical for\nobject manipulation tasks such as grasping and pushing. In this work, we\nintroduce the challenging task of estimating a set of tactile physical\nproperties from visual information. We aim to build a model that learns the\ncomplex mapping between visual information and tactile physical properties. We\nconstruct a first of its kind image-tactile dataset with over 400 multiview\nimage sequences and the corresponding tactile properties. A total of fifteen\ntactile physical properties across categories including friction, compliance,\nadhesion, texture, and thermal conductance are measured and then estimated by\nour models. We develop a cross-modal framework comprised of an adversarial\nobjective and a novel visuo-tactile joint classification loss. Additionally, we\ndevelop a neural architecture search framework capable of selecting optimal\ncombinations of viewing angles for estimating a given physical property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purri_M/0/1/0/all/0/1\">Matthew Purri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1\">Kristin Dana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Distributionally Robust Learning for Calibrated Uncertainties under Domain Shift. (arXiv:2010.05784v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.05784","description":"<p>We propose a deep distributionally robust learning framework for calibrated\nuncertainties under domain shifts. We consider cases where the source\n(training) distribution differs significantly from the target (test)\ndistribution. In addition to the standard class predictor, our framework\ncontains a binary domain classifier which estimates the density ratio between\nthe source and target domains. We incorporate both with neural networks and\ntrain them end-to-end. The framework is demonstrated to generate calibrated\nuncertainties that benefit many downstream tasks, including unsupervised domain\nadaptation (UDA) and semi-supervised learning (SSL) where methods such as\nself-training and FixMatch use uncertainties to select confident pseudo-labels.\nOur experiments show that the introduction of DRL to these methods leads to\nsignificant improvements in cross-domain performance. We also demonstrate that\nthe produced density ratio estimates show agreement with the human selection\nfrequencies, suggesting a match with the human perceived uncertainties. The\nsource code of this work will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Clinical Priori in 3D Convolutional Neural Networks for Prostate Cancer Detection in bpMRI. (arXiv:2011.00263v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.00263","description":"<p>We hypothesize that anatomical priors can be viable mediums to infuse\ndomain-specific clinical knowledge into state-of-the-art convolutional neural\nnetworks (CNN) based on the U-Net architecture. We introduce a probabilistic\npopulation prior which captures the spatial prevalence and zonal distinction of\nclinically significant prostate cancer (csPCa), in order to improve its\ncomputer-aided detection (CAD) in bi-parametric MR imaging (bpMRI). To evaluate\nperformance, we train 3D adaptations of the U-Net, U-SEResNet, UNet++ and\nAttention U-Net using 800 institutional training-validation scans, paired with\nradiologically-estimated annotations and our computed prior. For 200\nindependent testing bpMRI scans with histologically-confirmed delineations of\ncsPCa, our proposed method of encoding clinical priori demonstrates a strong\nability to improve patient-based diagnosis (upto 8.70% increase in AUROC) and\nlesion-level detection (average increase of 1.08 pAUC between 0.1-10 false\npositives per patient) across all four architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1\">Anindo Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1\">Matin Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Descriptor Visual Localization and Mapping. (arXiv:2012.01377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01377","description":"<p>Visual localization and mapping is the key technology underlying the majority\nof mixed reality and robotics systems. Most state-of-the-art approaches rely on\nlocal features to establish correspondences between images. In this paper, we\npresent three novel scenarios for localization and mapping which require the\ncontinuous update of feature representations and the ability to match across\ndifferent feature types. While localization and mapping is a fundamental\ncomputer vision problem, the traditional setup supposes the same local features\nare used throughout the evolution of a map. Thus, whenever the underlying\nfeatures are changed, the whole process is repeated from scratch. However, this\nis typically impossible in practice, because raw images are often not stored\nand re-building the maps could lead to loss of the attached digital content. To\novercome the limitations of current approaches, we present the first principled\nsolution to cross-descriptor localization and mapping. Our data-driven approach\nis agnostic to the feature descriptor type, has low computational requirements,\nand scales linearly with the number of description algorithms. Extensive\nexperiments demonstrate the effectiveness of our approach on state-of-the-art\nbenchmarks for a variety of handcrafted and learned features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1\">Mihai Dusmanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miksik_O/0/1/0/all/0/1\">Ondrej Miksik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1\">Johannes L. Sch&#xf6;nberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data InStance Prior (DISP) in Generative Adversarial Networks. (arXiv:2012.04256v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04256","description":"<p>Recent advances in generative adversarial networks (GANs) have shown\nremarkable progress in generating high-quality images. However, this gain in\nperformance depends on the availability of a large amount of training data. In\nlimited data regimes, training typically diverges, and therefore the generated\nsamples are of low quality and lack diversity. Previous works have addressed\ntraining in low data setting by leveraging transfer learning and data\naugmentation techniques. We propose a novel transfer learning method for GANs\nin the limited data domain by leveraging informative data prior derived from\nself-supervised/supervised pre-trained networks trained on a diverse source\ndomain. We perform experiments on several standard vision datasets using\nvarious GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the\nproposed method effectively transfers knowledge to domains with few target\nimages, outperforming existing state-of-the-art techniques in terms of image\nquality and diversity. We also show the utility of data instance prior in\nlarge-scale unconditional image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1\">Puneet Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_N/0/1/0/all/0/1\">Nupur Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Culture to Clothing: Discovering the World Events Behind A Century of Fashion Images. (arXiv:2102.01690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01690","description":"<p>Fashion is intertwined with external cultural factors, but identifying these\nlinks remains a manual process limited to only the most salient phenomena. We\npropose a data-driven approach to identify specific cultural factors affecting\nthe clothes people wear. Using large-scale datasets of news articles and\nvintage photos spanning a century, we present a multi-modal statistical model\nto detect influence relationships between happenings in the world and people's\nchoice of clothing. Furthermore, on two image datasets we apply our model to\nimprove the concrete vision tasks of visual style forecasting and photo\ntimestamping. Our work is a first step towards a computational, scalable, and\neasily refreshable approach to link culture to clothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_W/0/1/0/all/0/1\">Wei-Lin Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07358","description":"<p>Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association. (arXiv:2103.02440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02440","description":"<p>Many image-based perception tasks can be formulated as detecting, associating\nand tracking semantic keypoints, e.g., human body pose estimation and tracking.\nIn this work, we present a general framework that jointly detects and forms\nspatio-temporal keypoint associations in a single stage, making this the first\nreal-time pose detection and tracking algorithm. We present a generic neural\nnetwork architecture that uses Composite Fields to detect and construct a\nspatio-temporal pose which is a single, connected graph whose nodes are the\nsemantic keypoints (e.g., a person's body joints) in multiple frames. For the\ntemporal associations, we introduce the Temporal Composite Association Field\n(TCAF) which requires an extended network architecture and training method\nbeyond previous Composite Fields. Our experiments show competitive accuracy\nwhile being an order of magnitude faster on multiple publicly available\ndatasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We\nalso show that our method generalizes to any class of semantic keypoints such\nas car and animal parts to provide a holistic perception framework that is well\nsuited for urban mobility such as self-driving cars and delivery robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_S/0/1/0/all/0/1\">Sven Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertoni_L/0/1/0/all/0/1\">Lorenzo Bertoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quadruple Augmented Pyramid Network for Multi-class COVID-19 Segmentation via CT. (arXiv:2103.05546v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.05546","description":"<p>COVID-19, a new strain of coronavirus disease, has been one of the most\nserious and infectious disease in the world. Chest CT is essential in\nprognostication, diagnosing this disease, and assessing the complication. In\nthis paper, a multi-class COVID-19 CT segmentation is proposed aiming at\nhelping radiologists estimate the extent of effected lung volume. We utilized\nfour augmented pyramid networks on an encoder-decoder segmentation framework.\nQuadruple Augmented Pyramid Network (QAP-Net) not only enable CNN capture\nfeatures from variation size of CT images, but also act as spatial\ninterconnections and down-sampling to transfer sufficient feature information\nfor semantic segmentation. Experimental results achieve competitive performance\nin segmentation with the Dice of 0.8163, which outperforms other\nstate-of-the-art methods, demonstrating the proposed framework can segments of\nconsolidation as well as glass, ground area via COVID-19 chest CT efficiently\nand accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher Order Recurrent Space-Time Transformer for Video Action Prediction. (arXiv:2104.08665v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08665","description":"<p>Endowing visual agents with predictive capability is a key step towards video\nintelligence at scale. The predominant modeling paradigm for this is sequence\nlearning, mostly implemented through LSTMs. Feed-forward Transformer\narchitectures have replaced recurrent model designs in ML applications of\nlanguage processing and also partly in computer vision. In this paper we\ninvestigate on the competitiveness of Transformer-style architectures for video\npredictive tasks. To do so we propose HORST, a novel higher order recurrent\nlayer design whose core element is a spatial-temporal decomposition of\nself-attention for video. HORST achieves state of the art competitive\nperformance on Something-Something early action recognition and EPIC-Kitchens\naction anticipation, showing evidence of predictive capability that we\nattribute to our recurrent higher order design of self-attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_T/0/1/0/all/0/1\">Tsung-Ming Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheng-Kuang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Example Detection for DNN Models: A Review and Experimental Comparison. (arXiv:2105.00203v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00203","description":"<p>Deep learning (DL) has shown great success in many human-related tasks, which\nhas led to its adoption in many computer vision based applications, such as\nsecurity surveillance systems, autonomous vehicles and healthcare. Such\nsafety-critical applications have to draw their path to success deployment once\nthey have the capability to overcome safety-critical challenges. Among these\nchallenges are the defense against or/and the detection of the adversarial\nexamples (AEs). Adversaries can carefully craft small, often imperceptible,\nnoise called perturbations to be added to the clean image to generate the AE.\nThe aim of AE is to fool the DL model which makes it a potential risk for DL\napplications. Many test-time evasion attacks and countermeasures,i.e., defense\nor detection methods, are proposed in the literature. Moreover, few reviews and\nsurveys were published and theoretically showed the taxonomy of the threats and\nthe countermeasure methods with little focus in AE detection methods. In this\npaper, we focus on image classification tasks and attempt to provide a survey\nfor detection methods of test-time evasion attacks on neural network\nclassifiers. A detailed discussion for such methods is provided with\nexperimental results for eight state-of-the-art detectors under different\nscenarios on four datasets. We also provide potential challenges and future\nperspectives for this research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Discovery and Attribution of Open-world GAN Generated Images. (arXiv:2105.04580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04580","description":"<p>With the recent progress in Generative Adversarial Networks (GANs), it is\nimperative for media and visual forensics to develop detectors which can\nidentify and attribute images to the model generating them. Existing works have\nshown to attribute images to their corresponding GAN sources with high\naccuracy. However, these works are limited to a closed set scenario, failing to\ngeneralize to GANs unseen during train time and are therefore, not scalable\nwith a steady influx of new GANs. We present an iterative algorithm for\ndiscovering images generated from previously unseen GANs by exploiting the fact\nthat all GANs leave distinct fingerprints on their generated images. Our\nalgorithm consists of multiple components including network training,\nout-of-distribution detection, clustering, merge and refine steps. Through\nextensive experiments, we show that our algorithm discovers unseen GANs with\nhigh accuracy and also generalizes to GANs trained on unseen real datasets. We\nadditionally apply our algorithm to attribution and discovery of GANs in an\nonline fashion as well as to the more standard task of real/fake detection. Our\nexperiments demonstrate the effectiveness of our approach to discover new GANs\nand can be used in an open-world setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girish_S/0/1/0/all/0/1\">Sharath Girish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1\">Saksham Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CI-dataset and DetDSCI methodology for detecting too small and too large critical infrastructures in satellite images: Airports and electrical substations as case study. (arXiv:2105.11844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11844","description":"<p>The detection of critical infrastructures in large territories represented by\naerial and satellite images is of high importance in several fields such as in\nsecurity, anomaly detection, land use planning and land use change detection.\nHowever, the detection of such infrastructures is complex as they have highly\nvariable shapes and sizes, i.e., some infrastructures, such as electrical\nsubstations, are too small while others, such as airports, are too large.\nBesides, airports can have a surface area either small or too large with\ncompletely different shapes, which makes its correct detection challenging. As\nfar as we know, these limitations have not been tackled yet in previous works.\nThis paper presents (1) a smart Critical Infrastructure dataset, named\nCI-dataset, organised into two scales, small and large scales critical\ninfrastructures and (2) a two-level resolution-independent critical\ninfrastructure detection (DetDSCI) methodology that first determines the\nspatial resolution of the input image using a classification model, then\nanalyses the image using the appropriate detector for that spatial resolution.\nThe present study targets two representative classes, airports and electrical\nsubstations. Our experiments show that DetDSCI methodology achieves up to\n37,53% F1 improvement with respect to Faster R-CNN, one of the most influential\ndetection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Hernandez_F/0/1/0/all/0/1\">Francisco P&#xe9;rez-Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Ortega_J/0/1/0/all/0/1\">Jos&#xe9; Rodr&#xed;guez-Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhammou_Y/0/1/0/all/0/1\">Yassir Benhammou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabik_S/0/1/0/all/0/1\">Siham Tabik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03776","description":"<p>Background modeling and subtraction is a promising research area with a\nvariety of applications for video surveillance. Recent years have witnessed a\nproliferation of effective learning-based deep neural networks in this area.\nHowever, the techniques have only provided limited descriptions of scenes'\nproperties while requiring heavy computations, as their single-valued mapping\nfunctions are learned to approximate the temporal conditional averages of\nobserved target backgrounds and foregrounds. On the other hand, statistical\nlearning in imagery domains has been a prevalent approach with high adaptation\nto dynamic context transformation, notably using Gaussian Mixture Models (GMM)\nwith its generalization capabilities. By leveraging both, we propose a novel\nmethod called CDN-MEDAL-net for background modeling and subtraction with two\nconvolutional neural networks. The first architecture, CDN-GM, is grounded on\nan unsupervised GMM statistical learning strategy to describe observed scenes'\nsalient features. The second one, MEDAL-net, implements a light-weighted\npipeline of online video background subtraction. Our two-stage architecture is\nsmall, but it is very effective with rapid convergence to representations of\nintricate motion patterns. Our experiments show that the proposed approach is\nnot only capable of effectively extracting regions of moving objects in unseen\ncases, but it is also very efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Synh Viet-Uyen Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Ngoc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nhat Minh Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Hoai Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Edge-Aware Interactive Colorization against Color-Bleeding Effects. (arXiv:2107.01619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01619","description":"<p>Deep neural networks for automatic image colorization often suffer from the\ncolor-bleeding artifact, a problematic color spreading near the boundaries\nbetween adjacent objects. Such color-bleeding artifacts debase the reality of\ngenerated outputs, limiting the applicability of colorization models in\npractice. Although previous approaches have attempted to address this problem\nin an automatic manner, they tend to work only in limited cases where a high\ncontrast of gray-scale values are given in an input image. Alternatively,\nleveraging user interactions would be a promising approach for solving this\ncolor-breeding artifacts. In this paper, we propose a novel edge-enhancing\nnetwork for the regions of interest via simple user scribbles indicating where\nto enhance. In addition, our method requires a minimal amount of effort from\nusers for their satisfactory enhancement. Experimental results demonstrate that\nour interactive edge-enhancing approach effectively improves the color-bleeding\nartifacts compared to the existing baselines across various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eungyeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeonghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Somi Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Choonghyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic and Geometric Depth: Detecting Objects in Perspective. (arXiv:2107.14160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14160","description":"<p>3D object detection is an important capability needed in various practical\napplications such as driver assistance systems. Monocular 3D detection, as a\nrepresentative general setting among image-based approaches, provides a more\neconomical solution than conventional settings relying on LiDARs. It has drawn\nincreasing attention recently but still yields unsatisfactory results. This\npaper first presents a systematic study on this problem. We observe that the\ncurrent monocular 3D detection can be simplified as an instance depth\nestimation problem: The inaccurate instance depth blocks all the other 3D\nattribute predictions from improving the overall detection performance.\nHowever, recent methods directly estimate the depth based on isolated instances\nor pixels while ignoring the geometric relations across different objects.\nThese geometric relations can be valuable constraints as the key information\nabout depth is not directly manifest in the monocular image. Therefore, we\nconstruct geometric relation graphs across predicted objects and use the graph\nto facilitate depth estimation. As the preliminary depth estimation of each\ninstance is usually inaccurate in this ill-posed setting, we incorporate a\nprobabilistic representation to capture the uncertainty. It provides an\nimportant indicator to identify confident predictions and further guide the\ndepth propagation. Despite the simplicity of the basic idea, our method obtains\nsignificant improvements on KITTI and nuScenes benchmarks, achieving 1st place\nout of all monocular vision-only methods while still maintaining real-time\nefficiency. Code and models will be released at\nhttps://github.com/open-mmlab/mmdetection3d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00107","description":"<p>Deep convolutional neural networks (DCNNs) and the ventral visual pathway\nshare vast architectural and functional similarities in visual challenges such\nas object recognition. Recent insights have demonstrated that both hierarchical\ncascades can be compared in terms of both exerted behavior and underlying\nactivation. However, these approaches ignore key differences in spatial\npriorities of information processing. In this proof-of-concept study, we\ndemonstrate a comparison of human observers (N = 45) and three feedforward\nDCNNs through eye tracking and saliency maps. The results reveal fundamentally\ndifferent resolutions in both visualization methods that need to be considered\nfor an insightful comparison. Moreover, we provide evidence that a DCNN with\nbiologically plausible receptive field sizes called vNet reveals higher\nagreement with human viewing behavior as contrasted with a standard ResNet\narchitecture. We find that image-specific factors such as category, animacy,\narousal, and valence have a direct link to the agreement of spatial object\nrecognition priorities in humans and DCNNs, while other measures such as\ndifficulty and general image properties do not. With this approach, we try to\nopen up new perspectives at the intersection of biological and computer vision\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dyck_L/0/1/0/all/0/1\">Leonard E. van Dyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_S/0/1/0/all/0/1\">Sebastian J. Denzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_W/0/1/0/all/0/1\">Walter R. Gruber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARAPReg: An As-Rigid-As Possible Regularization Loss for Learning Deformable Shape Generators. (arXiv:2108.09432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09432","description":"<p>This paper introduces an unsupervised loss for training parametric\ndeformation shape generators. The key idea is to enforce the preservation of\nlocal rigidity among the generated shapes. Our approach builds on an\napproximation of the as-rigid-as possible (or ARAP) deformation energy. We show\nhow to develop the unsupervised loss via a spectral decomposition of the\nHessian of the ARAP energy. Our loss nicely decouples pose and shape variations\nthrough a robust norm. The loss admits simple closed-form expressions. It is\neasy to train and can be plugged into any standard generation models, e.g.,\nvariational auto-encoder (VAE) and auto-decoder (AD). Experimental results show\nthat our approach outperforms existing shape generation approaches considerably\non public benchmark datasets of various shape categories such as human, animal\nand bone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiangru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zaiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junfeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Pooling Driven Instance Segmentation Framework for Baggage Threat Recognition. (arXiv:2108.09603v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09603","description":"<p>Automated systems designed for screening contraband items from the X-ray\nimagery are still facing difficulties with high clutter, concealment, and\nextreme occlusion. In this paper, we addressed this challenge using a novel\nmulti-scale contour instance segmentation framework that effectively identifies\nthe cluttered contraband data within the baggage X-ray scans. Unlike standard\nmodels that employ region-based or keypoint-based techniques to generate\nmultiple boxes around objects, we propose to derive proposals according to the\nhierarchy of the regions defined by the contours. The proposed framework is\nrigorously validated on three public datasets, dubbed GDXray, SIXray, and\nOPIXray, where it outperforms the state-of-the-art methods by achieving the\nmean average precision score of 0.9779, 0.9614, and 0.8396, respectively.\nFurthermore, to the best of our knowledge, this is the first contour instance\nsegmentation framework that leverages multi-scale information to recognize\ncluttered and concealed contraband data from the colored and grayscale security\nX-ray imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt for Vision-Language Models. (arXiv:2109.01134v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01134","description":"<p>Vision-language pre-training has recently emerged as a promising alternative\nfor representation learning. It shifts from the tradition of using images and\ndiscrete labels for learning a fixed set of weights, seen as visual concepts,\nto aligning images and raw text for two separate encoders. Such a paradigm\nbenefits from a broader source of supervision and allows zero-shot transfer to\ndownstream tasks since visual concepts can be diametrically generated from\nnatural language, known as prompt. In this paper, we identify that a major\nchallenge of deploying such models in practice is prompt engineering. This is\nbecause designing a proper prompt, especially for context words surrounding a\nclass name, requires domain expertise and typically takes a significant amount\nof time for words tuning since a slight change in wording could have a huge\nimpact on performance. Moreover, different downstream tasks require specific\ndesigns, further hampering the efficiency of deployment. To overcome this\nchallenge, we propose a novel approach named context optimization (CoOp). The\nmain idea is to model context in prompts using continuous representations and\nperform end-to-end learning from data while keeping the pre-trained parameters\nfixed. In this way, the design of task-relevant prompts can be fully automated.\nExperiments on 11 datasets show that CoOp effectively turns pre-trained\nvision-language models into data-efficient visual learners, requiring as few as\none or two shots to beat hand-crafted prompts with a decent margin and able to\ngain significant improvements when using more shots (e.g., at 16 shots the\naverage gain is around 17% with the highest reaching over 50%). CoOp also\nexhibits strong robustness to distribution shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01827","description":"<p>In this paper, we propose GOHOME, a method leveraging graph representations\nof the High Definition Map and sparse projections to generate a heatmap output\nrepresenting the future position probability distribution for a given agent in\na traffic scene. This heatmap output yields an unconstrained 2D grid\nrepresentation of agent future possible locations, allowing inherent\nmultimodality and a measure of the uncertainty of the prediction. Our\ngraph-oriented model avoids the high computation burden of representing the\nsurrounding context as squared images and processing it with classical CNNs,\nbut focuses instead only on the most probable lanes where the agent could end\nup in the immediate future. GOHOME reaches 2$nd$ on Argoverse Motion\nForecasting Benchmark on the MissRate$_6$ metric while achieving significant\nspeed-up and memory burden diminution compared to Argoverse 1$^{st}$ place\nmethod HOME. We also highlight that heatmap output enables multimodal\nensembling and improve 1$^{st}$ place MissRate$_6$ by more than 15$\\%$ with our\nbest ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art\nperformance on the other trajectory prediction datasets nuScenes and\nInteraction, demonstrating the generalizability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04022","description":"<p>This work studies the problem of high-dimensional data (referred to as\ntensors) completion from partially observed samplings. We consider that a\ntensor is a superposition of multiple low-rank components. In particular, each\ncomponent can be represented as multilinear connections over several latent\nfactors and naturally mapped to a specific tensor network (TN) topology. In\nthis paper, we propose a fundamental tensor decomposition (TD) framework:\nMulti-Tensor Network Representation (MTNR), which can be regarded as a linear\ncombination of a range of TD models, e.g., CANDECOMP/PARAFAC (CP)\ndecomposition, Tensor Train (TT), and Tensor Ring (TR). Specifically, MTNR\nrepresents a high-order tensor as the addition of multiple TN models, and the\ntopology of each TN is automatically generated instead of manually\npre-designed. For the optimization phase, an adaptive topology learning (ATL)\nalgorithm is presented to obtain latent factors of each TN based on a rank\nincremental strategy and a projection error measurement strategy. In addition,\nwe theoretically establish the fundamental multilinear operations for the\ntensors with TN representation, and reveal the structural transformation of\nMTNR to a single TN. Finally, MTNR is applied to a typical task, tensor\ncompletion, and two effective algorithms are proposed for the exact recovery of\nincomplete data based on the Alternating Least Squares (ALS) scheme and\nAlternating Direction Method of Multiplier (ADMM) framework. Extensive\nnumerical experiments on synthetic data and real-world datasets demonstrate the\neffectiveness of MTNR compared with the start-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Chang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.06274v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.06274","description":"<p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the\ncochlea from magnetic resonance imaging (MRI) are critical to VS treatment\nplanning. Although supervised methods have achieved satisfactory performance in\nVS segmentation, they require full annotations by experts, which is laborious\nand time-consuming. In this work, we aim to tackle the VS and cochlea\nsegmentation problem in an unsupervised domain adaptation setting. Our proposed\nmethod leverages both the image-level domain alignment to minimize the domain\ndivergence and semi-supervised training to further boost the performance.\nFurthermore, we propose to fuse the labels predicted from multiple models via\nnoisy label correction. Our results on the challenge validation leaderboard\nshowed that our unsupervised method has achieved promising VS and cochlea\nsegmentation performance with mean dice score of 0.8261 $\\pm$ 0.0416; The mean\ndice value for the tumor is 0.8302 $\\pm$ 0.0772. This is comparable to the\nweakly-supervised based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McNeil_A/0/1/0/all/0/1\">Andrew McNeil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M.Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SketchHairSalon: Deep Sketch-based Hair Image Synthesis. (arXiv:2109.07874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07874","description":"<p>Recent deep generative models allow real-time generation of hair images from\nsketch inputs. Existing solutions often require a user-provided binary mask to\nspecify a target hair shape. This not only costs users extra labor but also\nfails to capture complicated hair boundaries. Those solutions usually encode\nhair structures via orientation maps, which, however, are not very effective to\nencode complex structures. We observe that colored hair sketches already\nimplicitly define target hair shapes as well as hair appearance and are more\nflexible to depict hair structures than orientation maps. Based on these\nobservations, we present SketchHairSalon, a two-stage framework for generating\nrealistic hair images directly from freehand sketches depicting desired hair\nstructure and appearance. At the first stage, we train a network to predict a\nhair matte from an input hair sketch, with an optional set of non-hair strokes.\nAt the second stage, another network is trained to synthesize the structure and\nappearance of hair images from the input sketch and the generated matte. To\nmake the networks in the two stages aware of long-term dependency of strokes,\nwe apply self-attention modules to them. To train these networks, we present a\nnew dataset containing thousands of annotated hair sketch-image pairs and\ncorresponding hair mattes. Two efficient methods for sketch completion are\nproposed to automatically complete repetitive braided parts and hair strokes,\nrespectively, thus reducing the workload of users. Based on the trained\nnetworks and the two sketch completion strategies, we build an intuitive\ninterface to allow even novice users to design visually pleasing hair images\nexhibiting various hair structures and appearance via freehand sketches. The\nqualitative and quantitative evaluations show the advantages of the proposed\nsystem over the existing or alternative solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chufeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Deng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-SlowFast Network for Efficient Visual Sound Separation. (arXiv:2109.08867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08867","description":"<p>The objective of this paper is to perform visual sound separation: i) we\nstudy visual sound separation on spectrograms of different temporal\nresolutions; ii) we propose a new light yet efficient three-stream framework\nV-SlowFast that operates on Visual frame, Slow spectrogram, and Fast\nspectrogram. The Slow spectrogram captures the coarse temporal resolution while\nthe Fast spectrogram contains the fine-grained temporal resolution; iii) we\nintroduce two contrastive objectives to encourage the network to learn\ndiscriminative visual features for separating sounds; iv) we propose an\naudio-visual global attention module for audio and visual feature fusion; v)\nthe introduced V-SlowFast model outperforms previous state-of-the-art in\nsingle-frame based visual sound separation on small- and large-scale datasets:\nMUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture\nvariant, which achieves 74.2% reduction in the number of model parameters and\n81.4% reduction in GMACs compared to the previous multi-stage models. Project\npage: https://ly-zhu.github.io/V-SlowFast\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DECORAS: detection and characterization of radio-astronomical sources using deep learning. (arXiv:2109.09077v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2109.09077","description":"<p>We present DECORAS, a deep learning based approach to detect both point and\nextended sources from Very Long Baseline Interferometry (VLBI) observations.\nOur approach is based on an encoder-decoder neural network architecture that\nuses a low number of convolutional layers to provide a scalable solution for\nsource detection. In addition, DECORAS performs source characterization in\nterms of the position, effective radius and peak brightness of the detected\nsources. We have trained and tested the network with images that are based on\nrealistic Very Long Baseline Array (VLBA) observations at 20 cm. Also, these\nimages have not gone through any prior de-convolution step and are directly\nrelated to the visibility data via a Fourier transform. We find that the source\ncatalog generated by DECORAS has a better overall completeness and purity, when\ncompared to a traditional source detection algorithm. DECORAS is complete at\nthe 7.5$\\sigma$ level, and has an almost factor of two improvement in\nreliability at 5.5$\\sigma$. We find that DECORAS can recover the position of\nthe detected sources to within 0.61 $\\pm$ 0.69 mas, and the effective radius\nand peak surface brightness are recovered to within 20 per cent for 98 and 94\nper cent of the sources, respectively. Overall, we find that DECORAS provides a\nreliable source detection and characterization solution for future wide-field\nVLBI surveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Rezaei_S/0/1/0/all/0/1\">S.Rezaei</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+McKean_J/0/1/0/all/0/1\">J.P.McKean</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Biehl_M/0/1/0/all/0/1\">M.Biehl</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Javadpour_A/0/1/0/all/0/1\">A.Javadpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09129","description":"<p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that\naffect patients' social abilities. In recent years, deep learning methods have\nbeen employed to detect ASD through functional MRI (fMRI). However, existing\napproaches solely concentrated on the abnormal brain functional connections but\nignored the importance of regional activities. Due to this biased prior\nknowledge, previous diagnosis models suffered from inter-site heterogeneity and\ninter-individual phenotypical differences. To address this issue, we propose a\nnovel feature extraction method for fMRI that can learn a personalized\nlowe-resolution representation of the entire brain networking regarding both\nthe functional connections and regional activities. First, we abstract the\nbrain imaging as a graph structure, where nodes represent brain areas and edges\ndenote functional connections, and downsample it to a sparse network by\nhierarchical graph pooling. Subsequently, by assigning each subject with the\nextracted features and building edges through inter-individual non-imaging\ncharacteristics, we build a population graph. The non-identically distributed\nnode features are further recalibrated to node embeddings learned by graph\nconvolutional networks. By these means, our framework can extract features\ndirectly and efficiently from the entire fMRI and be aware of implicit\ninter-individual differences. We have evaluated our framework on the ABIDE-I\ndataset with 10-fold cross-validation. The present model has achieved a mean\nclassification accuracy of 85.95\\% and a mean AUC of 0.92, which is better than\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Li Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jundong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_M/0/1/0/all/0/1\">Mingqin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_C/0/1/0/all/0/1\">Chi Wah Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_K/0/1/0/all/0/1\">Kei Hang Katie Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening. (arXiv:2109.09395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09395","description":"<p>Deep learning based pan-sharpening has received significant research interest\nin recent years. Most of existing methods fall into the supervised learning\nframework in which they down-sample the multi-spectral (MS) and panchromatic\n(PAN) images and regard the original MS images as ground truths to form\ntraining samples. Although impressive performance could be achieved, they have\ndifficulties generalizing to the original full-scale images due to the scale\ngap, which makes them lack of practicability. In this paper, we propose an\nunsupervised generative adversarial framework that learns from the full-scale\nimages without the ground truths to alleviate this problem. We extract the\nmodality-specific features from the PAN and MS images with a two-stream\ngenerator, perform fusion in the feature domain, and then reconstruct the\npan-sharpened images. Furthermore, we introduce a novel hybrid loss based on\nthe cycle-consistency and adversarial scheme to improve the performance.\nComparison experiments with the state-of-the-art methods are conducted on\nGaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed\nmethod can greatly improve the pan-sharpening performance on the full-scale\nimages, which clearly show its practical value. Codes and datasets will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huanyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Annotation Uncertainty with Gaussian Heatmaps in Landmark Localization. (arXiv:2109.09533v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09533","description":"<p>In landmark localization, due to ambiguities in defining their exact\nposition, landmark annotations may suffer from large observer variabilities,\nwhich result in uncertain annotations. To model the annotation ambiguities of\nthe training dataset, we propose to learn anisotropic Gaussian parameters\nmodeling the shape of the target heatmap during optimization. Furthermore, our\nmethod models the prediction uncertainty of individual samples by fitting\nanisotropic Gaussian functions to the predicted heatmaps during inference.\nBesides state-of-the-art results, our experiments on datasets of hand\nradiographs and lateral cephalograms also show that Gaussian functions are\ncorrelated with both localization accuracy and observer variability. As a final\nexperiment, we show the importance of integrating the uncertainty into decision\nmaking by measuring the influence of the predicted location uncertainty on the\nclassification of anatomical abnormalities in lateral cephalograms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thaler_F/0/1/0/all/0/1\">Franz Thaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1\">Martin Urschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_D/0/1/0/all/0/1\">Darko Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Trash Detection for Modern Societies using CCTV to Identifying Trash by utilizing Deep Convolutional Neural Network. (arXiv:2109.09611v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09611","description":"<p>To protect the environment from trash pollution, especially in societies, and\nto take strict action against the red-handed people who throws the trash. As\nmodern societies are developing and these societies need a modern solution to\nmake the environment clean. Artificial intelligence (AI) evolution, especially\nin Deep Learning, gives an excellent opportunity to develop real-time trash\ndetection using CCTV cameras. The inclusion of this project is real-time trash\ndetection using a deep model of Convolutional Neural Network (CNN). It is used\nto obtain eight classes mask, tissue papers, shoppers, boxes, automobile parts,\npampers, bottles, and juices boxes. After detecting the trash, the camera\nrecords the video of that person for ten seconds who throw trash in society.\nThe challenging part of this paper is preparing a complex custom dataset that\ntook too much time. The dataset consists of more than 2100 images. The CNN\nmodel was created, labeled, and trained. The detection time accuracy and\naverage mean precision (mAP) benchmark both models' performance. In\nexperimental phase the mAP performance and accuracy of the improved CNN model\nwas superior in all aspects. The model is used on a CCTV camera to detect trash\nin real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Syed Muhammad Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Syed Muhammad Ghazi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Syed Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Soo Young Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR. (arXiv:2109.09628v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09628","description":"<p>Self-supervised monocular depth prediction provides a cost-effective solution\nto obtain the 3D location of each pixel. However, the existing approaches\nusually lead to unsatisfactory accuracy, which is critical for autonomous\nrobots. In this paper, we propose a novel two-stage network to advance the\nself-supervised monocular dense depth learning by leveraging low-cost sparse\n(e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly\nin a manner of time-consuming iterative post-processing, our model fuses\nmonocular image features and sparse LiDAR features to predict initial depth\nmaps. Then, an efficient feed-forward refine network is further designed to\ncorrect the errors in these initial depth maps in pseudo-3D space with\nreal-time performance. Extensive experiments show that our proposed model\nsignificantly outperforms all the state-of-the-art self-supervised methods, as\nwell as the sparse-LiDAR-based methods on both self-supervised monocular depth\nprediction and completion tasks. With the accurate dense depth prediction, our\nmodel outperforms the state-of-the-art sparse-LiDAR-based method\n(Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object\ndetection on the KITTI Leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging. (arXiv:2109.09658v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09658","description":"<p>The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Despite these concerns and risks, there are\ncurrently no concrete guidelines and best practices for guiding future AI\ndevelopments in medical imaging towards increased trust, safety and adoption.\nTo bridge this gap, this paper introduces a careful selection of guiding\nprinciples drawn from the accumulated experiences, consensus, and best\npractices from five large European projects on AI in Health Imaging. These\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\nand (vi) Explainability. In a step-by-step approach, these guidelines are\nfurther translated into a framework of concrete recommendations for specifying,\ndeveloping, evaluating, and deploying technically, clinically and ethically\ntrustworthy AI solutions into clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallin_C/0/1/0/all/0/1\">Catherine Gallin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazrak_N/0/1/0/all/0/1\">Noussair Lazrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakou_G/0/1/0/all/0/1\">Gianna Tsakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ausso_S/0/1/0/all/0/1\">Susanna Auss&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberich_L/0/1/0/all/0/1\">Leonor Cerd&#xe1; Alberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marias_K/0/1/0/all/0/1\">Konstantinos Marias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tskinakis_M/0/1/0/all/0/1\">Manolis Tskinakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1\">Sara Colantonio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_N/0/1/0/all/0/1\">Nickolas Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salahuddin_Z/0/1/0/all/0/1\">Zohaib Salahuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_H/0/1/0/all/0/1\">Henry C Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambin_P/0/1/0/all/0/1\">Philippe Lambin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marti_Bonmati_L/0/1/0/all/0/1\">Luis Mart&#xed;-Bonmat&#xed;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes. (arXiv:2109.09690v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.09690","description":"<p>This paper presents a probabilistic framework to obtain both reliable and\nfast uncertainty estimates for predictions with Deep Neural Networks (DNNs).\nOur main contribution is a practical and principled combination of DNNs with\nsparse Gaussian Processes (GPs). We prove theoretically that DNNs can be seen\nas a special case of sparse GPs, namely mixtures of GP experts (MoE-GP), and we\ndevise a learning algorithm that brings the derived theory into practice. In\nexperiments from two different robotic tasks -- inverse dynamics of a\nmanipulator and object detection on a micro-aerial vehicle (MAV) -- we show the\neffectiveness of our approach in terms of predictive uncertainty, improved\nscalability, and run-time efficiency on a Jetson TX2. We thus argue that our\napproach can pave the way towards reliable and fast robot learning systems with\nuncertainty awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humt_M/0/1/0/all/0/1\">Matthias Humt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Marcus G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Precise Pruning Points Detection using Semantic-Instance-Aware Plant Models for Grapevine Winter Pruning Automation. (arXiv:2109.07247v1 [cs.RO] CROSS LISTED)","link":"http://arxiv.org/abs/2109.07247","description":"<p>Grapevine winter pruning is a complex task, that requires skilled workers to\nexecute it correctly. The complexity makes it time consuming. It is an\noperation that requires about 80-120 hours per hectare annually, making an\nautomated robotic system that helps in speeding up the process a crucial tool\nin large-size vineyards. We will describe (a) a novel expert annotated dataset\nfor grapevine segmentation, (b) a state of the art neural network\nimplementation and (c) generation of pruning points following agronomic rules,\nleveraging the simplified structure of the plant. With this approach, we are\nable to generate a set of pruning points on the canes, paving the way towards a\ncorrect automation of grapevine winter pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_M/0/1/0/all/0/1\">Miguel Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaldaferri_A/0/1/0/all/0/1\">Antonello Scaldaferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guadagna_P/0/1/0/all/0/1\">Paolo Guadagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_T/0/1/0/all/0/1\">Tao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatti_M/0/1/0/all/0/1\">Matteo Gatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poni_S/0/1/0/all/0/1\">Stefano Poni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1\">Claudio Semini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldwell_D/0/1/0/all/0/1\">Darwin Caldwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Correlation Aggregation: on the Path to Better Graph Neural Networks. (arXiv:2109.09300v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2109.09300","description":"<p>Prior to the introduction of Graph Neural Networks (GNNs), modeling and\nanalyzing irregular data, particularly graphs, was thought to be the Achilles'\nheel of deep learning. The core concept of GNNs is to find a representation by\nrecursively aggregating the representations of a central node and those of its\nneighbors. The core concept of GNNs is to find a representation by recursively\naggregating the representations of a central node and those of its neighbor,\nand its success has been demonstrated by many GNNs' designs. However, most of\nthem only focus on using the first-order information between a node and its\nneighbors. In this paper, we introduce a central node permutation variant\nfunction through a frustratingly simple and innocent-looking modification to\nthe core operation of a GNN, namely the Feature cOrrelation aGgregation (FOG)\nmodule which learns the second-order information from feature correlation\nbetween a node and its neighbors in the pipeline. By adding FOG into existing\nvariants of GNNs, we empirically verify this second-order information\ncomplements the features generated by original GNNs across a broad set of\nbenchmarks. A tangible boost in performance of the model is observed where the\nmodel surpasses previous state-of-the-art results by a significant margin while\nemploying fewer parameters. (e.g., 33.116% improvement on a real-world\nmolecular dataset using graph convolutional networks).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jieming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}