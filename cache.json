{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Can Foundation Models Talk Causality?. (arXiv:2206.10591v1 [cs.AI])","link":"http://arxiv.org/abs/2206.10591","description":"<p>Foundation models are subject to an ongoing heated debate, leaving open the\nquestion of progress towards AGI and dividing the community into two camps: the\nones who see the arguably impressive results as evidence to the scaling\nhypothesis, and the others who are worried about the lack of interpretability\nand reasoning capabilities. By investigating to which extent causal\nrepresentations might be captured by these large scale language models, we make\na humble efforts towards resolving the ongoing philosophical conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willig_M/0/1/0/all/0/1\">Moritz Willig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10658","description":"<p>We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic Parsing. (arXiv:2206.10668v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10668","description":"<p>We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model\nParsing, which produces semantic outputs based on the analysis of input text\nthrough constrained decoding of a prompted or fine-tuned language model.\nDevelopers of pretrained language models currently benchmark on classification,\nspan extraction and free-text generation tasks. Semantic parsing is neglected\nin language model evaluation because of the complexity of handling\ntask-specific architectures and representations. Recent work has shown that\ngeneration from a prompted or fine-tuned language model can perform well at\nsemantic parsing when the output is constrained to be a valid semantic\nrepresentation. BenchCLAMP includes context-free grammars for six semantic\nparsing datasets with varied output meaning representations, as well as a\nconstrained decoding interface to generate outputs covered by these grammars.\nWe provide low, medium, and high resource splits for each dataset, allowing\naccurate comparison of various language models under different data regimes.\nOur benchmark supports both prompt-based learning as well as fine-tuning, and\nprovides an easy-to-use toolkit for language model developers to evaluate on\nsemantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhro Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_S/0/1/0/all/0/1\">Sam Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_A/0/1/0/all/0/1\">Adam Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the case for audience design in conversational AI: Rapport expectations and language ideologies in a task-oriented chatbot. (arXiv:2206.10694v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10694","description":"<p>Chatbots are more and more prevalent in commercial and science contexts. They\nhelp customers complain about a product or service or support them to find the\nbest travel deals. Other bots provide mental health support or help book\nmedical appointments. This paper argues that insights into users' language\nideologies and their rapport expectations can be used to inform the audience\ndesign of the bot's language and interaction patterns and ensure equitable\naccess to the services provided by bots. The argument is underpinned by three\nkinds of data: simulated user interactions with a chatbot facilitating health\nappointment bookings, users' introspective comments on their interactions and\nusers' qualitative survey comments post engagement with the booking bot. In\nclosing, I will define audience design for conversational AI and discuss how\nuser-centred analyses of chatbot interactions and sociolinguistically informed\ntheoretical approaches, such as rapport management, can be used to support\naudience design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dippold_D/0/1/0/all/0/1\">Doris Dippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraSE: Towards Tackling Authorial Style from a Cognitive Science Perspective. (arXiv:2206.10706v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10706","description":"<p>Stylistic analysis of text is a key task in research areas ranging from\nauthorship attribution to forensic analysis and personality profiling. The\nexisting approaches for stylistic analysis are plagued by issues like topic\ninfluence, lack of discriminability for large number of authors and the\nrequirement for large amounts of diverse data. In this paper, the source of\nthese issues are identified along with the necessity for a cognitive\nperspective on authorial style in addressing them. A novel feature\nrepresentation, called Trajectory-based Style Estimation (TraSE), is introduced\nto support this purpose. Authorship attribution experiments with over 27,000\nauthors and 1.4 million samples in a cross-domain scenario resulted in 90%\nattribution accuracy suggesting that the feature representation is immune to\nsuch negative influences and an excellent candidate for stylistic analysis.\nFinally, a qualitative analysis is performed on TraSE using physical human\ncharacteristics, like age, to validate its claim on capturing cognitive traits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_R/0/1/0/all/0/1\">Ronald Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandarkar_A/0/1/0/all/0/1\">Avanti Bhandarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1\">Damon Woodard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information. (arXiv:2206.10744v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10744","description":"<p>The representations in large language models contain multiple types of gender\ninformation. We focus on two types of such signals in English texts: factual\ngender information, which is a grammatical or semantic property, and gender\nbias, which is the correlation between a word and specific gender. We can\ndisentangle the model's embeddings and identify components encoding both types\nof information with probing. We aim to diminish the stereotypical bias in the\nrepresentations while preserving the factual gender signal. Our filtering\nmethod shows that it is possible to decrease the bias of gender-neutral\nprofession names without significant deterioration of language modeling\ncapabilities. The findings can be applied to language generation to mitigate\nreliance on stereotypes while preserving gender agreement in coreferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and effective training of language and graph neural network models. (arXiv:2206.10781v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10781","description":"<p>Can we combine heterogenous graph structure with text to learn high-quality\nsemantic and behavioural representations? Graph neural networks (GNN)s encode\nnumerical node attributes and graph structure to achieve impressive performance\nin a variety of supervised learning tasks. Current GNN approaches are\nchallenged by textual features, which typically need to be encoded to a\nnumerical vector before provided to the GNN that may incur some information\nloss. In this paper, we put forth an efficient and effective framework termed\nlanguage model GNN (LM-GNN) to jointly train large-scale language models and\ngraph neural networks. The effectiveness in our framework is achieved by\napplying stage-wise fine-tuning of the BERT model first with heterogenous graph\ninformation and then with a GNN model. Several system and design optimizations\nare proposed to enable scalable and efficient training. LM-GNN accommodates\nnode and edge classification as well as link prediction tasks. We evaluate the\nLM-GNN framework in different datasets performance and showcase the\neffectiveness of the proposed approach. LM-GNN provides competitive results in\nan Amazon query-purchase-product application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ioannidis_V/0/1/0/all/0/1\">Vassilis N. Ioannidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Da Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Houyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities. (arXiv:2206.10883v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10883","description":"<p>With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lauren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlberg_N/0/1/0/all/0/1\">Nathan Dahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlanger_M/0/1/0/all/0/1\">Margo Schlanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-based Approach to Zero-shot Intent Recognition. (arXiv:2206.10914v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10914","description":"<p>The recent advances in transfer learning techniques and pre-training of large\ncontextualized encoders foster innovation in real-life applications, including\ndialog assistants. Practical needs of intent recognition require effective data\nusage and the ability to constantly update supported intents, adopting new\nones, and abandoning outdated ones. In particular, the generalized zero-shot\nparadigm, in which the model is trained on the seen intents and tested on both\nseen and unseen intents, is taking on new importance. In this paper, we explore\nthe generalized zero-shot setup for intent recognition. Following best\npractices for zero-shot text classification, we treat the task with a sentence\npair modeling approach. We outperform previous state-of-the-art f1-measure by\nup to 16\\% for unseen intents, using intent labels and user utterances and\nwithout accessing external sources (such as knowledge bases). Further\nenhancement includes lexicalization of intent labels, which improves\nperformance by up to 7\\%. By using task transferring from other sentence pair\ntasks, such as Natural Language Inference, we gain additional improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamanov_D/0/1/0/all/0/1\">Dmitry Lamanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Formulaic Language in Human and Machine Translation: Insight from a Parliamentary Corpus. (arXiv:2206.10919v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10919","description":"<p>A recent study has shown that, compared to human translations, neural machine\ntranslations contain more strongly-associated formulaic sequences made of\nrelatively high-frequency words, but far less strongly-associated formulaic\nsequences made of relatively rare words. These results were obtained on the\nbasis of translations of quality newspaper articles in which human translations\ncan be thought to be not very literal. The present study attempts to replicate\nthis research using a parliamentary corpus. The text were translated from\nFrench to English by three well-known neural machine translation systems:\nDeepL, Google Translate and Microsoft Translator. The results confirm the\nobservations on the news corpus, but the differences are less strong. They\nsuggest that the use of text genres that usually result in more literal\ntranslations, such as parliamentary corpora, might be preferable when comparing\nhuman and machine translations. Regarding the differences between the three\nneural machine systems, it appears that Google translations contain fewer\nhighly collocational bigrams, identified by the CollGram technique, than Deepl\nand Microsoft translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Networking Cipher Algorithms with Natural Language. (arXiv:2206.10924v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10924","description":"<p>This work provides a survey of several networking cipher algorithms and\nproposes a method for integrating natural language processing (NLP) as a\nprotective agent for them. Two main proposals are covered for the use of NLP in\nnetworking. First, NLP is considered as the weakest link in a networking\nencryption model; and, second, as a hefty deterrent when combined as an extra\nlayer over what could be considered a strong type of encryption -- the stream\ncipher. This paper summarizes how languages can be integrated into symmetric\nencryption as a way to assist in the encryption of vulnerable streams that may\nbe found under attack due to the natural frequency distribution of letters or\nwords in a local language stream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John E. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Embedding Models for Automatic Extraction and Classification of Acknowledged Entities in Scientific Documents. (arXiv:2206.10939v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10939","description":"<p>Acknowledgments in scientific papers may give an insight into aspects of the\nscientific community, such as reward systems, collaboration patterns, and\nhidden research trends. The aim of the paper is to evaluate the performance of\ndifferent embedding models for the task of automatic extraction and\nclassification of acknowledged entities from the acknowledgment text in\nscientific papers. We trained and implemented a named entity recognition (NER)\ntask using the Flair NLP-framework. The training was conducted using three\ndefault Flair NER models with two differently-sized corpora. The Flair\nEmbeddings model trained on the larger training corpus showed the best accuracy\nof 0.77. Our model is able to recognize six entity types: funding agency, grant\nnumber, individuals, university, corporation and miscellaneous. The model works\nmore precise for some entity types than the others, thus, individuals and grant\nnumbers showed very good F1-Score over 0.9. Most of the previous works on\nacknowledgement analysis were limited by the manual evaluation of data and\ntherefore by the amount of processed data. This model can be applied for the\ncomprehensive analysis of the acknowledgement texts and may potentially make a\ngreat contribution to the field of automated acknowledgement analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smirnova_N/0/1/0/all/0/1\">Nina Smirnova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward An Optimal Selection of Dialogue Strategies: A Target-Driven Approach for Intelligent Outbound Robots. (arXiv:2206.10953v1 [cs.CL])","link":"http://arxiv.org/abs/2206.10953","description":"<p>With the growth of the economy and society, enterprises, especially in the\nFinTech industry, have increasing demands of outbound calls for customers such\nas debt collection, marketing, anti-fraud calls, and so on. But a large amount\nof repetitive and mechanical work occupies most of the time of human agents, so\nthe cost of equipment and labor for enterprises is increasing accordingly. At\nthe same time, with the development of artificial intelligence technology in\nthe past few decades, it has become quite common for companies to use new\ntechnologies such as Big Data and artificial intelligence to empower outbound\ncall businesses. The intelligent outbound robot is a typical application of the\nartificial intelligence technology in the field of outbound call businesses. It\nis mainly used to communicate with customers in order to accomplish a certain\ntarget. It has the characteristics of low cost, high reuse, and easy\ncompliance, which has attracted more attention from the industry.\n</p>\n<p>At present, there are two kinds of intelligent outbound robots in the\nindustry but both of them still leave large room for improvement. One kind of\nthem is based on a finite state machine relying on the configuration of jump\nconditions and corresponding nodes based on manual experience. This kind of\nintelligent outbound robot is also called a flow-based robot. For example, the\nschematic diagram of the working model of a flow-based robot for debt\ncollection is shown in Fig.\\ref{fig:label}. In each round, the robot will reply\nto the user with the words corresponding to each node.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Ruifeng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_M/0/1/0/all/0/1\">Mengjiao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Y/0/1/0/all/0/1\">Yu Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting a French Dictionary from the Beginning of the 20th Century to Wikidata. (arXiv:2206.11022v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11022","description":"<p>The \\textit{Petit Larousse illustr\\'e} is a French dictionary first published\nin 1905. Its division in two main parts on language and on history and\ngeography corresponds to a major milestone in French lexicography as well as a\nrepository of general knowledge from this period. Although the value of many\nentries from 1905 remains intact, some descriptions now have a dimension that\nis more historical than contemporary. They are nonetheless significant to\nanalyze and understand cultural representations from this time. A comparison\nwith more recent information or a verification of these entries would require a\ntedious manual work. In this paper, we describe a new lexical resource, where\nwe connected all the dictionary entries of the history and geography part to\ncurrent data sources. For this, we linked each of these entries to a wikidata\nidentifier. Using the wikidata links, we can automate more easily the\nidentification, comparison, and verification of historically-situated\nrepresentations. We give a few examples on how to process wikidata identifiers\nand we carried out a small analysis of the entities described in the dictionary\nto outline possible applications. The resource, i.e. the annotation of 20,245\ndictionary entries with wikidata links, is available from GitHub\n(\\url{https://github.com/pnugues/petit_larousse_1905/})\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nugues_P/0/1/0/all/0/1\">Pierre Nugues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer Fast: Accelerating BERT on the Tensor Streaming Processor. (arXiv:2206.11062v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11062","description":"<p>Transformers have become a predominant machine learning workload, they are\nnot only the de-facto standard for natural language processing tasks, but they\nare also being deployed in other domains such as vision and speech recognition.\nMany of the transformer-based applications are real-time systems such as\nmachine translation and web search. These real time systems often come with\nstrict end-to-end inference latency requirements. Unfortunately, while the\nmajority of the transformer computation comes from matrix multiplications,\ntransformers also include several non-linear components that tend to become the\nbottleneck during an inference. In this work, we accelerate the inference of\nBERT models on the tensor streaming processor. By carefully fusing all the\nnonlinear components with the matrix multiplication components, we are able to\nefficiently utilize the on-chip matrix multiplication units resulting in a\ndeterministic tail latency of 130 $\\mu$s for a batch-1 inference through\nBERT-base, which is 6X faster than the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1\">Ibrahim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_S/0/1/0/all/0/1\">Sahil Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_M/0/1/0/all/0/1\">Matthew Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beidler_M/0/1/0/all/0/1\">Michael Beidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kris Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bill Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roach_K/0/1/0/all/0/1\">Kyle Roach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">John Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abts_D/0/1/0/all/0/1\">Dennis Abts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Benefits of Free-Form Rationales. (arXiv:2206.11083v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11083","description":"<p>Free-form rationales aim to aid model interpretability by supplying the\nbackground knowledge that can help understand model decisions. Crowdsourced\nrationales are provided for commonsense QA instances in popular datasets such\nas CoS-E and ECQA, but their utility remains under-investigated. We present\nhuman studies which show that ECQA rationales indeed provide additional\nbackground information to understand a decision, while over 88% of CoS-E\nrationales do not. Inspired by this finding, we ask: can the additional context\nprovided by free-form rationales benefit models, similar to human users? We\ninvestigate the utility of rationales as an additional source of supervision,\nby varying the quantity and quality of rationales during training. After\ncontrolling for instances where rationales leak the correct answer while not\nproviding additional background knowledge, we find that incorporating only 5%\nof rationales during training can boost model performance by 47.22% for CoS-E\nand 57.14% for ECQA during inference. Moreover, we also show that rationale\nquality matters: compared to crowdsourced rationales, T5-generated rationales\nprovide not only weaker supervision to models, but are also not helpful for\nhumans in aiding model interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Multimodal Pre-training into Multilingual via Language Acquisition. (arXiv:2206.11091v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11091","description":"<p>English-based Vision-Language Pre-training (VLP) has achieved great success\nin various downstream tasks. Some efforts have been taken to generalize this\nsuccess to non-English languages through Multilingual Vision-Language\nPre-training (M-VLP). However, due to the large number of languages, M-VLP\nmodels often require huge computing resources and cannot be flexibly extended\nto new languages. In this work, we propose a \\textbf{M}ulti\\textbf{L}ingual\n\\textbf{A}cquisition (MLA) framework that can easily generalize a monolingual\nVision-Language Pre-training model into multilingual. Specifically, we design a\nlightweight language acquisition encoder based on state-of-the-art monolingual\nVLP models. We further propose a two-stage training strategy to optimize the\nlanguage acquisition encoder, namely the Native Language Transfer stage and the\nLanguage Exposure stage. With much less multilingual training data and\ncomputing resources, our model achieves state-of-the-art performance on\nmultilingual image-text and video-text retrieval benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Clustering for Open Knowledge Base Canonicalization. (arXiv:2206.11130v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11130","description":"<p>Open information extraction (OIE) methods extract plenty of OIE triples &lt;noun\nphrase, relation phrase, noun phrase&gt; from unstructured text, which compose\nlarge open knowledge bases (OKBs). Noun phrases and relation phrases in such\nOKBs are not canonicalized, which leads to scattered and redundant facts. It is\nfound that two views of knowledge (i.e., a fact view based on the fact triple\nand a context view based on the fact triple's source context) provide\ncomplementary information that is vital to the task of OKB canonicalization,\nwhich clusters synonymous noun phrases and relation phrases into the same group\nand assigns them unique identifiers. However, these two views of knowledge have\nso far been leveraged in isolation by existing works. In this paper, we propose\nCMVC, a novel unsupervised framework that leverages these two views of\nknowledge jointly for canonicalizing OKBs without the need of manually\nannotated labels. To achieve this goal, we propose a multi-view CH K-Means\nclustering algorithm to mutually reinforce the clustering of view-specific\nembeddings learned from each view by considering their different clustering\nqualities. In order to further enhance the canonicalization performance, we\npropose a training data optimization strategy in terms of data quantity and\ndata quality respectively in each particular view to refine the learned\nview-specific embeddings in an iterative manner. Additionally, we propose a\nLog-Jump algorithm to predict the optimal number of clusters in a data-driven\nway without requiring any labels. We demonstrate the superiority of our\nframework through extensive experiments on multiple real-world OKB data sets\nagainst state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process. (arXiv:2206.11146v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11146","description":"<p>We introduce FiLex, a self-reinforcing stochastic process which models finite\nlexicons in emergent language experiments. The central property of FiLex is\nthat it is a self-reinforcing process, parallel to the intuition that the more\na word is used in a language, the more its use will continue. As a theoretical\nmodel, FiLex serves as a way to both explain and predict the behavior of the\nemergent language system. We empirically test FiLex's ability to capture the\nrelationship between the emergent language's hyperparameters and the lexicon's\nShannon entropy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boldt_B/0/1/0/all/0/1\">Brendon Boldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"reStructured Pre-training. (arXiv:2206.11147v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11147","description":"<p>In this work, we try to decipher the internal connection of NLP technology\ndevelopment in the past decades, searching for essence, which rewards us with a\n(potential) new learning paradigm for NLP tasks, dubbed as reStructured\nPre-training (RST). In such a paradigm, the role of data will be re-emphasized,\nand model pre-training and fine-tuning of downstream tasks are viewed as a\nprocess of data storing and accessing. Based on that, we operationalize the\nsimple principle that a good storage mechanism should not only have the ability\nto cache a large amount of data but also consider the ease of access. We\nachieve this by pre-training models over restructured data that consist of a\nvariety of valuable information instead of raw data after overcoming several\nengineering challenges. Experimentally, RST models not only surpass strong\ncompetitors (e.g., T0) on 52/55 popular datasets from a variety of NLP tasks,\nbut also achieve superior performance in National College Entrance Examination\n- English (Gaokao-English),the most authoritative examination in China.\nSpecifically, the proposed system Qin achieves 40 points higher than the\naverage scores made by students and 15 points higher than GPT3 with 1/16\nparameters. In particular, Qin gets a high score of 138.5 (the full mark is\n150) in the 2018 English exam (national paper III). We have released the Gaokao\nBenchmark with an online submission platform.\n</p>\n<p>In addition, we test our model in the 2022 College Entrance Examination\nEnglish that happened a few days ago (2022.06.08), and it gets a total score of\n134 (v.s. GPT3's 108).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Then and Now: Quantifying the Longitudinal Validity of Self-Disclosed Depression Diagnoses. (arXiv:2206.11155v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11155","description":"<p>Self-disclosed mental health diagnoses, which serve as ground truth\nannotations of mental health status in the absence of clinical measures,\nunderpin the conclusions behind most computational studies of mental health\nlanguage from the last decade. However, psychiatric conditions are dynamic; a\nprior depression diagnosis may no longer be indicative of an individual's\nmental health, either due to treatment or other mitigating factors. We ask: to\nwhat extent are self-disclosures of mental health diagnoses actually relevant\nover time? We analyze recent activity from individuals who disclosed a\ndepression diagnosis on social media over five years ago and, in turn, acquire\na new understanding of how presentations of mental health status on social\nmedia manifest longitudinally. We also provide expanded evidence for the\npresence of personality-related biases in datasets curated using self-disclosed\ndiagnoses. Our findings motivate three practical recommendations for improving\nmental health datasets curated using self-disclosed diagnoses: 1) Annotate\ndiagnosis dates and psychiatric comorbidities; 2) Sample control groups using\npropensity score matching; 3) Identify and remove spurious correlations\nintroduced by selection bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrigian_K/0/1/0/all/0/1\">Keith Harrigian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Problem of Semantic Shift in Longitudinal Monitoring of Social Media: A Case Study on Mental Health During the COVID-19 Pandemic. (arXiv:2206.11160v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11160","description":"<p>Social media allows researchers to track societal and cultural changes over\ntime based on language analysis tools. Many of these tools rely on statistical\nalgorithms which need to be tuned to specific types of language. Recent studies\nhave shown the absence of appropriate tuning, specifically in the presence of\nsemantic shift, can hinder robustness of the underlying methods. However,\nlittle is known about the practical effect this sensitivity may have on\ndownstream longitudinal analyses. We explore this gap in the literature through\na timely case study: understanding shifts in depression during the course of\nthe COVID-19 pandemic. We find that inclusion of only a small number of\nsemantically-unstable features can promote significant changes in longitudinal\nestimates of our target outcome. At the same time, we demonstrate that a\nrecently-introduced method for measuring semantic shift may be used to\nproactively identify failure points of language-based models and, in turn,\nimprove predictive generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrigian_K/0/1/0/all/0/1\">Keith Harrigian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unsupervised Content Disentanglement in Sentence Representations via Syntactic Roles. (arXiv:2206.11184v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11184","description":"<p>Linking neural representations to linguistic factors is crucial in order to\nbuild and analyze NLP models interpretable by humans. Among these factors,\nsyntactic roles (e.g. subjects, direct objects,$\\dots$) and their realizations\nare essential markers since they can be understood as a decomposition of\npredicative structures and thus the meaning of sentences. Starting from a deep\nprobabilistic generative model with attention, we measure the interaction\nbetween latent variables and realizations of syntactic roles and show that it\nis possible to obtain, without supervision, representations of sentences where\ndifferent syntactic roles correspond to clearly identified different latent\nvariables. The probabilistic model we propose is an Attention-Driven\nVariational Autoencoder (ADVAE). Drawing inspiration from Transformer-based\nmachine translation models, ADVAEs enable the analysis of the interactions\nbetween latent variables and input tokens through attention. We also develop an\nevaluation protocol to measure disentanglement with regard to the realizations\nof syntactic roles. This protocol is based on attention maxima for the encoder\nand on latent variable perturbations for the decoder. Our experiments on raw\nEnglish text from the SNLI dataset show that $\\textit{i)}$ disentanglement of\nsyntactic roles can be induced without supervision, $\\textit{ii)}$ ADVAE\nseparates syntactic roles better than classical sequence VAEs and Transformer\nVAEs, $\\textit{iii)}$ realizations of syntactic roles can be separately\nmodified in sentences by mere intervention on the associated latent variables.\nOur work constitutes a first step towards unsupervised controllable content\ngeneration. The code for our work is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felhi_G/0/1/0/all/0/1\">Ghazi Felhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Joseph Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11212","description":"<p>Many past works aim to improve visual reasoning in models by supervising\nfeature importance (estimated by model explanation techniques) with human\nannotations such as highlights of important image regions. However, recent work\nhas shown that performance gains from feature importance (FI) supervision for\nVisual Question Answering (VQA) tasks persist even with random supervision,\nsuggesting that these methods do not meaningfully align model FI with human FI.\nIn this paper, we show that model FI supervision can meaningfully improve VQA\nmodel accuracy as well as performance on several Right-for-the-Right-Reason\n(RRR) metrics by optimizing for four key model objectives: (1) accurate\npredictions given limited but sufficient information (Sufficiency); (2)\nmax-entropy predictions given no important information (Uncertainty); (3)\ninvariance of predictions to changes in unimportant features (Invariance); and\n(4) alignment between model FI explanations and human FI explanations\n(Plausibility). Our best performing method, Visual Feature Importance\nSupervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in\nterms of both in-distribution and out-of-distribution accuracy. While past work\nsuggests that the mechanism for improved accuracy is through improved\nexplanation plausibility, we show that this relationship depends crucially on\nexplanation faithfulness (whether explanations truly represent the model's\ninternal reasoning). Predictions are more accurate when explanations are\nplausible and faithful, and not when they are plausible but not faithful.\nLastly, we show that, surprisingly, RRR metrics are not predictive of\nout-of-distribution model accuracy when controlling for a model's\nin-distribution accuracy, which calls into question the value of these metrics\nfor evaluating model reasoning. All supporting code is available at\nhttps://github.com/zfying/visfis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhuofan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Context Tagging for Utterance Rewriting. (arXiv:2206.11218v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11218","description":"<p>Utterance rewriting aims to recover coreferences and omitted information from\nthe latest turn of a multi-turn dialogue. Recently, methods that tag rather\nthan linearly generate sequences have proven stronger in both in- and\nout-of-domain rewriting settings. This is due to a tagger's smaller search\nspace as it can only copy tokens from the dialogue context. However, these\nmethods may suffer from low coverage when phrases that must be added to a\nsource utterance cannot be covered by a single context span. This can occur in\nlanguages like English that introduce tokens such as prepositions into the\nrewrite for grammaticality. We propose a hierarchical context tagger (HCT) that\nmitigates this issue by predicting slotted rules (e.g., \"besides _\") whose\nslots are later filled with context spans. HCT (i) tags the source string with\ntoken-level edit actions and slotted rules and (ii) fills in the resulting rule\nslots with spans from the dialogue context. This rule tagging allows HCT to add\nout-of-context tokens and multiple spans at once; we further cluster the rules\nto truncate the long tail of the rule distribution. Experiments on several\nbenchmarks show that HCT can outperform state-of-the-art rewriting systems by\n~2 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Properties of Generated Corpora. (arXiv:2206.11219v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11219","description":"<p>Models for text generation have become focal for many research tasks and\nespecially for the generation of sentence corpora. However, understanding the\nproperties of an automatically generated text corpus remains challenging. We\npropose a set of tools that examine the properties of generated text corpora.\nApplying these tools on various generated corpora allowed us to gain new\ninsights into the properties of the generative models. As part of our\ncharacterization process, we found remarkable differences in the corpora\ngenerated by two leading generative technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zwerdling_N/0/1/0/all/0/1\">Naama Zwerdling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlomov_S/0/1/0/all/0/1\">Segev Shlomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldbraich_E/0/1/0/all/0/1\">Esther Goldbraich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kour_G/0/1/0/all/0/1\">George Kour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tepper_N/0/1/0/all/0/1\">Naama Tepper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_I/0/1/0/all/0/1\">Inbal Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabershinsky_V/0/1/0/all/0/1\">Vitaly Zabershinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMv2: Multilingual NLG Benchmarking in a Single Line of Code. (arXiv:2206.11249v1 [cs.CL])","link":"http://arxiv.org/abs/2206.11249","description":"<p>Evaluation in machine learning is usually informed by past choices, for\nexample which datasets or metrics to use. This standardization enables the\ncomparison on equal footing using leaderboards, but the evaluation choices\nbecome sub-optimal as better alternatives arise. This problem is especially\npertinent in natural language generation which requires ever-improving suites\nof datasets, metrics, and human evaluation to make definitive claims. To make\nfollowing best model evaluation practices easier, we introduce GEMv2. The new\nversion of the Generation, Evaluation, and Metrics Benchmark introduces a\nmodular infrastructure for dataset, model, and metric developers to benefit\nfrom each others work. GEMv2 supports 40 documented datasets in 51 languages.\nModels for all datasets can be evaluated online and our interactive data card\ncreation and rendering tools make it easier to add new datasets to the living\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendiran_A/0/1/0/all/0/1\">Abinaya Mahendiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Anna Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_A/0/1/0/all/0/1\">Ashish Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaobin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clive_J/0/1/0/all/0/1\">Jordan Clive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pushkarna_M/0/1/0/all/0/1\">Mahima Pushkarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creutz_M/0/1/0/all/0/1\">Mathias Creutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Sanjay Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ondrej Dusek</a>, et al. (27 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedFilter: Improving Extraction of Task-relevant Utterances from Doctor-Patient Conversations through Integration of Discourse Structure and Ontological Knowledge. (arXiv:2010.02246v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.02246","description":"<p>Information extraction from conversational data is particularly challenging\nbecause the task-centric nature of conversation allows for effective\ncommunication of implicit information by humans, but is challenging for\nmachines. The challenges may differ between utterances depending on the role of\nthe speaker within the conversation, especially when relevant expertise is\ndistributed asymmetrically across roles. Further, the challenges may also\nincrease over the conversation as more shared context is built up through\ninformation communicated implicitly earlier in the dialogue. In this paper, we\npropose the novel modeling approach MedFilter, which addresses these insights\nin order to increase performance at identifying and categorizing task-relevant\nutterances, and in so doing, positively impacts performance at a downstream\ninformation extraction task. We evaluate this approach on a corpus of nearly\n7,000 doctor-patient conversations where MedFilter is used to identify\nmedically relevant contributions to the discussion (achieving a 10% improvement\nover SOTA baselines in terms of area under the PR curve). Identifying\ntask-relevant utterances benefits downstream medical processing, achieving\nimprovements of 15%, 105%, and 23% respectively for the extraction of symptoms,\nmedications, and complaints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1\">Sopan Khosla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Jill Fain Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diff-Explainer: Differentiable Convex Optimization for Explainable Multi-hop Inference. (arXiv:2105.03417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03417","description":"<p>This paper presents Diff-Explainer, the first hybrid framework for\nexplainable multi-hop inference that integrates explicit constraints with\nneural architectures through differentiable convex optimization. Specifically,\nDiff-Explainer allows for the fine-tuning of neural representations within a\nconstrained optimization framework to answer and explain multi-hop questions in\nnatural language. To demonstrate the efficacy of the hybrid framework, we\ncombine existing ILP-based solvers for multi-hop Question Answering (QA) with\nTransformer-based representations. An extensive empirical evaluation on\nscientific and commonsense QA tasks demonstrates that the integration of\nexplicit constraints in an end-to-end differentiable framework can\nsignificantly improve the performance of non-differentiable ILP solvers (8.91%\n- 13.3%). Moreover, additional analysis reveals that Diff-Explainer is able to\nachieve strong performance when compared to standalone Transformers and\nprevious multi-hop approaches while still providing structured explanations in\nsupport of its predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14769","description":"<p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Alzheimer's is the most common cause of dementia,\nwhich constitutes a general term for loss of memory. Due to the fact that\ndementia affects speech, existing research initiatives focus on detecting\ndementia from spontaneous speech. However, little work has been done regarding\nthe conversion of speech data to Log-Mel spectrograms and Mel-frequency\ncepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,\nlittle work has been done in terms of both the usage of transformer networks\nand the way the two modalities, i.e., speech and transcripts, are combined in a\nsingle neural network. To address these limitations, first we employ several\npretrained models, with Vision Transformer (ViT) achieving the highest\nevaluation results. Secondly, we propose multimodal models. More specifically,\nour introduced models include Gated Multimodal Unit in order to control the\ninfluence of each modality towards the final classification and crossmodal\nattention so as to capture in an effective way the relationships between the\ntwo modalities. Extensive experiments conducted on the ADReSS Challenge dataset\ndemonstrate the effectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07991","description":"<p>This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surfer100: Generating Surveys From Web Resources, Wikipedia-style. (arXiv:2112.06377v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06377","description":"<p>Fast-developing fields such as Artificial Intelligence (AI) often outpace the\nefforts of encyclopedic sources such as Wikipedia, which either do not\ncompletely cover recently-introduced topics or lack such content entirely. As a\nresult, methods for automatically producing content are valuable tools to\naddress this information overload. We show that recent advances in pretrained\nlanguage modeling can be combined for a two-stage extractive and abstractive\napproach for Wikipedia lead paragraph generation. We extend this approach to\ngenerate longer Wikipedia-style summaries with sections and examine how such\nmethods struggle in this application through detailed studies with 100\nreference human-collected surveys. This is the first study on utilizing web\nresources for long Wikipedia-style summaries to the best of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamura_R/0/1/0/all/0/1\">Rina Kawamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_J/0/1/0/all/0/1\">Jaesung Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sally Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1\">Tomoe Mizutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06604","description":"<p>The Electronic Health Record (EHR) is an essential part of the modern medical\nsystem and impacts healthcare delivery, operations, and research. Unstructured\ntext is attracting much attention despite structured information in the EHRs\nand has become an exciting research field. The success of the recent neural\nNatural Language Processing (NLP) method has led to a new direction for\nprocessing unstructured clinical notes. In this work, we create a python\nlibrary for clinical texts, EHRKit. This library contains two main parts:\nMIMIC-III-specific functions and tasks specific functions. The first part\nintroduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,\nincluding basic search, information retrieval, and information extraction. The\nsecond part integrates many third-party libraries for up to 12 off-shelf NLP\ntasks such as named entity recognition, summarization, machine translation,\netc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Keen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yujie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lucas Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chia-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosand_B/0/1/0/all/0/1\">Benjamin Rosand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.03715","description":"<p>Commonsense reasoning systems should be able to generalize to diverse\nreasoning cases. However, most state-of-the-art approaches depend on expensive\ndata annotations and overfit to a specific benchmark without learning how to\nperform general semantic reasoning. To overcome these drawbacks, zero-shot QA\nsystems have shown promise as a robust learning scheme by transforming a\ncommonsense knowledge graph (KG) into synthetic QA-form samples for model\ntraining. Considering the increasing type of different commonsense KGs, this\npaper aims to extend the zero-shot transfer learning scenario into\nmultiple-source settings, where different KGs can be utilized synergetically.\nTowards this goal, we propose to mitigate the loss of knowledge from the\ninterference among the different knowledge sources, by developing a modular\nvariant of the knowledge aggregation as a new zero-shot commonsense reasoning\nframework. Results on five commonsense reasoning benchmarks demonstrate the\nefficacy of our framework, improving the performance with multiple KGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yu Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Beong-woo Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAAMA 2.0: An Integrated System that Answers Boolean and Extractive Questions. (arXiv:2206.08441v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.08441","description":"<p>Recent machine reading comprehension datasets include extractive and boolean\nquestions but current approaches do not offer integrated support for answering\nboth question types. We present a multilingual machine reading comprehension\nsystem and front-end demo that handles boolean questions by providing both a\nYES/NO answer and highlighting supporting evidence, and handles extractive\nquestions by highlighting the answer in the passage. Our system, GAAMA 2.0, is\nranked first on the Tydi QA leaderboard at the time of this writing. We\ncontrast two different implementations of our approach. The first includes\nseveral independent stacks of transformers allowing easy deployment of each\ncomponent. The second is a single stack of transformers utilizing adapters to\nreduce GPU memory footprint in a resource-constrained environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCarley_S/0/1/0/all/0/1\">Scott McCarley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferritto_A/0/1/0/all/0/1\">Anthony Ferritto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework. (arXiv:2206.10620v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10620","description":"<p>There is a growing demand for shifting the delivery of AI capability from\ndata centers on the cloud to edge or end devices, exemplified by the fast\nemerging real-time AI-based apps running on smartphones, AR/VR devices,\nautonomous vehicles, and various IoT devices. The shift has however been\nseriously hampered by the large growing gap between DNN computing demands and\nthe computing power on edge or end devices. This article presents the design of\nXGen, an optimizing framework for DNN designed to bridge the gap. XGen takes\ncross-cutting co-design as its first-order consideration. Its full-stack\nAI-oriented optimizations consist of a number of innovative optimizations at\nevery layer of the DNN software stack, all designed in a cooperative manner.\nThe unique technology makes XGen able to optimize various DNNs, including those\nwith an extreme depth (e.g., BERT, GPT, other transformers), and generate code\nthat runs several times faster than those from existing DNN frameworks, while\ndelivering the same level of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaofeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xipeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BOSS: A Benchmark for Human Belief Prediction in Object-context Scenarios. (arXiv:2206.10665v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10665","description":"<p>Humans with an average level of social cognition can infer the beliefs of\nothers based solely on the nonverbal communication signals (e.g. gaze, gesture,\npose and contextual information) exhibited during social interactions. This\nsocial cognitive ability to predict human beliefs and intentions is more\nimportant than ever for ensuring safe human-robot interaction and\ncollaboration. This paper uses the combined knowledge of Theory of Mind (ToM)\nand Object-Context Relations to investigate methods for enhancing collaboration\nbetween humans and autonomous systems in environments where verbal\ncommunication is prohibited. We propose a novel and challenging multimodal\nvideo dataset for assessing the capability of artificial intelligence (AI)\nsystems in predicting human belief states in an object-context scenario. The\nproposed dataset consists of precise labelling of human belief state\nground-truth and multimodal inputs replicating all nonverbal communication\ninputs captured by human perception. We further evaluate our dataset with\nexisting deep learning models and provide new insights into the effects of the\nvarious input modalities and object-context relations on the performance of the\nbaseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samson Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_N/0/1/0/all/0/1\">Nicholas Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCIM: Simultaneous Clustering, Inference, and Mapping for Open-World Semantic Scene Understanding. (arXiv:2206.10670v1 [cs.RO])","link":"http://arxiv.org/abs/2206.10670","description":"<p>In order to operate in human environments, a robot's semantic perception has\nto overcome open-world challenges such as novel objects and domain gaps.\nAutonomous deployment to such environments therefore requires robots to update\ntheir knowledge and learn without supervision. We investigate how a robot can\nautonomously discover novel semantic classes and improve accuracy on known\nclasses when exploring an unknown environment. To this end, we develop a\ngeneral framework for mapping and clustering that we then use to generate a\nself-supervised learning signal to update a semantic segmentation model. In\nparticular, we show how clustering parameters can be optimized during\ndeployment and that fusion of multiple observation modalities improves novel\nobject discovery compared to prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1\">Hermann Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Marcus G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawel_A/0/1/0/all/0/1\">Abel Gawel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Backdoor Datasets. (arXiv:2206.10673v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10673","description":"<p>Extensive literature on backdoor poison attacks has studied attacks and\ndefenses for backdoors using \"digital trigger patterns.\" In contrast, \"physical\nbackdoors\" use physical objects as triggers, have only recently been\nidentified, and are qualitatively different enough to resist all defenses\ntargeting digital trigger backdoors. Research on physical backdoors is limited\nby access to large datasets containing real images of physical objects\nco-located with targets of classification. Building these datasets is time- and\nlabor-intensive. This works seeks to address the challenge of accessibility for\nresearch on physical backdoor attacks. We hypothesize that there may be\nnaturally occurring physically co-located objects already present in popular\ndatasets such as ImageNet. Once identified, a careful relabeling of these data\ncan transform them into training samples for physical backdoor attacks. We\npropose a method to scalably identify these subsets of potential triggers in\nexisting datasets, along with the specific classes they can poison. We call\nthese naturally occurring trigger-class subsets natural backdoor datasets. Our\ntechniques successfully identify natural backdoors in widely-available\ndatasets, and produce models behaviorally equivalent to those trained on\nmanually curated datasets. We release our code to allow the research community\nto create their own datasets for research on physical backdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1\">Roma Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1\">Arjun Nitin Bhagoji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passananti_J/0/1/0/all/0/1\">Josephine Passananti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andere_E/0/1/0/all/0/1\">Emilio Andere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Continuous Rotation Canonicalization with Radial Beam Sampling. (arXiv:2206.10690v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10690","description":"<p>Nearly all state of the art vision models are sensitive to image rotations.\nExisting methods often compensate for missing inductive biases by using\naugmented training data to learn pseudo-invariances. Alongside the resource\ndemanding data inflation process, predictions often poorly generalize. The\ninductive biases inherent to convolutional neural networks allow for\ntranslation equivariance through kernels acting parallely to the horizontal and\nvertical axes of the pixel grid. This inductive bias, however, does not allow\nfor rotation equivariance. We propose a radial beam sampling strategy along\nwith radial kernels operating on these beams to inherently incorporate\ncenter-rotation covariance. Together with an angle distance loss, we present a\nradial beam-based image canonicalization model, short BIC. Our model allows for\nmaximal continuous angle regression and canonicalizes arbitrary center-rotated\ninput images. As a pre-processing model, this enables rotation-invariant vision\npipelines with model-agnostic rotation-sensitive downstream predictions. We\nshow that our end-to-end trained angle regressor is able to predict continuous\nrotation angles on several vision datasets, i.e. FashionMNIST, CIFAR10,\nCOIL100, and LFW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1\">Johann Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1\">Sebastian Stober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Domain Adaptation for Lane Detection. (arXiv:2206.10692v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10692","description":"<p>We focus on bridging domain discrepancy in lane detection among different\nscenarios to greatly reduce extra annotation and re-training costs for\nautonomous driving. Critical factors hinder the performance improvement of\ncross-domain lane detection that conventional methods only focus on pixel-wise\nloss while ignoring shape and position priors of lanes. To address the issue,\nwe propose the Multi-level Domain Adaptation (MLDA) framework, a new\nperspective to handle cross-domain lane detection at three complementary\nsemantic levels of pixel, instance and category. Specifically, at pixel level,\nwe propose to apply cross-class confidence constraints in self-training to\ntackle the imbalanced confidence distribution of lane and background. At\ninstance level, we go beyond pixels to treat segmented lanes as instances and\nfacilitate discriminative features in target domain with triplet learning,\nwhich effectively rebuilds the semantic context of lanes and contributes to\nalleviating the feature confusion. At category level, we propose an adaptive\ninter-domain embedding module to utilize the position prior of lanes during\nadaptation. In two challenging datasets, ie TuSimple and CULane, our approach\nimproves lane detection performance by a large margin with gains of 8.8% on\naccuracy and 7.4% on F1-score respectively, compared with state-of-the-art\ndomain adaptation algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning. (arXiv:2206.10698v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10698","description":"<p>We present Transformation Invariance and Covariance Contrast (TiCo) for\nself-supervised visual representation learning. Similar to other recent\nself-supervised learning methods, our method is based on maximizing the\nagreement among embeddings of different distorted versions of the same image,\nwhich pushes the encoder to produce transformation invariant representations.\nTo avoid the trivial solution where the encoder generates constant vectors, we\nregularize the covariance matrix of the embeddings from different images by\npenalizing low rank solutions. By jointly minimizing the transformation\ninvariance loss and covariance contrast loss, we get an encoder that is able to\nproduce useful representations for downstream tasks. We analyze our method and\nshow that it can be viewed as a variant of MoCo with an implicit memory bank of\nunlimited size at no extra memory cost. This makes our method perform better\nthan alternative methods when using small batch sizes. TiCo can also be seen as\na modification of Barlow Twins. By connecting the contrastive and\nredundancy-reduction methods together, TiCo gives us new insights into how\njoint embedding methods work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiachen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraes_R/0/1/0/all/0/1\">Rafael M. Moraes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakulak_S/0/1/0/all/0/1\">Serkan Karakulak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobol_V/0/1/0/all/0/1\">Vlad Sobol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canziani_A/0/1/0/all/0/1\">Alfredo Canziani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoramic Panoptic Segmentation: Insights Into Surrounding Parsing for Mobile Agents via Unsupervised Contrastive Learning. (arXiv:2206.10711v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10711","description":"<p>In this work, we introduce panoramic panoptic segmentation, as the most\nholistic scene understanding, both in terms of Field of View (FoV) and\nimage-level understanding for standard camera-based input. A complete\nsurrounding understanding provides a maximum of information to a mobile agent,\nwhich is essential for any intelligent vehicle in order to make informed\ndecisions in a safety-critical dynamic environment such as real-world traffic.\nIn order to overcome the lack of annotated panoramic images, we propose a\nframework which allows model training on standard pinhole images and transfers\nthe learned features to a different domain in a cost-minimizing way. Using our\nproposed method with dense contrastive learning, we manage to achieve\nsignificant improvements over a non-adapted approach. Depending on the\nefficient panoptic segmentation architecture, we can improve 3.5-6.5% measured\nin Panoptic Quality (PQ) over non-adapted models on our established Wild\nPanoramic Panoptic Segmentation (WildPPS) dataset. Furthermore, our efficient\nframework does not need access to the images of the target domain, making it a\nfeasible domain generalization approach suitable for a limited hardware\nsetting. As additional contributions, we publish WildPPS: The first panoramic\npanoptic image dataset to foster progress in surrounding perception and explore\na novel training procedure combining supervised and contrastive training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaus_A/0/1/0/all/0/1\">Alexander Jaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Color Embeddings for Splicing Localization in Severely Degraded Images. (arXiv:2206.10737v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10737","description":"<p>One common task in image forensics is to detect spliced images, where\nmultiple source images are composed to one output image. Most of the currently\nbest performing splicing detectors leverage high-frequency artifacts. However,\nafter an image underwent strong compression, most of the high frequency\nartifacts are not available anymore. In this work, we explore an alternative\napproach to splicing detection, which is potentially better suited for images\nin-the-wild, subject to strong compression and downsampling. Our proposal is to\nmodel the color formation of an image. The color formation largely depends on\nvariations at the scale of scene objects, and is hence much less dependent on\nhigh-frequency artifacts. We learn a deep metric space that is on one hand\nsensitive to illumination color and camera white-point estimation, but on the\nother hand insensitive to variations in object color. Large distances in the\nembedding space indicate that two image regions either stem from different\nscenes or different cameras. In our evaluation, we show that the proposed\nembedding space outperforms the state of the art on images that have been\nsubject to strong compression and downsampling. We confirm in two further\nexperiments the dual nature of the metric space, namely to both characterize\nthe acquisition camera and the scene illuminant color. As such, this work\nresides at the intersection of physics-based and statistical forensics with\nbenefits from both sides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hadwiger_B/0/1/0/all/0/1\">Benjamin Hadwiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Floor Map Reconstruction Through Radio Sensing and Learning By a Large Intelligent Surface. (arXiv:2206.10750v1 [eess.SP])","link":"http://arxiv.org/abs/2206.10750","description":"<p>Environmental scene reconstruction is of great interest for autonomous\nrobotic applications, since an accurate representation of the environment is\nnecessary to ensure safe interaction with robots. Equally important, it is also\nvital to ensure reliable communication between the robot and its controller.\nLarge Intelligent Surface (LIS) is a technology that has been extensively\nstudied due to its communication capabilities. Moreover, due to the number of\nantenna elements, these surfaces arise as a powerful solution to radio sensing.\nThis paper presents a novel method to translate radio environmental maps\nobtained at the LIS to floor plans of the indoor environment built of\nscatterers spread along its area. The usage of a Least Squares (LS) based\nmethod, U-Net (UN) and conditional Generative Adversarial Networks (cGANs) were\nleveraged to perform this task. We show that the floor plan can be correctly\nreconstructed using both local and global measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vaca_Rubio_C/0/1/0/all/0/1\">Cristian J. Vaca-Rubio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1\">Roberto Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mestre_X/0/1/0/all/0/1\">Xavier Mestre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gregoratti_D/0/1/0/all/0/1\">David Gregoratti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carvalho_E/0/1/0/all/0/1\">Elisabeth de Carvalho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Popovski_P/0/1/0/all/0/1\">Petar Popovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Ground Truth for Single Image Deraining. (arXiv:2206.10779v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10779","description":"<p>We propose a large-scale dataset of real-world rainy and clean image pairs\nand a method to remove degradations, induced by rain streaks and rain\naccumulation, from the image. As there exists no real-world dataset for\nderaining, current state-of-the-art methods rely on synthetic data and thus are\nlimited by the sim2real domain gap; moreover, rigorous evaluation remains a\nchallenge due to the absence of a real paired dataset. We fill this gap by\ncollecting the first real paired deraining dataset through meticulous control\nof non-rain variations. Our dataset enables paired training and quantitative\nevaluation for diverse real-world rain phenomena (e.g. rain streaks and rain\naccumulation). To learn a representation invariant to rain phenomena, we\npropose a deep neural network that reconstructs the underlying scene by\nminimizing a rain-invariant loss between rainy and clean images. Extensive\nexperiments demonstrate that the proposed dataset benefits existing derainers,\nand our model can outperform the state-of-the-art deraining methods on real\nrainy images under various conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1\">Yunhao Ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Howard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Ethan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_A/0/1/0/all/0/1\">Akira Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahnl_A/0/1/0/all/0/1\">Arnold Pfahnl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrappa_C/0/1/0/all/0/1\">Chethan Chinder Chandrappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_C/0/1/0/all/0/1\">Celso de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1\">Achuta Kadambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. (arXiv:2206.10789v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10789","description":"<p>We present the Pathways Autoregressive Text-to-Image (Parti) model, which\ngenerates high-fidelity photorealistic images and supports content-rich\nsynthesis involving complex compositions and world knowledge. Parti treats\ntext-to-image generation as a sequence-to-sequence modeling problem, akin to\nmachine translation, with sequences of image tokens as the target outputs\nrather than text tokens in another language. This strategy can naturally tap\ninto the rich body of prior work on large language models, which have seen\ncontinued advances in capabilities and performance through scaling data and\nmodel sizes. Our approach is simple: First, Parti uses a Transformer-based\nimage tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens.\nSecond, we achieve consistent quality improvements by scaling the\nencoder-decoder Transformer model up to 20B parameters, with a new\nstate-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on\nMS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts\n(P2), a new holistic benchmark of over 1600 English prompts, demonstrate the\neffectiveness of Parti across a wide variety of categories and difficulty\naspects. We also explore and highlight limitations of our models in order to\ndefine and exemplify key areas of focus for further improvements. See\nhttps://parti.research.google/ for high-resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baid_G/0/1/0/all/0/1\">Gunjan Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayan_B/0/1/0/all/0/1\">Burcu Karagol Ayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imitation Learning for Generalizable Self-driving Policy with Sim-to-real Transfer. (arXiv:2206.10797v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10797","description":"<p>Imitation Learning uses the demonstrations of an expert to uncover the\noptimal policy and it is suitable for real-world robotics tasks as well. In\nthis case, however, the training of the agent is carried out in a simulation\nenvironment due to safety, economic and time constraints. Later, the agent is\napplied in the real-life domain using sim-to-real methods. In this paper, we\napply Imitation Learning methods that solve a robotics task in a simulated\nenvironment and use transfer learning to apply these solutions in the\nreal-world environment. Our task is set in the Duckietown environment, where\nthe robotic agent has to follow the right lane based on the input images of a\nsingle forward-facing camera. We present three Imitation Learning and two\nsim-to-real methods capable of achieving this task. A detailed comparison is\nprovided on these techniques to highlight their advantages and disadvantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorincz_Z/0/1/0/all/0/1\">Zolt&#xe1;n L&#x151;rincz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szemenyei_M/0/1/0/all/0/1\">M&#xe1;rton Szemenyei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moni_R/0/1/0/all/0/1\">R&#xf3;bert Moni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI. (arXiv:2206.10802v1 [eess.IV])","link":"http://arxiv.org/abs/2206.10802","description":"<p>Volumetric reconstruction of fetal brains from multiple stacks of MR slices,\nacquired in the presence of almost unpredictable and often severe subject\nmotion, is a challenging task that is highly sensitive to the initialization of\nslice-to-volume transformations. We propose a novel slice-to-volume\nregistration method using Transformers trained on synthetically transformed\ndata, which model multiple stacks of MR slices as a sequence. With the\nattention mechanism, our model automatically detects the relevance between\nslices and predicts the transformation of one slice using information from\nother slices. We also estimate the underlying 3D volume to assist\nslice-to-volume registration and update the volume and transformations\nalternately to improve accuracy. Results on synthetic data show that our method\nachieves lower registration error and better reconstruction quality compared\nwith existing state-of-the-art methods. Experiments with real-world MRI data\nare also performed to demonstrate the ability of the proposed model to improve\nthe quality of 3D reconstruction under severe fetal motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSMI: How to Make Objects of Interest Disappear without Accessing Object Detectors?. (arXiv:2206.10809v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10809","description":"<p>Most black-box adversarial attack schemes for object detectors mainly face\ntwo shortcomings: requiring access to the target model and generating\ninefficient adversarial examples (failing to make objects disappear in large\nnumbers). To overcome these shortcomings, we propose a black-box adversarial\nattack scheme based on semantic segmentation and model inversion (SSMI). We\nfirst locate the position of the target object using semantic segmentation\ntechniques. Next, we design a neighborhood background pixel replacement to\nreplace the target region pixels with background pixels to ensure that the\npixel modifications are not easily detected by human vision. Finally, we\nreconstruct a machine-recognizable example and use the mask matrix to select\npixels in the reconstructed example to modify the benign image to generate an\nadversarial example. Detailed experimental results show that SSMI can generate\nefficient adversarial examples to evade human-eye perception and make objects\nof interest disappear. And more importantly, SSMI outperforms existing same\nkinds of attacks. The maximum increase in new and disappearing labels is 16%,\nand the maximum decrease in mAP metrics for object detection is 36%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Hui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuliang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Attention is Needed: Grouped Spatial-temporal Shift for Simple and Efficient Video Restorers. (arXiv:2206.10810v1 [eess.IV])","link":"http://arxiv.org/abs/2206.10810","description":"<p>Video restoration, aiming at restoring clear frames from degraded videos, has\nbeen attracting increasing attention. Video restoration is required to\nestablish the temporal correspondences from multiple misaligned frames. To\nachieve that end, existing deep methods generally adopt complicated network\narchitectures, such as integrating optical flow, deformable convolution,\ncross-frame or cross-pixel self-attention layers, resulting in expensive\ncomputational cost. We argue that with proper design, temporal information\nutilization in video restoration can be much more efficient and effective. In\nthis study, we propose a simple, fast yet effective framework for video\nrestoration. The key of our framework is the grouped spatial-temporal shift,\nwhich is simple and lightweight, but can implicitly establish inter-frame\ncorrespondences and achieve multi-frame aggregation. Coupled with basic 2D\nU-Nets for frame-wise encoding and decoding, such an efficient spatial-temporal\nshift module can effectively tackle the challenges in video restoration.\nExtensive experiments show that our framework surpasses previous\nstate-of-the-art method with 43% of its computational cost on both video\ndeblurring and video denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Dasong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting Fire with Fire: Avoiding DNN Shortcuts through Priming. (arXiv:2206.10816v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10816","description":"<p>Across applications spanning supervised classification and sequential\ncontrol, deep learning has been reported to find \"shortcut\" solutions that fail\ncatastrophically under minor changes in the data distribution. In this paper,\nwe show empirically that DNNs can be coaxed to avoid poor shortcuts by\nproviding an additional \"priming\" feature computed from key input features,\nusually a coarse output estimate. Priming relies on approximate domain\nknowledge of these task-relevant key input features, which is often easy to\nobtain in practical settings. For example, one might prioritize recent frames\nover past frames in a video input for visual imitation learning, or salient\nforeground over background pixels for image classification. On NICO image\nclassification, MuJoCo continuous control, and CARLA autonomous driving, our\npriming strategy works significantly better than several popular\nstate-of-the-art approaches for feature selection and data augmentation. We\nconnect these empirical findings to recent theoretical results on DNN\noptimization, and argue theoretically that priming distracts the optimizer away\nfrom poor shortcuts by creating better, simpler shortcuts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jierui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1\">Jiaye Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupling Visual Semantics of Artificial Neural Networks and Human Brain Function via Synchronized Activations. (arXiv:2206.10821v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10821","description":"<p>Artificial neural networks (ANNs), originally inspired by biological neural\nnetworks (BNNs), have achieved remarkable successes in many tasks such as\nvisual representation learning. However, whether there exists semantic\ncorrelations/connections between the visual representations in ANNs and those\nin BNNs remains largely unexplored due to both the lack of an effective tool to\nlink and couple two different domains, and the lack of a general and effective\nframework of representing the visual semantics in BNNs such as human functional\nbrain networks (FBNs). To answer this question, we propose a novel\ncomputational framework, Synchronized Activations (Sync-ACT), to couple the\nvisual representation spaces and semantics between ANNs and BNNs in human brain\nbased on naturalistic functional magnetic resonance imaging (nfMRI) data. With\nthis approach, we are able to semantically annotate the neurons in ANNs with\nbiologically meaningful description derived from human brain imaging for the\nfirst time. We evaluated the Sync-ACT framework on two publicly available\nmovie-watching nfMRI datasets. The experiments demonstrate a) the significant\ncorrelation and similarity of the semantics between the visual representations\nin FBNs and those in a variety of convolutional neural networks (CNNs) models;\nb) the close relationship between CNN's visual representation similarity to\nBNNs and its performance in image classification tasks. Overall, our study\nintroduces a general and effective paradigm to couple the ANNs and BNNs and\nprovides novel insights for future studies such as brain-inspired artificial\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenxiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">David Weizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing. (arXiv:2206.10830v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10830","description":"<p>Recent advances in the industrial inspection of textured surfaces-in the form\nof visual inspection-have made such inspections possible for efficient,\nflexible manufacturing systems. We propose an unsupervised feature memory\nrearrangement network (FMR-Net) to accurately detect various textural defects\nsimultaneously. Consistent with mainstream methods, we adopt the idea of\nbackground reconstruction; however, we innovatively utilize artificial\nsynthetic defects to enable the model to recognize anomalies, while traditional\nwisdom relies only on defect-free samples. First, we employ an encoding module\nto obtain multiscale features of the textured surface. Subsequently, a\ncontrastive-learning-based memory feature module (CMFM) is proposed to obtain\ndiscriminative representations and construct a normal feature memory bank in\nthe latent space, which can be employed as a substitute for defects and fast\nanomaly scores at the patch level. Next, a novel global feature rearrangement\nmodule (GFRM) is proposed to further suppress the reconstruction of residual\ndefects. Finally, a decoding module utilizes the restored features to\nreconstruct the normal texture background. In addition, to improve inspection\nperformance, a two-phase training strategy is utilized for accurate defect\nrestoration refinement, and we exploit a multimodal inspection method to\nachieve noise-robust defect localization. We verify our method through\nextensive experiments and test its practical deployment in collaborative\nedge--cloud intelligent manufacturing scenarios by means of a multilevel\ndetection method, demonstrating that FMR-Net exhibits state-of-the-art\ninspection accuracy and shows great potential for use in edge-computing-enabled\nsmart industries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Haiming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenyong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEarth 2022 Deforestation Challenge -- ForestGump. (arXiv:2206.10831v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10831","description":"<p>The estimation of deforestation in the Amazon Forest is challenge task\nbecause of the vast size of the area and the difficulty of direct human access.\nHowever, it is a crucial problem in that deforestation results in serious\nenvironmental problems such as global climate change, reduced biodiversity,\netc. In order to effectively solve the problems, satellite imagery would be a\ngood alternative to estimate the deforestation of the Amazon. With a\ncombination of optical images and Synthetic aperture radar (SAR) images,\nobservation of such a massive area regardless of weather conditions become\npossible. In this paper, we present an accurate deforestation estimation method\nwith conventional UNet and comprehensive data processing. The diverse channels\nof Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to\ntrain deep neural networks. With the proposed method, deforestation status for\nnovel queries are successfully estimated with high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yeonju Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Debiased Classifier with Biased Committee. (arXiv:2206.10843v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10843","description":"<p>Neural networks are prone to be biased towards spurious correlations between\nclasses and latent attributes exhibited in a major portion of training data,\nwhich ruins their generalization capability. This paper proposes a new method\nfor training debiased classifiers with no spurious attribute label. The key\nidea of the method is to employ a committee of classifiers as an auxiliary\nmodule that identifies bias-conflicting data, i.e., data without spurious\ncorrelations, and assigns large weights to them when training the main\nclassifier. The committee is learned as a bootstrapped ensemble so that a\nmajority of its classifiers are biased as well as being diverse, and\nintentionally fail to predict classes of bias-conflicting data accordingly. The\nconsensus within the committee on prediction difficulty thus provides a\nreliable cue for identifying and weighting bias-conflicting data. Moreover, the\ncommittee is also trained with knowledge transferred from the main classifier\nso that it gradually becomes debiased along with the main classifier and\nemphasizes more difficult data as training progresses. On five real-world\ndatasets, our method outperforms existing methods using no spurious attribute\nlabel like ours and even surpasses those relying on bias labels occasionally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nayeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sehyun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation. (arXiv:2206.10845v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10845","description":"<p>Recently, Synthetic data-based Instance Segmentation has become an\nexceedingly favorable optimization paradigm since it leverages simulation\nrendering and physics to generate high-quality image-annotation pairs. In this\npaper, we propose a Parallel Pre-trained Transformers (PPT) framework to\naccomplish the synthetic data-based Instance Segmentation task. Specifically,\nwe leverage the off-the-shelf pre-trained vision Transformers to alleviate the\ngap between natural and synthetic data, which helps to provide good\ngeneralization in the downstream synthetic data scene with few samples.\nSwin-B-based CBNet V2, SwinL-based CBNet V2 and Swin-L-based Uniformer are\nemployed for parallel feature learning, and the results of these three models\nare fused by pixel-level Non-maximum Suppression (NMS) algorithm to obtain more\nrobust results. The experimental results reveal that PPT ranks first in the\nCVPR2022 AVA Accessibility Vision and Autonomy Challenge, with a 65.155% mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xin Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniCon+: ICTCAS-UCAS Submission to the AVA-ActiveSpeaker Task at ActivityNet Challenge 2022. (arXiv:2206.10861v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10861","description":"<p>This report presents a brief description of our winning solution to the AVA\nActive Speaker Detection (ASD) task at ActivityNet Challenge 2022. Our\nunderlying model UniCon+ continues to build on our previous work, the Unified\nContext Network (UniCon) and Extended UniCon which are designed for robust\nscene-level ASD. We augment the architecture with a simple GRU-based module\nthat allows information of recurring identities to flow across scenes through\nread and update operations. We report a best result of 94.47% mAP on the\nAVA-ActiveSpeaker test set, which continues to rank first on this year's\nchallenge leaderboard and significantly pushes the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Susan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NVIDIA-UNIBZ Submission for EPIC-KITCHENS-100 Action Anticipation Challenge 2022. (arXiv:2206.10869v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10869","description":"<p>In this report, we describe the technical details of our submission for the\nEPIC-Kitchen-100 action anticipation challenge. Our modelings, the higher-order\nrecurrent space-time transformer and the message-passing neural network with\nedge learning, are both recurrent-based architectures which observe only 2.5\nseconds inference context to form the action anticipation prediction. By\naveraging the prediction scores from a set of models compiled with our proposed\ntraining pipeline, we achieved strong performance on the test set, which is\n19.61% overall mean top-5 recall, recorded as second place on the public\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_T/0/1/0/all/0/1\">Tsung-Ming Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yi-Kwan Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1\">Sze-Sen Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheng-Kuang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka-Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Re-calibration based MIL for Whole Slide Image Classification. (arXiv:2206.10878v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10878","description":"<p>Whole slide image (WSI) classification is a fundamental task for the\ndiagnosis and treatment of diseases; but, curation of accurate labels is\ntime-consuming and limits the application of fully-supervised methods. To\naddress this, multiple instance learning (MIL) is a popular method that poses\nclassification as a weakly supervised learning task with slide-level labels\nonly. While current MIL methods apply variants of the attention mechanism to\nre-weight instance features with stronger models, scant attention is paid to\nthe properties of the data distribution. In this work, we propose to\nre-calibrate the distribution of a WSI bag (instances) by using the statistics\nof the max-instance (critical) feature. We assume that in binary MIL, positive\nbags have larger feature magnitudes than negatives, thus we can enforce the\nmodel to maximize the discrepancy between bags with a metric feature loss that\nmodels positive bags as out-of-distribution. To achieve this, unlike existing\nMIL methods that use single-batch training modes, we propose balanced-batch\nsampling to effectively use the feature loss i.e., (+/-) bags simultaneously.\nFurther, we employ a position encoding module (PEM) to model\nspatial/morphological information, and perform pooling by multi-head\nself-attention (PSMA) with a Transformer encoder. Experimental results on\nexisting benchmark datasets show our approach is effective and improves over\nstate-of-the-art MIL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Soo Jeong Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Heounjeong Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Meejeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_H/0/1/0/all/0/1\">Hyun Jung Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetric Network with Spatial Relationship Modeling for Natural Language-based Vehicle Retrieval. (arXiv:2206.10879v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10879","description":"<p>Natural language (NL) based vehicle retrieval aims to search specific vehicle\ngiven text description. Different from the image-based vehicle retrieval,\nNL-based vehicle retrieval requires considering not only vehicle appearance,\nbut also surrounding environment and temporal relations. In this paper, we\npropose a Symmetric Network with Spatial Relationship Modeling (SSM) method for\nNL-based vehicle retrieval. Specifically, we design a symmetric network to\nlearn the unified cross-modal representations between text descriptions and\nvehicle images, where vehicle appearance details and vehicle trajectory global\ninformation are preserved. Besides, to make better use of location information,\nwe propose a spatial relationship modeling methods to take surrounding\nenvironment and mutual relationship between vehicles into consideration. The\nqualitative and quantitative experiments verify the effectiveness of the\nproposed method. We achieve 43.92% MRR accuracy on the test set of the 6th AI\nCity Challenge on natural language-based vehicle retrieval track, yielding the\n1st place among all valid submissions on the public leaderboard. The code is\navailable at https://github.com/hbchen121/AICITY2022_Track2_SSM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haobo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yadong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boxun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KiloNeuS: Implicit Neural Representations with Real-Time Global Illumination. (arXiv:2206.10885v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10885","description":"<p>The latest trends in inverse rendering techniques for reconstruction use\nneural networks to learn 3D representations as neural fields. NeRF-based\ntechniques fit multi-layer perceptrons (MLPs) to a set of training images to\nestimate a radiance field which can then be rendered from any virtual camera by\nmeans of volume rendering algorithms. Major drawbacks of these representations\nare the lack of well-defined surfaces and non-interactive rendering times, as\nwide and deep MLPs must be queried millions of times per single frame. These\nlimitations have recently been singularly overcome, but managing to accomplish\nthis simultaneously opens up new use cases. We present KiloNeuS, a new neural\nobject representation that can be rendered in path-traced scenes at interactive\nframe rates. KiloNeuS enables the simulation of realistic light interactions\nbetween neural and classic primitives in shared scenes, and it demonstrably\nperforms in real-time with plenty of room for future optimizations and\nextensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esposito_S/0/1/0/all/0/1\">Stefano Esposito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baieri_D/0/1/0/all/0/1\">Daniele Baieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellmann_S/0/1/0/all/0/1\">Stefan Zellmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinkenjann_A/0/1/0/all/0/1\">Andr&#xe9; Hinkenjann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation. (arXiv:2206.10886v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10886","description":"<p>Recent works have shown the ability of Implicit Neural Representations (INR)\nto carry meaningful representations of signal derivatives. In this work, we\nleverage this property to perform Video Frame Interpolation (VFI) by explicitly\nconstraining the derivatives of the INR to satisfy the optical flow constraint\nequation. We achieve state of the art VFI on limited motion ranges using only a\ntarget video and its optical flow, without learning the interpolation operator\nfrom additional training data. We further show that constraining the INR\nderivatives not only allows to better interpolate intermediate frames but also\nimproves the ability of narrow networks to fit the observed frames, which\nsuggests potential applications to video compression and INR optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weihao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hascoet_T/0/1/0/all/0/1\">Tristan Hascoet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_R/0/1/0/all/0/1\">Ryoichi Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takiguchi_T/0/1/0/all/0/1\">Tetsuya Takiguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation. (arXiv:2206.10892v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10892","description":"<p>In this paper, we present the Intra- and Inter-Human Relation Networks\n(I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules.\nFirst, the Intra-Human Relation Module operates on a single person and aims to\ncapture Intra-Human dependencies. Second, the Inter-Human Relation Module\nconsiders the relation between multiple instances and focuses on capturing\nInter-Human interactions. The Inter-Human Relation Module can be designed very\nlightweight by reducing the resolution of feature map, yet learn useful\nrelation information to significantly boost the performance of the Intra-Human\nRelation Module. Even without bells and whistles, our method can compete or\noutperform current competition winners. We conduct extensive experiments on\nCOCO, CrowdPose, and OCHuman datasets. The results demonstrate that the\nproposed model surpasses all the state-of-the-art methods. Concretely, the\nproposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman\ndataset respectively, outperforming existing methods by a large margin.\nAdditionally, the ablation study and visualization analysis also prove the\neffectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Wenjin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving. (arXiv:2206.10902v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10902","description":"<p>To safely and rationally participate in dense and heterogeneous traffic,\nautonomous vehicles require to sufficiently analyze the motion patterns of\nsurrounding traffic-agents and accurately predict their future trajectories.\nThis is challenging because the trajectories of traffic-agents are not only\ninfluenced by the traffic-agents themselves but also by spatial interaction\nwith each other. Previous methods usually rely on the sequential step-by-step\nprocessing of Long Short-Term Memory networks (LSTMs) and merely extract the\ninteractions between spatial neighbors for single type traffic-agents. We\npropose the Spatio-Temporal Transformer Networks (S2TNet), which models the\nspatio-temporal interactions by spatio-temporal Transformer and deals with the\ntemporel sequences by temporal Transformer. We input additional category, shape\nand heading information into our networks to handle the heterogeneity of\ntraffic-agents. The proposed methods outperforms state-of-the-art methods on\nApolloScape Trajectory dataset by more than 7\\% on both the weighted sum of\nAverage and Final Displacement Error. Our code is available at\nhttps://github.com/chenghuang66/s2tnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangfang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongbin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniUD-FBK-UB-UniBZ Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.10903v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10903","description":"<p>This report presents the technical details of our submission to the\nEPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022. To participate in\nthe challenge, we designed an ensemble consisting of different models trained\nwith two recently developed relevance-augmented versions of the widely used\ntriplet loss. Our submission, visible on the public leaderboard, obtains an\naverage score of 61.02% nDCG and 49.77% mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1\">Alex Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10910","description":"<p>In this paper, we propose an end-to-end SpA-Former to recover a shadow-free\nimage from a single shaded image. Unlike traditional methods that require two\nsteps for shadow detection and then shadow removal, the SpA-Former unifies\nthese steps into one, which is a one-stage network capable of directly learning\nthe mapping function between shadows and no shadows, it does not require a\nseparate shadow detection. Thus, SpA-former is adaptable to real image\nde-shadowing for shadows projected on different semantic regions. SpA-Former\nconsists of transformer layer and a series of joint Fourier transform residual\nblocks and two-wheel joint spatial attention. The network in this paper is able\nto handle the task while achieving a very fast processing efficiency.\n</p>\n<p>Our code is relased on https://github.com/\nzhangbaijin/Spatial-Transformer-shadow-removal\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chao Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shan Ying Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence of uncertainty estimation techniques on false-positive reduction in liver lesion detection. (arXiv:2206.10911v1 [eess.IV])","link":"http://arxiv.org/abs/2206.10911","description":"<p>Deep learning techniques show success in detecting objects in medical images,\nbut still suffer from false-positive predictions that may hinder accurate\ndiagnosis. The estimated uncertainty of the neural network output has been used\nto flag incorrect predictions. We study the role played by features computed\nfrom neural network uncertainty estimates and shape-based features computed\nfrom binary predictions in reducing false positives in liver lesion detection\nby developing a classification-based post-processing step for different\nuncertainty estimation methods. We demonstrate an improvement in the lesion\ndetection performance of the neural network (with respect to F1-score) for all\nuncertainty estimation methods on two datasets, comprising abdominal MR and CT\nimages respectively. We show that features computed from neural network\nuncertainty estimates tend not to contribute much toward reducing false\npositives. Our results show that factors like class imbalance (true over false\npositive ratio) and shape-based features extracted from uncertainty maps play\nan important role in distinguishing false positive from true positive\npredictions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhat_I/0/1/0/all/0/1\">Ishaan Bhat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P.W. Pluim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viergerver_M/0/1/0/all/0/1\">Max A. Viergerver</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based software for lung nodule detection in chest X-rays -- Time for a second reader approach?. (arXiv:2206.10912v1 [eess.IV])","link":"http://arxiv.org/abs/2206.10912","description":"<p>Objectives: To compare artificial intelligence (AI) as a second reader in\ndetecting lung nodules on chest X-rays (CXR) versus radiologists of two\nbinational institutions, and to evaluate AI performance when using two\ndifferent modes: automated versus assisted (additional remote radiologist\nreview).\n</p>\n<p>Methods: The CXR public database (n = 247) of the Japanese Society of\nRadiological Technology with various types and sizes of lung nodules was\nanalyzed. Eight radiologists evaluated the CXR images with regard to the\npresence of lung nodules and nodule conspicuity. After radiologist review, the\nAI software processed and flagged the CXR with the highest probability of\nmissed nodules. The calculated accuracy metrics were the area under the curve\n(AUC), sensitivity, specificity, F1 score, false negative case number (FN), and\nthe effect of different AI modes (automated/assisted) on the accuracy of nodule\ndetection.\n</p>\n<p>Results: For radiologists, the average AUC value was 0.77 $\\pm$ 0.07, while\nthe average FN was 52.63 $\\pm$ 17.53 (all studies) and 32 $\\pm$ 11.59 (studies\ncontaining a nodule of malignant etiology = 32% rate of missed malignant\nnodules). Both AI modes -- automated and assisted -- produced an average\nincrease in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a\ndecrease in specificity (by 10% and 3%, respectively).\n</p>\n<p>Conclusions: Both AI modes flagged the pulmonary nodules missed by\nradiologists in a significant number of cases. AI as a second reader has a high\npotential to improve diagnostic accuracy and radiology workflow. AI might\ndetect certain pulmonary nodules earlier than radiologists, with a potentially\nsignificant impact on patient outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ohlmann_Knafo_S/0/1/0/all/0/1\">Susanne Ohlmann-Knafo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanauskas_N/0/1/0/all/0/1\">Naglis Ramanauskas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huettinger_S/0/1/0/all/0/1\">Sebastian Huettinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeyakumar_E/0/1/0/all/0/1\">Emil Johnson Jeyakumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barusauskas_D/0/1/0/all/0/1\">Darius Baru&#x161;auskas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bielskiene_N/0/1/0/all/0/1\">Neringa Bielskien&#x117;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naujalis_V/0/1/0/all/0/1\">Vytautas Naujalis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bialopetravicius_J/0/1/0/all/0/1\">Jonas Bialopetravi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razanskas_J/0/1/0/all/0/1\">Jonas Ra&#x17e;anskas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samuilis_A/0/1/0/all/0/1\">Art&#x16b;ras Samuilis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dementaviciene_J/0/1/0/all/0/1\">J&#x16b;rat&#x117; Dementavi&#x10d;ien&#x117;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pickuth_D/0/1/0/all/0/1\">Dirk Pickuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the effect of sparsity on neural networks robustness. (arXiv:2206.10915v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10915","description":"<p>This paper examines the impact of static sparsity on the robustness of a\ntrained network to weight perturbations, data corruption, and adversarial\nexamples. We show that, up to a certain sparsity achieved by increasing network\nwidth and depth while keeping the network capacity fixed, sparsified networks\nconsistently match and often outperform their initially dense versions.\nRobustness and accuracy decline simultaneously for very high sparsity due to\nloose connectivity between network layers. Our findings show that a rapid\nrobustness drop caused by network compression observed in the literature is due\nto a reduced network capacity rather than sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timpl_L/0/1/0/all/0/1\">Lukas Timpl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on the Evaluation of Generative Models. (arXiv:2206.10935v1 [cs.LG])","link":"http://arxiv.org/abs/2206.10935","description":"<p>Implicit generative models, which do not return likelihood values, such as\ngenerative adversarial networks and diffusion models, have become prevalent in\nrecent years. While it is true that these models have shown remarkable results,\nevaluating their performance is challenging. This issue is of vital importance\nto push research forward and identify meaningful gains from random noise.\nCurrently, heuristic metrics such as the Inception score (IS) and Frechet\nInception Distance (FID) are the most common evaluation metrics, but what they\nmeasure is not entirely clear. Additionally, there are questions regarding how\nmeaningful their score actually is. In this work, we study the evaluation\nmetrics of generative models by generating a high-quality synthetic dataset on\nwhich we can estimate classical metrics for comparison. Our study shows that\nwhile FID and IS do correlate to several f-divergences, their ranking of close\nmodels can vary considerably making them problematic when used for fain-grained\ncomparison. We further used this experimental setting to study which evaluation\nmetric best correlates with our probabilistic metrics. Lastly, we look into the\nbase features used for metrics such as FID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betzalel_E/0/1/0/all/0/1\">Eyal Betzalel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penso_C/0/1/0/all/0/1\">Coby Penso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polar Parametrization for Vision-based Surround-View 3D Detection. (arXiv:2206.10965v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10965","description":"<p>3D detection based on surround-view camera system is a critical technique in\nautopilot. In this work, we present Polar Parametrization for 3D detection,\nwhich reformulates position parametrization, velocity decomposition, perception\nrange, label assignment and loss function in polar coordinate system. Polar\nParametrization establishes explicit associations between image patterns and\nprediction targets, exploiting the view symmetry of surround-view cameras as\ninductive bias to ease optimization and boost performance. Based on Polar\nParametrization, we propose a surround-view 3D DEtection TRansformer, named\nPolarDETR. PolarDETR achieves promising performance-speed trade-off on\ndifferent backbone configurations. Besides, PolarDETR ranks 1st on the\nleaderboard of nuScenes benchmark in terms of both 3D detection and 3D tracking\nat the submission time (Mar. 4th, 2022). Code will be released at\n\\url{https://github.com/hustvl/PolarDETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Morphing Attack Detection using Siamese Network and Few-shot Learning. (arXiv:2206.10969v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10969","description":"<p>Face morphing attack detection is challenging and presents a concrete and\nsevere threat for face verification systems. Reliable detection mechanisms for\nsuch attacks, which have been tested with a robust cross-database protocol and\nunknown morphing tools still is a research challenge. This paper proposes a\nframework following the Few-Shot-Learning approach that shares image\ninformation based on the siamese network using triplet-semi-hard-loss to tackle\nthe morphing attack detection and boost the clustering classification process.\nThis network compares a bona fide or potentially morphed image with triplets of\nmorphing and bona fide face images. Our results show that this new network\ncluster the data points, and assigns them to classes in order to obtain a lower\nequal error rate in a cross-database scenario sharing only small image numbers\nfrom an unknown database. Few-shot learning helps to boost the learning\nprocess. Experimental results using a cross-datasets trained with FRGCv2 and\ntested with FERET and the AMSL open-access databases reduced the BPCER10 from\n43% to 4.91% using ResNet50 and 5.50% for MobileNetV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_D/0/1/0/all/0/1\">Daniel Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdvSmo: Black-box Adversarial Attack by Smoothing Linear Structure of Texture. (arXiv:2206.10988v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10988","description":"<p>Black-box attacks usually face two problems: poor transferability and the\ninability to evade the adversarial defense. To overcome these shortcomings, we\ncreate an original approach to generate adversarial examples by smoothing the\nlinear structure of the texture in the benign image, called AdvSmo. We\nconstruct the adversarial examples without relying on any internal information\nto the target model and design the imperceptible-high attack success rate\nconstraint to guide the Gabor filter to select appropriate angles and scales to\nsmooth the linear texture from the input images to generate adversarial\nexamples. Benefiting from the above design concept, AdvSmo will generate\nadversarial examples with strong transferability and solid evasiveness.\nFinally, compared to the four advanced black-box adversarial attack methods,\nfor the eight target models, the results show that AdvSmo improves the average\nattack success rate by 9% on the CIFAR-10 and 16% on the Tiny-ImageNet dataset\ncompared to the best of these attack methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Hui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuliang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zi Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Documents Authentication based on Forgery Detection of Guilloche Pattern. (arXiv:2206.10989v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10989","description":"<p>In cases such as digital enrolment via mobile and online services, identity\ndocument verification is critical in order to efficiently detect forgery and\ntherefore build user trust in the digital world. In this paper, an\nauthentication model for identity documents based on forgery detection of\nguilloche patterns is proposed. The proposed approach is made up of two steps:\nfeature extraction and similarity measure between a pair of feature vectors of\nidentity documents. The feature extraction step involves learning the\nsimilarity between a pair of identity documents via a convolutional neural\nnetwork (CNN) architecture and ends by extracting highly discriminative\nfeatures between them. While, the similarity measure step is applied to decide\nif a given identity document is authentic or forged. In this work, these two\nsteps are combined together to achieve two objectives: (i) extracted features\nshould have good anticollision (discriminative) capabilities to distinguish\nbetween a pair of identity documents belonging to different classes, (ii)\nchecking out the conformity of the guilloche pattern of a given identity\ndocument and its similarity to the guilloche pattern of an authentic version of\nthe same country. Experiments are conducted in order to analyze and identify\nthe most proper parameters to achieve higher authentication performance. The\nexperimental results are performed on the MIDV-2020 dataset. The results show\nthe ability of the proposed approach to extract the relevant characteristics of\nthe processed pair of identity documents in order to model the guilloche\npatterns, and thus distinguish them correctly. The implementation code and the\nforged dataset are provided here (https://drive.google.com/id-FDGP-1)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Ghadi_M/0/1/0/all/0/1\">Musab Al-Ghadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Kramer_P/0/1/0/all/0/1\">Petra Gomez-Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burie_J/0/1/0/all/0/1\">Jean-Christophe Burie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Contrastive Language Image Pretraining. (arXiv:2206.10996v1 [cs.CV])","link":"http://arxiv.org/abs/2206.10996","description":"<p>Contrastive Language Image Pretraining (CLIP) received widespread attention\nsince its learned representations can be transferred well to various downstream\ntasks. During CLIP training, the InfoNCE objective aims to align positive\nimage-text pairs and separate negative ones. In this paper, we show a\nrepresentation grouping effect during this process: the InfoNCE objective\nindirectly groups semantically similar representations together via randomly\nemerged within-modal anchors. We introduce Prototypical Contrastive Language\nImage Pretraining (ProtoCLIP) to enhance such grouping by boosting its\nefficiency and increasing its robustness against modality gap. Specifically,\nProtoCLIP sets up prototype-level discrimination between image and text spaces,\nwhich efficiently transfers higher-level structural knowledge. We further\npropose Prototypical Back Translation (PBT) to decouple representation grouping\nfrom representation alignment, resulting in effective learning of meaningful\nrepresentations under large modality gap. PBT also enables us to introduce\nadditional external teachers with richer prior knowledge. ProtoCLIP is trained\nwith an online episodic training strategy, which makes it can be scaled up to\nunlimited amounts of data. Combining the above novel designs, we train our\nProtoCLIP on Conceptual Captions and achieved an +5.81% ImageNet linear probing\nimprovement and an +2.01% ImageNet zero-shot classification improvement. Codes\nare available at https://github.com/megvii-research/protoclip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaiquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yixiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yiping Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Action Localization via Hierarchical Mining. (arXiv:2206.11011v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11011","description":"<p>Weakly-supervised action localization aims to localize and classify action\ninstances in the given videos temporally with only video-level categorical\nlabels. Thus, the crucial issue of existing weakly-supervised action\nlocalization methods is the limited supervision from the weak annotations for\nprecise predictions. In this work, we propose a hierarchical mining strategy\nunder video-level and snippet-level manners, i.e., hierarchical supervision and\nhierarchical consistency mining, to maximize the usage of the given annotations\nand prediction-wise consistency. To this end, a Hierarchical Mining Network\n(HiM-Net) is proposed. Concretely, it mines hierarchical supervision for\nclassification in two grains: one is the video-level existence for ground truth\ncategories captured by multiple instance learning; the other is the\nsnippet-level inexistence for each negative-labeled category from the\nperspective of complementary labels, which is optimized by our proposed\ncomplementary label learning. As for hierarchical consistency, HiM-Net explores\nvideo-level co-action feature similarity and snippet-level\nforeground-background opposition, for discriminative representation learning\nand consistent foreground-background separation. Specifically, prediction\nvariance is viewed as uncertainty to select the pairs with high consensus for\nproposed foreground-background collaborative learning. Comprehensive\nexperimental results show that HiM-Net outperforms existing methods on THUMOS14\nand ActivityNet1.3 datasets with large margins by hierarchically mining the\nsupervision and consistency. Code will be available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jia-Chang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jia-Run Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianping Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated GI tract segmentation using deep learning. (arXiv:2206.11048v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11048","description":"<p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the\ntumor and at the same time avoid the stomach and intestines. With MR-Linacs\n(magnetic resonance imaging and linear accelerator systems), oncologists can\nvisualize the position of the tumor and allow for precise dose according to\ntumor cell presence which can vary from day to day. The current job of\noutlining the position of the stomach and intestines to adjust the X-ray beams\ndirection for the dose delivery to the tumor while avoiding the organs. This is\na time-consuming and labor-intensive process that can easily prolong treatments\nfrom 15 minutes to an hour a day unless deep learning methods can automate the\nsegmentation process. This paper discusses an automated segmentation process\nusing deep learning to make this process faster and allow more patients to get\neffective treatment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_M/0/1/0/all/0/1\">Manhar Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer. (arXiv:2206.11053v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11053","description":"<p>Visual question answering (VQA) in surgery is largely unexplored. Expert\nsurgeons are scarce and are often overloaded with clinical and academic\nworkloads. This overload often limits their time answering questionnaires from\npatients, medical students or junior residents related to surgical procedures.\nAt times, students and junior residents also refrain from asking too many\nquestions during classes to reduce disruption. While computer-aided simulators\nand recording of past surgical procedures have been made available for them to\nobserve and improve their skills, they still hugely rely on medical experts to\nanswer their questions. Having a Surgical-VQA system as a reliable 'second\nopinion' could act as a backup and ease the load on the medical experts in\nanswering these questions. The lack of annotated medical data and the presence\nof domain-specific terms has limited the exploration of VQA for surgical\nprocedures. In this work, we design a Surgical-VQA task that answers\nquestionnaires on surgical procedures based on the surgical scene. Extending\nthe MICCAI endoscopic vision challenge 2018 dataset and workflow recognition\ndataset further, we introduce two Surgical-VQA datasets with classification and\nsentence-based answers. To perform Surgical-VQA, we employ vision-text\ntransformers models. We further introduce a residual MLP-based VisualBert\nencoder model that enforces interaction between visual and text tokens,\nimproving performance in classification-based answering. Furthermore, we study\nthe influence of the number of input image patches and temporal visual features\non the model performance in both classification and sentence-based answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seenivasan_L/0/1/0/all/0/1\">Lalithkumar Seenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Adithya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified and Biologically-Plausible Relational Graph Representation of Vision Transformers. (arXiv:2206.11073v1 [cs.NE])","link":"http://arxiv.org/abs/2206.11073","description":"<p>Vision transformer (ViT) and its variants have achieved remarkable successes\nin various visual tasks. The key characteristic of these ViT models is to adopt\ndifferent aggregation strategies of spatial patch information within the\nartificial neural networks (ANNs). However, there is still a key lack of\nunified representation of different ViT architectures for systematic\nunderstanding and assessment of model representation performance. Moreover, how\nthose well-performing ViT ANNs are similar to real biological neural networks\n(BNNs) is largely unexplored. To answer these fundamental questions, we, for\nthe first time, propose a unified and biologically-plausible relational graph\nrepresentation of ViT models. Specifically, the proposed relational graph\nrepresentation consists of two key sub-graphs: aggregation graph and affine\ngraph. The former one considers ViT tokens as nodes and describes their spatial\ninteraction, while the latter one regards network channels as nodes and\nreflects the information communication between channels. Using this unified\nrelational graph representation, we found that: a) a sweet spot of the\naggregation graph leads to ViTs with significantly improved predictive\nperformance; b) the graph measures of clustering coefficient and average path\nlength are two effective indicators of model prediction performance, especially\nwhen applying on the datasets with small samples; c) our findings are\nconsistent across various ViT architectures and multiple datasets; d) the\nproposed relational graph representation of ViT has high similarity with real\nBNNs derived from brain science data. Overall, our work provides a novel\nunified and biologically-plausible paradigm for more interpretable and\neffective representation of ViT ANNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenxiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">David Weizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Gait: Gait Recognition via Motion Excitation. (arXiv:2206.11080v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11080","description":"<p>Gait recognition, which can realize long-distance and contactless\nidentification, is an important biometric technology. Recent gait recognition\nmethods focus on learning the pattern of human movement or appearance during\nwalking, and construct the corresponding spatio-temporal representations.\nHowever, different individuals have their own laws of movement patterns, simple\nspatial-temporal features are difficult to describe changes in motion of human\nparts, especially when confounding variables such as clothing and carrying are\nincluded, thus distinguishability of features is reduced. In this paper, we\npropose the Motion Excitation Module (MEM) to guide spatio-temporal features to\nfocus on human parts with large dynamic changes, MEM learns the difference\ninformation between frames and intervals, so as to obtain the representation of\ntemporal motion changes, it is worth mentioning that MEM can adapt to frame\nsequences with uncertain length, and it does not add any additional parameters.\nFurthermore, we present the Fine Feature Extractor (FFE), which independently\nlearns the spatio-temporal representations of human body according to different\nhorizontal parts of individuals. Benefiting from MEM and FFE, our method\ninnovatively combines motion change information, significantly improving the\nperformance of the model under cross appearance conditions. On the popular\ndataset CASIA-B, our proposed Motion Gait is better than the existing gait\nrecognition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shanna Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes. (arXiv:2206.11095v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11095","description":"<p>Immersive displays such as VR headsets, AR glasses, Multiview displays, Free\npoint televisions have emerged as a new class of display technologies in recent\nyears, offering a better visual experience and viewer engagement as compared to\nconventional displays. With the evolution of 3D video and display technologies,\nthe consumer market for High Dynamic Range (HDR) cameras and displays is\nquickly growing. The lack of appropriate experimental data is a critical\nhindrance for the development of primary research efforts in the field of 3D\nHDR video technology. Also, the unavailability of sufficient real world\nmulti-exposure experimental dataset is a major bottleneck for HDR imaging\nresearch, thereby limiting the quality of experience (QoE) for the viewers. In\nthis paper, we introduce a diversified stereoscopic multi-exposure dataset\ncaptured within the campus of Indian Institute of Technology Madras, which is\nhome to a diverse flora and fauna. The dataset is captured using ZED\nstereoscopic camera and provides intricate scenes of outdoor locations such as\ngardens, roadside views, festival venues, buildings and indoor locations such\nas academic and residential areas. The proposed dataset accommodates wide depth\nrange, complex depth structure, complicate object movement, illumination\nvariations, rich color dynamics, texture discrepancy in addition to significant\nrandomness introduced by moving camera and background motion. The proposed\ndataset is made publicly available to the research community. Furthermore, the\nprocedure for capturing, aligning and calibrating multi-exposure stereo videos\nand images is described in detail. Finally, we have discussed the progress,\nchallenges, potential use cases and future research opportunities with respect\nto HDR imaging, depth estimation, consistent tone mapping and 3D HDR coding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_R/0/1/0/all/0/1\">Rohit Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mansi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadaskar_A/0/1/0/all/0/1\">Aditya Wadaskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICC++: Explainable Image Retrieval for Art Historical Corpora using Image Composition Canvas. (arXiv:2206.11115v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11115","description":"<p>Image compositions are helpful in the study of image structures and assist in\ndiscovering the semantics of the underlying scene portrayed across art forms\nand styles. With the digitization of artworks in recent years, thousands of\nimages of a particular scene or narrative could potentially be linked together.\nHowever, manually linking this data with consistent objectiveness can be a\nhighly challenging and time-consuming task. In this work, we present a novel\napproach called Image Composition Canvas (ICC++) to compare and retrieve images\nhaving similar compositional elements. ICC++ is an improvement over ICC\nspecializing in generating low and high-level features (compositional elements)\nmotivated by Max Imdahl's work. To this end, we present a rigorous quantitative\nand qualitative comparison of our approach with traditional and\nstate-of-the-art (SOTA) methods showing that our proposed method outperforms\nall of them. In combination with deep features, our method outperforms the best\ndeep learning-based method, opening the research direction for explainable\nmachine learning for digital humanities. We will release the code and the data\npost-publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhu_P/0/1/0/all/0/1\">Prathmesh Madhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquart_T/0/1/0/all/0/1\">Tilman Marquart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosti_R/0/1/0/all/0/1\">Ronak Kosti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suckow_D/0/1/0/all/0/1\">Dirk Suckow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based fully automatic wrist cartilage volume quantification in MR Image. (arXiv:2206.11127v1 [eess.IV])","link":"http://arxiv.org/abs/2206.11127","description":"<p>Detection of cartilage loss is crucial for the diagnosis of osteo- and\nrheumatoid arthritis. A large number of automatic segmentation tools have been\nreported so far for cartilage assessment in magnetic resonance images of large\njoints. As compared to knee or hip, wrist cartilage has a more complex\nstructure so that automatic tools developed for large joints are not expected\nto be operational for wrist cartilage segmentation. In that respect, a fully\nautomatic wrist cartilage segmentation method would be of high clinical\ninterest. We assessed the performance of four optimized variants of the U-Net\narchitecture with truncation of its depth and addition of attention layers\n(U-Net_AL). The corresponding results were compared to those from a patch-based\nconvolutional neural network (CNN) we previously designed. The segmentation\nquality was assessed on the basis of a comparative analysis with manual\nsegmentation using several morphological (2D DSC, 3D DSC, precision) and a\nvolumetric metrics. The four networks outperformed the patch-based CNN in terms\nof segmentation homogeneity and quality. The median 3D DSC value computed with\nthe U-Net_AL (0.817) was significantly larger than the corresponding DSC values\ncomputed with the other networks. In addition, the U-Net_AL CNN provided the\nlowest mean volume error (17%) and the highest Pearson correlation coefficient\n(0.765) with respect to the ground truth. Of interest, the reproducibility\ncomputed from using U-Net_AL was larger than the reproducibility of the manual\nsegmentation. U-net convolutional neural network with additional attention\nlayers provides the best wrist cartilage segmentation performance. In order to\nbe used in clinical conditions, the trained network can be fine-tuned on a\ndataset representing a group of specific patients. The error of cartilage\nvolume measurement should be assessed independently using a non-MRI method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vladimirov_N/0/1/0/all/0/1\">Nikita Vladimirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brui_E/0/1/0/all/0/1\">Ekaterina Brui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levchuk_A/0/1/0/all/0/1\">Anatoliy Levchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fokin_V/0/1/0/all/0/1\">Vladimir Fokin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Efimtcev_A/0/1/0/all/0/1\">Aleksandr Efimtcev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendahan_D/0/1/0/all/0/1\">David Bendahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization. (arXiv:2206.11134v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11134","description":"<p>Open-vocabulary object detection (OVD) aims to scale up vocabulary size to\ndetect objects of novel categories beyond the training vocabulary. Recent work\nresorts to the rich knowledge in pre-trained vision-language models. However,\nexisting methods are ineffective in proposal-level vision-language alignment.\nMeanwhile, the models usually suffer from confidence bias toward base\ncategories and perform worse on novel ones. To overcome the challenges, we\npresent MEDet, a novel and effective OVD framework with proposal mining and\nprediction equalization. First, we design an online proposal mining to refine\nthe inherited vision-semantic knowledge from coarse to fine, allowing for\nproposal-level detection-oriented feature alignment. Second, based on causal\ninference theory, we introduce a class-wise backdoor adjustment to reinforce\nthe predictions on novel categories to improve the overall OVD performance.\nExtensive experiments on COCO and LVIS benchmarks verify the superiority of\nMEDet over the competing approaches in detecting objects of novel categories,\ne.g., 32.6% AP50 on COCO and 22.4% mask mAP on LVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Physical Metric For 6-DoF Grasp Pose Detection. (arXiv:2206.11141v1 [cs.RO])","link":"http://arxiv.org/abs/2206.11141","description":"<p>6-DoF grasp pose detection of multi-grasp and multi-object is a challenge\ntask in the field of intelligent robot. To imitate human reasoning ability for\ngrasping objects, data driven methods are widely studied. With the introduction\nof large-scale datasets, we discover that a single physical metric usually\ngenerates several discrete levels of grasp confidence scores, which cannot\nfinely distinguish millions of grasp poses and leads to inaccurate prediction\nresults. In this paper, we propose a hybrid physical metric to solve this\nevaluation insufficiency. First, we define a novel metric is based on the\nforce-closure metric, supplemented by the measurement of the object flatness,\ngravity and collision. Second, we leverage this hybrid physical metric to\ngenerate elaborate confidence scores. Third, to learn the new confidence scores\neffectively, we design a multi-resolution network called Flatness Gravity\nCollision GraspNet (FGC-GraspNet). FGC-GraspNet proposes a multi-resolution\nfeatures learning architecture for multiple tasks and introduces a new joint\nloss function that enhances the average precision of the grasp detection. The\nnetwork evaluation and adequate real robot experiments demonstrate the\neffectiveness of our hybrid physical metric and FGC-GraspNet. Our method\nachieves 90.5\\% success rate in real-world cluttered scenes. Our code is\navailable at https://github.com/luyh20/FGC-GraspNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuhao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Beixing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_P/0/1/0/all/0/1\">Peiyuan Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal transport meets noisy label robust loss and MixUp regularization for domain adaptation. (arXiv:2206.11180v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11180","description":"<p>It is common in computer vision to be confronted with domain shift: images\nwhich have the same class but different acquisition conditions. In domain\nadaptation (DA), one wants to classify unlabeled target images using source\nlabeled images. Unfortunately, deep neural networks trained on a source\ntraining set perform poorly on target images which do not belong to the\ntraining domain. One strategy to improve these performances is to align the\nsource and target image distributions in an embedded space using optimal\ntransport (OT). However OT can cause negative transfer, i.e. aligning samples\nwith different labels, which leads to overfitting especially in the presence of\nlabel shift between domains. In this work, we mitigate negative alignment by\nexplaining it as a noisy label assignment to target images. We then mitigate\nits effect by appropriate regularization. We propose to couple the MixUp\nregularization \\citep{zhang2018mixup} with a loss that is robust to noisy\nlabels in order to improve domain adaptation performance. We show in an\nextensive ablation study that a combination of the two techniques is critical\nto achieve improved performance. Finally, we evaluate our method, called\n\\textsc{mixunbot}, on several benchmarks and real-world DA problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1\">Kilian Fatras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naganuma_H/0/1/0/all/0/1\">Hiroki Naganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1\">Ioannis Mitliagkas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facke: a Survey on Generative Models for Face Swapping. (arXiv:2206.11203v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11203","description":"<p>In this work, we investigate into the performance of mainstream neural\ngenerative models on the very task of swapping faces. We have experimented on\nCVAE, CGAN, CVAE-GAN, and conditioned diffusion models. Existing finely trained\nmodels have already managed to produce fake faces (Facke) indistinguishable to\nthe naked eye as well as achieve high objective metrics. We perform a\ncomparison among them and analyze their pros and cons. Furthermore, we proposed\nsome promising tricks though they do not apply to this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wentao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11212","description":"<p>Many past works aim to improve visual reasoning in models by supervising\nfeature importance (estimated by model explanation techniques) with human\nannotations such as highlights of important image regions. However, recent work\nhas shown that performance gains from feature importance (FI) supervision for\nVisual Question Answering (VQA) tasks persist even with random supervision,\nsuggesting that these methods do not meaningfully align model FI with human FI.\nIn this paper, we show that model FI supervision can meaningfully improve VQA\nmodel accuracy as well as performance on several Right-for-the-Right-Reason\n(RRR) metrics by optimizing for four key model objectives: (1) accurate\npredictions given limited but sufficient information (Sufficiency); (2)\nmax-entropy predictions given no important information (Uncertainty); (3)\ninvariance of predictions to changes in unimportant features (Invariance); and\n(4) alignment between model FI explanations and human FI explanations\n(Plausibility). Our best performing method, Visual Feature Importance\nSupervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in\nterms of both in-distribution and out-of-distribution accuracy. While past work\nsuggests that the mechanism for improved accuracy is through improved\nexplanation plausibility, we show that this relationship depends crucially on\nexplanation faithfulness (whether explanations truly represent the model's\ninternal reasoning). Predictions are more accurate when explanations are\nplausible and faithful, and not when they are plausible but not faithful.\nLastly, we show that, surprisingly, RRR metrics are not predictive of\nout-of-distribution model accuracy when controlling for a model's\nin-distribution accuracy, which calls into question the value of these metrics\nfor evaluating model reasoning. All supporting code is available at\nhttps://github.com/zfying/visfis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhuofan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correct and Certify: A New Approach to Self-Supervised 3D-Object Perception. (arXiv:2206.11215v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11215","description":"<p>We consider an object pose estimation and model fitting problem, where -\ngiven a partial point cloud of an object - the goal is to estimate the object\npose by fitting a CAD model to the sensor data. We solve this problem by\ncombining (i) a semantic keypoint-based pose estimation model, (ii) a novel\nself-supervised training approach, and (iii) a certification procedure, that\nnot only verifies whether the output produced by the model is correct or not,\nbut also flags uniqueness of the produced solution. The semantic keypoint\ndetector model is initially trained in simulation and does not perform well on\nreal-data due to the domain gap. Our self-supervised training procedure uses a\ncorrector and a certification module to improve the detector. The corrector\nmodule corrects the detected keypoints to compensate for the domain gap, and is\nimplemented as a declarative layer, for which we develop a simple\ndifferentiation rule. The certification module declares whether the corrected\noutput produced by the model is certifiable (i.e. correct) or not. At each\niteration, the approach optimizes over the loss induced only by the certifiable\ninput-output pairs. As training progresses, we see that the fraction of outputs\nthat are certifiable increases, eventually reaching near $100\\%$ in many cases.\nWe also introduce the notion of strong certifiability wherein the model can\ndetermine if the predicted object model fit is unique or not. The detected\nsemantic keypoints help us implement this in the forward pass. We conduct\nextensive experiments to evaluate the performance of the corrector, the\ncertification, and the proposed self-supervised training using the ShapeNet and\nYCB datasets, and show the proposed approach achieves performance comparable to\nfully supervised baselines while not requiring pose or keypoint supervision on\nreal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lisa Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Business Document Information Extraction: Towards Practical Benchmarks. (arXiv:2206.11229v1 [cs.IR])","link":"http://arxiv.org/abs/2206.11229","description":"<p>Information extraction from semi-structured documents is crucial for\nfrictionless business-to-business (B2B) communication. While machine learning\nproblems related to Document Information Extraction (IE) have been studied for\ndecades, many common problem definitions and benchmarks do not reflect\ndomain-specific aspects and practical needs for automating B2B document\ncommunication. We review the landscape of Document IE problems, datasets and\nbenchmarks. We highlight the practical aspects missing in the common\ndefinitions and define the Key Information Localization and Extraction (KILE)\nand Line Item Recognition (LIR) problems. There is a lack of relevant datasets\nand benchmarks for Document IE on semi-structured business documents as their\ncontent is typically legally protected or sensitive. We discuss potential\nsources of available documents including synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skalicky_M/0/1/0/all/0/1\">Maty&#xe1;&#x161; Skalick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsa_S/0/1/0/all/0/1\">&#x160;t&#x11b;p&#xe1;n &#x160;imsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal U&#x159;i&#x10d;&#xe1;&#x159;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan &#x160;ulc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-aware Glass Surface Detection with Cross-modal Context Mining. (arXiv:2206.11250v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11250","description":"<p>Glass surfaces are becoming increasingly ubiquitous as modern buildings tend\nto use a lot of glass panels. This however poses substantial challenges on the\noperations of autonomous systems such as robots, self-driving cars and drones,\nas the glass panels can become transparent obstacles to the navigation.Existing\nworks attempt to exploit various cues, including glass boundary context or\nreflections, as a prior. However, they are all based on input RGB images.We\nobserve that the transmission of 3D depth sensor light through glass surfaces\noften produces blank regions in the depth maps, which can offer additional\ninsights to complement the RGB image features for glass surface detection. In\nthis paper, we propose a novel framework for glass surface detection by\nincorporating RGB-D information, with two novel modules: (1) a cross-modal\ncontext mining (CCM) module to adaptively learn individual and mutual context\nfeatures from RGB and depth information, and (2) a depth-missing aware\nattention (DAA) module to explicitly exploit spatial locations where missing\ndepths occur to help detect the presence of glass surfaces. In addition, we\npropose a large-scale RGB-D glass surface detection dataset, called\n\\textit{RGB-D GSD}, for RGB-D glass surface detection. Our dataset comprises\n3,009 real-world RGB-D glass surface images with precise annotations. Extensive\nexperimental results show that our proposed model outperforms state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yuen Hei Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavior Transformers: Cloning $k$ modes with one stone. (arXiv:2206.11251v1 [cs.LG])","link":"http://arxiv.org/abs/2206.11251","description":"<p>While behavior learning has made impressive progress in recent times, it lags\nbehind computer vision and natural language processing due to its inability to\nleverage large, human-generated datasets. Human behaviors have wide variance,\nmultiple modes, and human demonstrations typically do not come with reward\nlabels. These properties limit the applicability of current methods in Offline\nRL and Behavioral Cloning to learn from large, pre-collected datasets. In this\nwork, we present Behavior Transformer (BeT), a new technique to model unlabeled\ndemonstration data with multiple modes. BeT retrofits standard transformer\narchitectures with action discretization coupled with a multi-task action\ncorrection inspired by offset prediction in object detection. This allows us to\nleverage the multi-modal modeling ability of modern transformers to predict\nmulti-modal continuous actions. We experimentally evaluate BeT on a variety of\nrobotic manipulation and self-driving behavior datasets. We show that BeT\nsignificantly improves over prior state-of-the-art work on solving demonstrated\ntasks while capturing the major modes present in the pre-collected datasets.\nFinally, through an extensive ablation study, we analyze the importance of\nevery crucial component in BeT. Videos of behavior generated by BeT are\navailable at https://notmahi.github.io/bet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafiullah_N/0/1/0/all/0/1\">Nur Muhammad Mahi Shafiullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zichen Jeff Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altanzaya_A/0/1/0/all/0/1\">Ariuntuya Altanzaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Blind Face Restoration with Codebook Lookup Transformer. (arXiv:2206.11253v1 [cs.CV])","link":"http://arxiv.org/abs/2206.11253","description":"<p>Blind face restoration is a highly ill-posed problem that often requires\nauxiliary guidance to 1) improve the mapping from degraded inputs to desired\noutputs, or 2) complement high-quality details lost in the inputs. In this\npaper, we demonstrate that a learned discrete codebook prior in a small proxy\nspace largely reduces the uncertainty and ambiguity of restoration mapping by\ncasting blind face restoration as a code prediction task, while providing rich\nvisual atoms for generating high-quality faces. Under this paradigm, we propose\na Transformer-based prediction network, named CodeFormer, to model global\ncomposition and context of the low-quality faces for code prediction, enabling\nthe discovery of natural faces that closely approximate the target faces even\nwhen the inputs are severely degraded. To enhance the adaptiveness for\ndifferent degradation, we also propose a controllable feature transformation\nmodule that allows a flexible trade-off between fidelity and quality. Thanks to\nthe expressive codebook prior and global modeling, CodeFormer outperforms state\nof the arts in both quality and fidelity, showing superior robustness to\ndegradation. Extensive experimental results on synthetic and real-world\ndatasets verify the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kelvin C.K. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.10217","description":"<p>Most existing point cloud instance and semantic segmentation methods rely\nheavily on strong supervision signals, which require point-level labels for\nevery point in the scene. However, such strong supervision suffers from large\nannotation costs, arousing the need to study efficient annotating. In this\npaper, we discover that the locations of instances matter for both instance and\nsemantic 3D scene segmentation. By fully taking advantage of locations, we\ndesign a weakly supervised point cloud segmentation algorithm that only\nrequires clicking on one point per instance to indicate its location for\nannotation. With over-segmentation for pre-processing, we extend these location\nannotations into segments as seg-level labels. We further design a segment\ngrouping network (SegGroup) to generate point-level pseudo labels under\nseg-level labels by hierarchically grouping the unlabeled segments into the\nrelevant nearby labeled segments, so that existing point-level supervised\nsegmentation models can directly consume these pseudo labels for training.\nExperimental results show that our seg-level supervised method (SegGroup)\nachieves comparable results with the fully annotated point-level supervised\nmethods. Moreover, it outperforms the recent weakly supervised methods given a\nfixed annotation budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">An Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation. (arXiv:2101.07253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07253","description":"<p>Domain adaptation is an important task to enable learning when labels are\nscarce. While most works focus only on the image modality, there are many\nimportant multi-modal datasets. In order to leverage multi-modality for domain\nadaptation, we propose cross-modal learning, where we enforce consistency\nbetween the predictions of two modalities via mutual mimicking. We constrain\nour network to make correct predictions on labeled data and consistent\npredictions across modalities on unlabeled target-domain data. Experiments in\nunsupervised and semi-supervised domain adaptation settings prove the\neffectiveness of this novel domain adaptation strategy. Specifically, we\nevaluate on the task of 3D semantic segmentation from either the 2D image, the\n3D point cloud or from both. We leverage recent driving datasets to produce a\nwide variety of domain adaptation scenarios including changes in scene layout,\nlighting, sensor setup and weather, as well as the synthetic-to-real setup. Our\nmethod significantly improves over previous uni-modal adaptation baselines on\nall adaption scenarios. Our code is publicly available at\nhttps://github.com/valeoai/xmuda_journal\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaritz_M/0/1/0/all/0/1\">Maximilian Jaritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirbel_E/0/1/0/all/0/1\">&#xc9;milie Wirbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Generated Images Hard To Spot: A Transferable Attack On Synthetic Image Detectors. (arXiv:2104.12069v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12069","description":"<p>Visually realistic GAN-generated images have recently emerged as an important\nmisinformation threat. Research has shown that these synthetic images contain\nforensic traces that are readily identifiable by forensic detectors.\nUnfortunately, these detectors are built upon neural networks, which are\nvulnerable to recently developed adversarial attacks. In this paper, we propose\na new anti-forensic attack capable of fooling GAN-generated image detectors.\nOur attack uses an adversarially trained generator to synthesize traces that\nthese detectors associate with real images. Furthermore, we propose a technique\nto train our attack so that it can achieve transferability, i.e. it can fool\nunknown CNNs that it was not explicitly trained against. We evaluate our attack\nthrough an extensive set of experiments, where we show that our attack can fool\neight state-of-the-art detection CNNs with synthetic images created using seven\ndifferent GANs, and outperform other alternative attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamm_M/0/1/0/all/0/1\">Matthew C. Stamm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11541","description":"<p>Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11214","description":"<p>Conventional methods for human pose estimation either require a high degree\nof instrumentation, by relying on many inertial measurement units (IMUs), or\nconstraint the recording space, by relying on extrinsic cameras. These deficits\nare tackled through the approach of human pose estimation from sparse IMU data.\nWe define adjacency adaptive graph convolutional long-short term memory\nnetworks (AAGC-LSTM), to tackle human pose estimation based on six IMUs, while\nincorporating the human body graph structure directly into the network. The\nAAGC-LSTM combines both spatial and temporal dependency in a single network\noperation, more memory efficiently than previous approaches. This is made\npossible by equipping graph convolutions with adjacency adaptivity, which\neliminates the problem of information loss in deep or recurrent graph networks,\nwhile it also allows for learning unknown dependencies between the human body\njoints. To further boost accuracy, we propose longitudinal loss weighting to\nconsider natural movement patterns. With our presented approach, we are able to\nutilize the inherent graph nature of the human body, and thus can outperform\nthe state of the art (SOTA) for human pose estimation from sparse IMU data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04281","description":"<p>This paper presents a semantic planar SLAM system that improves pose\nestimation and mapping using cues from an instance planar segmentation network.\nWhile the mainstream approaches are using RGB-D sensors, employing a monocular\ncamera with such a system still faces challenges such as robust data\nassociation and precise geometric model fitting. In the majority of existing\nwork, geometric model estimation problems such as homography estimation and\npiece-wise planar reconstruction (PPR) are usually solved by standard (greedy)\nRANSAC separately and sequentially. However, setting the inlier-outlier\nthreshold is difficult in absence of information about the scene (i.e. the\nscale). In this work, we revisit these problems and argue that two mentioned\ngeometric models (homographies/3D planes) can be solved by minimizing an energy\nfunction that exploits the spatial coherence, i.e. with graph-cut optimization,\nwhich also tackles the practical issue when the output of a trained CNN is\ninaccurate. Moreover, we propose an adaptive parameter setting strategy based\non our experiments, and report a comprehensive evaluation on various\nopen-source datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaxu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust fine-tuning of zero-shot models. (arXiv:2109.01903v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01903","description":"<p>Large pre-trained models such as CLIP or ALIGN offer consistent accuracy\nacross a range of data distributions when performing zero-shot inference (i.e.,\nwithout fine-tuning on a specific dataset). Although existing fine-tuning\nmethods substantially improve accuracy on a given target distribution, they\noften reduce robustness to distribution shifts. We address this tension by\nintroducing a simple and effective method for improving robustness while\nfine-tuning: ensembling the weights of the zero-shot and fine-tuned models\n(WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy\nimprovements under distribution shift, while preserving high accuracy on the\ntarget distribution. On ImageNet and five derived distribution shifts, WiSE-FT\nimproves accuracy under distribution shift by 4 to 6 percentage points (pp)\nover prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves\nsimilarly large robustness gains (2 to 23 pp) on a diverse set of six further\ndistribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard\nfine-tuning on seven commonly used transfer learning datasets. These\nimprovements come at no additional computational cost during fine-tuning or\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining for Strong Gravitational Lenses with Self-supervised Learning. (arXiv:2110.00023v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2110.00023","description":"<p>We employ self-supervised representation learning to distill information from\n76 million galaxy images from the Dark Energy Spectroscopic Instrument Legacy\nImaging Surveys' Data Release 9. Targeting the identification of new strong\ngravitational lens candidates, we first create a rapid similarity search tool\nto discover new strong lenses given only a single labelled example. We then\nshow how training a simple linear classifier on the self-supervised\nrepresentations, requiring only a few minutes on a CPU, can automatically\nclassify strong lenses with great efficiency. We present 1192 new strong lens\ncandidates that we identified through a brief visual identification campaign,\nand release an interactive web-based similarity search tool and the top network\npredictions to facilitate crowd-sourcing rapid discovery of additional strong\ngravitational lenses and other rare objects:\nhttps://github.com/georgestein/ssl-legacysurvey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Stein_G/0/1/0/all/0/1\">George Stein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Blaum_J/0/1/0/all/0/1\">Jacqueline Blaum</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Harrington_P/0/1/0/all/0/1\">Peter Harrington</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Medan_T/0/1/0/all/0/1\">Tomislav Medan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lukic_Z/0/1/0/all/0/1\">Zarija Lukic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14769","description":"<p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Alzheimer's is the most common cause of dementia,\nwhich constitutes a general term for loss of memory. Due to the fact that\ndementia affects speech, existing research initiatives focus on detecting\ndementia from spontaneous speech. However, little work has been done regarding\nthe conversion of speech data to Log-Mel spectrograms and Mel-frequency\ncepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,\nlittle work has been done in terms of both the usage of transformer networks\nand the way the two modalities, i.e., speech and transcripts, are combined in a\nsingle neural network. To address these limitations, first we employ several\npretrained models, with Vision Transformer (ViT) achieving the highest\nevaluation results. Secondly, we propose multimodal models. More specifically,\nour introduced models include Gated Multimodal Unit in order to control the\ninfluence of each modality towards the final classification and crossmodal\nattention so as to capture in an effective way the relationships between the\ntwo modalities. Extensive experiments conducted on the ADReSS Challenge dataset\ndemonstrate the effectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiT: Zero-Shot Transfer with Locked-image text Tuning. (arXiv:2111.07991v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07991","description":"<p>This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11011","description":"<p>The Transformer-based encoder-decoder framework is becoming popular in scene\ntext recognition, largely because it naturally integrates recognition clues\nfrom both visual and semantic domains. However, recent studies show that the\ntwo kinds of clues are not always well registered and therefore, feature and\ncharacter might be misaligned in the difficult text (e.g., with rare shapes).\nAs a result, constraints such as character position are introduced to alleviate\nthis problem. Despite certain success, a content-free positional embedding\nhardly associates stably with meaningful local image regions. In this paper, we\npropose a novel module called Multi-Domain Character Distance Perception\n(MDCDP) to establish a visual and semantic related positional encoding. MDCDP\nuses positional embedding to query both visual and semantic features following\nthe attention mechanism. The two kinds of constrained features are then fused\nto produce a reinforced feature, generating a content-aware embedding that well\nperceives spacing variations and semantic affinities among characters, i.e.,\nmulti-domain character distance. We develop a novel network named CDistNet that\nstacks multiple MDCDPs to guide a gradually precise distance modeling. Thus,\nthe feature-character alignment is well built even various recognition\ndifficulties presented. We create two series of augmented datasets with\nincreasing recognition difficulties and apply CDistNet to both them and six\npublic benchmarks. The experiments demonstrate that CDistNet outperforms recent\npopular methods by large margins in challenging recognition scenarios. It also\nachieves state-of-the-art accuracy on standard benchmarks. In addition, the\nvisualization shows that CDistNet achieves proper information utilization in\nboth visual and semantic domains. Our code is given in\nhttps://github.com/simplify23/CDistNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianlun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shancheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk-to-Resolve: Combining scene understanding and spatial dialogue to resolve granular task ambiguity for a collocated robot. (arXiv:2111.11099v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.11099","description":"<p>The utility of collocating robots largely depends on the easy and intuitive\ninteraction mechanism with the human. If a robot accepts task instruction in\nnatural language, first, it has to understand the user's intention by decoding\nthe instruction. However, while executing the task, the robot may face\nunforeseeable circumstances due to the variations in the observed scene and\ntherefore requires further user intervention. In this article, we present a\nsystem called Talk-to-Resolve (TTR) that enables a robot to initiate a coherent\ndialogue exchange with the instructor by observing the scene visually to\nresolve the impasse. Through dialogue, it either finds a cue to move forward in\nthe original plan, an acceptable alternative to the original plan, or\naffirmation to abort the task altogether. To realize the possible stalemate, we\nutilize the dense captions of the observed scene and the given instruction\njointly to compute the robot's next action. We evaluate our system based on a\ndata set of initial instruction and situational scene pairs. Our system can\nidentify the stalemate and resolve them with appropriate dialogue exchange with\n82% accuracy. Additionally, a user study reveals that the questions from our\nsystems are more natural (4.02 on average on a scale of 1 to 5) as compared to\na state-of-the-art (3.08 on average).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Snehasis Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_B/0/1/0/all/0/1\">Brojeshwar Bhowmick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intuitive Shape Editing in Latent Space. (arXiv:2111.12488v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12488","description":"<p>The use of autoencoders for shape editing or generation through latent space\nmanipulation suffers from unpredictable changes in the output shape. Our\nautoencoder-based method enables intuitive shape editing in latent space by\ndisentangling latent sub-spaces into style variables and control points on the\nsurface that can be manipulated independently. The key idea is adding a\nLipschitz-type constraint to the loss function, i.e. bounding the change of the\noutput shape proportionally to the change in latent space, leading to\ninterpretable latent space representations. The control points on the surface\nthat are part of the latent code of an object can then be freely moved,\nallowing for intuitive shape editing directly in latent space. We evaluate our\nmethod by comparing to state-of-the-art data-driven shape editing methods. We\nfurther demonstrate the expressiveness of our learned latent space by\nleveraging it for unsupervised part segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsner_T/0/1/0/all/0/1\">Tim Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_V/0/1/0/all/0/1\">Victor Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nehring_Wirxel_J/0/1/0/all/0/1\">Julius Nehring-Wirxel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07225","description":"<p>The long-tailed class distribution in visual recognition tasks poses great\nchallenges for neural networks on how to handle the biased predictions between\nhead and tail classes, i.e., the model tends to classify tail classes as head\nclasses. While existing research focused on data resampling and loss function\nengineering, in this paper, we take a different perspective: the classification\nmargins. We study the relationship between the margins and logits\n(classification scores) and empirically observe the biased margins and the\nbiased logits are positively correlated. We propose MARC, a simple yet\neffective MARgin Calibration function to dynamically calibrate the biased\nmargins for unbiased logits. We validate MARC through extensive experiments on\ncommon long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and\niNaturalist-LT. Experimental results demonstrate that our MARC achieves\nfavorable results on these benchmarks. In addition, MARC is extremely easy to\nimplement with just three lines of code. We hope this simple method will\nmotivate people to rethink the biased margins and biased logits in long-tailed\nvisual recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Instance Segmentation of MVS Buildings. (arXiv:2112.09902v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09902","description":"<p>We present a novel 3D instance segmentation framework for Multi-View Stereo\n(MVS) buildings in urban scenes. Unlike existing works focusing on semantic\nsegmentation of urban scenes, the emphasis of this work lies in detecting and\nsegmenting 3D building instances even if they are attached and embedded in a\nlarge and imprecise 3D surface model. Multi-view RGB images are first enhanced\nto RGBH images by adding a heightmap and are segmented to obtain all roof\ninstances using a fine-tuned 2D instance segmentation neural network. Instance\nmasks from different multi-view images are then clustered into global masks.\nOur mask clustering accounts for spatial occlusion and overlapping, which can\neliminate segmentation ambiguities among multi-view images. Based on these\nglobal masks, 3D roof instances are segmented out by mask back-projections and\nextended to the entire building instances through a Markov random field\noptimization. A new dataset that contains instance-level annotation for both 3D\nurban scenes (roofs and buildings) and drone images (roofs) is provided. To the\nbest of our knowledge, it is the first outdoor dataset dedicated to 3D instance\nsegmentation with much more annotations of attached 3D buildings than existing\ndatasets. Quantitative evaluations and ablation studies have shown the\neffectiveness of all major steps and the advantages of our multi-view framework\nover the orthophoto-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiazhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shufang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graph Generation: A Comprehensive Survey. (arXiv:2201.00443v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00443","description":"<p>Deep learning techniques have led to remarkable breakthroughs in the field of\ngeneric object detection and have spawned a lot of scene-understanding tasks in\nrecent years. Scene graph has been the focus of research because of its\npowerful semantic representation and applications to scene understanding. Scene\nGraph Generation (SGG) refers to the task of automatically mapping an image\ninto a semantic structural scene graph, which requires the correct labeling of\ndetected objects and their relationships. Although this is a challenging task,\nthe community has proposed a lot of SGG approaches and achieved good results.\nIn this paper, we provide a comprehensive survey of recent achievements in this\nfield brought about by deep learning techniques. We review 138 representative\nworks that cover different input modalities, and systematically summarize\nexisting methods of image-based SGG from the perspective of feature extraction\nand fusion. We attempt to connect and systematize the existing visual\nrelationship detection methods, to summarize, and interpret the mechanisms and\nthe strategies of SGG in a comprehensive way. Finally, we finish this survey\nwith deep discussions about current existing problems and future research\ndirections. This survey will help readers to develop a better understanding of\nthe current research status and ideas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Youliang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yixuan Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1\">Haoran Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiguang Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Face Morphing Attacks: Generation, Vulnerability and Detection. (arXiv:2201.03454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03454","description":"<p>Face Recognition systems (FRS) have been found vulnerable to morphing\nattacks, where the morphed face image is generated by blending the face images\nfrom contributory data subjects. This work presents a novel direction towards\ngenerating face morphing attacks in 3D. To this extent, we have introduced a\nnovel approach based on blending the 3D face point clouds corresponding to the\ncontributory data subjects. The proposed method will generate the 3D face\nmorphing by projecting the input 3D face point clouds to depth-maps \\&amp; 2D color\nimages followed by the image blending and wrapping operations performed\nindependently on the color images and depth maps. We then back-project the 2D\nmorphing color-map and the depth-map to the point cloud using the canonical\n(fixed) view. Given that the generated 3D face morphing models will result in\nthe holes due to a single canonical view, we have proposed a new algorithm for\nhole filling that will result in a high-quality 3D face morphing model.\nExtensive experiments are carried out on the newly generated 3D face dataset\ncomprised of 675 3D scans corresponding to 41 unique data subjects. Experiments\nare performed to benchmark the vulnerability of automatic 2D and 3D FRS and\nhuman observer analysis. We also present the quantitative assessment of the\nquality of the generated 3D face morphing models using eight different quality\nmetrics. Finally, we have proposed three different 3D face Morphing Attack\nDetection (3D-MAD) algorithms to benchmark the performance of the 3D MAD\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jag Mohan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Masking for Self-Supervised Learning. (arXiv:2201.13100v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13100","description":"<p>We propose ADIOS, a masked image model (MIM) framework for self-supervised\nlearning, which simultaneously learns a masking function and an image encoder\nusing an adversarial objective. The image encoder is trained to minimise the\ndistance between representations of the original and that of a masked image.\nThe masking function, conversely, aims at maximising this distance. ADIOS\nconsistently improves on state-of-the-art self-supervised learning (SSL)\nmethods on a variety of tasks and datasets -- including classification on\nImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and\niNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao\net al., 2021) -- while generating semantically meaningful masks. Unlike modern\nMIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch\ntokenisation construction of Vision Transformers, and can be implemented with\nconvolutional backbones. We further demonstrate that the masks learned by ADIOS\nare more effective in improving representation learning of SSL methods than\nmasking schemes used in popular MIM models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosiorek_A/0/1/0/all/0/1\">Adam R. Kosiorek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metrics for saliency map evaluation of deep learning explanation methods. (arXiv:2201.13291v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13291","description":"<p>Due to the black-box nature of deep learning models, there is a recent\ndevelopment of solutions for visual explanations of CNNs. Given the high cost\nof user studies, metrics are necessary to compare and evaluate these different\nmethods. In this paper, we critically analyze the Deletion Area Under Curve\n(DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al.\n(2018). These metrics were designed to evaluate the faithfulness of saliency\nmaps generated by generic methods such as Grad-CAM or RISE. First, we show that\nthe actual saliency score values given by the saliency map are ignored as only\nthe ranking of the scores is taken into account. This shows that these metrics\nare insufficient by themselves, as the visual appearance of a saliency map can\nchange significantly without the ranking of the scores being modified.\nSecondly, we argue that during the computation of DAUC and IAUC, the model is\npresented with images that are out of the training distribution which might\nlead to an unreliable behavior of the model being explained. To complement\nDAUC/IAUC, we propose new metrics that quantify the sparsity and the\ncalibration of explanation methods, two previously unstudied properties.\nFinally, we give general remarks about the metrics studied in this paper and\ndiscuss how to evaluate them in a user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GiraffeDet: A Heavy-Neck Paradigm for Object Detection. (arXiv:2202.04256v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04256","description":"<p>In conventional object detection frameworks, a backbone body inherited from\nimage recognition models extracts deep latent features and then a neck module\nfuses these latent features to capture information at different scales. As the\nresolution in object detection is much larger than in image recognition, the\ncomputational cost of the backbone often dominates the total inference cost.\nThis heavy-backbone design paradigm is mostly due to the historical legacy when\ntransferring image recognition models to object detection rather than an\nend-to-end optimized design for object detection. In this work, we show that\nsuch paradigm indeed leads to sub-optimal object detection models. To this end,\nwe propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for\nefficient object detection. The GiraffeDet uses an extremely lightweight\nbackbone and a very deep and large neck module which encourages dense\ninformation exchange among different spatial scales as well as different levels\nof latent semantics simultaneously. This design paradigm allows detectors to\nprocess the high-level semantic information and low-level spatial information\nat the same priority even in the early stage of the network, making it more\neffective in detection tasks. Numerical evaluations on multiple popular object\ndetection benchmarks show that GiraffeDet consistently outperforms previous\nSOTA models across a wide spectrum of resource constraints. The source code is\navailable at https://github.com/jyqi/GiraffeDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Underwater Image Enhancement via Content and Style Separation. (arXiv:2202.08537v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08537","description":"<p>Underwater image suffer from color cast, low contrast and hazy effect due to\nlight absorption, refraction and scattering, which degraded the high-level\napplication, e.g, object detection and object tracking. Recent learning-based\nmethods demonstrate astonishing performance on underwater image enhancement,\nhowever, most of these works use synthetic pair data for supervised learning\nand ignore the domain gap to real-world data. To solve this problem, we propose\na domain adaptation framework for underwater image enhancement via content and\nstyle separation, different from prior works of domain adaptation for\nunderwater image enhancement, which target to minimize the latent discrepancy\nof synthesis and real-world data, we aim to separate encoded feature into\ncontent and style latent and distinguish style latent from different domains,\ni.e. synthesis, real-world underwater and clean domain, and process domain\nadaptation and image enhancement in latent space. By latent manipulation, our\nmodel provide a user interact interface to adjust different enhanced level for\ncontinuous change. Experiment on various public real-world underwater\nbenchmarks demonstrate that the proposed framework is capable to perform domain\nadaptation for underwater image enhancement and outperform various\nstate-of-the-art underwater image enhancement algorithms in quantity and\nquality. The model and source code will be available at\nhttps://github.com/fordevoted/UIESS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Soo-Chang Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning. (arXiv:2202.11915v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11915","description":"<p>Semi-supervised learning (SSL) has long been proved to be an effective\ntechnique to construct powerful models with limited labels. In the existing\nliterature, consistency regularization-based methods, which force the perturbed\nsamples to have similar predictions with the original ones have attracted much\nattention for their promising accuracy. However, we observe that, the\nperformance of such methods decreases drastically when the labels get extremely\nlimited, e.g., 2 or 3 labels for each category. Our empirical study finds that\nthe main problem lies with the drifting of semantic information in the\nprocedure of data augmentation. The problem can be alleviated when enough\nsupervision is provided. However, when little guidance is available, the\nincorrect regularization would mislead the network and undermine the\nperformance of the algorithm. To tackle the problem, we (1) propose an\ninterpolation-based method to construct more reliable positive sample pairs;\n(2) design a novel contrastive loss to guide the embedding of the learned\nnetwork to change linearly between samples so as to improve the discriminative\ncapability of the network by enlarging the margin decision boundaries. Since no\ndestructive regularization is introduced, the performance of our proposed\nalgorithm is largely improved. Specifically, the proposed algorithm outperforms\nthe second best algorithm (Comatch) with 5.3% by achieving 88.73%\nclassification accuracy when only two labels are available for each class on\nthe CIFAR-10 dataset. Moreover, we further prove the generality of the proposed\nmethod by improving the performance of the existing state-of-the-art algorithms\nconsiderably with our proposed strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaochang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding person identification via gait. (arXiv:2203.04179v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2203.04179","description":"<p>Gait recognition is the process of identifying humans from their bipedal\nlocomotion such as walking or running. As such, gait data is privacy sensitive\ninformation and should be anonymized where possible. With the rise of higher\nquality gait recording techniques, such as depth cameras or motion capture\nsuits, an increasing amount of detailed gait data is captured and processed.\nIntroduction and rise of the Metaverse is but one popular application scenario\nin which the gait of users is transferred onto digital avatars. As a first step\ntowards developing effective anonymization techniques for high-quality gait\ndata, we study different aspects of movement data to quantify their\ncontribution to gait recognition. We first extract categories of features from\nthe literature on human gait perception and then design experiments for each\ncategory to assess how much the information they contain contributes to\nrecognition success. Our results show that gait anonymization will be\nchallenging, as the data is highly redundant and interdependent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanisch_S/0/1/0/all/0/1\">Simon Hanisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muschter_E/0/1/0/all/0/1\">Evelyn Muschter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatzipanayioti_A/0/1/0/all/0/1\">Admantini Hatzipanayioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu-Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strufe_T/0/1/0/all/0/1\">Thorsten Strufe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09215","description":"<p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and\nefficiently create a dynamic digital world, containing large-scale\nindoor-outdoor scenes, diverse human motions, and rich interactions between\nhumans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is\nspace-free without any external devices' constraints and map-free without\npre-built maps. Considering that IMUs can capture human poses but always drift\nfor long-period use, while LiDAR is stable for global localization but rough\nfor local positions and orientations, HSC4D makes both sensors complement each\nother by a joint optimization and achieves promising results for long-term\ncapture. Relationships between humans and environments are also explored to\nmake their interaction more realistic. To facilitate many down-stream tasks,\nlike AR, VR, robots, autonomous driving, etc., we propose a dataset containing\nthree large scenes (1k-5k $m^2$) with accurate dynamic human motions and\nlocations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)\nand challenging human activities (exercising, walking up/down stairs, climbing,\netc.) demonstrate the effectiveness and the generalization ability of HSC4D.\nThe dataset and code are available at <a href=\"http://www.lidarhumanmotion.net/hsc4d/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yudi Dai</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yitai Lin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chenglu Wen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a> (1) ((1) Xiamen University, China, (2) ShanghaiTech University, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09550","description":"<p>Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhe Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Miao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianghong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization. (arXiv:2204.04215v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.04215","description":"<p>Data-free quantization is a task that compresses the neural network to low\nbit-width without access to original training data. Most existing data-free\nquantization methods cause severe performance degradation due to inaccurate\nactivation clipping range and quantization error, especially for low bit-width.\nIn this paper, we present a simple yet effective data-free quantization method\nwith accurate activation clipping and adaptive batch normalization. Accurate\nactivation clipping (AAC) improves the model accuracy by exploiting accurate\nactivation information from the full-precision model. Adaptive batch\nnormalization firstly proposes to address the quantization error from\ndistribution changes by updating the batch normalization layer adaptively.\nExtensive experiments demonstrate that the proposed data-free quantization\nmethod can yield surprisingly performance, achieving 64.33% top-1 accuracy of\nResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the\nexisting state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yefei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-agent. (arXiv:2204.05513v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.05513","description":"<p>Focusing on the task of point-to-point navigation for an autonomous driving\nvehicle, we propose a novel deep learning model trained with end-to-end and\nmulti-task learning manners to perform both perception and control tasks\nsimultaneously. The model is used to drive the ego vehicle safely by following\na sequence of routes defined by the global planner. The perception part of the\nmodel is used to encode high-dimensional observation data provided by an RGBD\ncamera while performing semantic segmentation, semantic depth cloud (SDC)\nmapping, and traffic light state and stop sign prediction. Then, the control\npart decodes the encoded features along with additional information provided by\nGPS and speedometer to predict waypoints that come with a latent feature space.\nFurthermore, two agents are employed to process these outputs and make a\ncontrol policy that determines the level of steering, throttle, and brake as\nthe final action. The model is evaluated on CARLA simulator with various\nscenarios made of normal-adversarial situations and different weathers to mimic\nreal-world conditions. In addition, we do a comparative study with some recent\nmodels to justify the performance in multiple aspects of driving. Moreover, we\nalso conduct an ablation study on SDC mapping and multi-agent to understand\ntheir roles and behavior. As a result, our model achieves the highest driving\nscore even with fewer parameters and computation load. To support future\nstudies, we share our codes at\nhttps://github.com/oskarnatan/end-to-end-driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Natan_O/0/1/0/all/0/1\">Oskar Natan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1\">Jun Miura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08759","description":"<p>With the recently massive development in convolution neural networks,\nnumerous lightweight CNN-based image super-resolution methods have been\nproposed for practical deployments on edge devices. However, most existing\nmethods focus on one specific aspect: network or loss design, which leads to\nthe difficulty of minimizing the model size. To address the issue, we conclude\nblock devising, architecture searching, and loss design to obtain a more\nefficient SR structure. In this paper, we proposed an edge-enhanced feature\ndistillation network, named EFDN, to preserve the high-frequency information\nunder constrained resources. In detail, we build an edge-enhanced convolution\nblock based on the existing reparameterization methods. Meanwhile, we propose\nedge-enhanced gradient loss to calibrate the reparameterized path training.\nExperimental results show that our edge-enhanced strategies preserve the edge\nand significantly improve the final restoration quality. Code is available at\nhttps://github.com/icandle/EFDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Purification for Unsupervised Person Re-identification. (arXiv:2204.09931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09931","description":"<p>Unsupervised person re-identification is a challenging and promising task in\ncomputer vision. Nowadays unsupervised person re-identification methods have\nachieved great progress by training with pseudo labels. However, how to purify\nfeature and label noise is less explicitly studied in the unsupervised manner.\nTo purify the feature, we take into account two types of additional features\nfrom different local views to enrich the feature representation. The proposed\nmulti-view features are carefully integrated into our cluster contrast learning\nto leverage more discriminative cues that the global feature easily ignored and\nbiased. To purify the label noise, we propose to take advantage of the\nknowledge of teacher model in an offline scheme. Specifically, we first train a\nteacher model from noisy pseudo labels, and then use the teacher model to guide\nthe learning of our student model. In our setting, the student model could\nconverge fast with the supervision of the teacher model thus reduce the\ninterference of noisy labels as the teacher model greatly suffered. After\ncarefully handling the noise and bias in the feature learning, our purification\nmodules are proven to be very effective for unsupervised person\nre-identification. Extensive experiments on three popular person\nre-identification datasets demonstrate the superiority of our method.\nEspecially, our approach achieves a state-of-the-art accuracy 85.8\\% @mAP and\n94.5\\% @Rank-1 on the challenging Market-1501 benchmark with ResNet-50 under\nthe fully unsupervised setting. The code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Long Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1\">Xiao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases. (arXiv:2205.04800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04800","description":"<p>We propose a principled approach for non-isometric landmark-preserving\nnon-rigid shape matching. Our method is based on the functional maps framework,\nbut rather than promoting isometries we focus instead on near-conformal maps\nthat preserve landmarks exactly. We achieve this, first, by introducing a novel\nlandmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.\nSecond, we establish the functional decomposition of conformal maps expressed\nin this basis. Finally, we formulate a conformally-invariant energy that\npromotes high-quality landmark-preserving maps, and show how it can be solved\nvia a variant of the recently proposed ZoomOut method that we extend to our\nsetting. Our method is descriptor-free, efficient and robust to significant\nmesh variability. We evaluate our approach on a range of benchmark datasets and\ndemonstrate state-of-the-art performance on non-isometric benchmarks and near\nstate-of-the-art performance on isometric ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panine_M/0/1/0/all/0/1\">Mikhail Panine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirgo_M/0/1/0/all/0/1\">Maxime Kirgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXACT: How to Train Your Accuracy. (arXiv:2205.09615v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.09615","description":"<p>Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, Hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on image classification show that the proposed optimization method\nis a powerful alternative to widely used classification losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_I/0/1/0/all/0/1\">Ivan Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dereka_S/0/1/0/all/0/1\">Stanislav Dereka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1\">Sergey Kolesnikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search. (arXiv:2205.09676v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09676","description":"<p>To track the target in a video, current visual trackers usually adopt greedy\nsearch for target object localization in each frame, that is, the candidate\nregion with the maximum response score will be selected as the tracking result\nof each frame. However, we found that this may be not an optimal choice,\nespecially when encountering challenging tracking scenarios such as heavy\nocclusion and fast motion. To address this issue, we propose to maintain\nmultiple tracking trajectories and apply beam search strategy for visual\ntracking, so that the trajectory with fewer accumulated errors can be\nidentified. Accordingly, this paper introduces a novel multi-agent\nreinforcement learning based beam search tracking strategy, termed\nBeamTracking. It is mainly inspired by the image captioning task, which takes\nan image as input and generates diverse descriptions using beam search\nalgorithm. Accordingly, we formulate the tracking as a sample selection problem\nfulfilled by multiple parallel decision-making processes, each of which aims at\npicking out one sample as their tracking result in each frame. Each maintained\ntrajectory is associated with an agent to perform the decision-making and\ndetermine what actions should be taken to update related information. When all\nthe frames are processed, we select the trajectory with the maximum accumulated\nscore as the tracking result. Extensive experiments on seven popular tracking\nbenchmark datasets validated the effectiveness of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition. (arXiv:2205.10338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10338","description":"<p>Deep neural networks have surpassed human performance in key visual\nchallenges such as object recognition, but require a large amount of energy,\ncomputation, and memory. In contrast, spiking neural networks (SNNs) have the\npotential to improve both the efficiency and biological plausibility of object\nrecognition systems. Here we present a SNN model that uses spike-latency coding\nand winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli\nfrom the Fashion MNIST dataset. Stimuli were preprocessed with center-surround\nreceptive fields and then fed to a layer of spiking neurons whose synaptic\nweights were updated using spike-timing-dependent-plasticity (STDP). We\ninvestigate how the quality of the represented objects changes under different\nWTA-I schemes and demonstrate that a network of 150 spiking neurons can\nefficiently represent objects with as little as 40 spikes. Studying how core\nobject recognition may be implemented using biologically plausible learning\nrules in SNNs may not only further our understanding of the brain, but also\nlead to novel and efficient artificial vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_M/0/1/0/all/0/1\">Melani Sanchez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_T/0/1/0/all/0/1\">Tushar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cottereau_B/0/1/0/all/0/1\">Benoit R. Cottereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy. (arXiv:2205.10683v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10683","description":"<p>Large convolutional neural networks (CNN) can be difficult to train in the\ndifferentially private (DP) regime, since the optimization algorithms require a\ncomputationally expensive operation, known as the per-sample gradient clipping.\nWe propose an efficient and scalable implementation of this clipping on\nconvolutional layers, termed as the mixed ghost clipping, that significantly\neases the private training in terms of both time and space complexities,\nwithout affecting the accuracy. The improvement in efficiency is rigorously\nstudied through the first complexity analysis for the mixed ghost clipping and\nexisting DP training algorithms.\n</p>\n<p>Extensive experiments on vision classification tasks, with large ResNet, VGG,\nand Vision Transformers, demonstrate that DP training with mixed ghost clipping\nadds $1\\sim 10\\%$ memory overhead and $&lt;2\\times$ slowdown to the standard\nnon-private training. Specifically, when training VGG19 on CIFAR10, the mixed\nghost clipping is $3\\times$ faster than state-of-the-art Opacus library with\n$18\\times$ larger maximum batch size. To emphasize the significance of\nefficient DP training on convolutional layers, we achieve 96.7\\% accuracy on\nCIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous\nbest results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy\nengine (\\url{https://github.com/JialinMao/private_CNN}) that implements DP\ntraining of CNN with a few lines of code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jialin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shiyun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Fusion Method to Enhance the Robustness of CNN. (arXiv:2205.15582v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15582","description":"<p>With the development of technology rapidly, applications of convolutional\nneural networks have improved the convenience of our life. However, in image\nclassification field, it has been found that when some perturbations are added\nto images, the CNN would misclassify it. Thus various defense methods have been\nproposed. The previous approach only considered how to incorporate modules in\nthe network to improve robustness, but did not focus on the way the modules\nwere incorporated. In this paper, we design a new fusion method to enhance the\nrobustness of CNN. We use a dot product-based approach to add the denoising\nmodule to ResNet18 and the attention mechanism to further improve the\nrobustness of the model. The experimental results on CIFAR10 have shown that\nour method is effective and better than the state-of-the-art methods under the\nattack of FGSM and PGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yating Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.01736","description":"<p>It is known that Deep Neural networks (DNNs) are vulnerable to adversarial\nattacks, and the adversarial robustness of DNNs could be improved by adding\nadversarial noises to training data (e.g., the standard adversarial training\n(SAT)). However, inappropriate noises added to training data may reduce a\nmodel's performance, which is termed the trade-off between accuracy and\nrobustness. This problem has been sufficiently studied for the classification\nof whole images but has rarely been explored for image analysis tasks in the\nmedical application domain, including image segmentation, landmark detection,\nand object detection tasks. In this study, we show that, for those medical\nimage analysis tasks, the SAT method has a severe issue that limits its\npractical use: it generates a fixed and unified level of noise for all training\nsamples for robust DNN training. A high noise level may lead to a large\nreduction in model performance and a low noise level may not be effective in\nimproving robustness. To resolve this issue, we design an adaptive-margin\nadversarial training (AMAT) method that generates sample-wise adaptive\nadversarial noises for robust DNN training. In contrast to the existing,\nclassification-oriented adversarial training methods, our AMAT method uses a\nloss-defined-margin strategy so that it can be applied to different tasks as\nlong as the loss functions are well-defined. We successfully apply our AMAT\nmethod to state-of-the-art DNNs, using five publicly available datasets. The\nexperimental results demonstrate that: (1) our AMAT method can be applied to\nthe three seemingly different tasks in the medical image application domain;\n(2) AMAT outperforms the SAT method in adversarial robustness; (3) AMAT has a\nminimal reduction in prediction accuracy on clean data, compared with the SAT\nmethod; and (4) AMAT has almost the same training time cost as SAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning. (arXiv:2206.03010v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03010","description":"<p>Spatiotemporal predictive learning, which predicts future frames through\nhistorical prior knowledge with the aid of deep learning, is widely used in\nmany fields. Previous work essentially improves the model performance by\nwidening or deepening the network, but it also brings surging memory overhead,\nwhich seriously hinders the development and application of this technology. In\norder to improve the performance without increasing memory consumption, we\nfocus on scale, which is another dimension to improve model performance but\nwith low memory requirement. The effectiveness has been widely proved in many\nCNN-based tasks such as image classification and semantic segmentation, but it\nhas not been fully explored in recent RNN models. In this paper, learning from\nthe benefit of multi-scale, we propose a general framework named Multi-Scale\nRNN (MS-RNN) to boost recent RNN models for spatiotemporal predictive learning.\nBy integrating different scales, we enhance the existing models with both\nimproved performance and greatly reduced overhead. We verify our MS-RNN\nframework by exhaustive experiments with 6 popular RNN models (ConvLSTM,\nTrajGRU, PredRNN, PredRNN++, MIM, and MotionRNN) on 4 different datasets\n(Moving MNIST, KTH, TaxiBJ, and HKO-7). The results show the efficiency that\nthe RNN models incorporating our framework have much lower memory cost but\nbetter performance than before. Our code is released at\n\\url{https://github.com/mazhf/MS-RNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhifeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer. (arXiv:2206.05763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05763","description":"<p>Clinically, the accurate annotation of lesions/tissues can significantly\nfacilitate the disease diagnosis. For example, the segmentation of optic\ndisc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the\nsegmentation of skin lesions on dermoscopic images is helpful to the melanoma\ndiagnosis, etc. With the advancement of deep learning techniques, a wide range\nof methods proved the lesions/tissues segmentation can also facilitate the\nautomated disease diagnosis models. However, existing methods are limited in\nthe sense that they can only capture static regional correlations in the\nimages. Inspired by the global and dynamic nature of Vision Transformer, in\nthis paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans)\nto transfer the segmentation knowledge to the disease diagnosis network.\nSpecifically, we first propose an asymmetric multi-scale interaction strategy\nto correlate each single low-level diagnosis feature with multi-scale\nsegmentation features. Then, an effective strategy called SeA-block is adopted\nto vitalize diagnosis feature via correlated segmentation features. To model\nthe segmentation-diagnosis interaction, SeA-block first embeds the diagnosis\nfeature based on the segmentation information via the encoder, and then\ntransfers the embedding back to the diagnosis feature space by a decoder.\nExperimental results demonstrate that SeATrans surpasses a wide range of\nstate-of-the-art (SOTA) segmentation-assisted diagnosis methods on several\ndisease diagnosis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Transformer with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis. (arXiv:2206.05782v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.05782","description":"<p>The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a\nchallenging task. Most existing approaches focus solely on single-resolution\nimages. The multi-resolution schemes, utilizing image pyramids to enhance WSI\nvisual representations, have not yet been paid enough attention to. In order to\nexplore a multi-resolution solution for improving cancer prognosis accuracy,\nthis paper proposes a dual-stream architecture to model WSIs by an image\npyramid strategy. This architecture consists of two sub-streams: one for\nlow-resolution WSIs, and the other especially for high-resolution ones.\nCompared to other approaches, our scheme has three highlights: (i) there exists\na one-to-one relation between stream and resolution; (ii) a square pooling\nlayer is added to align the patches from two resolution streams, largely\nreducing computation cost and enabling a natural stream feature fusion; (iii) a\ncross-attention-based method is proposed to pool high-resolution patches\nspatially under the guidance of low-resolution ones. We validate our scheme on\nthree publicly-available datasets with a total number of 3,101 WSIs from 1,911\npatients. Experimental results verify that (i) hierarchical dual-stream\nrepresentation is more effective than single-stream ones for cancer prognosis,\ngaining an average C-Index rise of 5.0% and 1.8% on a single low-resolution and\nhigh-resolution stream, respectively; (ii) our dual-stream scheme could\noutperform current state-of-the-art ones, by an average C-Index improvement of\n5.1%; (iii) the cancer diseases with observable survival differences could have\ndifferent preferences for model complexity. Our scheme could serve as an\nalternative tool for further facilitating WSI prognosis research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_B/0/1/0/all/0/1\">Bo Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_F/0/1/0/all/0/1\">Feng Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_L/0/1/0/all/0/1\">Luping Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Generative Model of Neonatal Cortical Surface Development. (arXiv:2206.07542v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2206.07542","description":"<p>The neonatal cortical surface is known to be affected by preterm birth, and\nthe subsequent changes to cortical organisation have been associated with\npoorer neurodevelopmental outcomes. Deep Generative models have the potential\nto lead to clinically interpretable models of disease, but developing these on\nthe cortical surface is challenging since established techniques for learning\nconvolutional filters are inappropriate on non-flat topologies. To close this\ngap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to\ntranslate sphericalised neonatal cortical surface features (curvature and\nT1w/T2w cortical myelin) between different stages of cortical maturity. Results\nshow our method is able to reliably predict changes in individual patterns of\ncortical organisation at later stages of gestation, validated by comparison to\nlongitudinal data; and translate appearance between preterm and term gestation\n(&gt; 37 weeks gestation), validated through comparison with a trained\nterm/preterm classifier. Simulated differences in cortical maturation are\nconsistent with observations in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. Williams</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Edwards_A/0/1/0/all/0/1\">A. David Edwards</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Using Privileged Information for Zero-Shot Action Recognition. (arXiv:2206.08632v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08632","description":"<p>Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have\nnever been seen during training. Most existing methods assume a shared semantic\nspace between seen and unseen actions and intend to directly learn a mapping\nfrom a visual space to the semantic space. This approach has been challenged by\nthe semantic gap between the visual space and semantic space. This paper\npresents a novel method that uses object semantics as privileged information to\nnarrow the semantic gap and, hence, effectively, assist the learning. In\nparticular, a simple hallucination network is proposed to implicitly extract\nobject semantics during testing without explicitly extracting objects and a\ncross-attention module is developed to augment visual feature with the object\nsemantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have\nshown that the proposed method outperforms the state-of-the-art methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yonghong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Slot Attention for Vision-and-Language Navigation. (arXiv:2206.08645v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08645","description":"<p>Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\n</p>\n<p>Recently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\n</p>\n<p>However, there exist two problems in current transformer-based models.\n</p>\n<p>1) The models process each view independently without taking the integrity of\nthe objects into account.\n</p>\n<p>2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\n</p>\n<p>To address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yifeng Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution. (arXiv:2206.09114v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09114","description":"<p>Visual grounding is a task that aims to locate a target object according to a\nnatural language expression. As a multi-modal task, feature interaction between\ntextual and visual inputs is vital. However, previous solutions mainly handle\neach modality independently before fusing them together, which does not take\nfull advantage of relevant textual information while extracting visual\nfeatures. To better leverage the textual-visual relationship in visual\ngrounding, we propose a Query-conditioned Convolution Module (QCM) that\nextracts query-aware visual features by incorporating query information into\nthe generation of convolutional kernels. With our proposed QCM, the downstream\nfusion module receives visual features that are more discriminative and focused\non the desired object described in the expression, leading to more accurate\npredictions. Extensive experiments on three popular visual grounding datasets\ndemonstrate that our method achieves state-of-the-art performance. In addition,\nthe query-aware visual features are informative enough to achieve comparable\nperformance to the latest methods when directly used for prediction without\nfurther multi-modal fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chonghan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chih-Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Noel Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modality Image Super-Resolution using Generative Adversarial Networks. (arXiv:2206.09193v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.09193","description":"<p>Over the past few years deep learning-based techniques such as Generative\nAdversarial Networks (GANs) have significantly improved solutions to image\nsuper-resolution and image-to-image translation problems. In this paper, we\npropose a solution to the joint problem of image super-resolution and\nmulti-modality image-to-image translation. The problem can be stated as the\nrecovery of a high-resolution image in a modality, given a low-resolution\nobservation of the same image in an alternative modality. Our paper offers two\nmodels to address this problem and will be evaluated on the recovery of\nhigh-resolution day images given low-resolution night images of the same scene.\nPromising qualitative and quantitative results will be presented for each\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abedjooy_A/0/1/0/all/0/1\">Aref Abedjooy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mehran Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modality Image Inpainting using Generative Adversarial Networks. (arXiv:2206.09210v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.09210","description":"<p>Deep learning techniques, especially Generative Adversarial Networks (GANs)\nhave significantly improved image inpainting and image-to-image translation\ntasks over the past few years. To the best of our knowledge, the problem of\ncombining the image inpainting task with the multi-modality image-to-image\ntranslation remains intact. In this paper, we propose a model to address this\nproblem. The model will be evaluated on combined night-to-day image translation\nand inpainting, along with promising qualitative and quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abedjooy_A/0/1/0/all/0/1\">Aref Abedjooy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mehran Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAViR-T: Spatially Attentive Visual Reasoning with Transformers. (arXiv:2206.09265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09265","description":"<p>We present a novel computational model, \"SAViR-T\", for the family of visual\nreasoning problems embodied in the Raven's Progressive Matrices (RPM). Our\nmodel considers explicit spatial semantics of visual elements within each image\nin the puzzle, encoded as spatio-visual tokens, and learns the intra-image as\nwell as the inter-image token dependencies, highly relevant for the visual\nreasoning task. Token-wise relationship, modeled through a transformer-based\nSAViR-T architecture, extract group (row or column) driven representations by\nleveraging the group-rule coherence and use this as the inductive bias to\nextract the underlying rule representations in the top two row (or column) per\ntoken in the RPM. We use this relation representations to locate the correct\nchoice image that completes the last row or column for the RPM. Extensive\nexperiments across both synthetic RPM benchmarks, including RAVEN, I-RAVEN,\nRAVEN-FAIR, and PGM, and the natural image-based \"V-PROM\" demonstrate that\nSAViR-T sets a new state-of-the-art for visual reasoning, exceeding prior\nmodels' performance by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pritish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach. (arXiv:2206.09597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09597","description":"<p>Affordance-centric Question-driven Task Completion for Egocentric\nAssistant(AQTC) is a novel task which helps AI assistant learn from\ninstructional videos and scripts and guide the user step-by-step. In this\npaper, we deal with the AQTC via a two-stage Function-centric approach, which\nconsists of Question2Function Module to ground the question with the related\nfunction and Function2Answer Module to predict the action based on the\nhistorical steps. We evaluated several possible solutions in each module and\nobtained significant gains compared to the given baselines. Our code is\navailable at \\url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weidong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers. (arXiv:2206.09731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09731","description":"<p>Semantic segmentation necessitates approaches that learn high-level\ncharacteristics while dealing with enormous amounts of data. Convolutional\nneural networks (CNNs) can learn unique and adaptive features to achieve this\naim. However, due to the large size and high spatial resolution of remote\nsensing images, these networks cannot analyze an entire scene efficiently.\nRecently, deep transformers have proven their capability to record global\ninteractions between different objects in the image. In this paper, we propose\na new segmentation model that combines convolutional neural networks with\ntransformers, and show that this mixture of local and global feature extraction\ntechniques provides significant advantages in remote sensing segmentation. In\naddition, the proposed model includes two fusion layers that are designed to\nrepresent multi-modal inputs and output of the network efficiently. The input\nfusion layer extracts feature maps summarizing the relationship between image\ncontent and elevation maps (DSM). The output fusion layer uses a novel\nmulti-task segmentation strategy where class labels are identified using\nclass-specific feature extraction layers and loss functions. Finally, a\nfast-marching method is used to convert all unidentified class labels to their\nclosest known neighbors. Our results demonstrate that the proposed methodology\nimproves segmentation accuracy compared to state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlMarzouqi_H/0/1/0/all/0/1\">Hasan AlMarzouqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saoud_L/0/1/0/all/0/1\">Lyes Saad Saoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning. (arXiv:2206.10137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10137","description":"<p>Contrastive self-supervised learning methods learn to map data points such as\nimages into non-parametric representation space without requiring labels. While\nhighly successful, current methods require a large amount of data in the\ntraining phase. In situations where the target training set is limited in size,\ngeneralization is known to be poor. Pretraining on a large source data set and\nfine-tuning on the target samples is prone to overfitting in the few-shot\nregime, where only a small number of target samples are available. Motivated by\nthis, we propose a domain adaption method for self-supervised contrastive\nlearning, termed Few-Max, to address the issue of adaptation to a target\ndistribution under few-shot learning. To quantify the representation quality,\nwe evaluate Few-Max on a range of source and target datasets, including\nImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms other\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaabad_A/0/1/0/all/0/1\">Ali Lotfi Rezaabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sidharth Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_S/0/1/0/all/0/1\">Sriram Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_J/0/1/0/all/0/1\">Jonathan I. Tamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Estimate and Refine Fluid Motion with Physical Dynamics. (arXiv:2206.10480v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.10480","description":"<p>Extracting information on fluid motion directly from images is challenging.\nFluid flow represents a complex dynamic system governed by the Navier-Stokes\nequations. General optical flow methods are typically designed for rigid body\nmotion, and thus struggle if applied to fluid motion estimation directly.\nFurther, optical flow methods only focus on two consecutive frames without\nutilising historical temporal information, while the fluid motion (velocity\nfield) can be considered a continuous trajectory constrained by time-dependent\npartial differential equations (PDEs). This discrepancy has the potential to\ninduce physically inconsistent estimations. Here we propose an unsupervised\nlearning based prediction-correction scheme for fluid flow estimation. An\nestimate is first given by a PDE-constrained optical flow predictor, which is\nthen refined by a physical based corrector. The proposed approach outperforms\noptical flow methods and shows competitive results compared to existing\nsupervised learning based methods on a benchmark dataset. Furthermore, the\nproposed approach can generalize to complex real-world fluid scenarios where\nground truth information is effectively unknowable. Finally, experiments\ndemonstrate that the physical corrector can refine flow estimates by mimicking\nthe operator splitting method commonly utilised in fluid dynamical simulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tlhomole_J/0/1/0/all/0/1\">James Tlhomole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piggott_M/0/1/0/all/0/1\">Matthew D. Piggott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}