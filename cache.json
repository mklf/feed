{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"LIP: Lightweight Intelligent Preprocessor for meaningful text-to-speech. (arXiv:2207.07118v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07118","description":"<p>Existing Text-to-Speech (TTS) systems need to read messages from the email\nwhich may have Personal Identifiable Information (PII) to text messages that\ncan have a streak of emojis and punctuation. 92% of the world's online\npopulation use emoji with more than 10 billion emojis sent everyday. Lack of\npreprocessor leads to messages being read as-is including punctuation and\ninfographics like emoticons. This problem worsens if there is a continuous\nsequence of punctuation/emojis that are quite common in real-world\ncommunications like messaging, Social Networking Site (SNS) interactions, etc.\nIn this work, we aim to introduce a lightweight intelligent preprocessor (LIP)\nthat can enhance the readability of a message before being passed downstream to\nexisting TTS systems. We propose multiple sub-modules including: expanding\ncontraction, censoring swear words, and masking of PII, as part of our\npreprocessor to enhance the readability of text. With a memory footprint of\nonly 3.55 MB and inference time of 4 ms for up to 50-character text, our\nsolution is suitable for real-time deployment. This work being the first of its\nkind, we try to benchmark with an open independent survey, the result of which\nshows 76.5% preference towards LIP enabled TTS engine as compared to standard\nTTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1\">Harshvardhan Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Begam_N/0/1/0/all/0/1\">Nansi Begam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Richa Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B.S.S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition in Conversation using Probabilistic Soft Logic. (arXiv:2207.07238v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07238","description":"<p>Creating agents that can both appropriately respond to conversations and\nunderstand complex human linguistic tendencies and social cues has been a long\nstanding challenge in the NLP community. A recent pillar of research revolves\naround emotion recognition in conversation (ERC); a sub-field of emotion\nrecognition that focuses on conversations or dialogues that contain two or more\nutterances. In this work, we explore an approach to ERC that exploits the use\nof neural embeddings along with complex structures in dialogues. We implement\nour approach in a framework called Probabilistic Soft Logic (PSL), a\ndeclarative templating language that uses first-order like logical rules, that\nwhen combined with data, define a particular class of graphical model.\nAdditionally, PSL provides functionality for the incorporation of results from\nneural models into PSL models. This allows our model to take advantage of\nadvanced neural methods, such as sentence embeddings, and logical reasoning\nover the structure of a dialogue. We compare our method with state-of-the-art\npurely neural ERC systems, and see almost a 20% improvement. With these\nresults, we provide an extensive qualitative and quantitative analysis over the\nDailyDialog conversation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Augustine_E/0/1/0/all/0/1\">Eriq Augustine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1\">Connor Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickens_C/0/1/0/all/0/1\">Charles Dickens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07243","description":"<p>Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1\">Anita Mahinpei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights. (arXiv:2207.07255v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07255","description":"<p>Investigating cooperativity of interlocutors is central in studying\npragmatics of dialogue. Models of conversation that only assume cooperative\nagents fail to explain the dynamics of strategic conversations. Thus, we\ninvestigate the ability of agents to identify non-cooperative interlocutors\nwhile completing a concurrent visual-dialogue task. Within this novel setting,\nwe study the optimality of communication strategies for achieving this\nmulti-task objective. We use the tools of learning theory to develop a\ntheoretical model for identifying non-cooperative interlocutors and apply this\ntheory to analyze different communication strategies. We also introduce a\ncorpus of non-cooperative conversations about images in the GuessWhat?! dataset\nproposed by De Vries et al. (2017). We use reinforcement learning to implement\nmultiple communication strategies in this context and find empirical results\nvalidate our theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maidment_T/0/1/0/all/0/1\">Tristan Maidment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healy_P/0/1/0/all/0/1\">Pat Healy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Flexible Schema-Guided Dialogue Management Framework: From Friendly Peer to Virtual Standardized Cancer Patient. (arXiv:2207.07276v1 [cs.AI])","link":"http://arxiv.org/abs/2207.07276","description":"<p>A schema-guided approach to dialogue management has been shown in recent work\nto be effective in creating robust customizable virtual agents capable of\nacting as friendly peers or task assistants. However, successful applications\nof these methods in open-ended, mixed-initiative domains remain elusive --\nparticularly within medical domains such as virtual standardized patients,\nwhere such complex interactions are commonplace -- and require more extensive\nand flexible dialogue management capabilities than previous systems provide. In\nthis paper, we describe a general-purpose schema-guided dialogue management\nframework used to develop SOPHIE, a virtual standardized cancer patient that\nallows a doctor to conveniently practice for interactions with patients. We\nconduct a crowdsourced evaluation of conversations between medical students and\nSOPHIE. Our agent is judged to produce responses that are natural, emotionally\nappropriate, and consistent with her role as a cancer patient. Furthermore, it\nsignificantly outperforms an end-to-end neural model fine-tuned on a human\nstandardized patient corpus, attesting to the advantages of a schema-guided\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kane_B/0/1/0/all/0/1\">Benjamin Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giugno_C/0/1/0/all/0/1\">Catherine Giugno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_L/0/1/0/all/0/1\">Lenhart Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haut_K/0/1/0/all/0/1\">Kurtis Haut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wohn_C/0/1/0/all/0/1\">Caleb Wohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Ehsan Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07278","description":"<p>With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weibo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text. (arXiv:2207.07308v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07308","description":"<p>The wide use of social media and digital technologies facilitates sharing\nvarious news and information about events and activities. Despite sharing\npositive information misleading and false information is also spreading on\nsocial media. There have been efforts in identifying such misleading\ninformation both manually by human experts and automatic tools. Manual effort\ndoes not scale well due to the high volume of information, containing factual\nclaims, are appearing online. Therefore, automatically identifying check-worthy\nclaims can be very useful for human experts. In this study, we describe our\nparticipation in Subtask-1A: Check-worthiness of tweets (English, Dutch and\nSpanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing\nsteps and applied different models to identify whether a given text is worthy\nof fact checking or not. We use the oversampling technique to balance the\ndataset and applied SVM and Random Forest (RF) with TF-IDF representations. We\nalso used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models\nfor the experiments. We used BERT-m for the official submissions and our\nsystems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,\nrespectively. In further experiments, our evaluation shows that transformer\nmodels (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and\nEnglish languages where a different scenario is observed for Spanish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarannum_P/0/1/0/all/0/1\">Prerona Tarannum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noori_S/0/1/0/all/0/1\">Sheak Rashed Haider Noori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning about Actions over Visual and Linguistic Modalities: A Survey. (arXiv:2207.07568v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07568","description":"<p>'Actions' play a vital role in how humans interact with the world and enable\nthem to achieve desired goals. As a result, most common sense (CS) knowledge\nfor humans revolves around actions. While 'Reasoning about Actions &amp; Change'\n(RAC) has been widely studied in the Knowledge Representation community, it has\nrecently piqued the interest of NLP and computer vision researchers. This paper\nsurveys existing tasks, benchmark datasets, various techniques and models, and\ntheir respective performance concerning advancements in RAC in the vision and\nlanguage domain. Towards the end, we summarize our key takeaways, discuss the\npresent challenges facing this research area, and outline potential directions\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sampat_S/0/1/0/all/0/1\">Shailaja Keyur Sampat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Maitreya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Subhasish Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Twitter know your political views? POLiTweets dataset and semi-automatic method for political leaning discovery. (arXiv:2207.07586v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07586","description":"<p>Every day, the world is flooded by millions of messages and statements posted\non Twitter or Facebook. Social media platforms try to protect users' personal\ndata, but there still is a real risk of misuse, including elections\nmanipulation. Did you know, that only 13 posts addressing important or\ncontroversial topics for society are enough to predict one's political\naffiliation with a 0.85 F1-score? To examine this phenomenon, we created a\nnovel universal method of semi-automated political leaning discovery. It relies\non a heuristical data annotation procedure, which was evaluated to achieve 0.95\nagreement with human annotators (counted as an accuracy metric). We also\npresent POLiTweets - the first publicly open Polish dataset for political\naffiliation discovery in a multi-party setup, consisting of over 147k tweets\nfrom almost 10k Polish-writing users annotated heuristically and almost 40k\ntweets from 166 users annotated manually as a test set. We used our data to\nstudy the aspects of domain shift in the context of topics and the type of\ncontent writers - ordinary citizens vs. professional politicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1\">Joanna Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajstura_M/0/1/0/all/0/1\">Micha&#x142; Kajstura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziolkowski_M/0/1/0/all/0/1\">Maciej Zi&#xf3;&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajda_K/0/1/0/all/0/1\">Krzysztof Rajda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text. (arXiv:2207.07597v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07597","description":"<p>In recent years, creating and managing knowledge bases have become crucial to\nthe retail product and enterprise domains. We present an automatic knowledge\nbase construction system that mines data from documents. This system can\ngenerate training data during the training process without human intervention.\nTherefore, it is domain-agnostic trainable using only the target domain text\ncorpus and a pre-defined knowledge base. This system is called OASYS and is the\nfirst system built with the Korean language in mind. In addition, we also have\nconstructed a new human-annotated benchmark dataset of the Korean Wikipedia\ncorpus paired with a Korean DBpedia to aid system evaluation. The system\nperformance results on human-annotated benchmark test dataset are meaningful\nand show that the generated knowledge base from OASYS trained on only\nauto-generated data is useful. We provide both a human-annotated test dataset\nand an auto-generated dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Je_S/0/1/0/all/0/1\">Sang-hyun Je</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunjoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Fine-tuning of Language Models. (arXiv:2110.06500v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06500","description":"<p>We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Saurabh Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1\">Gautam Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1\">Andre Manoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation. (arXiv:2110.08214v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08214","description":"<p>Speech-to-speech translation (S2ST) converts input speech to speech in\nanother language. A challenge of delivering S2ST in real time is the\naccumulated delay between the translation and speech synthesis modules. While\nrecently incremental text-to-speech (iTTS) models have shown large quality\nimprovements, they typically require additional future text inputs to reach\noptimal performance. In this work, we minimize the initial waiting time of iTTS\nby adapting the upstream speech translator to generate high-quality pseudo\nlookahead for the speech synthesizer. After mitigating the initial delay, we\ndemonstrate that the duration of synthesized speech also plays a crucial role\non latency. We formalize this as a latency metric and then present a simple yet\neffective duration-scaling approach for latency reduction. Our approaches\nconsistently reduce latency by 0.2-0.5 second without sacrificing speech\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Visual-Language Models for Efficient Video Understanding. (arXiv:2112.04478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04478","description":"<p>Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chen Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tengda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kunhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v5 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.00842","description":"<p>This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output branches, the\nt-SOT model has only a single output branch that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of ``virtual''\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Theoretically Grounded Benchmark for Evaluating Machine Commonsense. (arXiv:2203.12184v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12184","description":"<p>Programming machines with commonsense reasoning (CSR) abilities is a\nlongstanding challenge in the Artificial Intelligence community. Current CSR\nbenchmarks use multiple-choice (and in relatively fewer cases, generative)\nquestion-answering instances to evaluate machine commonsense. Recent progress\nin transformer-based language representation models suggest that considerable\nprogress has been made on existing benchmarks. However, although tens of CSR\nbenchmarks currently exist, and are growing, it is not evident that the full\nsuite of commonsense capabilities have been systematically evaluated.\nFurthermore, there are doubts about whether language models are 'fitting' to a\nbenchmark dataset's training partition by picking up on subtle, but normatively\nirrelevant (at least for CSR), statistical features to achieve good performance\non the testing partition. To address these challenges, we propose a benchmark\ncalled Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based\non discriminative question answering, but with questions designed to evaluate\ndiverse aspects of commonsense, such as space, time, and world states. TG-CSR\nis based on a subset of commonsense categories first proposed as a viable\ntheory of commonsense by Gordon and Hobbs. The benchmark is also designed to be\nfew-shot (and in the future, zero-shot), with only a few training and\nvalidation examples provided. This report discusses the structure and\nconstruction of the benchmark. Preliminary results suggest that the benchmark\nis challenging even for advanced language representation models designed for\ndiscriminative CSR question answering tasks.\n</p>\n<p>Benchmark access and leaderboard:\nhttps://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:\nhttps://usc-isi-i2.github.io/TGCSR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1\">Henrique Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Ke Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulvehill_A/0/1/0/all/0/1\">Alice M. Mulvehill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razeghi_Y/0/1/0/all/0/1\">Yasaman Razeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about a children's growth and development. The\ncoronavirus pandemic has highlighted the necessity of online assessment for\npreschool children. One of the areas that should be tested is the ability to\nspeak. Because of the differences between children's and adults' voices,\nemploying Automatic Speech Recognition(ASR) systems is difficult since they are\npre-trained on adults' voices. We constructed an ASR for our cognitive test\nsystem to solve this issue using the Wav2Vec 2.0 model with a new pre-training\nobjective called Random Frequency Pitch(RFP). In addition, we used our new\ndataset to fine-tune our model for Meaningless Words(MW) and Rapid Automatic\nNaming(RAN) tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on\nthe Persian section of the CommonVoice dataset. Furthermore, our novel\nmethodology produces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings. (arXiv:2203.16685v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16685","description":"<p>This paper presents a streaming speaker-attributed automatic speech\nrecognition (SA-ASR) model that can recognize ``who spoke what'' with low\nlatency even when multiple people are speaking simultaneously. Our model is\nbased on token-level serialized output training (t-SOT) which was recently\nproposed to transcribe multi-talker speech in a streaming fashion. To further\nrecognize speaker identities, we propose an encoder-decoder based speaker\nembedding extractor that can estimate a speaker representation for each\nrecognized token not only from non-overlapping speech but also from overlapping\nspeech. The proposed speaker embedding, named t-vector, is extracted\nsynchronously with the t-SOT ASR model, enabling joint execution of speaker\nidentification (SID) or speaker diarization (SD) with the multi-talker\ntranscription with low latency. We evaluate the proposed model for a joint task\nof ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed\nmodel achieves substantially better accuracy than a prior streaming model and\nshows comparable or sometimes even superior results to the state-of-the-art\noffline SA-ASR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.17152","description":"<p>Speech enhancement (SE) performance has improved considerably owing to the\nuse of deep learning models as a base function. Herein, we propose a perceptual\ncontrast stretching (PCS) approach to further improve SE performance. The PCS\nis derived based on the critical band importance function and is applied to\nmodify the targets of the SE model. Specifically, the contrast of target\nfeatures is stretched based on perceptual importance, thereby improving the\noverall SE performance. Compared with post-processing-based implementations,\nincorporating PCS into the training phase preserves performance and reduces\nonline computation. Notably, PCS can be combined with different SE model\narchitectures and training criteria. Furthermore, PCS does not affect the\ncausality or convergence of SE model training. Experimental results on the\nVoiceBank-DEMAND dataset show that the proposed method can achieve\nstate-of-the-art performance on both causal (PESQ score = 3.07) and noncausal\n(PESQ score = 3.35) SE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_R/0/1/0/all/0/1\">Rong Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03117","description":"<p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to align aspects and corresponding sentiments for\naspect-specific sentiment polarity inference. It is challenging because a\nsentence may contain multiple aspects or complicated (e.g., conditional,\ncoordinating, or adversative) relations. Recently, exploiting dependency syntax\ninformation with graph neural networks has been the most popular trend. Despite\nits success, methods that heavily rely on the dependency tree pose challenges\nin accurately modeling the alignment of the aspects and their words indicative\nof sentiment, since the dependency tree may provide noisy signals of unrelated\nassociations (e.g., the \"conj\" relation between \"great\" and \"dreadful\" in\nFigure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax\naware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully\nexploits the syntax information (e.g., phrase segmentation and hierarchical\nstructure) of the constituent tree of a sentence to model the sentiment-aware\ncontext of every single aspect (called intra-context) and the sentiment\nrelations across aspects (called inter-context) for learning. Experiments on\nfour benchmark datasets demonstrate that BiSyn-GAT+ outperforms the\nstate-of-the-art methods consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiyong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailment Graph Learning with Textual Entailment and Soft Transitivity. (arXiv:2204.03286v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03286","description":"<p>Typed entailment graphs try to learn the entailment relations between\npredicates from text and model them as edges between predicate nodes. The\nconstruction of entailment graphs usually suffers from severe sparsity and\nunreliability of distributional similarity. We propose a two-stage method,\nEntailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns\nlocal entailment relations by recognizing possible textual entailment between\ntemplate sentences formed by typed CCG-parsed predicates. Based on the\ngenerated local graph, EGT2 then uses three novel soft transitivity constraints\nto consider the logical transitivity in entailment structures. Experiments on\nbenchmark datasets show that EGT2 can well model the transitivity in entailment\ngraph to alleviate the sparsity issue, and lead to significant improvement over\ncurrent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Natural Language Understanding Models with Venn--ABERS Predictors. (arXiv:2205.10586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10586","description":"<p>Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovannotti_P/0/1/0/all/0/1\">Patrizio Giovannotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.11349","description":"<p>Recent works have shown that attaching prompts to the input is effective at\nconditioning Language Models (LM) to perform specific tasks. However, prompts\nare always included in the input text during inference, thus incurring\nsubstantial computational and memory overhead. Also, there is currently no\nstraightforward method of utilizing prompts that are longer than the maximum\ninput length of the LMs without incurring additional costs during inference. We\npropose Prompt Injection (PI), a novel formulation of injecting the prompt into\nthe parameters of an LM to be an efficient alternative to attaching fixed\nprompts to the input. We show that in scenarios with long fixed prompts, PI can\nbe up to 280 times more efficient in terms of total FLOPs than previous\napproaches. We further explore methodologies for PI and show promising results\nin persona-dependent conversation, semantic parsing, and zero-shot learning\nwith task instructions. Through these explorations, we show that PI can be a\npromising direction for conditioning language models, especially in scenarios\nwith long and fixed prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunbi Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-aware multimodal page classification of Brazilian legal documents. (arXiv:2207.00748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00748","description":"<p>The Brazilian Supreme Court receives tens of thousands of cases each\nsemester. Court employees spend thousands of hours to execute the initial\nanalysis and classification of those cases -- which takes effort away from\nposterior, more complex stages of the case management workflow. In this paper,\nwe explore multimodal classification of documents from Brazil's Supreme Court.\nWe train and evaluate our methods on a novel multimodal dataset of 6,510\nlawsuits (339,478 pages) with manual annotation assigning each page to one of\nsix classes. Each lawsuit is an ordered sequence of pages, which are stored\nboth as an image and as a corresponding text extracted through optical\ncharacter recognition. We first train two unimodal classifiers: a ResNet\npre-trained on ImageNet is fine-tuned on the images, and a convolutional\nnetwork with filters of multiple kernel sizes is trained from scratch on\ndocument texts. We use them as extractors of visual and textual features, which\nare then combined through our proposed Fusion Module. Our Fusion Module can\nhandle missing textual or visual input by using learned embeddings for missing\ndata. Moreover, we experiment with bi-directional Long Short-Term Memory\n(biLSTM) networks and linear-chain conditional random fields to model the\nsequential nature of the pages. The multimodal approaches outperform both\ntextual and visual classifiers, especially when leveraging the sequential\nnature of the pages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_P/0/1/0/all/0/1\">Pedro H. Luz de Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_A/0/1/0/all/0/1\">Ana Paula G. S. de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braz_F/0/1/0/all/0/1\">Fabricio A. Braz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nilton C. da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_F/0/1/0/all/0/1\">Flavio de Barros Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation. (arXiv:2207.04906v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.04906","description":"<p>Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear-time calculation of the expected sum of edge lengths in planar linearizations of trees. (arXiv:2207.05564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05564","description":"<p>Dependency graphs have proven to be a very successful model to represent the\nsyntactic structure of sentences of human languages. In these graphs, widely\naccepted to be trees, vertices are words and arcs connect\nsyntactically-dependent words. The tendency of these dependencies to be short\nhas been demonstrated using random baselines for the sum of the lengths of the\nedges or its variants. A ubiquitous baseline is the expected sum in projective\norderings (wherein edges do not cross and the root word of the sentence is not\ncovered by any edge). It was shown that said expected value can be computed in\n$O(n)$ time. In this article we focus on planar orderings (where the root word\ncan be covered) and present two main results. First, we show the relationship\nbetween the expected sum in planar arrangements and the expected sum in\nprojective arrangements. Second, we also derive a $O(n)$-time algorithm to\ncalculate the expected value of the sum of edge lengths. These two results stem\nfrom another contribution of the present article, namely a characterization of\nplanarity that, given a sentence, yields either the number of planar\npermutations or an efficient algorithm to generate uniformly random planar\npermutations of the words. Our research paves the way for replicating past\nresearch on dependency distance minimization using random planar linearizations\nas random baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.06414","description":"<p>Building models for health prediction based on Electronic Health Records\n(EHR) has become an active research area. EHR patient journey data consists of\npatient time-ordered clinical events/visits from patients. Most existing\nstudies focus on modeling long-term dependencies between visits, without\nexplicitly taking short-term correlations between consecutive visits into\naccount, where irregular time intervals, incorporated as auxiliary information,\nare fed into health prediction models to capture latent progressive patterns of\npatient journeys. We present a novel deep neural network with four modules to\ntake into account the contributions of various variables for health prediction:\ni) the Stacked Attention module strengthens the deep semantics in clinical\nevents within each patient journey and generates visit embeddings, ii) the\nShort-Term Temporal Attention module models short-term correlations between\nconsecutive visit embeddings while capturing the impact of time intervals\nwithin those visit embeddings, iii) the Long-Term Temporal Attention module\nmodels long-term dependencies between visit embeddings while capturing the\nimpact of time intervals within those visit embeddings, iv) and finally, the\nCoupled Attention module adaptively aggregates the outputs of Short-Term\nTemporal Attention and Long-Term Temporal Attention modules to make health\npredictions. Experimental results on MIMIC-III demonstrate superior predictive\naccuracy of our model compared to existing state-of-the-art methods, as well as\nthe interpretability and robustness of this approach. Furthermore, we found\nthat modeling short-term correlations contributes to local priors generation,\nleading to improved predictive modeling of patient journeys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Image Clustering with Contrastive Learning and Multi-scale Graph Convolutional Networks. (arXiv:2207.07173v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07173","description":"<p>Deep clustering has recently attracted significant attention. Despite the\nremarkable progress, most of the previous deep clustering works still suffer\nfrom two limitations. First, many of them focus on some distribution-based\nclustering loss, lacking the ability to exploit sample-wise (or\naugmentation-wise) relationships via contrastive learning. Second, they often\nneglect the indirect sample-wise structure information, overlooking the rich\npossibilities of multi-scale neighborhood structure learning. In view of this,\nthis paper presents a new deep clustering approach termed Image clustering with\ncontrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN),\nwhich bridges the gap between convolutional neural network (CNN) and graph\nconvolutional network (GCN) as well as the gap between contrastive learning and\nmulti-scale neighborhood structure learning for the image clustering task. The\nproposed IcicleGCN framework consists of four main modules, namely, the\nCNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster\nStructure Learning and Instance reconstruction Module (JC-SLIM), and the\nMulti-scale GCN module (M-GCN). Specifically, with two random augmentations\nperformed on each image, the backbone network with two weight-sharing views is\nutilized to learn the representations for the augmented samples, which are then\nfed to ISM and JC-SLIM for instance-level and cluster-level contrastive\nlearning, respectively. Further, to enforce multi-scale neighborhood structure\nlearning, two streams of GCNs and an auto-encoder are simultaneously trained\nvia (i) the layer-wise interaction with representation fusion and (ii) the\njoint self-adaptive learning that ensures their last-layer output distributions\nto be consistent. Experiments on multiple image datasets demonstrate the\nsuperior clustering performance of IcicleGCN over the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanku Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification. (arXiv:2207.07189v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07189","description":"<p>We present 'AiTLAS: Benchmark Arena' -- an open-source benchmark framework\nfor evaluating state-of-the-art deep learning approaches for image\nclassification in Earth Observation (EO). To this end, we present a\ncomprehensive comparative analysis of more than 400 models derived from nine\ndifferent state-of-the-art architectures, and compare them to a variety of\nmulti-class and multi-label classification tasks from 22 datasets with\ndifferent sizes and properties. In addition to models trained entirely on these\ndatasets, we also benchmark models trained in the context of transfer learning,\nleveraging pre-trained model variants, as it is typically performed in\npractice. All presented approaches are general and can be easily extended to\nmany other remote sensing image classification tasks not considered in this\nstudy. To ensure reproducibility and facilitate better usability and further\ndevelopments, all of the experimental resources including the trained models,\nmodel configurations and processing details of the datasets (with their\ncorresponding splits used for training and evaluating the models) are publicly\navailable on the repository: https://github.com/biasvariancelabs/aitlas-arena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitrovski_I/0/1/0/all/0/1\">Ivica Dimitrovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitanovski_I/0/1/0/all/0/1\">Ivan Kitanovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1\">Nikola Simidjievski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lipschitz Bound Analysis of Neural Networks. (arXiv:2207.07232v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07232","description":"<p>Lipschitz Bound Estimation is an effective method of regularizing deep neural\nnetworks to make them robust against adversarial attacks. This is useful in a\nvariety of applications ranging from reinforcement learning to autonomous\nsystems. In this paper, we highlight the significant gap in obtaining a\nnon-trivial Lipschitz bound certificate for Convolutional Neural Networks\n(CNNs) and empirically support it with extensive graphical analysis. We also\nshow that unrolling Convolutional layers or Toeplitz matrices can be employed\nto convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.\nFurther, we propose a simple algorithm to show the existing 20x-50x gap in a\nparticular data distribution between the actual lipschitz constant and the\nobtained tight bound. We also ran sets of thorough experiments on various\nnetwork architectures and benchmark them on datasets like MNIST and CIFAR-10.\nAll these proposals are supported by extensive testing, graphs, histograms and\ncomparative analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_S/0/1/0/all/0/1\">Sarosij Bose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Model Uncertainty Estimation via Stochastic Data Centering. (arXiv:2207.07235v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07235","description":"<p>We are interested in estimating the uncertainties of deep neural networks,\nwhich play an important role in many scientific and engineering problems. In\nthis paper, we present a striking new finding that an ensemble of neural\nnetworks with the same weight initialization, trained on datasets that are\nshifted by a constant bias gives rise to slightly inconsistent trained models,\nwhere the differences in predictions are a strong indicator of epistemic\nuncertainties. Using the neural tangent kernel (NTK), we demonstrate that this\nphenomena occurs in part because the NTK is not shift-invariant. Since this is\nachieved via a trivial input transformation, we show that it can therefore be\napproximated using just a single neural network -- using a technique that we\ncall $\\Delta-$UQ -- that estimates uncertainty around prediction by\nmarginalizing out the effect of the biases. We show that $\\Delta-$UQ's\nuncertainty estimates are superior to many of the current methods on a variety\nof benchmarks -- outlier rejection, calibration under distribution shift, and\nsequential design optimization of black box functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanaswamy_V/0/1/0/all/0/1\">Vivek Narayanaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremer_P/0/1/0/all/0/1\">Peer-Timo Bremer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Bark Beetle-Induced Forest Tree Mortality using Deep Learning. (arXiv:2207.07241v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07241","description":"<p>Bark beetle outbreaks can dramatically impact forest ecosystems and services\naround the world. For the development of effective forest policies and\nmanagement plans, the early detection of infested trees is essential. Despite\nthe visual symptoms of bark beetle infestation, this task remains challenging,\nconsidering overlapping tree crowns and non-homogeneity in crown foliage\ndiscolouration. In this work, a deep learning based method is proposed to\neffectively classify different stages of bark beetle attacks at the individual\ntree level. The proposed method uses RetinaNet architecture (exploiting a\nrobust feature extraction backbone pre-trained for tree crown detection) to\ntrain a shallow subnetwork for classifying the different attack stages of\nimages captured by unmanned aerial vehicles (UAVs). Moreover, various data\naugmentation strategies are examined to address the class imbalance problem,\nand consequently, the affine transformation is selected to be the most\neffective one for this purpose. Experimental evaluations demonstrate the\neffectiveness of the proposed method by achieving an average accuracy of\n98.95%, considerably outperforming the baseline method by approximately 10%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapil_R/0/1/0/all/0/1\">Rudraksh Kapil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1\">Devin Goodsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1\">Nadir Erbilgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07243","description":"<p>Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1\">Anita Mahinpei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Recognition from Detection: Single Shot Self-Reliant Scene Text Spotter. (arXiv:2207.07253v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07253","description":"<p>Typical text spotters follow the two-stage spotting strategy: detect the\nprecise boundary for a text instance first and then perform text recognition\nwithin the located text region. While such strategy has achieved substantial\nprogress, there are two underlying limitations. 1) The performance of text\nrecognition depends heavily on the precision of text detection, resulting in\nthe potential error propagation from detection to recognition. 2) The RoI\ncropping which bridges the detection and recognition brings noise from\nbackground and leads to information loss when pooling or interpolating from\nfeature maps. In this work we propose the single shot Self-Reliant Scene Text\nSpotter (SRSTS), which circumvents these limitations by decoupling recognition\nfrom detection. Specifically, we conduct text detection and recognition in\nparallel and bridge them by the shared positive anchor point. Consequently, our\nmethod is able to recognize the text instances correctly even though the\nprecise text boundaries are challenging to detect. Additionally, our method\nreduces the annotation cost for text detection substantially. Extensive\nexperiments on regular-shaped benchmark and arbitrary-shaped benchmark\ndemonstrate that our SRSTS compares favorably to previous state-of-the-art\nspotters in terms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingjing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pengyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleNet: Searching for the Model to Scale. (arXiv:2207.07267v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07267","description":"<p>Recently, community has paid increasing attention on model scaling and\ncontributed to developing a model family with a wide spectrum of scales.\nCurrent methods either simply resort to a one-shot NAS manner to construct a\nnon-structural and non-scalable model family or rely on a manual yet fixed\nscaling strategy to scale an unnecessarily best base model. In this paper, we\nbridge both two components and propose ScaleNet to jointly search base model\nand scaling strategy so that the scaled large model can have more promising\nperformance. Concretely, we design a super-supernet to embody models with\ndifferent spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be\nlearned interactively with the base model via a Markov chain-based evolution\nalgorithm and generalized to develop even larger models. To obtain a decent\nsuper-supernet, we design a hierarchical sampling strategy to enhance its\ntraining sufficiency and alleviate the disturbance. Experimental results show\nour scaled networks enjoy significant performance superiority on various FLOPs,\nbut with at least 2.53x reduction on search cost. Codes are available at\nhttps://github.com/luminolx/ScaleNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Vision Transformer with Cross Feature Attention. (arXiv:2207.07268v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07268","description":"<p>Recent advances in vision transformers (ViTs) have achieved great performance\nin visual recognition tasks. Convolutional neural networks (CNNs) exploit\nspatial inductive bias to learn visual representations, but these networks are\nspatially local. ViTs can learn global representations with their\nself-attention mechanism, but they are usually heavy-weight and unsuitable for\nmobile devices. In this paper, we propose cross feature attention (XFA) to\nbring down computation cost for transformers, and combine efficient mobile CNNs\nto form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can\nserve as a general-purpose backbone to learn both global and local\nrepresentation. Experimental results show that XFormer outperforms numerous CNN\nand ViT-based models across different tasks and datasets. On ImageNet1K\ndataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,\nwhich is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT\n(ViT-based) for similar number of parameters. Our model also performs well when\ntransferring to object detection and semantic segmentation tasks. On MS COCO\ndataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -&gt; 33.2 AP) in YOLOv3\nframework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with\nonly a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,\nsurpassing state-of-the-art lightweight segmentation networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Youpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huadong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_Y/0/1/0/all/0/1\">Yong A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Video Salient Object Detection via Point Supervision. (arXiv:2207.07269v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07269","description":"<p>Video salient object detection models trained on pixel-wise dense annotation\nhave achieved excellent performance, yet obtaining pixel-by-pixel annotated\ndatasets is laborious. Several works attempt to use scribble annotations to\nmitigate this problem, but point supervision as a more labor-saving annotation\nmethod (even the most labor-saving method among manual annotation methods for\ndense prediction), has not been explored. In this paper, we propose a strong\nbaseline model based on point supervision. To infer saliency maps with temporal\ninformation, we mine inter-frame complementary information from short-term and\nlong-term perspectives, respectively. Specifically, we propose a hybrid token\nattention module, which mixes optical flow and image information from\northogonal directions, adaptively highlighting critical optical flow\ninformation (channel dimension) and critical token information (spatial\ndimension). To exploit long-term cues, we develop the Long-term Cross-Frame\nAttention module (LCFA), which assists the current frame in inferring salient\nobjects based on multi-frame tokens. Furthermore, we label two point-supervised\ndatasets, P-DAVIS and P-DAVSOD, by relabeling the DAVIS and the DAVSOD dataset.\nExperiments on the six benchmark datasets illustrate our method outperforms the\nprevious state-of-the-art weakly supervised methods and even is comparable with\nsome fully supervised approaches. Source code and datasets are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Haozhe Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07278","description":"<p>With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weibo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP. (arXiv:2207.07284v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07284","description":"<p>Vision multi-layer perceptrons (MLPs) have shown promising performance in\ncomputer vision tasks, and become the main competitor of CNNs and vision\nTransformers. They use token-mixing layers to capture cross-token interactions,\nas opposed to the multi-head self-attention mechanism used by Transformers.\nHowever, the heavily parameterized token-mixing layers naturally lack\nmechanisms to capture local information and multi-granular non-local relations,\nthus their discriminative power is restrained. To tackle this issue, we propose\na new positional spacial gating unit (PoSGU). It exploits the attention\nformulations used in the classical relative positional encoding (RPE), to\nefficiently encode the cross-token relations for token mixing. It can\nsuccessfully reduce the current quadratic parameter complexity $O(N^2)$ of\nvision MLPs to $O(N)$ and $O(1)$. We experiment with two RPE mechanisms, and\nfurther propose a group-wise extension to improve their expressive power with\nthe accomplishment of multi-granular contexts. These then serve as the key\nbuilding blocks of a new type of vision MLP, referred to as PosMLP. We evaluate\nthe effectiveness of the proposed approach by conducting thorough experiments,\ndemonstrating an improved or comparable performance with reduced parameter\ncomplexity. For instance, for a model trained on ImageNet1K, we achieve a\nperformance improvement from 72.14\\% to 74.02\\% and a learnable parameter\nreduction from $19.4M$ to $18.2M$. Code could be found at\n\\href{https://github.com/Zhicaiwww/PosMLP}{https://github.com/Zhicaiwww/PosMLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tingting Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval. (arXiv:2207.07285v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07285","description":"<p>Video-text retrieval has been a crucial and fundamental task in multi-modal\nresearch. The development of video-text retrieval has been considerably\npromoted by large-scale multi-modal contrastive pre-training, which primarily\nfocuses on coarse-grained or fine-grained contrast. However, cross-grained\ncontrast, which is the contrast between coarse-grained representations and\nfine-grained representations, has rarely been explored in prior research.\nCompared with fine-grained or coarse-grained contrasts, cross-grained contrast\ncalculate the correlation between coarse-grained features and each fine-grained\nfeature, and is able to filter out the unnecessary fine-grained features guided\nby the coarse-grained feature during similarity calculation, thus improving the\naccuracy of retrieval. To this end, this paper presents a novel multi-grained\ncontrastive model, namely X-CLIP, for video-text retrieval. However, another\nchallenge lies in the similarity aggregation problem, which aims to aggregate\nfine-grained and cross-grained similarity matrices to instance-level\nsimilarity. To address this challenge, we propose the Attention Over Similarity\nMatrix (AOSM) module to make the model focus on the contrast between essential\nframes and words, thus lowering the impact of unnecessary frames and words on\nretrieval results. With multi-grained contrast and the proposed AOSM module,\nX-CLIP achieves outstanding performance on five widely-used video-text\nretrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1\nR@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous\nstate-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on\nthese benchmarks, demonstrating the superiority of multi-grained contrast and\nAOSM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yiwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation. (arXiv:2207.07288v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07288","description":"<p>Existing few-shot image generation approaches typically employ fusion-based\nstrategies, either on the image or the feature level, to produce new images.\nHowever, previous approaches struggle to synthesize high-frequency signals with\nfine details, deteriorating the synthesis quality. To address this, we propose\nWaveGAN, a frequency-aware model for few-shot image generation. Concretely, we\ndisentangle encoded features into multiple frequency components and perform\nlow-frequency skip connections to preserve outline and structural information.\nThen we alleviate the generator's struggles of synthesizing fine details by\nemploying high-frequency skip connections, thus providing informative frequency\ninformation to the generator. Moreover, we utilize a frequency L1-loss on the\ngenerated and real images to further impede frequency information loss.\nExtensive experiments demonstrate the effectiveness and advancement of our\nmethod on three datasets. Noticeably, we achieve new state-of-the-art with FID\n42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822\nrespectively on Flower, Animal Faces, and VGGFace. GitHub:\nhttps://github.com/kobeshegu/ECCV2022_WaveGAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Ziqiu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenyi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Deep Compressive Sensing with Recurrent-Residual Structural Constraints. (arXiv:2207.07301v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07301","description":"<p>Existing deep compressive sensing (CS) methods either ignore adaptive online\noptimization or depend on costly iterative optimizer during reconstruction.\nThis work explores a novel image CS framework with recurrent-residual\nstructural constraint, termed as R$^2$CS-NET. The R$^2$CS-NET first\nprogressively optimizes the acquired samplings through a novel recurrent neural\nnetwork. The cascaded residual convolutional network then fully reconstructs\nthe image from optimized latent representation. As the first deep CS framework\nefficiently bridging adaptive online optimization, the R$^2$CS-NET integrates\nthe robustness of online optimization with the efficiency and nonlinear\ncapacity of deep learning methods. Signal correlation has been addressed\nthrough the network architecture. The adaptive sensing nature further makes it\nan ideal candidate for color image CS via leveraging channel correlation.\nNumerical experiments verify the proposed recurrent latent optimization design\nnot only fulfills the adaptation motivation, but also outperforms classic long\nshort-term memory (LSTM) architecture in the same scenario. The overall\nframework demonstrates hardware implementation feasibility, with leading\nrobustness and generalization capability among existing deep CS benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Niu_J/0/1/0/all/0/1\">Jun Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Dermoscopic Image Feature Representation Learning for Melanoma Classification. (arXiv:2207.07303v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07303","description":"<p>Deep learning-based melanoma classification with dermoscopic images has\nrecently shown great potential in automatic early-stage melanoma diagnosis.\nHowever, limited by the significant data imbalance and obvious extraneous\nartifacts, i.e., the hair and ruler markings, discriminative feature extraction\nfrom dermoscopic images is very challenging. In this study, we seek to resolve\nthese problems respectively towards better representation learning for lesion\nfeatures. Specifically, a GAN-based data augmentation (GDA) strategy is adapted\nto generate synthetic melanoma-positive images, in conjunction with the\nproposed implicit hair denoising (IHD) strategy. Wherein the hair-related\nrepresentations are implicitly disentangled via an auxiliary classifier network\nand reversely sent to the melanoma-feature extraction backbone for better\nmelanoma-specific representation learning. Furthermore, to train the IHD\nmodule, the hair noises are additionally labeled on the ISIC2020 dataset,\nmaking it the first large-scale dermoscopic dataset with annotation of\nhair-like artifacts. Extensive experiments demonstrate the superiority of the\nproposed framework as well as the effectiveness of each component. The improved\ndataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">ChengHui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_M/0/1/0/all/0/1\">MingKang Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">ShengGe Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">MingQing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1\">JiangPeng Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">HanMo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiao-Jun Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Privacy-Preserving Person Re-identification via Person Identify Shift. (arXiv:2207.07311v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07311","description":"<p>Recently privacy concerns of person re-identification (ReID) raise more and\nmore attention and preserving the privacy of the pedestrian images used by ReID\nmethods become essential. De-identification (DeID) methods alleviate privacy\nissues by removing the identity-related of the ReID data. However, most of the\nexisting DeID methods tend to remove all personal identity-related information\nand compromise the usability of de-identified data on the ReID task. In this\npaper, we aim to develop a technique that can achieve a good trade-off between\nprivacy protection and data usability for person ReID. To achieve this, we\npropose a novel de-identification method designed explicitly for person ReID,\nnamed Person Identify Shift (PIS). PIS removes the absolute identity in a\npedestrian image while preserving the identity relationship between image\npairs. By exploiting the interpolation property of variational auto-encoder,\nPIS shifts each pedestrian image from the current identity to another with a\nnew identity, resulting in images still preserving the relative identities.\nExperimental results show that our method has a better trade-off between\nprivacy-preserving and model performance than existing de-identification\nmethods and can defend against human and model attacks for data privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shuguang Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingsong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Cairong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07316","description":"<p>Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiazhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">ShengChuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancement by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for Low-Light Images. (arXiv:2207.07317v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07317","description":"<p>Low-light image enhancement is an inherently subjective process whose targets\nvary with the user's aesthetic. Motivated by this, several personalized\nenhancement methods have been investigated. However, the enhancement process\nbased on user preferences in these techniques is invisible, i.e., a \"black\nbox\". In this work, we propose an intelligible unsupervised personalized\nenhancer (iUPEnhancer) for low-light images, which establishes the correlations\nbetween the low-light and the unpaired reference images with regard to three\nuser-friendly attributions (brightness, chromaticity, and noise). The proposed\niUP-Enhancer is trained with the guidance of these correlations and the\ncorresponding unsupervised loss functions. Rather than a \"black box\" process,\nour iUP-Enhancer presents an intelligible enhancement process with the above\nattributions. Extensive experiments demonstrate that the proposed algorithm\nproduces competitive qualitative and quantitative results while maintaining\nexcellent flexibility and scalability. This can be validated by personalization\nwith single/multiple references, cross-attribution references, or merely\nadjusting parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Naishan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Man Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Co-capture System for Recording and Tracking Fish with Frame- and Event Cameras. (arXiv:2207.07332v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07332","description":"<p>This work introduces a co-capture system for multi-animal visual data\nacquisition using conventional cameras and event cameras. Event cameras offer\nmultiple advantages over frame-based cameras, such as a high temporal\nresolution and temporal redundancy suppression, which enable us to efficiently\ncapture the fast and erratic movements of fish. We furthermore present an\nevent-based multi-animal tracking algorithm, which proves the feasibility of\nthe approach and sets the baseline for further exploration of combining the\nadvantages of event cameras and conventional cameras for multi-animal tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamann_F/0/1/0/all/0/1\">Friedhelm Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rain Rate Estimation with SAR using NEXRAD measurements with Convolutional Neural Networks. (arXiv:2207.07333v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07333","description":"<p>Remote sensing of rainfall events is critical for both operational and\nscientific needs, including for example weather forecasting, extreme flood\nmitigation, water cycle monitoring, etc. Ground-based weather radars, such as\nNOAA's Next-Generation Radar (NEXRAD), provide reflectivity and precipitation\nmeasurements of rainfall events. However, the observation range of such radars\nis limited to a few hundred kilometers, prompting the exploration of other\nremote sensing methods, paricularly over the open ocean, that represents large\nareas not covered by land-based radars. For a number of decades, C-band SAR\nimagery such a such as Sentinel-1 imagery has been known to exhibit rainfall\nsignatures over the sea surface. However, the development of SAR-derived\nrainfall products remains a challenge. Here we propose a deep learning approach\nto extract rainfall information from SAR imagery. We demonstrate that a\nconvolutional neural network, such as U-Net, trained on a colocated and\npreprocessed Sentinel-1/NEXRAD dataset clearly outperforms state-of-the-art\nfiltering schemes. Our results indicate high performance in segmenting\nprecipitation regimes, delineated by thresholds at 1, 3, and 10 mm/h. Compared\nto current methods that rely on Koch filters to draw binary rainfall maps,\nthese multi-threshold learning-based models can provide rainfall estimation for\nhigher wind speeds and thus may be of great interest for data assimilation\nweather forecasting or for improving the qualification of SAR-derived wind\nfield data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colin_A/0/1/0/all/0/1\">Aur&#xe9;lien Colin</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tandeo_P/0/1/0/all/0/1\">Pierre Tandeo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Peureux_C/0/1/0/all/0/1\">Charles Peureux</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Husson_R/0/1/0/all/0/1\">Romain Husson</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Longepe_N/0/1/0/all/0/1\">Nicolas Longepe</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1\">Ronan Fablet</a> (1) ((1) IMT Atlantique, Lab-STICC, UMR CNRS, France, (2) Collecte Localisation Satellites, Brest, France, (3) &#x3a6;-lab Explore Office, ESRIN, European Space Agency (ESA), Frascati, Italy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Parallax Transformer Network for Stereo Image JPEG Artifacts Removal. (arXiv:2207.07335v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07335","description":"<p>Under stereo settings, the performance of image JPEG artifacts removal can be\nfurther improved by exploiting the additional information provided by a second\nview. However, incorporating this information for stereo image JPEG artifacts\nremoval is a huge challenge, since the existing compression artifacts make\npixel-level view alignment difficult. In this paper, we propose a novel\nparallax transformer network (PTNet) to integrate the information from stereo\nimage pairs for stereo image JPEG artifacts removal. Specifically, a\nwell-designed symmetric bi-directional parallax transformer module is proposed\nto match features with similar textures between different views instead of\npixel-level view alignment. Due to the issues of occlusions and boundaries, a\nconfidence-based cross-view fusion module is proposed to achieve better feature\nfusion for both views, where the cross-view features are weighted with\nconfidence maps. Especially, we adopt a coarse-to-fine design for the\ncross-view interaction, leading to better performance. Comprehensive\nexperimental results demonstrate that our PTNet can effectively remove\ncompression artifacts and achieves superior performance than other testing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ri Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. (arXiv:2207.07340v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07340","description":"<p>With the wide application of face recognition systems, there is rising\nconcern that original face images could be exposed to malicious intents and\nconsequently cause personal privacy breaches. This paper presents DuetFace, a\nnovel privacy-preserving face recognition method that employs collaborative\ninference in the frequency domain. Starting from a counterintuitive discovery\nthat face recognition can achieve surprisingly good performance with only\nvisually indistinguishable high-frequency channels, this method designs a\ncredible split of frequency channels by their cruciality for visualization and\noperates the server-side model on non-crucial channels. However, the model\ndegrades in its attention to facial features due to the missing visual\ninformation. To compensate, the method introduces a plug-in interactive block\nto allow attention transfer from the client-side by producing a feature mask.\nThe mask is further refined by deriving and overlaying a facial region of\ninterest (ROI). Extensive experiments on multiple datasets validate the\neffectiveness of the proposed method in protecting face images from undesired\nvisual inspection, reconstruction, and identification while maintaining high\ntask availability and performance. Results show that the proposed method\nachieves a comparable recognition accuracy and computation cost to the\nunprotected ArcFace and outperforms the state-of-the-art privacy-preserving\nmethods. The source code is available at\nhttps://github.com/Tencent/TFace/tree/master/recognition/tasks/duetface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_Y/0/1/0/all/0/1\">Yuxi Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiazhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection. (arXiv:2207.07347v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07347","description":"<p>Standard approaches for adversarial patch generation lead to noisy\nconspicuous patterns, which are easily recognizable by humans. Recent research\nhas proposed several approaches to generate naturalistic patches using\ngenerative adversarial networks (GANs), yet only a few of them were evaluated\non the object detection use case. Moreover, the state of the art mostly focuses\non suppressing a single large bounding box in input by overlapping it with the\npatch directly. Suppressing objects near the patch is a different, more complex\ntask. In this work, we have evaluated the existing approaches to generate\ninconspicuous patches. We have adapted methods, originally developed for\ndifferent computer vision tasks, to the object detection use case with YOLOv3\nand the COCO dataset. We have evaluated two approaches to generate naturalistic\npatches: by incorporating patch generation into the GAN training process and by\nusing the pretrained GAN. For both cases, we have assessed a trade-off between\nperformance and naturalistic patch appearance. Our experiments have shown, that\nusing a pre-trained GAN helps to gain realistic-looking patches while\npreserving the performance similar to conventional adversarial patches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlitskaya_S/0/1/0/all/0/1\">Svetlana Pavlitskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codau_B/0/1/0/all/0/1\">Bianca-Marina Cod&#x103;u</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an Auxiliary Space. (arXiv:2207.07351v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07351","description":"<p>Diverse human motion prediction aims at predicting multiple possible future\npose sequences from a sequence of observed poses. Previous approaches usually\nemploy deep generative networks to model the conditional distribution of data,\nand then randomly sample outcomes from the distribution. While different\nresults can be obtained, they are usually the most likely ones which are not\ndiverse enough. Recent work explicitly learns multiple modes of the conditional\ndistribution via a deterministic network, which however can only cover a fixed\nnumber of modes within a limited range. In this paper, we propose a novel\nsampling strategy for sampling very diverse results from an imbalanced\nmultimodal distribution learned by a deep generative model. Our method works by\ngenerating an auxiliary space and smartly making randomly sampling from the\nauxiliary space equivalent to the diverse sampling from the target\ndistribution. We propose a simple yet effective network architecture that\nimplements this novel sampling strategy, which incorporates a Gumbel-Softmax\ncoefficient matrix sampling method and an aggressive diversity promoting hinge\nloss function. Extensive experiments demonstrate that our method significantly\nimproves both the diversity and accuracy of the samplings compared with\nprevious state-of-the-art sampling approaches. Code and pre-trained models are\navailable at https://github.com/Droliven/diverse_sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Lingwei Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Registration based Few-Shot Anomaly Detection. (arXiv:2207.07361v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07361","description":"<p>This paper considers few-shot anomaly detection (FSAD), a practical yet\nunder-studied setting for anomaly detection (AD), where only a limited number\nof normal images are provided for each category at training. So far, existing\nFSAD studies follow the one-model-per-category learning paradigm used for\nstandard AD, and the inter-category commonality has not been explored. Inspired\nby how humans detect anomalies, i.e., comparing an image in question to normal\nimages, we here leverage registration, an image alignment task that is\ninherently generalizable across categories, as the proxy task, to train a\ncategory-agnostic anomaly detection model. During testing, the anomalies are\nidentified by comparing the registered features of the test image and its\ncorresponding support (normal) images. As far as we know, this is the first\nFSAD method that trains a single generalizable model and requires no\nre-training or parameter fine-tuning for new categories. Experimental results\nhave shown that the proposed method outperforms the state-of-the-art FSAD\nmethods by 3%-8% in AUC on the MVTec and MPDD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Haoyan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aofan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1\">Michael Spratling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07368","description":"<p>Low-dose computed tomography (CT) denoising algorithms aim to enable reduced\npatient dose in routine CT acquisitions while maintaining high image quality.\nRecently, deep learning~(DL)-based methods were introduced, outperforming\nconventional denoising algorithms on this task due to their high model\ncapacity. However, for the transition of DL-based denoising to clinical\npractice, these data-driven approaches must generalize robustly beyond the seen\ntraining data. We, therefore, propose a hybrid denoising approach consisting of\na set of trainable joint bilateral filters (JBFs) combined with a convolutional\nDL-based denoising network to predict the guidance image. Our proposed\ndenoising pipeline combines the high model capacity enabled by DL-based feature\nextraction with the reliability of the conventional JBF. The pipeline's ability\nto generalize is demonstrated by training on abdomen CT scans without metal\nimplants and testing on abdomen scans with metal implants as well as on head CT\ndata. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in\nour pipeline, the denoising performance is improved by $10\\,\\%$/$82\\,\\%$ (RMSE)\nand $3\\,\\%$/$81\\,\\%$ (PSNR) in regions containing metal and by $6\\,\\%$/$78\\,\\%$\n(RMSE) and $2\\,\\%$/$4\\,\\%$ (PSNR) on head CT data, compared to the respective\nvanilla model. Concluding, the proposed trainable JBFs limit the error bound of\ndeep neural networks to facilitate the applicability of DL-based denoisers in\nlow-dose CT pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1\">Fabian Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1\">Mareike Thies</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1\">Mingxuan Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patwari_M/0/1/0/all/0/1\">Mayank Patwari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ploner_S/0/1/0/all/0/1\">Stefan Ploner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maul_N/0/1/0/all/0/1\">Noah Maul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfaff_L/0/1/0/all/0/1\">Laura Pfaff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation. (arXiv:2207.07370v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07370","description":"<p>Brain tumor segmentation (BTS) in magnetic resonance image (MRI) is crucial\nfor brain tumor diagnosis, cancer management and research purposes. With the\ngreat success of the ten-year BraTS challenges as well as the advances of CNN\nand Transformer algorithms, a lot of outstanding BTS models have been proposed\nto tackle the difficulties of BTS in different technical aspects. However,\nexisting studies hardly consider how to fuse the multi-modality images in a\nreasonable manner. In this paper, we leverage the clinical knowledge of how\nradiologists diagnose brain tumors from multiple MRI modalities and propose a\nclinical knowledge-driven brain tumor segmentation model, called CKD-TransBTS.\nInstead of directly concatenating all the modalities, we re-organize the input\nmodalities by separating them into two groups according to the imaging\nprinciple of MRI. A dual-branch hybrid encoder with the proposed\nmodality-correlated cross-attention block (MCCA) is designed to extract the\nmulti-modality image features. The proposed model inherits the strengths from\nboth Transformer and CNN with the local feature representation ability for\nprecise lesion boundaries and long-range feature extraction for 3D volumetric\nimages. To bridge the gap between Transformer and CNN features, we propose a\nTrans&amp;CNN Feature Calibration block (TCFC) in the decoder. We compare the\nproposed model with five CNN-based models and six transformer-based models on\nthe BraTS 2021 challenge dataset. Extensive experiments demonstrate that the\nproposed model achieves state-of-the-art brain tumor segmentation performance\ncompared with all the competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jianwei Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1\">Bingjiang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1\">Biao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Instances as 1D Kernels. (arXiv:2207.07372v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07372","description":"<p>We introduce a 3D instance representation, termed instance kernels, where\ninstances are represented by one-dimensional vectors that encode the semantic,\npositional, and shape information of 3D instances. We show that instance\nkernels enable easy mask inference by simply scanning kernels over the entire\nscenes, avoiding the heavy reliance on proposals or heuristic clustering\nalgorithms in standard 3D instance segmentation pipelines. The idea of instance\nkernel is inspired by recent success of dynamic convolutions in 2D/3D instance\nsegmentation. However, we find it non-trivial to represent 3D instances due to\nthe disordered and unstructured nature of point cloud data, e.g., poor instance\nlocalization can significantly degrade instance representation. To remedy this,\nwe construct a novel 3D instance encoding paradigm. First, potential instance\ncentroids are localized as candidates. Then, a candidate merging scheme is\ndevised to simultaneously aggregate duplicated candidates and collect context\naround the merged centroids to form the instance kernels. Once instance kernels\nare available, instance masks can be reconstructed via dynamic convolutions\nwhose weights are conditioned on instance kernels. The whole pipeline is\ninstantiated with a dynamic kernel network (DKNet). Results show that DKNet\noutperforms the state of the arts on both ScanNetV2 and S3DIS datasets with\nbetter instance localization. Code is available:\nhttps://github.com/W1zheng/DKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yizheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shuaiyuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Weicai Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion. (arXiv:2207.07381v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07381","description":"<p>Multi-person motion capture can be challenging due to ambiguities caused by\nsevere occlusion, fast body movement, and complex interactions. Existing\nframeworks build on 2D pose estimations and triangulate to 3D coordinates via\nreasoning the appearance, trajectory, and geometric consistencies among\nmulti-camera observations. However, 2D joint detection is usually incomplete\nand with wrong identity assignments due to limited observation angle, which\nleads to noisy 3D triangulation results. To overcome this issue, we propose to\nexplore the short-range autoregressive characteristics of skeletal motion using\ntransformer. First, we propose an adaptive, identity-aware triangulation module\nto reconstruct 3D joints and identify the missing joints for each identity. To\ngenerate complete 3D skeletal motion, we then propose a Dual-Masked\nAuto-Encoder (D-MAE) which encodes the joint status with both\nskeletal-structural and temporal position encoding for trajectory completion.\nD-MAE's flexible masking and encoding mechanism enable arbitrary skeleton\ndefinitions to be conveniently deployed under the same framework. In order to\ndemonstrate the proposed model's capability in dealing with severe data loss\nscenarios, we contribute a high-accuracy and challenging motion capture dataset\nof multi-person interactions with severe occlusion. Evaluations on both\nbenchmark and our new dataset demonstrate the efficiency of our proposed model,\nas well as its advantage against the other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junkun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes. (arXiv:2207.07418v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07418","description":"<p>The semantic segmentation of surgical scenes is a prerequisite for task\nautomation in robot assisted interventions. We propose LapSeg3D, a novel\nDNN-based approach for the voxel-wise annotation of point clouds representing\nsurgical scenes. As the manual annotation of training data is highly time\nconsuming, we introduce a semi-autonomous clustering-based pipeline for the\nannotation of the gallbladder, which is used to generate segmented labels for\nthe DNN. When evaluated against manually annotated data, LapSeg3D achieves an\nF1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo\nporcine livers. We show LapSeg3D to generalize accurately across different\ngallbladders and datasets recorded with different RGB-D camera systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alt_B/0/1/0/all/0/1\">Benjamin Alt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_C/0/1/0/all/0/1\">Christian Kunz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katic_D/0/1/0/all/0/1\">Darko Katic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Younis_R/0/1/0/all/0/1\">Rayan Younis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakel_R/0/1/0/all/0/1\">Rainer J&#xe4;kel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat Peter M&#xfc;ller-Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathis_Ullrich_F/0/1/0/all/0/1\">Franziska Mathis-Ullrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Object Tracking and Segmentation via Neural Message Passing. (arXiv:2207.07454v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07454","description":"<p>Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and\nMultiple Object Tracking and Segmentation (MOTS) within the\ntracking-by-detection paradigm. However, they also introduce a major challenge\nfor learning methods, as defining a model that can operate on such structured\ndomain is not trivial. In this work, we exploit the classical network flow\nformulation of MOT to define a fully differentiable framework based on Message\nPassing Networks (MPNs). By operating directly on the graph domain, our method\ncan reason globally over an entire set of detections and exploit contextual\nfeatures. It then jointly predicts both final solutions for the data\nassociation problem and segmentation masks for all objects in the scene while\nexploiting synergies between the two tasks. We achieve state-of-the-art results\nfor both tracking and segmentation in several publicly available datasets. Our\ncode is available at github.com/ocetintas/MPNTrackSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1\">Guillem Braso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetintas_O/0/1/0/all/0/1\">Orcun Cetintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSolar tracker: towards unsupervised assessment with open-source data of the accuracy of deep learning-based distributed PV mapping. (arXiv:2207.07466v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07466","description":"<p>Photovoltaic (PV) energy is key to mitigating the current energy crisis.\nHowever, distributed PV generation, which amounts to half of the PV energy\ngeneration, makes it increasingly difficult for transmission system operators\n(TSOs) to balance the load and supply and avoid grid congestions. Indeed, in\nthe absence of measurements, estimating the distributed PV generation is tough.\nIn recent years, many remote sensing-based approaches have been proposed to map\ndistributed PV installations. However, to be applicable in industrial settings,\none needs to assess the accuracy of the mapping over the whole deployment area.\nWe build on existing work to propose an automated PV registry pipeline. This\npipeline automatically generates a dataset recording all distributed PV\ninstallations' location, area, installed capacity, and tilt angle. It only\nrequires aerial orthoimagery and topological data, both of which are freely\naccessible online. In order to assess the accuracy of the registry, we propose\nan unsupervised method based on the {\\it Registre national d'installation}\n(RNI), that centralizes all individual PV systems aggregated at communal level,\nenabling practitioners to assess the accuracy of the registry and eventually\nremove outliers. We deploy our model on 9 French {\\it d\\'epartements} covering\nmore than 50 000 square kilometers, providing the largest mapping of\ndistributed PV panels with this level of detail to date. We then demonstrate\nhow practitioners can use our unsupervised accuracy assessment method to assess\nthe accuracy of the outputs. In particular, we show how it can easily identify\noutliers in the detections. Overall, our approach paves the way for a safer\nintegration of deep learning-based pipelines for remote PV mapping. Code is\navailable at {\\tt https://github.com/gabrielkasmi/dsfrance}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1\">Gabriel Kasmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1\">Laurent Dubus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1\">Philippe Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saint_Drenan_Y/0/1/0/all/0/1\">Yves-Marie Saint-Drenan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USegScene: Unsupervised Learning of Depth, Optical Flow and Ego-Motion with Semantic Guidance and Coupled Networks. (arXiv:2207.07469v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07469","description":"<p>In this paper we propose USegScene, a framework for semantically guided\nunsupervised learning of depth, optical flow and ego-motion estimation for\nstereo camera images using convolutional neural networks. Our framework\nleverages semantic information for improved regularization of depth and optical\nflow maps, multimodal fusion and occlusion filling considering dynamic rigid\nobject motions as independent SE(3) transformations. Furthermore, complementary\nto pure photo-metric matching, we propose matching of semantic features,\npixel-wise classes and object instance borders between the consecutive images.\nIn contrast to previous methods, we propose a network architecture that jointly\npredicts all outputs using shared encoders and allows passing information\nacross the task-domains, e.g., the prediction of optical flow can benefit from\nthe prediction of the depth. Furthermore, we explicitly learn the depth and\noptical flow occlusion maps inside the network, which are leveraged in order to\nimprove the predictions in therespective regions. We present results on the\npopular KITTI dataset and show that our approach outperforms other methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vertens_J/0/1/0/all/0/1\">Johan Vertens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RESECT-SEG: Open access annotations of intra-operative brain tumor ultrasound images. (arXiv:2207.07494v1 [physics.med-ph])","link":"http://arxiv.org/abs/2207.07494","description":"<p>Purpose: Registration and segmentation of magnetic resonance (MR) and\nultrasound (US) images play an essential role in surgical planning and\nresection of brain tumors. However, validating these techniques is challenging\ndue to the scarcity of publicly accessible sources with high-quality ground\ntruth information. To this end, we propose a unique annotation dataset of tumor\ntissues and resection cavities from the previously published RESECT dataset\n(Xiao et al. 2017) to encourage a more rigorous assessments of image processing\ntechniques. Acquisition and validation methods: The RESECT database consists of\nMR and intraoperative US (iUS) images of 23 patients who underwent resection\nsurgeries. The proposed dataset contains tumor tissues and resection cavity\nannotations of the iUS images. The quality of annotations were validated by two\nhighly experienced neurosurgeons through several assessment criteria. Data\nformat and availability: Annotations of tumor tissues and resection cavities\nare provided in 3D NIFTI formats. Both sets of annotations are accessible\nonline in the \\url{https://osf.io/6y4db}. Discussion and potential\napplications: The proposed database includes tumor tissue and resection cavity\nannotations from real-world clinical ultrasound brain images to evaluate\nsegmentation and registration methods. These labels could also be used to train\ndeep learning approaches. Eventually, this dataset should further improve the\nquality of image guidance in neurosurgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Behboodi_B/0/1/0/all/0/1\">Bahareh Behboodi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Carton_F/0/1/0/all/0/1\">Francois-Xavier Carton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chabanas_M/0/1/0/all/0/1\">Matthieu Chabanas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ribaupierre_S/0/1/0/all/0/1\">Sandrine De Ribaupierre</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Solheim_O/0/1/0/all/0/1\">Ole Solheim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Munkvold_B/0/1/0/all/0/1\">Bodil K. R. Munkvold</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xiao_Y/0/1/0/all/0/1\">Yiming Xiao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reinertsen_I/0/1/0/all/0/1\">Ingerid Reinertsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07506","description":"<p>Detecting out-of-distribution (OOD) data is a task that is receiving an\nincreasing amount of research attention in the domain of deep learning for\ncomputer vision. However, the performance of detection methods is generally\nevaluated on the task in isolation, rather than also considering potential\ndownstream tasks in tandem. In this work, we examine selective classification\nin the presence of OOD data (SCOD). That is to say, the motivation for\ndetecting OOD samples is to reject them so their impact on the quality of\npredictions is reduced. We show under this task specification, that existing\npost-hoc methods perform quite differently compared to when evaluated only on\nOOD detection. This is because it is no longer an issue to conflate\nin-distribution (ID) data with OOD data if the ID data is going to be\nmisclassified. However, the conflation within ID data of correct and incorrect\npredictions becomes undesirable. We also propose a novel method for SCOD,\nSoftmax Information Retaining Combination (SIRC), that augments softmax-based\nconfidence scores with feature-agnostic information such that their ability to\nidentify OOD samples is improved without sacrificing separation between correct\nand incorrect ID predictions. Experiments on a wide variety of ImageNet-scale\ndatasets and convolutional neural network architectures show that SIRC is able\nto consistently match or outperform the baseline for SCOD, whilst existing OOD\ndetection methods fail to do so.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Guoxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1\">Christos-Savvas Bouganis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection. (arXiv:2207.07517v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07517","description":"<p>The ability to detect Out-of-Distribution (OOD) data is important in\nsafety-critical applications of deep learning. The aim is to separate\nIn-Distribution (ID) data drawn from the training distribution from OOD data\nusing a measure of uncertainty extracted from a deep neural network. Deep\nEnsembles are a well-established method of improving the quality of uncertainty\nestimates produced by deep neural networks, and have been shown to have\nsuperior OOD detection performance compared to single models. An existing\nintuition in the literature is that the diversity of Deep Ensemble predictions\nindicates distributional shift, and so measures of diversity such as Mutual\nInformation (MI) should be used for OOD detection. We show experimentally that\nthis intuition is not valid on ImageNet-scale OOD detection -- using MI leads\nto 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.\nWe suggest an alternative explanation for Deep Ensembles' better OOD detection\nperformance -- OOD detection is binary classification and we are ensembling\ndiverse classifiers. As such we show that practically, even better OOD\ndetection performance can be achieved for Deep Ensembles by averaging\ntask-specific detection scores such as Energy over the ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Guoxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1\">Christos-Savvas Bouganis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation. (arXiv:2207.07522v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07522","description":"<p>Scene flow estimation, which extracts point-wise motion between scenes, is\nbecoming a crucial task in many computer vision tasks. However, all of the\nexisting estimation methods utilize only the unidirectional features,\nrestricting the accuracy and generality. This paper presents a novel scene flow\nestimation architecture using bidirectional flow embedding layers. The proposed\nbidirectional layer learns features along both forward and backward directions,\nenhancing the estimation performance. In addition, hierarchical feature\nextraction and warping improve the performance and reduce computational\noverhead. Experimental results show that the proposed architecture achieved a\nnew state-of-the-art record by outperforming other approaches with large margin\nin both FlyingThings3D and KITTI benchmarks. Codes are available at\nhttps://github.com/cwc1260/BiFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wencan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jong Hwan Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models. (arXiv:2207.07539v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07539","description":"<p>3D point cloud models are widely applied in safety-critical scenes, which\ndelivers an urgent need to obtain more solid proofs to verify the robustness of\nmodels. Existing verification method for point cloud model is time-expensive\nand computationally unattainable on large networks. Additionally, they cannot\nhandle the complete PointNet model with joint alignment network (JANet) that\ncontains multiplication layers, which effectively boosts the performance of 3D\nmodels. This motivates us to design a more efficient and general framework to\nverify various architectures of point cloud models. The key challenges in\nverifying the large-scale complete PointNet models are addressed as dealing\nwith the cross-non-linearity operations in the multiplication layers and the\nhigh computational complexity of high-dimensional point cloud inputs and added\nlayers. Thus, we propose an efficient verification framework, 3DVerifier, to\ntackle both challenges by adopting a linear relaxation function to bound the\nmultiplication layer and combining forward and backward propagation to compute\nthe certified bounds of the outputs of the point cloud models. Our\ncomprehensive experiments demonstrate that 3DVerifier outperforms existing\nverification algorithms for 3D models in terms of both efficiency and accuracy.\nNotably, our approach achieves an orders-of-magnitude improvement in\nverification efficiency for the large network, and the obtained certified\nbounds are also significantly tighter than the state-of-the-art verifiers. We\nrelease our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use\nby the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_R/0/1/0/all/0/1\">Ronghui Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1\">Wenjie Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcolino_L/0/1/0/all/0/1\">Leandro S. Marcolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Q/0/1/0/all/0/1\">Qiang Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN. (arXiv:2207.07553v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07553","description":"<p>Deep learning models used in medical image analysis are prone to raising\nreliability concerns due to their black-box nature. To shed light on these\nblack-box models, previous works predominantly focus on identifying the\ncontribution of input features to the diagnosis, i.e., feature attribution. In\nthis work, we explore counterfactual explanations to identify what patterns the\nmodels rely on for diagnosis. Specifically, we investigate the effect of\nchanging features within chest X-rays on the classifier's output to understand\nits decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to\ncreate counterfactual explanations for chest X-rays by manipulating specific\nlatent directions in their latent space. In addition, we propose EigenFind to\nsignificantly reduce the computation time of generated explanations. We\nclinically evaluate the relevancy of our counterfactual explanations with the\nhelp of radiologists. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Atad_M/0/1/0/all/0/1\">Matan Atad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dmytrenko_V/0/1/0/all/0/1\">Vitalii Dmytrenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan Kirschke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1\">Bene Wiestler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mobile Keystroke Biometrics Using Transformers. (arXiv:2207.07596v1 [cs.CR])","link":"http://arxiv.org/abs/2207.07596","description":"<p>Behavioural biometrics have proven to be effective against identity theft as\nwell as be considered user-friendly authentication methods. One of the most\npopular traits in the literature is keystroke dynamics due to the large\ndeployment of computers and mobile devices in our society. This paper focuses\non improving keystroke biometric systems on the free-text scenario. This\nscenario is characterised as very challenging due to the uncontrolled text\nconditions, the influential of the user's emotional and physical state, and the\nin-use application. To overcome these drawbacks, methods based on deep learning\nsuch as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks\n(RNNs) have been proposed in the literature, outperforming traditional machine\nlearning methods. However, these architectures still have aspects that need to\nbe reviewed and improved. To the best of our knowledge, this is the first study\nthat proposes keystroke biometric systems based on Transformers. The proposed\nTransformer architecture has achieved Equal Error Rate (EER) values of 3.84% in\nthe popular Aalto mobile keystroke database using only 5 enrolment sessions,\noutperforming in large margin other state-of-the-art approaches in the\nliterature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1\">Giuseppe Stragapede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1\">Paula Delgado-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guest_R/0/1/0/all/0/1\">Richard Guest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. (arXiv:2207.07601v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07601","description":"<p>Many existing autonomous driving paradigms involve a multi-stage discrete\npipeline of tasks. To better predict the control signals and enhance user\nsafety, an end-to-end approach that benefits from joint spatial-temporal\nfeature learning is desirable. While there are some pioneering works on\nLiDAR-based input or implicit design, in this paper we formulate the problem in\nan interpretable vision-based setting. In particular, we propose a\nspatial-temporal feature learning scheme towards a set of more representative\nfeatures for perception, prediction and planning tasks simultaneously, which is\ncalled ST-P3. Specifically, an egocentric-aligned accumulation technique is\nproposed to preserve geometry information in 3D space before the bird's eye\nview transformation for perception; a dual pathway modeling is devised to take\npast motion variations into account for future prediction; a temporal-based\nrefinement unit is introduced to compensate for recognizing vision-based\nelements for planning. To the best of our knowledge, we are the first to\nsystematically investigate each part of an interpretable end-to-end\nvision-based autonomous driving system. We benchmark our approach against\nprevious state-of-the-arts on both open-loop nuScenes dataset as well as\nclosed-loop CARLA simulation. The results show the effectiveness of our method.\nSource code, model and protocol details are made publicly available at\nhttps://github.com/OpenPerceptionX/ST-P3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengchao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Penghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image and Texture Independent Deep Learning Noise Estimation using Multiple Frames. (arXiv:2207.07604v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07604","description":"<p>In this study, a novel multiple-frame based image and texture independent\nconvolutional Neural Network (CNN) noise estimator is introduced. The estimator\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirmizitas_H/0/1/0/all/0/1\">Hikmet Kirmizitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besli_N/0/1/0/all/0/1\">Nurettin Besli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving. (arXiv:2207.07609v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07609","description":"<p>Vehicle-to-Everything (V2X) network has enabled collaborative perception in\nautonomous driving, which is a promising solution to the fundamental defect of\nstand-alone intelligence including blind zones and long-range perception.\nHowever, the lack of datasets has severely blocked the development of\ncollaborative perception algorithms. In this work, we release DOLPHINS: Dataset\nfor cOllaborative Perception enabled Harmonious and INterconnected\nSelf-driving, as a new simulated large-scale various-scenario multi-view\nmulti-modality autonomous driving dataset, which provides a ground-breaking\nbenchmark platform for interconnected autonomous driving. DOLPHINS outperforms\ncurrent datasets in six dimensions: temporally-aligned images and point clouds\nfrom both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle\n(V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6\ntypical scenarios with dynamic weather conditions make the most various\ninterconnected autonomous driving dataset; meticulously selected viewpoints\nproviding full coverage of the key areas and every object; 42376 frames and\n292549 objects, as well as the corresponding 3D annotations, geo-positions, and\ncalibrations, compose the largest dataset for collaborative perception; Full-HD\nimages and 64-line LiDARs construct high-resolution data with sufficient\ndetails; well-organized APIs and open-source codes ensure the extensibility of\nDOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and\nmulti-view collaborative perception tasks on DOLPHINS. The experiment results\nshow that the raw-level fusion scheme through V2X communication can help to\nimprove the precision as well as to reduce the necessity of expensive LiDAR\nequipment on vehicles when RSUs exist, which may accelerate the popularity of\ninterconnected self-driving vehicles. DOLPHINS is now available on\nhttps://dolphins-dataset.net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Ruiqing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yukuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhisheng Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Prediction as an Effective Pretraining Strategy. (arXiv:2207.07611v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07611","description":"<p>Transformers have gained increasing popularity in a wide range of\napplications, including Natural Language Processing (NLP), Computer Vision and\nSpeech Recognition, because of their powerful representational capacity.\nHowever, harnessing this representational capacity effectively requires a large\namount of data, strong regularization, or both, to mitigate overfitting.\nRecently, the power of the Transformer has been unlocked by self-supervised\npretraining strategies based on masked autoencoders which rely on\nreconstructing masked inputs, directly, or contrastively from unmasked content.\nThis pretraining strategy which has been used in BERT models in NLP, Wav2Vec\nmodels in Speech and, recently, in MAE models in Vision, forces the model to\nlearn about relationships between the content in different parts of the input\nusing autoencoding related objectives. In this paper, we propose a novel, but\nsurprisingly simple alternative to content reconstruction~-- that of predicting\nlocations from content, without providing positional information for it. Doing\nso requires the Transformer to understand the positional relationships between\ndifferent parts of the input, from their content alone. This amounts to an\nefficient implementation where the pretext task is a classification problem\namong all possible positions for each input token. We experiment on both Vision\nand Speech benchmarks, where our approach brings improvements over strong\nsupervised training baselines and is comparable to modern\nunsupervised/self-supervised pretraining methods. Our method also enables\nTransformers trained without position embeddings to outperform ones trained\nwith full position information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1\">Navdeep Jaitly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1\">Dan Busbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Joseph Yitan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Non-Anatomical Graph Structure for isolated hand gesture separation in continuous gesture sequences. (arXiv:2207.07619v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07619","description":"<p>Continuous Hand Gesture Recognition (CHGR) has been extensively studied by\nresearchers in the last few decades. Recently, one model has been presented to\ndeal with the challenge of the boundary detection of isolated gestures in a\ncontinuous gesture video [17]. To enhance the model performance and also\nreplace the handcrafted feature extractor in the presented model in [17], we\npropose a GCN model and combine it with the stacked Bi-LSTM and Attention\nmodules to push the temporal information in the video stream. Considering the\nbreakthroughs of GCN models for skeleton modality, we propose a two-layer GCN\nmodel to empower the 3D hand skeleton features. Finally, the class\nprobabilities of each isolated gesture are fed to the post-processing module,\nborrowed from [17]. Furthermore, we replace the anatomical graph structure with\nsome non-anatomical graph structures. Due to the lack of a large dataset,\nincluding both the continuous gesture sequences and the corresponding isolated\ngestures, three public datasets in Dynamic Hand Gesture Recognition (DHGR),\nRKS-PERSIANSIGN, and ASLVID, are used for evaluation. Experimental results show\nthe superiority of the proposed model in dealing with isolated gesture\nboundaries detection in continuous gesture sequences\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MegaPortraits: One-shot Megapixel Neural Head Avatars. (arXiv:2207.07621v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07621","description":"<p>In this work, we advance the neural head avatar technology to the megapixel\nresolution while focusing on the particularly challenging task of cross-driving\nsynthesis, i.e., when the appearance of the driving image is substantially\ndifferent from the animated source image. We propose a set of new neural\narchitectures and training methods that can leverage both medium-resolution\nvideo data and high-resolution image data to achieve the desired levels of\nrendered image quality and generalization to novel views and motion. We\ndemonstrate that suggested architectures and methods produce convincing\nhigh-resolution neural avatars, outperforming the competitors in the\ncross-driving scenario. Lastly, we show how a trained high-resolution neural\navatar model can be distilled into a lightweight student model which runs in\nreal-time and locks the identities of neural avatars to several dozens of\npre-defined source images. Real-time operation and identity lock are essential\nfor many practical applications head avatar systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drobyshev_N/0/1/0/all/0/1\">Nikita Drobyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelishev_J/0/1/0/all/0/1\">Jenya Chelishev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khakhulin_T/0/1/0/all/0/1\">Taras Khakhulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivakhnenko_A/0/1/0/all/0/1\">Aleksei Ivakhnenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_E/0/1/0/all/0/1\">Egor Zakharov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain MRI study for glioma segmentation using convolutional neural networks and original post-processing techniques with low computational demand. (arXiv:2207.07622v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07622","description":"<p>Gliomas are brain tumors composed of different highly heterogeneous\nhistological subregions. Image analysis techniques to identify relevant tumor\nsubstructures have high potential for improving patient diagnosis, treatment\nand prognosis. However, due to the high heterogeneity of gliomas, the\nsegmentation task is currently a major challenge in the field of medical image\nanalysis. In the present work, the database of the Brain Tumor Segmentation\n(BraTS) Challenge 2018, composed of multimodal MRI scans of gliomas, was\nstudied. A segmentation methodology based on the design and application of\nconvolutional neural networks (CNNs) combined with original post-processing\ntechniques with low computational demand was proposed. The post-processing\ntechniques were the main responsible for the results obtained in the\nsegmentations. The segmented regions were the whole tumor, the tumor core, and\nthe enhancing tumor core, obtaining averaged Dice coefficients equal to 0.8934,\n0.8376, and 0.8113, respectively. These results reached the state of the art in\nglioma segmentation determined by the winners of the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hernandez_Lopez_J/0/1/0/all/0/1\">Jos&#xe9; Gerardo Su&#xe1;rez-Garc&#xed;a Javier Miguel Hern&#xe1;ndez-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_Barbosa_E/0/1/0/all/0/1\">Eduardo Moreno-Barbosa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Celis_Alonso_B/0/1/0/all/0/1\">Benito de Celis-Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences. (arXiv:2207.07629v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07629","description":"<p>Supervised and unsupervised deep trackers that rely on deep learning\ntechnologies are popular in recent years. Yet, they demand high computational\ncomplexity and a high memory cost. A green unsupervised single-object tracker,\ncalled GUSOT, that aims at object tracking for long videos under a\nresource-constrained environment is proposed in this work. Built upon a\nbaseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT\ncontains two additional new modules: 1) lost object recovery, and 2)\ncolor-saliency-based shape proposal. They help resolve the tracking loss\nproblem and offer a more flexible object proposal, respectively. Thus, they\nenable GUSOT to achieve higher tracking accuracy in the long run. We conduct\nexperiments on the large-scale dataset LaSOT with long video sequences, and\nshow that GUSOT offers a lightweight high-performance tracking solution that\nfinds applications in mobile and edge computing platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning. (arXiv:2207.07635v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07635","description":"<p>The development of CLIP [Radford et al., 2021] has sparked a debate on\nwhether language supervision can result in vision models with more transferable\nrepresentations than traditional image-only methods. Our work studies this\nquestion through a carefully controlled comparison of two approaches in terms\nof their ability to learn representations that generalize to downstream\nclassification tasks. We find that when the pre-training dataset meets certain\ncriteria -- it is sufficiently large and contains descriptive captions with low\nvariability -- image-only methods do not match CLIP's transfer performance,\neven when they are trained with more image data. However, contrary to what one\nmight expect, there are practical settings in which these criteria are not met,\nwherein added supervision through captions is actually detrimental. Motivated\nby our findings, we devise simple prescriptions to enable CLIP to better\nleverage the language information present in existing pre-training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1\">Shibani Santurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1\">Rohan Taori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models. (arXiv:2207.07646v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07646","description":"<p>Utilizing vision and language models (VLMs) pre-trained on large-scale\nimage-text pairs is becoming a promising paradigm for open-vocabulary visual\nrecognition. In this work, we extend this paradigm by leveraging motion and\naudio that naturally exist in video. We present \\textbf{MOV}, a simple yet\neffective method for \\textbf{M}ultimodal \\textbf{O}pen-\\textbf{V}ocabulary\nvideo classification. In MOV, we directly use the vision encoder from\npre-trained VLMs with minimal modifications to encode video, optical flow and\naudio spectrogram. We design a cross-modal fusion mechanism to aggregate\ncomplimentary multimodal information. Experiments on Kinetics-700 and VGGSound\nshow that introducing flow or audio modality brings large performance gains\nover the pre-trained VLM and existing methods. Specifically, MOV greatly\nimproves the accuracy on base classes, while generalizes better on novel\nclasses. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video\nclassification benchmarks, significantly outperforming both traditional\nzero-shot methods and recent methods based on VLMs. Code and models will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.02658","description":"<p>Limiting failures of machine learning systems is of paramount importance for\nsafety-critical applications. In order to improve the robustness of machine\nlearning systems, Distributionally Robust Optimization (DRO) has been proposed\nas a generalization of Empirical Risk Minimization (ERM). However, its use in\ndeep learning has been severely restricted due to the relative inefficiency of\nthe optimizers available for DRO in comparison to the wide-spread variants of\nStochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with\nhardness weighted sampling, a principled and efficient optimization method for\nDRO in machine learning that is particularly suited in the context of deep\nlearning. Similar to a hard example mining strategy in practice, the proposed\nalgorithm is straightforward to implement and computationally as efficient as\nSGD-based optimizers used for deep learning, requiring minimal overhead\ncomputation. In contrast to typical ad hoc hard mining approaches, we prove the\nconvergence of our DRO algorithm for over-parameterized deep learning networks\nwith ReLU activation and a finite number of layers and parameters. Our\nexperiments on fetal brain 3D MRI segmentation and brain tumor segmentation in\nMRI demonstrate the feasibility and the usefulness of our approach. Using our\nhardness weighted sampling for training a state-of-the-art deep learning\npipeline leads to improved robustness to anatomical variabilities in automatic\nfetal brain 3D MRI segmentation using deep learning and to improved robustness\nto the image protocol variations in brain tumor segmentation. Our code is\navailable at https://github.com/LucasFidon/HardnessWeightedSampler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guffens_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Guffens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elslander_E/0/1/0/all/0/1\">Esther Van Elslander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Ernst Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_M/0/1/0/all/0/1\">Michael Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prayer_D/0/1/0/all/0/1\">Daniela Prayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasprian_G/0/1/0/all/0/1\">Gregor Kasprian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langs_G/0/1/0/all/0/1\">Georg Langs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"clDice -- A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.07311","description":"<p>Accurate segmentation of tubular, network-like structures, such as vessels,\nneurons, or roads, is relevant to many fields of research. For such structures,\nthe topology is their most important characteristic; particularly preserving\nconnectedness: in the case of vascular networks, missing a connected vessel\nentirely alters the blood-flow dynamics. We introduce a novel similarity\nmeasure termed centerlineDice (short clDice), which is calculated on the\nintersection of the segmentation masks and their (morphological) skeleta. We\ntheoretically prove that clDice guarantees topology preservation up to homotopy\nequivalence for binary 2D and 3D segmentation. Extending this, we propose a\ncomputationally efficient, differentiable loss function (soft-clDice) for\ntraining arbitrary neural segmentation networks. We benchmark the soft-clDice\nloss on five public datasets, including vessels, roads and neurons (2D and 3D).\nTraining on soft-clDice leads to segmentation with more accurate connectivity\ninformation, higher graph similarity, and better volumetric scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes C. Paetzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1\">Anjany Sekuboyina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unger_A/0/1/0/all/0/1\">Alexander Unger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhylka_A/0/1/0/all/0/1\">Andrey Zhylka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_U/0/1/0/all/0/1\">Ulrich Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern H. Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ODFNet: Using orientation distribution functions to characterize 3D point clouds. (arXiv:2012.04708v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04708","description":"<p>Learning new representations of 3D point clouds is an active research area in\n3D vision, as the order-invariant point cloud structure still presents\nchallenges to the design of neural network architectures. Recent works explored\nlearning either global or local features or both for point clouds, however none\nof the earlier methods focused on capturing contextual shape information by\nanalysing local orientation distribution of points. In this paper, we leverage\non point orientation distributions around a point in order to obtain an\nexpressive local neighborhood representation for point clouds. We achieve this\nby dividing the spherical neighborhood of a given point into predefined cone\nvolumes, and statistics inside each volume are used as point features. In this\nway, a local patch can be represented by not only the selected point's nearest\nneighbors, but also considering a point density distribution defined along\nmultiple orientations around the point. We are then able to construct an\norientation distribution function (ODF) neural network that involves an\nODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet\nmodel achieves state-of the-art accuracy for object classification on\nModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertan_A/0/1/0/all/0/1\">Alican Mertan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks. (arXiv:2101.07555v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07555","description":"<p>The paper proposes a solution based on Generative Adversarial Network (GAN)\nfor solving jigsaw puzzles. The problem assumes that an image is divided into\nequal square pieces, and asks to recover the image according to information\nprovided by the pieces. Conventional jigsaw puzzle solvers often determine the\nrelationships based on the boundaries of pieces, which ignore the important\nsemantic information. In this paper, we propose JigsawGAN, a GAN-based\nauxiliary learning method for solving jigsaw puzzles with unpaired images (with\nno prior knowledge of the initial images). We design a multi-task pipeline that\nincludes, (1) a classification branch to classify jigsaw permutations, and (2)\na GAN branch to recover features to images in correct orders. The\nclassification branch is constrained by the pseudo-labels generated according\nto the shuffled pieces. The GAN branch concentrates on the image semantic\ninformation, where the generator produces the natural images to fool the\ndiscriminator, while the discriminator distinguishes whether a given image\nbelongs to the synthesized or the real target domain. These two branches are\nconnected by a flow-based warp module that is applied to warp features to\ncorrect the order according to the classification results. The proposed method\ncan solve jigsaw puzzles more efficiently by utilizing both semantic\ninformation and boundary information simultaneously. Qualitative and\nquantitative comparisons against several representative jigsaw puzzle solvers\ndemonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.01850","description":"<p>The paper proposes a method to effectively fuse multi-exposure inputs and\ngenerate high-quality high dynamic range (HDR) images with unpaired datasets.\nDeep learning-based HDR image generation methods rely heavily on paired\ndatasets. The ground truth images play a leading role in generating reasonable\nHDR images. Datasets without ground truth are hard to be applied to train deep\nneural networks. Recently, Generative Adversarial Networks (GAN) have\ndemonstrated their potentials of translating images from source domain X to\ntarget domain Y in the absence of paired examples. In this paper, we propose a\nGAN-based network for solving such problems while generating enjoyable HDR\nresults, named UPHDR-GAN. The proposed method relaxes the constraint of the\npaired dataset and learns the mapping from the LDR domain to the HDR domain.\nAlthough the pair data are missing, UPHDR-GAN can properly handle the ghosting\nartifacts caused by moving objects or misalignments with the help of the\nmodified GAN loss, the improved discriminator network and the useful\ninitialization phase. The proposed method preserves the details of important\nregions and improves the total image perceptual quality. Qualitative and\nquantitative comparisons against the representative methods demonstrate the\nsuperiority of the proposed UPHDR-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ru Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Heng-Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMix: Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13027","description":"<p>Data mixing augmentation have proved to be effective for improving the\ngeneralization ability of deep neural networks. While early methods mix samples\nby hand-crafted policies (e.g., linear interpolation), recent methods utilize\nsaliency information to match the mixed samples and labels via complex offline\noptimization. However, there arises a trade-off between precise mixing policies\nand optimization complexity. To address this challenge, we propose a novel\nautomatic mixup (AutoMix) framework, where the mixup policy is parameterized\nand serves the ultimate classification goal directly. Specifically, AutoMix\nreformulates the mixup classification into two sub-tasks (i.e., mixed sample\ngeneration and mixup classification) with corresponding sub-networks and solves\nthem in a bi-level optimization framework. For the generation, a learnable\nlightweight mixup generator, Mix Block, is designed to generate mixed samples\nby modeling patch-wise relationships under the direct supervision of the\ncorresponding mixed labels. To prevent the degradation and instability of\nbi-level optimization, we further introduce a momentum pipeline to train\nAutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks\nprove the superiority of AutoMix compared with state-of-the-arts in various\nclassification scenarios and downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12636","description":"<p>Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14178","description":"<p>Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EKTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA often requires reading and\nreasoning about rarely seen or completely unseen scene-text content of an\nimage. We address this zero-shot nature of the problem by proposing the\ngeneralized use of external knowledge to augment our understanding of the scene\ntext. We design a framework to extract, validate, and reason with knowledge\nusing a standard multimodal transformer for vision language understanding\ntasks. Through empirical evidence and qualitative results, we demonstrate how\nexternal knowledge can highlight instance-only cues and thus help deal with\ntraining data bias, improve answer entity type correctness, and detect\nmultiword named entities. We generate results comparable to the\nstate-of-the-art on three publicly available datasets, under the constraints of\nsimilar upstream OCR systems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Diverse Feature Priors. (arXiv:2110.08220v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08220","description":"<p>To improve model generalization, model designers often restrict the features\nthat their models use, either implicitly or explicitly. In this work, we\nexplore the design space of leveraging such feature priors by viewing them as\ndistinct perspectives on the data. Specifically, we find that models trained\nwith diverse sets of feature priors have less overlapping failure modes, and\ncan thus be combined more effectively. Moreover, we demonstrate that jointly\ntraining such models on additional (unlabeled) data allows them to correct each\nother's mistakes, which, in turn, leads to better generalization and resilience\nto spurious correlations. Code available at\nhttps://github.com/MadryLab/copriors\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1\">Dimitris Tsipras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Batch Norm Initialization. (arXiv:2110.13989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13989","description":"<p>Batch normalization (BN) is comprised of a normalization component followed\nby an affine transformation and has become essential for training deep neural\nnetworks. Standard initialization of each BN in a network sets the affine\ntransformation scale and shift to 1 and 0, respectively. However, after\ntraining we have observed that these parameters do not alter much from their\ninitialization. Furthermore, we have noticed that the normalization process can\nstill yield overly large values, which is undesirable for training. We revisit\nthe BN formulation and present a new initialization method and update approach\nfor BN to address the aforementioned issues. Experiments are designed to\nemphasize and demonstrate the positive influence of proper BN scale\ninitialization on performance, and use rigorous statistical significance tests\nfor evaluation. The approach can be used with existing implementations at no\nadditional computational cost. Source code is available at\nhttps://github.com/osu-cvl/revisiting-bn-init.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jim Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_L/0/1/0/all/0/1\">Logan Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. (arXiv:2111.15000v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15000","description":"<p>We present a deformable prototypical part network (Deformable ProtoPNet), an\ninterpretable image classifier that integrates the power of deep learning and\nthe interpretability of case-based reasoning. This model classifies input\nimages by comparing them with prototypes learned during training, yielding\nexplanations in the form of \"this looks like that.\" However, while previous\nmethods use spatially rigid prototypes, we address this shortcoming by\nproposing spatially flexible prototypes. Each prototype is made up of several\nprototypical parts that adaptively change their relative spatial positions\ndepending on the input image. Consequently, a Deformable ProtoPNet can\nexplicitly capture pose variations and context, improving both model accuracy\nand the richness of explanations provided. Compared to other case-based\ninterpretable models using prototypes, our approach achieves state-of-the-art\naccuracy and gives an explanation with greater context. The code is available\nat https://github.com/jdonnelly36/Deformable-ProtoPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Donnelly_J/0/1/0/all/0/1\">Jon Donnelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnett_A/0/1/0/all/0/1\">Alina Jade Barnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Visual-Language Models for Efficient Video Understanding. (arXiv:2112.04478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04478","description":"<p>Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chen Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tengda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kunhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images. (arXiv:2112.09131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09131","description":"<p>Existing state-of-the-art methods for Video Object Segmentation (VOS) learn\nlow-level pixel-to-pixel correspondences between frames to propagate object\nmasks across video. This requires a large amount of densely annotated video\ndata, which is costly to annotate, and largely redundant since frames within a\nvideo are highly correlated. In light of this, we propose HODOR: a novel method\nthat tackles VOS by effectively leveraging annotated static images for\nunderstanding object appearance and scene context. We encode object instances\nand scene information from an image frame into robust high-level descriptors\nwhich can then be used to re-segment those objects in different frames. As a\nresult, HODOR achieves state-of-the-art performance on the DAVIS and\nYouTube-VOS benchmarks compared to existing methods trained without video\nannotations. Without any architectural modification, HODOR can also learn from\nvideo context around single annotated video frames by utilizing cyclic\nconsistency, whereas other methods rely on dense, temporally consistent\nannotations. Source code is available at: https://github.com/Ali2500/HODOR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1\">Ali Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1\">Jonathon Luiten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1\">Alexander Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-distribution Detection with Boundary Aware Learning. (arXiv:2112.11648v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11648","description":"<p>There is an increasing need to determine whether inputs are\nout-of-distribution (\\emph{OOD}) for safely deploying machine learning models\nin the open world scenario. Typical neural classifiers are based on the closed\nworld assumption, where the training data and the test data are drawn\n\\emph{i.i.d.} from the same distribution, and as a result, give over-confident\npredictions even faced with \\emph{OOD} inputs. For tackling this problem,\nprevious studies either use real outliers for training or generate synthetic\n\\emph{OOD} data under strong assumptions, which are either costly or\nintractable to generalize. In this paper, we propose boundary aware learning\n(\\textbf{BAL}), a novel framework that can learn the distribution of \\emph{OOD}\nfeatures adaptively. The key idea of BAL is to generate \\emph{OOD} features\nfrom trivial to hard progressively with a generator, meanwhile, a discriminator\nis trained for distinguishing these synthetic \\emph{OOD} features and\nin-distribution (\\emph{ID}) features. Benefiting from the adversarial training\nscheme, the discriminator can well separate \\emph{ID} and \\emph{OOD} features,\nallowing more robust \\emph{OOD} detection. The proposed BAL achieves\n\\emph{state-of-the-art} performance on classification benchmarks, reducing up\nto 13.9\\% FPR95 compared with previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12175","description":"<p>Most action recognition models today are highly parameterized, and evaluated\non datasets with predominantly spatially distinct classes. It has also been\nshown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward\ntexture rather than shape in still image recognition tasks. Taken together,\nthis raises suspicion that large video models partly learn spurious\ncorrelations rather than to track relevant shapes over time to infer\ngeneralizable semantics from their movement. A natural way to avoid parameter\nexplosion when learning visual patterns over time is to make use of recurrence.\nIn this article, we empirically study whether the choice of low-level temporal\nmodeling has consequences for texture bias and cross-domain robustness. In\norder to enable a light-weight and systematic assessment of the ability to\ncapture temporal structure, not revealed from single frames, we provide the\nTemporal Shape (TS) dataset, as well as modified domains of Diving48 allowing\nfor the investigation of texture bias for video models. We find that across a\nvariety of model sizes, convolutional-recurrent and attention-based models show\nbetter out-of-domain robustness on TS than 3D CNNs. In domain shift experiments\non Diving48, our experiments indicate that 3D CNNs and attention-based models\nexhibit more texture bias than convolutional-recurrent models. Moreover,\nqualitative examples suggest that convolutional-recurrent models learn more\ncorrect class attributes from the diving data when compared to the other two\ntypes of models at the same global validation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1\">Ernest Pokropek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.01763","description":"<p>Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets. (arXiv:2202.11312v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.11312","description":"<p>Reliability of SLAM systems is considered one of the critical requirements in\nmodern autonomous systems. This directed the efforts to developing many\nstate-of-the-art systems, creating challenging datasets, and introducing\nrigorous metrics to measure SLAM performance. However, the link between\ndatasets and performance in the robustness/resilience context has rarely been\nexplored. In order to fill this void, characterization of the operating\nconditions of SLAM systems is essential in order to provide an environment for\nquantitative measurement of robustness and resilience. In this paper, we argue\nthat for proper evaluation of SLAM performance, the characterization of SLAM\ndatasets serves as a critical first step. The study starts by reviewing\nprevious efforts for quantitative characterization of SLAM datasets. Then, the\nproblem of perturbation characterization is discussed and the linkage to SLAM\nrobustness/resilience is established. After that, we propose a novel, generic\nand extendable framework for quantitative analysis and comparison of SLAM\ndatasets. Additionally, a description of different characterization parameters\nis provided. Finally, we demonstrate the application of our framework by\npresenting the characterization results of three SLAM datasets: KITTI,\nEuroC-MAV, and TUM-VI highlighting the level of insights achieved by the\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Islam Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanoFlow: Learning 360{\\deg} Optical Flow for Surrounding Temporal Understanding. (arXiv:2202.13388v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13388","description":"<p>Optical flow estimation is a basic task in self-driving and robotics systems,\nwhich enables to temporally interpret traffic scenes. Autonomous vehicles\nclearly benefit from the ultra-wide Field of View (FoV) offered by 360{\\deg}\npanoramic sensors. However, due to the unique imaging process of panoramic\ncameras, models designed for pinhole images do not directly generalize\nsatisfactorily to 360{\\deg} panoramic images. In this paper, we put forward a\nnovel network framework--PanoFlow, to learn optical flow for panoramic images.\nTo overcome the distortions introduced by equirectangular projection in\npanoramic transformation, we design a Flow Distortion Augmentation (FDA)\nmethod, which contains radial flow distortion (FDA-R) or equirectangular flow\ndistortion (FDA-E). We further look into the definition and properties of\ncyclic optical flow for panoramic videos, and hereby propose a Cyclic Flow\nEstimation (CFE) method by leveraging the cyclicity of spherical images to\ninfer 360{\\deg} optical flow and converting large displacement to relatively\nsmall displacement. PanoFlow is applicable to any existing flow estimation\nmethod and benefits from the progress of narrow-FoV flow estimation. In\naddition, we create and release a synthetic panoramic dataset Flow360 based on\nCARLA to facilitate training and quantitative analysis. PanoFlow achieves\nstate-of-the-art performance on the public OmniFlowNet and the established\nFlow360 benchmarks. Our proposed approach reduces the End-Point-Error (EPE) on\nFlow360 by 27.3%. On OmniFlowNet, PanoFlow achieves an EPE of 3.17 pixels, a\n55.5% error reduction from the best published result. We also qualitatively\nvalidate our method via a collection vehicle and a public real-world OmniPhotos\ndataset, indicating strong potential and robustness for real-world navigation\napplications. Code and dataset are publicly available at\nhttps://github.com/MasterHow/PanoFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yaozu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhe Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03041","description":"<p>We present a systematic study on a new task called dichotomous image\nsegmentation (DIS) , which aims to segment highly accurate objects from natural\nimages. To this end, we collected the first large-scale DIS dataset, called\nDIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images\ncovering camouflaged, salient, or meticulous objects in various backgrounds.\nDIS is annotated with extremely fine-grained labels. Besides, we introduce a\nsimple intermediate supervision baseline (IS-Net) using both feature-level and\nmask-level guidance for DIS model training. IS-Net outperforms various\ncutting-edge baselines on the proposed DIS5K, making it a general self-learned\nsupervision network that can facilitate future research in DIS. Further, we\ndesign a new metric called human correction efforts (HCE) which approximates\nthe number of mouse clicking operations required to correct the false positives\nand false negatives. HCE is utilized to measure the gap between models and\nreal-world applications and thus can complement existing metrics. Finally, we\nconduct the largest-scale benchmark, evaluating 16 representative segmentation\nmodels, providing a more insightful discussion regarding object complexities,\nand showing several potential applications (e.g., background removal, art\ndesign, 3D reconstruction). Hoping these efforts can open up promising\ndirections for both academic and industries. Project page:\nhttps://xuebinqin.github.io/dis/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_a/0/1/0/all/0/1\">and Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVFNet: Real-time 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06585","description":"<p>In recent years 3D object detection from LiDAR point clouds has made great\nprogress thanks to the development of deep learning technologies. Although\nvoxel or point based methods are popular in 3D object detection, they usually\ninvolve time-consuming operations such as 3D convolutions on voxels or ball\nquery among points, making the resulting network inappropriate for time\ncritical applications. On the other hand, 2D view-based methods feature high\ncomputing efficiency while usually obtaining inferior performance than the\nvoxel or point based methods. In this work, we present a real-time view-based\nsingle stage 3D object detector, namely CVFNet to fulfill this task. To\nstrengthen the cross-view feature learning under the condition of demanding\nefficiency, our framework extracts the features of different views and fuses\nthem in an efficient progressive way. We first propose a novel Point-Range\nfeature fusion module that deeply integrates point and range view features in\nmultiple stages. Then, a special Slice Pillar is designed to well maintain the\n3D geometry when transforming the obtained deep point-view features into bird's\neye view. To better balance the ratio of samples, a sparse pillar detection\nhead is presented to focus the detection on the nonempty grids. We conduct\nexperiments on the popular KITTI and NuScenes benchmark, and state-of-the-art\nperformances are achieved in terms of both accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STDAN: Deformable Attention Network for Space-Time Video Super-Resolution. (arXiv:2203.06841v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06841","description":"<p>The target of space-time video super-resolution (STVSR) is to increase the\nspatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)\nvideos. Recent approaches based on deep learning have made significant\nimprovements, but most of them only use two adjacent frames, that is,\nshort-term features, to synthesize the missing frame embedding, which cannot\nfully explore the information flow of consecutive input LR frames. In addition,\nexisting STVSR models hardly exploit the temporal contexts explicitly to assist\nhigh-resolution (HR) frame reconstruction. To address these issues, in this\npaper, we propose a deformable attention network called STDAN for STVSR. First,\nwe devise a long-short term feature interpolation (LSTFI) module, which is\ncapable of excavating abundant content from more neighboring input frames for\nthe interpolation process through a bidirectional RNN structure. Second, we put\nforward a spatial-temporal deformable feature aggregation (STDFA) module, in\nwhich spatial and temporal contexts in dynamic video frames are adaptively\ncaptured and aggregated to enhance SR reconstruction. Experimental results on\nseveral datasets demonstrate that our approach outperforms state-of-the-art\nSTVSR methods. The code is available at\nhttps://github.com/littlewhitesea/STDAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. (arXiv:2203.09091v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09091","description":"<p>This paper presents datasets utilised for synthetic near-infrared (NIR) image\ngeneration and bounding-box level fruit detection systems. It is undeniable\nthat high-calibre machine learning frameworks such as Tensorflow or Pytorch,\nand large-scale ImageNet or COCO datasets with the aid of accelerated GPU\nhardware have pushed the limit of machine learning techniques for more than\ndecades. Among these breakthroughs, a high-quality dataset is one of the\nessential building blocks that can lead to success in model generalisation and\nthe deployment of data-driven deep neural networks. In particular, synthetic\ndata generation tasks often require more training samples than other supervised\napproaches. Therefore, in this paper, we share the NIR+RGB datasets that are\nre-processed from two public datasets (i.e., nirscene and SEN12MS) and our\nnovel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and\nqualitatively demonstrate that these NIR+RGB datasets are sufficient to be used\nfor synthetic NIR image generation. We achieved Frechet Inception Distance\n(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper\ndatasets respectively. In addition, we release manual annotations of 11 fruit\nbounding boxes that can be exported as various formats using cloud service.\nFour newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel\nbounding box datasets on top of our previous work presented in the deepFruits\nproject [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The\ntotal number of bounding box instances of the dataset is 162k and it is ready\nto use from cloud service. For the evaluation of the dataset, Yolov5 single\nstage detector is exploited and reported impressive\nmean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope\nthese datasets are useful and serve as a baseline for the future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sa_I/0/1/0/all/0/1\">Inkyu Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">JongYoon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Ho Seok Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_B/0/1/0/all/0/1\">Bruce MacDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13694","description":"<p>We propose an action-conditional human motion generation method using\nvariational implicit neural representations (INR). The variational formalism\nenables action-conditional distributions of INRs, from which one can easily\nsample representations to generate novel human motion sequences. Our method\noffers variable-length sequence generation by construction because a part of\nINR is optimized for a whole sequence of arbitrary length with temporal\nembeddings. In contrast, previous works reported difficulties with modeling\nvariable-length sequences. We confirm that our method with a Transformer\ndecoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC\ndatasets in terms of realism and diversity of generated motions. Surprisingly,\neven our method with an MLP decoder consistently outperforms the\nstate-of-the-art Transformer-based auto-encoder. In particular, we show that\nvariable-length motions generated by our method are better than fixed-length\nmotions generated by the state-of-the-art method in terms of realism and\ndiversity. Code at https://github.com/PACerv/ImplicitMotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cervantes_P/0/1/0/all/0/1\">Pablo Cervantes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekikawa_Y/0/1/0/all/0/1\">Yusuke Sekikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Ikuro Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Koichi Shinoda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00559","description":"<p>We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. By incorporating\nexposure-adaptive novel view synthesis, our method successfully addresses\nphotometric distortions in outdoor environments that existing photometric-based\nmethods fail to handle. With domain-invariant feature matching, our solution\nimproves pose regression accuracy using semi-supervised learning on unlabeled\ndata. In particular, the pipeline consists of two components: Novel View\nSynthesizer and DFNet. The former synthesizes novel views compensating for\nchanges in exposure and the latter regresses camera poses and extracts robust\nfeatures that close the domain gap between real images and synthetic ones.\nFurthermore, we introduce an online synthetic data generation scheme. We show\nthat these approaches effectively enhance camera pose estimation both in indoor\nand outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by\noutperforming existing single-image APR methods by as much as 56%, comparable\nto 3D structure-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08721","description":"<p>Many adaptations of transformers have emerged to address the single-modal\nvision tasks, where self-attention modules are stacked to handle input sources\nlike images. Intuitively, feeding multiple modalities of data to vision\ntransformers could improve the performance, yet the inner-modal attentive\nweights may also be diluted, which could thus undermine the final performance.\nIn this paper, we propose a multimodal token fusion method (TokenFusion),\ntailored for transformer-based vision tasks. To effectively fuse multiple\nmodalities, TokenFusion dynamically detects uninformative tokens and\nsubstitutes these tokens with projected and aggregated inter-modal features.\nResidual positional alignment is also adopted to enable explicit utilization of\nthe inter-modal alignments after fusion. The design of TokenFusion allows the\ntransformer to learn correlations among multimodal features, while the\nsingle-modal transformer architecture remains largely intact. Extensive\nexperiments are conducted on a variety of homogeneous and heterogeneous\nmodalities and demonstrate that TokenFusion surpasses state-of-the-art methods\nin three typical vision tasks: multimodal image-to-image translation, RGB-depth\nsemantic segmentation, and 3D object detection with point cloud and images. Our\ncode is available at https://github.com/yikaiw/TokenFusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09817","description":"<p>Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1\">Shruthi Bannur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Daniel C. Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1\">Anton Schwaighofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1\">Stephanie Hyland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1\">Maria Wetscherek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1\">Aditya Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1\">Javier Alvarez-Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1\">Ozan Oktay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Fine-Grained Structured Sparsity for Image Restoration. (arXiv:2204.12266v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12266","description":"<p>Image restoration tasks have witnessed great performance improvement in\nrecent years by developing large deep models. Despite the outstanding\nperformance, the heavy computation demanded by the deep models has restricted\nthe application of image restoration. To lift the restriction, it is required\nto reduce the size of the networks while maintaining accuracy. Recently, N:M\nstructured pruning has appeared as one of the effective and practical pruning\napproaches for making the model efficient with the accuracy constraint.\nHowever, it fails to account for different computational complexities and\nperformance requirements for different layers of an image restoration network.\nTo further optimize the trade-off between the efficiency and the restoration\naccuracy, we propose a novel pruning method that determines the pruning ratio\nfor N:M structured sparsity at each layer. Extensive experimental results on\nsuper-resolution and deblurring tasks demonstrate the efficacy of our method\nwhich outperforms previous pruning methods significantly. PyTorch\nimplementation for the proposed methods will be publicly available at\nhttps://github.com/JungHunOh/SLS_CVPR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junghun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheeun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes. (arXiv:2205.01931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01931","description":"<p>Histopathological images provide the definitive source of cancer diagnosis,\ncontaining information used by pathologists to identify and subclassify\nmalignant disease, and to guide therapeutic choices. These images contain vast\namounts of information, much of which is currently unavailable to human\ninterpretation. Supervised deep learning approaches have been powerful for\nclassification tasks, but they are inherently limited by the cost and quality\nof annotations. Therefore, we developed Histomorphological Phenotype Learning,\nan unsupervised methodology, which requires no annotations and operates via the\nself-discovery of discriminatory image features in small image tiles. Tiles are\ngrouped into morphologically similar clusters which appear to represent\nrecurrent modes of tumor growth emerging under natural selection. These\nclusters have distinct features which can be identified using orthogonal\nmethods. Applied to lung cancer tissues, we show that they align closely with\npatient outcomes, with histopathologically recognised tumor types and growth\npatterns, and with transcriptomic measures of immunophenotype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1\">Adalberto Claudio Quiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1\">Nicolas Coudray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiriboga_L/0/1/0/all/0/1\">Luis Chiriboga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimkhan_A/0/1/0/all/0/1\">Afreen Karimkhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narula_N/0/1/0/all/0/1\">Navneet Narula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pass_H/0/1/0/all/0/1\">Harvey Pass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_A/0/1/0/all/0/1\">Andre L. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quesne_J/0/1/0/all/0/1\">John Le Quesne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1\">Aristotelis Tsirigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Ke Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Inter-Class Distance for Semantic Segmentation. (arXiv:2205.03650v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03650","description":"<p>Knowledge distillation is widely adopted in semantic segmentation to reduce\nthe computation cost.The previous knowledge distillation methods for semantic\nsegmentation focus on pixel-wise feature alignment and intra-class feature\nvariation distillation, neglecting to transfer the knowledge of the inter-class\ndistance in the feature space, which is important for semantic segmentation. To\naddress this issue, we propose an Inter-class Distance Distillation (IDD)\nmethod to transfer the inter-class distance in the feature space from the\nteacher network to the student network. Furthermore, semantic segmentation is a\nposition-dependent task,thus we exploit a position information distillation\nmodule to help the student network encode more position information. Extensive\nexperiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show\nthat our method is helpful to improve the accuracy of semantic segmentation\nmodels and achieves the state-of-the-art performance. E.g. it boosts the\nbenchmark model(\"PSPNet+ResNet18\") by 7.50% in accuracy on the Cityscapes\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunluan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT. (arXiv:2205.07180v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.07180","description":"<p>This paper investigates self-supervised pre-training for audio-visual speaker\nrepresentation learning where a visual stream showing the speaker's mouth area\nis used alongside speech as inputs. Our study focuses on the Audio-Visual\nHidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose\naudio-visual speech pre-training framework. We conducted extensive experiments\nprobing the effectiveness of pre-training and visual modality. Experimental\nresults suggest that AV-HuBERT generalizes decently to speaker related\ndownstream tasks, improving label efficiency by roughly ten fold for both\naudio-only and audio-visual speaker verification. We also show that\nincorporating visual information, even just the lip area, greatly improves the\nperformance and noise robustness, reducing EER by 38% in the clean condition\nand 75% in noisy conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08209","description":"<p>Deep convolutional neural networks have proven to be remarkably effective in\nsemantic segmentation tasks. Most popular loss functions were introduced\ntargeting improved volumetric scores, such as the Sorensen Dice coefficient. By\ndesign, DSC can tackle class imbalance; however, it does not recognize instance\nimbalance within a class. As a result, a large foreground instance can dominate\nminor instances and still produce a satisfactory Sorensen Dice coefficient.\nNevertheless, missing out on instances will lead to poor detection performance.\nThis represents a critical issue in applications such as disease progression\nmonitoring. For example, it is imperative to locate and surveil small-scale\nlesions in the follow-up of multiple sclerosis patients. We propose a novel\nfamily of loss functions, nicknamed blob loss, primarily aimed at maximizing\ninstance-level detection metrics, such as F1 score and sensitivity. Blob loss\nis designed for semantic segmentation problems in which the instances are the\nconnected components within a class. We extensively evaluate a DSC-based blob\nloss in five complex 3D semantic segmentation tasks featuring pronounced\ninstance heterogeneity in terms of texture and morphology. Compared to soft\nDice loss, we achieve 5 percent improvement for MS lesions, 3 percent\nimprovement for liver tumor, and an average 2 percent improvement for\nMicroscopy segmentation tasks considering F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_I/0/1/0/all/0/1\">Izabela Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maskari_R/0/1/0/all/0/1\">Rami Al-Maskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1\">Harsharan Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loehr_T/0/1/0/all/0/1\">Timo Loehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piraud_M/0/1/0/all/0/1\">Marie Piraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erturk_A/0/1/0/all/0/1\">Ali Erturk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan Kirschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peeken_J/0/1/0/all/0/1\">Jan Peeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_C/0/1/0/all/0/1\">Claus Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.08390","description":"<p>Ultrasonography is an important routine examination for breast cancer\ndiagnosis, due to its non-invasive, radiation-free and low-cost properties.\nHowever, the diagnostic accuracy of breast cancer is still limited due to its\ninherent limitations. It would be a tremendous success if we can precisely\ndiagnose breast cancer by breast ultrasound images (BUS). Many learning-based\ncomputer-aided diagnostic methods have been proposed to achieve breast cancer\ndiagnosis/lesion classification. However, most of them require a pre-define ROI\nand then classify the lesion inside the ROI. Conventional classification\nbackbones, such as VGG16 and ResNet50, can achieve promising classification\nresults with no ROI requirement. But these models lack interpretability, thus\nrestricting their use in clinical practice. In this study, we propose a novel\nROI-free model for breast cancer diagnosis in ultrasound images with\ninterpretable feature representations. We leverage the anatomical prior\nknowledge that malignant and benign tumors have different spatial relationships\nbetween different tissue layers, and propose a HoVer-Transformer to formulate\nthis prior knowledge. The proposed HoVer-Trans block extracts the inter- and\nintra-layer spatial information horizontally and vertically. We conduct and\nrelease an open dataset GDPH&amp;SYSUCC for breast cancer diagnosis in BUS. The\nproposed model is evaluated in three datasets by comparing with four CNN-based\nmodels and two vision transformer models via five-fold cross validation. It\nachieves state-of-the-art classification performance with the best model\ninterpretability. In the meanwhile, our proposed model outperforms two senior\nsonographers on the breast cancer diagnosis when only one BUS image is given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1\">Yuhao Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chunwang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1\">Bingjiang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1\">Yanfen Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaomei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SALAD: Source-free Active Label-Agnostic Domain Adaptation for Classification, Segmentation and Detection. (arXiv:2205.12840v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12840","description":"<p>We present a novel method, SALAD, for the challenging vision task of adapting\na pre-trained \"source\" domain network to a \"target\" domain, with a small budget\nfor annotation in the \"target\" domain and a shift in the label space. Further,\nthe task assumes that the source data is not available for adaptation, due to\nprivacy concerns or otherwise. We postulate that such systems need to jointly\noptimize the dual task of (i) selecting fixed number of samples from the target\ndomain for annotation and (ii) transfer of knowledge from the pre-trained\nnetwork to the target domain. To do this, SALAD consists of a novel Guided\nAttention Transfer Network (GATN) and an active learning function, HAL. The\nGATN enables feature distillation from pre-trained network to the target\nnetwork, complemented with the target samples mined by HAL using\ntransfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it\nis task-agnostic, and can be applied across various visual tasks such as\nclassification, segmentation and detection; (ii) it can handle shifts in output\nlabel space from the pre-trained source network to the target domain; (iii) it\ndoes not require access to source data for adaptation. We conduct extensive\nexperiments across 3 visual tasks, viz. digits classification (MNIST, SVHN,\nVISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document\nlayout detection (PubLayNet to DSSE). We show that our source-free approach,\nSALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over\nprior adaptation methods that assume access to large amounts of annotated\nsource data for adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghuhan_M/0/1/0/all/0/1\">Manoj Ghuhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_T/0/1/0/all/0/1\">Tripti Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping. (arXiv:2206.05963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05963","description":"<p>In this paper, a novel solution is introduced for visual Simultaneous\nLocalization and Mapping (vSLAM) that is built up of Deep Learning components.\nThe proposed architecture is a highly modular framework in which each component\noffers state of the art results in their respective fields of vision-based deep\nlearning solutions. The paper shows that with the synergic integration of these\nindividual building blocks, a functioning and efficient all-through deep neural\n(ATDN) vSLAM system can be created. The Embedding Distance Loss function is\nintroduced and using it the ATDN architecture is trained. The resulting system\nmanaged to achieve 4.4% translation and 0.0176 deg/m rotational error on a\nsubset of the KITTI dataset. The proposed architecture can be used for\nefficient and low-latency autonomous driving (AD) aiding database creation as\nwell as a basis for autonomous vehicle (AV) control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szanto_M/0/1/0/all/0/1\">M&#xe1;ty&#xe1;s Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogar_G/0/1/0/all/0/1\">Gy&#xf6;rgy R. Bog&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajta_L/0/1/0/all/0/1\">L&#xe1;szl&#xf3; Vajta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperRes: Efficient Hypernetwork-Based Continuous Image Restoration. (arXiv:2206.05970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05970","description":"<p>Continuous image restoration attempts to provide a model that can restore\nimages with unseen degradation levels during training at inference time.\nExisting methods are limited in terms of either the accuracy of the\nrestoration, the range of degradation levels they can support, or the size of\nthe model they require. We introduce a novel approach that achieves the optimal\naccuracy of multiple dedicated models for a wide range of degradation levels\nwith the same number of parameters as a single base model. We present a\nhypernetwork that can efficiently generate an image restoration network to best\nadapt to the required level of degradation. Experiments on popular datasets\nshow that our approach outperforms the state-of-the-art for a variety of image\nrestoration tasks, including denoising, DeJPEG, and super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aharon_S/0/1/0/all/0/1\">Shai Aharon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Artzi_G/0/1/0/all/0/1\">Gil Ben-Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied Scene-aware Human Pose Estimation. (arXiv:2206.09106v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09106","description":"<p>We propose embodied scene-aware human pose estimation where we estimate 3D\nposes based on a simulated agent's proprioception and scene awareness, along\nwith external third-person observations. Unlike prior methods that often resort\nto multistage optimization, non-causal inference, and complex contact modeling\nto estimate human pose and human scene interactions, our method is one stage,\ncausal, and recovers global 3D human poses in a simulated environment. Since 2D\nthird-person observations are coupled with the camera pose, we propose to\ndisentangle the camera pose and use a multi-step projection gradient defined in\nthe global coordinate frame as the movement cue for our embodied agent.\nLeveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we\nsimulate our agent in everyday environments (libraries, offices, bedrooms,\netc.) and equip our agent with environmental sensors to intelligently navigate\nand interact with scene geometries. Our method also relies only on 2D keypoints\nand can be trained on synthetic datasets derived from popular human motion\ndatabases. To evaluate, we use the popular H36M and PROX datasets and, for the\nfirst time, achieve a success rate of 96.7% on the challenging PROX dataset\nwithout ever using PROX motion sequences for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2207.03574","description":"<p>Neural networks' lack of robustness against attacks raises concerns in\nsecurity-sensitive settings such as autonomous vehicles. While many\ncountermeasures may look promising, only a few withstand rigorous evaluation.\nDefenses using random transformations (RT) have shown impressive results,\nparticularly BaRT (Raff et al., 2019) on ImageNet. However, this type of\ndefense has not been rigorously evaluated, leaving its robustness properties\npoorly understood. Their stochastic properties make evaluation more challenging\nand render many proposed attacks on deterministic models inapplicable. First,\nwe show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation\nis ineffective and likely overestimates its robustness. We then attempt to\nconstruct the strongest possible RT defense through the informed selection of\ntransformations and Bayesian optimization for tuning their parameters.\nFurthermore, we create the strongest possible attack to evaluate our RT\ndefense. Our new attack vastly outperforms the baseline, reducing the accuracy\nby 83% compared to the 19% reduction by the commonly used EoT attack\n($4.3\\times$ improvement). Our result indicates that the RT defense on the\nImagenette dataset (a ten-class subset of ImageNet) is not robust against\nadversarial examples. Extending the study further, we use our new attack to\nadversarially train RT defense (called AdvRT), resulting in a large robustness\ngain. Code is available at\nhttps://github.com/wagner-group/demystify-random-transform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1\">Chawin Sitawarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_Strieb_Z/0/1/0/all/0/1\">Zachary Golan-Strieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">David Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05249","description":"<p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is\ncritical for always-on action recognition on wearable devices with limited\ncomputing and battery resources. The commonly used fixed sampling strategy is\nnot context-aware and may under-sample the visual content, and thus adversely\nimpacts both computation efficiency and accuracy. Inspired by the concepts of\nfoveal vision and pre-attentive processing from the human visual perception\nmechanism, we introduce a novel adaptive spatiotemporal sampling scheme for\nefficient action recognition. Our system pre-scans the global scene context at\nlow-resolution and decides to skip or request high-resolution features at\nsalient regions for further processing. We validate the system on EPIC-KITCHENS\nand UCF-101 datasets for action recognition, and show that our proposed\napproach can greatly speed up inference with a tolerable loss of accuracy\ncompared with those from state-of-the-art baselines. Source code is available\nin https://github.com/knmac/adaptive_spatiotemporal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mac_K/0/1/0/all/0/1\">Khoi-Nguyen C. Mac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh P. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Shadow Generation Using Pixel Height Maps. (arXiv:2207.05385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05385","description":"<p>Shadows are essential for realistic image compositing. Physics-based shadow\nrendering methods require 3D geometries, which are not always available. Deep\nlearning-based shadow synthesis methods learn a mapping from the light\ninformation to an object's shadow without explicitly modeling the shadow\ngeometry. Still, they lack control and are prone to visual artifacts. We\nintroduce pixel heigh, a novel geometry representation that encodes the\ncorrelations between objects, ground, and camera pose. The pixel height can be\ncalculated from 3D geometries, manually annotated on 2D images, and can also be\npredicted from a single-view RGB image by a supervised approach. It can be used\nto calculate hard shadows in a 2D image based on the projective geometry,\nproviding precise control of the shadows' direction and shape. Furthermore, we\npropose a data-driven soft shadow generator to apply softness to a hard shadow\nbased on a softness input parameter. Qualitative and quantitative evaluations\ndemonstrate that the proposed pixel height significantly improves the quality\nof the shadow generation while allowing for controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Yichen Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztireli_A/0/1/0/all/0/1\">A. Cengiz Oztireli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benes_B/0/1/0/all/0/1\">Bedrich Benes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05518","description":"<p>Multi-object tracking (MOT) requires detecting and associating objects\nthrough frames. Unlike tracking via detected bounding boxes or tracking objects\nas points, we propose tracking objects as pixel-wise distributions. We\ninstantiate this idea on a transformer-based architecture, P3AFormer, with\npixel-wise propagation, prediction, and association. P3AFormer propagates\npixel-wise features guided by flow information to pass messages between frames.\nFurthermore, P3AFormer adopts a meta-architecture to produce multi-scale object\nfeature maps. During inference, a pixel-wise association procedure is proposed\nto recover object connections through frames based on the pixel-wise\nprediction. P3AFormer yields 81.2\\% in terms of MOTA on the MOT17 benchmark --\nthe first among all transformer networks to reach 80\\% MOTA in literature.\nP3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zelin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueqing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boxun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting. (arXiv:2207.06694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06694","description":"<p>End-to-end text spotting has attached great attention recently due to its\nbenefits on global optimization and high maintainability for real applications.\nHowever, the input scale has always been a tough trade-off since recognizing a\nsmall text instance usually requires enlarging the whole image, which brings\nhigh computational costs. In this paper, to address this problem, we propose a\nnovel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting\nframework, which aims to infer images in different small but recognizable\nresolutions and achieve a better balance between accuracy and efficiency.\nConcretely, we adopt a resolution selector to dynamically decide the input\nresolutions for different images, which is constraint by both inference\naccuracy and computational cost. Another sequential knowledge distillation\nstrategy is conducted on the text recognition branch, making the low-res input\nobtains comparable performance to a high-res image. The proposed method can be\noptimized end-to-end and adopted in any current text spotting framework to\nimprove the practicability. Extensive experiments on several text spotting\nbenchmarks show that the proposed method vastly improves the usability of\nlow-res models. The code is available at\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06831","description":"<p>Point-interactive image colorization aims to colorize grayscale images when a\nuser provides the colors for specific locations. It is essential for\npoint-interactive colorization methods to appropriately propagate user-provided\ncolors (i.e., user hints) in the entire image to obtain a reasonably colorized\nimage with minimal user effort. However, existing approaches often produce\npartially colorized results due to the inefficient design of stacking\nconvolutional layers to propagate hints to distant relevant regions. To address\nthis problem, we present iColoriT, a novel point-interactive colorization\nVision Transformer capable of propagating user hints to relevant regions,\nleveraging the global receptive field of Transformers. The self-attention\nmechanism of Transformers enables iColoriT to selectively colorize relevant\nregions with only a few local hints. Our approach colorizes images in real-time\nby utilizing pixel shuffling, an efficient upsampling technique that replaces\nthe decoder architecture. Also, in order to mitigate the artifacts caused by\npixel shuffling with large upsampling ratios, we present the local stabilizing\nlayer. Extensive quantitative and qualitative results demonstrate that our\napproach highly outperforms existing methods for point-interactive\ncolorization, producing accurately colorized images with a user's minimal\neffort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jooyeol Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Omni-Vision Representation through the Lens of Visual Realms. (arXiv:2207.07106v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07106","description":"<p>Though impressive performance has been achieved in specific visual realms\n(e.g. faces, dogs, and places), an omni-vision representation generalizing to\nmany natural visual domains is highly desirable. But, existing benchmarks are\nbiased and inefficient to evaluate the omni-vision representation -- these\nbenchmarks either only include several specific realms, or cover most realms at\nthe expense of subsuming numerous datasets that have extensive realm\noverlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It\nincludes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.\nWithout semantic overlapping, these datasets cover most visual realms\ncomprehensively and meanwhile efficiently. In addition, we propose a new\nsupervised contrastive learning framework, namely Relational Contrastive\nlearning (ReCo), for a better omni-vision representation. Beyond pulling two\ninstances from the same concept closer -- the typical supervised contrastive\nlearning framework -- ReCo also pulls two instances from the same semantic\nrealm closer, encoding the semantic relation between concepts, and facilitating\nomni-vision representation learning. We benchmark ReCo and other advances in\nomni-vision representation studies that are different in architectures (from\nCNNs to transformers) and in learning paradigms (from supervised learning to\nself-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo\nto other supervised contrastive learning methods and reveal multiple practical\nobservations to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}