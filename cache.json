{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Transformer Memory as a Differentiable Search Index. (arXiv:2202.06991v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06991","description":"<p>In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exhaustivity and anti-exhaustivity in the RSA framework: Testing the effect of prior beliefs. (arXiv:2202.07023v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07023","description":"<p>During communication, the interpretation of utterances is sensitive to a\nlistener's probabilistic prior beliefs, something which is captured by one\ncurrently influential model of pragmatics, the Rational Speech Act (RSA)\nframework. In this paper we focus on cases when this sensitivity to priors\nleads to counterintuitive predictions of the framework. Our domain of interest\nis exhaustivity effects, whereby a sentence such as \"Mary came\" is understood\nto mean that only Mary came. We show that in the baseline RSA model, under\ncertain conditions, anti-exhaustive readings are predicted (e.g., \"Mary came\"\nwould be used to convey that both Mary and Peter came). The specific question\nwe ask is the following: should exhaustive interpretations be derived as purely\npragmatic inferences (as in the classical Gricean view, endorsed in the\nbaseline RSA model), or should they rather be generated by an encapsulated\nsemantic mechanism (as argued in some of the recent formal literature)? To\nanswer this question, we provide a detailed theoretical analysis of different\nRSA models and evaluate them against data obtained in a new study which tested\nthe effects of prior beliefs on both production and comprehension, improving on\nprevious empirical work. We found no anti-exhaustivity effects, but observed\nthat message choice is sensitive to priors, as predicted by the RSA framework\noverall. The best models turn out to be those which include an encapsulated\nexhaustivity mechanism (as other studies concluded on the basis of very\ndifferent data). We conclude that, on the one hand, in the division of labor\nbetween semantics and pragmatics, semantics plays a larger role than is often\nthought, but, on the other hand, the tradeoff between informativity and cost\nwhich characterizes all RSA models does play a central role for genuine\npragmatic effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cremers_A/0/1/0/all/0/1\">Alexandre Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan G. Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_B/0/1/0/all/0/1\">Benjamin Spector</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v1 [cs.AI])","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 45% and\n70% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal. (arXiv:2202.07075v1 [cs.SI])","link":"http://arxiv.org/abs/2202.07075","description":"<p>While there is increasing global attention to data privacy, most of their\ncurrent theoretical understanding is based on research conducted in a few\ncountries. Prior work argues that people's cultural backgrounds might shape\ntheir privacy concerns; thus, we could expect people from different world\nregions to conceptualize them in diverse ways. We collected and analyzed a\nlarge-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanish\nand English to start exploring this hypothesis. We employed word embeddings and\nqualitative analysis to identify which information privacy concerns are present\nand characterize language and regional differences in emphasis on these\nconcerns. Our results suggest that related concepts, such as regulations, can\nbe added to current information privacy frameworks. We also observe a greater\nemphasis on data collection in English than in Spanish. Additionally, data from\nNorth America exhibits a narrower focus on awareness compared to other regions\nunder study. Our results call for more diverse sources of data and nuanced\nanalysis of data privacy concerns around the globe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Pizarro_F/0/1/0/all/0/1\">Felipe Gonz&#xe1;lez-Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueroa_A/0/1/0/all/0/1\">Andrea Figueroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Claudia L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragon_C/0/1/0/all/0/1\">Cecilia Aragon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing the ICBe Dataset: Very High Recall and Precision Event Extraction from Narratives about International Crises. (arXiv:2202.07081v1 [stat.AP])","link":"http://arxiv.org/abs/2202.07081","description":"<p>How do international crises unfold? We conceive of international affairs as a\nstrategic chess game between adversaries, necessitating a systematic way to\nmeasure pieces, moves, and gambits accurately and consistently over different\ncontexts and periods. We develop such a measurement strategy with an ontology\nof crisis actions and interactions and apply it to a high-quality corpus of\ncrisis narratives recorded by the International Crisis Behavior (ICB) Project.\nWe demonstrate that the ontology has high coverage over most of the thoughts,\nspeech, and actions contained in these narratives and produces high inter-coder\nagreement when applied by human coders. We introduce a new crisis event dataset\nICB Events (ICBe). We find that ICBe captures the process of a crisis with\ngreater accuracy and granularity than other well-regarded events or crisis\ndatasets. We make the data, replication material, and additional visualizations\navailable at a companion website www.crisisevents.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Douglass_R/0/1/0/all/0/1\">Rex W. Douglass</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scherer_T/0/1/0/all/0/1\">Thomas Leo Scherer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gannon_J/0/1/0/all/0/1\">J. Andr&#xe9;s Gannon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gartzke_E/0/1/0/all/0/1\">Erik Gartzke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindsay_J/0/1/0/all/0/1\">Jon Lindsay</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carcelli_S/0/1/0/all/0/1\">Shannon Carcelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilkenfeld_J/0/1/0/all/0/1\">Jonathan Wilkenfeld</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Quinn_D/0/1/0/all/0/1\">David M. Quinn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aiken_C/0/1/0/all/0/1\">Catherine Aiken</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Navarro_J/0/1/0/all/0/1\">Jose Miguel Cabezas Navarro</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lund_N/0/1/0/all/0/1\">Neil Lund</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Murauskaite_E/0/1/0/all/0/1\">Egle Murauskaite</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Partridge_D/0/1/0/all/0/1\">Diana Partridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Tweets With Applicable Fact-Checks Across Languages. (arXiv:2202.07094v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07094","description":"<p>An important challenge for news fact-checking is the effective dissemination\nof existing fact-checks. This in turn brings the need for reliable methods to\ndetect previously fact-checked claims. In this paper, we focus on automatically\nfinding existing fact-checks for claims made in social media posts (tweets). We\nconduct both classification and retrieval experiments, in monolingual (English\nonly), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)\nsettings using multilingual transformer models such as XLM-RoBERTa and\nmultilingual embeddings such as LaBSE and SBERT. We present promising results\nfor \"match\" classification (93% average accuracy) in four language pairs. We\nalso find that a BM25 baseline outperforms state-of-the-art multilingual\nembedding models for the retrieval task during our monolingual experiments. We\nhighlight and discuss NLP challenges while addressing this problem in different\nlanguages, and we introduce a novel curated dataset of fact-checks and\ncorresponding tweets for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1\">Ver&#xf3;nica P&#xe9;rez-Rosas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Dynamic Neural Networks for Natural Language Processing. (arXiv:2202.07101v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07101","description":"<p>Effectively scaling large Transformer models is a main driver of recent\nadvances in natural language processing. Dynamic neural networks, as an\nemerging research direction, are capable of scaling up neural networks with\nsub-linear increases in computation and time by dynamically adjusting their\ncomputational path based on the input. Dynamic neural networks could be a\npromising solution to the growing parameter numbers of pretrained language\nmodels, allowing both model pretraining with trillions of parameters and faster\ninference on mobile devices. In this survey, we summarize progress of three\ntypes of dynamic neural networks in NLP: skimming, mixture of experts, and\nearly exit. We also highlight current challenges in dynamic neural networks and\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Model Compression for Natural Language Processing. (arXiv:2202.07105v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07105","description":"<p>With recent developments in new architectures like Transformer and\npretraining techniques, significant progress has been made in applications of\nnatural language processing (NLP). However, the high energy cost and long\ninference delay of Transformer is preventing NLP from entering broader\nscenarios including edge and mobile computing. Efficient NLP research aims to\ncomprehensively consider computation, time and carbon emission for the entire\nlife-cycle of NLP, including data preparation, model training and inference. In\nthis survey, we focus on the inference stage and review the current state of\nmodel compression for NLP, including the benchmarks, metrics and methodology.\nWe outline the current obstacles and future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v1 [cs.AI])","link":"http://arxiv.org/abs/2202.07138","description":"<p>Automated planning focuses on strategies, building domain models and\nsynthesizing plans to transit initial states to goals. Natural language\nprocessing concerns with the interactions between agents and human language,\nespecially processing and analyzing large amounts of natural language data.\nThese two fields have abilities to generate explicit knowledge, e.g.,\npreconditions and effects of action models, and learn from tacit knowledge,\ne.g., neural models, respectively. Integrating AI planning and natural language\nprocessing effectively improves the communication between human and intelligent\nagents. This paper outlines the commons and relations between AI planning and\nnatural language processing, argues that each of them can effectively impact on\nthe other one by four areas: (1) planning-based text understanding, (2)\nplanning-based text generation, (3) text-based human-robot interaction, and (4)\ntext-based explainable planning. We also explore some potential future issues\nbetween AI planning and natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kebing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1\">Hankz Hankui Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsPod: Automatic and Interactive News Podcasts. (arXiv:2202.07146v1 [cs.HC])","link":"http://arxiv.org/abs/2202.07146","description":"<p>News podcasts are a popular medium to stay informed and dive deep into news\ntopics. Today, most podcasts are handcrafted by professionals. In this work, we\nadvance the state-of-the-art in automatically generated podcasts, making use of\nrecent advances in natural language processing and text-to-speech technology.\nWe present NewsPod, an automatically generated, interactive news podcast. The\npodcast is divided into segments, each centered on a news event, with each\nsegment structured as a Question and Answer conversation, whose goal is to\nengage the listener. A key aspect of the design is the use of distinct voices\nfor each role (questioner, responder), to better simulate a conversation.\nAnother novel aspect of NewsPod allows listeners to interact with the podcast\nby asking their own questions and receiving automatically generated answers. We\nvalidate the soundness of this system design through two usability studies,\nfocused on evaluating the narrative style and interactions with the podcast,\nrespectively. We find that NewsPod is preferred over a baseline by\nparticipants, with 80% claiming they would use the system in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elicia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korlakunta_S/0/1/0/all/0/1\">Srujay Korlakunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot Pools. (arXiv:2202.07156v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07156","description":"<p>Dialogue state tracking (DST) is a component of the task-oriented dialogue\nsystem. It is responsible for extracting and managing slot values according to\ndialogue utterances, where each slot represents an essential part of the\ninformation to accomplish a task, and slot value is updated recurrently in each\ndialogue turn. However, many DST models cannot update slot values\nappropriately. These models may repeatedly inherit wrong slot values extracted\nin previous turns, resulting in the fail of the entire DST task.They cannot\nupdate indirectly mentioned slots well, either. This study designed a model\nwith a mentioned slot pool (MSP) to tackle the update problem. The MSP is a\nslot-specific memory that records all mentioned slot values that may be\ninherited, and our model updates slot values according to the MSP and the\ndialogue context. Our model rejects inheriting the previous slot value when it\npredicates the value is wrong. Then, it re-extracts the slot value from the\ncurrent dialogue context. As the contextual information accumulates with the\ndialogue progress, the new value is more likely to be correct. It also can\ntrack the indirectly mentioned slot by picking a value from the MSP.\nExperimental results showed our model reached state-of-the-art DST performance\non MultiWOZ 2.1 and 2.2 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhoujian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning. (arXiv:2202.07206v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07206","description":"<p>Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razeghi_Y/0/1/0/all/0/1\">Yasaman Razeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval. (arXiv:2202.07247v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07247","description":"<p>We introduce CommerceMM - a multimodal model capable of providing a diverse\nand granular understanding of commerce topics associated to the given piece of\ncontent (image, text, image+text), and having the capability to generalize to a\nwide range of tasks, including Multimodal Categorization, Image-Text Retrieval,\nQuery-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the\npre-training + fine-tuning training regime and present 5 effective pre-training\ntasks on image-text pairs. To embrace more common and diverse commerce data\nwith text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal\nmapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks,\ncalled Omni-Retrieval pre-training. The pre-training is conducted in an\nefficient manner with only two forward/backward updates for the combined 14\ntasks. Extensive experiments and analysis show the effectiveness of each task.\nWhen combining all pre-training tasks, our model achieves state-of-the-art\nperformance on 7 commerce-related downstream tasks after fine-tuning.\nAdditionally, we propose a novel approach of modality randomization to\ndynamically adjust our model under different efficiency constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Animesh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao MJ Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hugo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Cross-lingual Prompting with Mask Token Augmentation. (arXiv:2202.07255v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07255","description":"<p>Prompting shows promising results in few-shot scenarios. However, its\nstrength for multilingual/cross-lingual problems has not been fully exploited.\nZhao and Sch\\\"utze (2021) made initial explorations in this direction by\npresenting that cross-lingual prompting outperforms cross-lingual finetuning.\nIn this paper, we conduct empirical analysis on the effect of each component in\ncross-lingual prompting and derive Universal Prompting across languages, which\nhelps alleviate the discrepancies between source-language training and\ntarget-language inference. Based on this, we propose a mask token augmentation\nframework to further improve the performance of prompt-based cross-lingual\ntransfer. Notably, for XNLI, our method achieves 46.54% with only 16 English\ntraining examples per class, significantly better than 34.99% of finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Meng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Media Slant is Contagious. (arXiv:2202.07269v1 [econ.GN])","link":"http://arxiv.org/abs/2202.07269","description":"<p>This paper analyzes the influence of partisan content from national cable TV\nnews on local reporting in U.S. newspapers. We provide a new\nmachine-learning-based measure of cable news slant, trained on a corpus of 40K\ntranscribed TV episodes from Fox News Channel (FNC), CNN, and MSNBC\n(2005-2008). Applying the method to a corpus of 24M local newspaper articles,\nwe find that in response to an exogenous increase in local viewership of FNC\nrelative to CNN/MSNBC, local newspaper articles become more similar to FNC\ntranscripts (and vice versa). Consistent with newspapers responding to changes\nin reader preferences, we see a shift in the framing of local news coverage\nrather than just direct borrowing of cable news content. Further, cable news\nslant polarizes local news content: right-leaning newspapers tend to adopt\nright-wing FNC language, while left-leaning newspapers tend to become more\nleft-wing. Media slant is contagious.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Widmer_P/0/1/0/all/0/1\">Philine Widmer</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Galletta_S/0/1/0/all/0/1\">Sergio Galletta</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saving Dense Retriever from Shortcut Dependency in Conversational Search. (arXiv:2202.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07280","description":"<p>In conversational search (CS), it needs holistic understanding over\nconversational inputs to retrieve relevant passages. In this paper, we\ndemonstrate the existence of a retrieval shortcut in CS, which causes models to\nretrieve passages solely relying on partial history while disregarding the\nlatest question. With in-depth analysis, we first show naively trained dense\nretrievers heavily exploit the shortcut and hence perform poorly when asked to\nanswer history-independent questions. To prevent models from solely relying on\nthe shortcut, we explore iterative hard negatives mined by pre-trained dense\nretrievers. Experimental results show that training with the iterative hard\nnegatives effectively mitigates the dependency on the shortcut and makes\nsubstantial improvement on recent CS benchmarks. Our retrievers achieve new\nstate-of-the-art results, outperforming the previous best models by 9.7 in\nRecall@10 on QReCC and 12.4 in Recall@5 on TopiOCQA. Furthermore, in our\nend-to-end QA experiments, FiD readers combined with our retrievers surpass the\nprevious state-of-the-art models by 3.7 and 1.0 EM scores on QReCC and\nTopiOCQA, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective Multi-Task Interaction for Entity-Relation Extraction: A Unified Framework with Selection Recurrent Network. (arXiv:2202.07281v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07281","description":"<p>Entity-relation extraction aims to jointly solve named entity recognition\n(NER) and relation extraction (RE). Recent approaches use either one-way\nsequential information propagation in a pipeline manner or two-way implicit\ninteraction with a shared encoder. However, they still suffer from poor\ninformation interaction due to the gap between the different task forms of NER\nand RE, raising a controversial question whether RE is really beneficial to\nNER. Motivated by this, we propose a novel and unified cascade framework that\ncombines the advantages of both sequential information propagation and implicit\ninteraction. Meanwhile, it eliminates the gap between the two tasks by\nreformulating entity-relation extraction as unified span-extraction tasks.\nSpecifically, we propose a selection recurrent network as a shared encoder to\nencode task-specific independent and shared representations and design two\nsequential information propagation strategies to realize the sequential\ninformation flow between NER and RE. Extensive experiments demonstrate that our\napproaches can achieve state-of-the-art results on two common benchmarks, ACE05\nand SciERC, and effectively model the multi-task interaction, which realizes\nsignificant mutual benefits of NER and RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Hanh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokota_H/0/1/0/all/0/1\">Haruo Yokota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07305","description":"<p>Image narrative generation describes the creation of stories regarding the\ncontent of image data from a subjective viewpoint. Given the importance of the\nsubjective feelings of writers, characters, and readers in storytelling, image\nnarrative generation methods must consider human emotion, which is their major\ndifference from descriptive caption generation tasks. The development of\nautomated methods to generate story-like text associated with images may be\nconsidered to be of considerable social significance, because stories serve\nessential functions both as entertainment and also for many practical purposes\nsuch as education and advertising. In this study, we propose a model called\nViNTER (Visual Narrative Transformer with Emotion arc Representation) to\ngenerate image narratives that focus on time series representing varying\nemotions as \"emotion arcs,\" to take advantage of recent advances in multimodal\nTransformer-based pre-trained models. We present experimental results of both\nmanual and automatic evaluations, which demonstrate the effectiveness of the\nproposed emotion-aware approach to image narrative generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_Y/0/1/0/all/0/1\">Yusuke Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"textless-lib: a Library for Textless Spoken Language Processing. (arXiv:2202.07359v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07359","description":"<p>Textless spoken language processing research aims to extend the applicability\nof standard NLP toolset onto spoken language and languages with few or no\ntextual resources. In this paper, we introduce textless-lib, a PyTorch-based\nlibrary aimed to facilitate research in this research area. We describe the\nbuilding blocks that the library provides and demonstrate its usability by\ndiscuss three different use-case examples: (i) speaker probing, (ii) speech\nresynthesis and compression, and (iii) speech continuation. We believe that\ntextless-lib substantially simplifies research the textless setting and will be\nhandful not only for speech researchers but also for the NLP community at\nlarge. The code, documentation, and pre-trained models are available at\nhttps://github.com/facebookresearch/textlesslib/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuLD: The Multitask Long Document Benchmark. (arXiv:2202.07362v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07362","description":"<p>The impressive progress in NLP techniques has been driven by the development\nof multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks\nfocus on tasks for one or two input sentences, there has been exciting work in\ndesigning efficient techniques for processing much longer inputs. In this\npaper, we present MuLD: a new long document benchmark consisting of only\ndocuments over 10,000 tokens. By modifying existing NLP tasks, we create a\ndiverse benchmark which requires models to successfully model long-term\ndependencies in the text. We evaluate how existing models perform, and find\nthat our benchmark is much more challenging than their `short document'\nequivalents. Furthermore, by evaluating both regular and efficient\ntransformers, we show that models with increased context length are better able\nto solve the tasks presented, suggesting that future improvements in these\nmodels are vital for solving similar long document problems. We release the\ndata and code for baselines to encourage further research on efficient NLP\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_G/0/1/0/all/0/1\">G Thomas Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Prompt Learning for Explainable Recommendation. (arXiv:2202.07371v1 [cs.IR])","link":"http://arxiv.org/abs/2202.07371","description":"<p>Providing user-understandable explanations to justify recommendations could\nhelp users better understand the recommended items, increase the system's ease\nof use, and gain users' trust. A typical approach to realize it is natural\nlanguage generation. However, previous works mostly adopt recurrent neural\nnetworks to meet the ends, leaving the potentially more effective pre-trained\nTransformer models under-explored. In fact, user and item IDs, as important\nidentifiers in recommender systems, are inherently in different semantic space\nas words that pre-trained models were already trained on. Thus, how to\neffectively fuse IDs into such models becomes a critical issue. Inspired by\nrecent advancement in prompt learning, we come up with two solutions: find\nalternative words to represent IDs (called discrete prompt learning), and\ndirectly input ID vectors to a pre-trained model (termed continuous prompt\nlearning). In the latter case, ID vectors are randomly initialized but the\nmodel is trained in advance on large corpora, so they are actually in different\nlearning stages. To bridge the gap, we further propose two training strategies:\nsequential tuning and recommendation as regularization. Extensive experiments\nshow that our continuous prompt learning approach equipped with the training\nstrategies consistently outperforms strong baselines on three datasets of\nexplainable recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07543","description":"<p>Memes are prevalent on the internet and continue to grow and evolve alongside\nour culture. An automatic understanding of memes propagating on the internet\ncan shed light on the general sentiment and cultural attitudes of people. In\nthis work, we present team BLUE's solution for the second edition of the\nMEMOTION competition. We showcase two approaches for meme classification (i.e.\nsentiment, humour, offensive, sarcasm and motivation levels) using a text-only\nmethod using BERT, and a Multi-Modal-Multi-Task transformer network that\noperates on both the meme image and its caption to output the final scores. In\nboth approaches, we leverage state-of-the-art pretrained models for text (BERT,\nSentence Transformer) and image processing (EfficientNetV4, CLIP). Through our\nefforts, we obtain first place in task A, second place in task B and third\nplace in task C. In addition, our team obtained the highest average score for\nall three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iordache_I/0/1/0/all/0/1\">Ioan-Bogdan Iordache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Trends of COVID-19 Tweet Sentiment with Respect to Voting Preferences in the 2020 Election Year of the United States. (arXiv:2202.07587v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07587","description":"<p>COVID-19 related policies were extensively politicized during the 2020\nelection year of the United States, resulting in polarizing viewpoints. Twitter\nusers were particularly engaged during the 2020 election year. Here we\ninvestigated whether COVID-19 related tweets were associated with the overall\nelection results at the state level during the period leading up to the\nelection day. We observed weak correlations between the average sentiment of\nCOVID-19 related tweets and popular votes in two-week intervals, and the trends\ngradually become opposite. We then compared the average sentiments of COVID-19\nrelated tweets between states called in favor of Republican (red states) or\nDemocratic parties (blue states). We found that at the beginning of lockdowns\nsentiments in the blue states were much more positive than those in the red\nstates. However, sentiments in the red states gradually become more positive\nduring the summer of 2020 and persisted until the election day.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doman_M/0/1/0/all/0/1\">Megan Doman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motley_J/0/1/0/all/0/1\">Jacob Motley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PILED: An Identify-and-Localize Framework for Few-Shot Event Detection. (arXiv:2202.07615v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07615","description":"<p>Practical applications of event extraction systems have long been hindered by\ntheir need for heavy human annotation. In order to scale up to new domains and\nevent types, models must learn to cope with limited supervision, as in few-shot\nlearning settings. To this end, the major challenge is to let the model master\nthe semantics of event types, without requiring abundant event mention\nannotations. In our study, we employ cloze prompts to elicit event-related\nknowledge from pretrained language models and further use event definitions and\nkeywords to pinpoint the trigger word. By formulating the event detection task\nas an identify-then-localize procedure, we minimize the number of type-specific\nparameters, enabling our model to quickly adapt to event detection tasks for\nnew types. Experiments on three event detection benchmark datasets (ACE,\nFewEvent, MAVEN) show that our proposed method performs favorably under fully\nsupervised settings and surpasses existing few-shot methods by 21% F1 on the\nFewEvent dataset and 20% on the MAVEN dataset when only 5 examples are provided\nfor each event type.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deeper into Cross-lingual Visual Question Answering. (arXiv:2202.07630v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07630","description":"<p>Visual question answering (VQA) is one of the crucial vision-and-language\ntasks. Yet, the bulk of research until recently has focused only on the English\nlanguage due to the lack of appropriate evaluation resources. Previous work on\ncross-lingual VQA has reported poor zero-shot transfer performance of current\nmultilingual multimodal Transformers and large gaps to monolingual performance,\nattributed mostly to misalignment of text embeddings between the source and\ntarget languages, without providing any additional deeper analyses. In this\nwork, we delve deeper and address different aspects of cross-lingual VQA\nholistically, aiming to understand the impact of input data, fine-tuning and\nevaluation regimes, and interactions between the two modalities in\ncross-lingual setups. 1) We tackle low transfer performance via novel methods\nthat substantially reduce the gap to monolingual English performance, yielding\n+10 accuracy points over existing transfer methods. 2) We study and dissect\ncross-lingual VQA across different question types of varying complexity, across\ndifferent multilingual multi-modal Transformers, and in zero-shot and few-shot\nscenarios. 3) We further conduct extensive analyses on modality biases in\ntraining data and models, aimed to further understand why zero-shot performance\ngaps remain for some question types and languages. We hope that the novel\nmethods and detailed analyses will guide further progress in multilingual VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vulic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Configuration to Rule Them All? Towards Hyperparameter Transfer in Topic Models using Multi-Objective Bayesian Optimization. (arXiv:2202.07631v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07631","description":"<p>Topic models are statistical methods that extract underlying topics from\ndocument collections. When performing topic modeling, a user usually desires\ntopics that are coherent, diverse between each other, and that constitute good\ndocument representations for downstream tasks (e.g. document classification).\nIn this paper, we conduct a multi-objective hyperparameter optimization of\nthree well-known topic models. The obtained results reveal the conflicting\nnature of different objectives and that the training corpus characteristics are\ncrucial for the hyperparameter selection, suggesting that it is possible to\ntransfer the optimal hyperparameter configurations between datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrando_I/0/1/0/all/0/1\">Ismail Harrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lisena_P/0/1/0/all/0/1\">Pasquale Lisena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troncy_R/0/1/0/all/0/1\">Raphael Troncy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fersini_E/0/1/0/all/0/1\">Elisabetta Fersini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Memorization Across Neural Language Models. (arXiv:2202.07646v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07646","description":"<p>Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n</p>\n<p>We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation. (arXiv:2202.07654v1 [cs.CL])","link":"http://arxiv.org/abs/2202.07654","description":"<p>The predictions of question answering (QA) systems are typically evaluated\nagainst manually annotated finite sets of one or more answers. This leads to a\ncoverage limitation that results in underestimating the true performance of\nsystems, and is typically addressed by extending over exact match (EM) with\npredefined rules or with the token-level F1 measure. In this paper, we present\nthe first systematic conceptual and data-driven analysis to examine the\nshortcomings of token-level equivalence measures.\n</p>\n<p>To this end, we define the asymmetric notion of answer equivalence (AE),\naccepting answers that are equivalent to or improve over the reference, and\ncollect over 26K human judgements for candidates produced by multiple QA\nsystems on SQuAD. Through a careful analysis of this data, we reveal and\nquantify several concrete limitations of the F1 measure, such as false\nimpression of graduality, missing dependence on question, and more.\n</p>\n<p>Since collecting AE annotations for each evaluated model is expensive, we\nlearn a BERT matching BEM measure to approximate this task. Being a simpler\ntask than QA, we find BEM to provide significantly better AE approximations\nthan F1, and more accurately reflect the performance of systems.\n</p>\n<p>Finally, we also demonstrate the practical utility of AE and BEM on the\nconcrete application of minimal accurate prediction sets, reducing the number\nof required answers by up to 2.6 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulian_J/0/1/0/all/0/1\">Jannis Bulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajewski_W/0/1/0/all/0/1\">Wojciech Gajewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Linearity of Cross-Lingual Word Embedding Mappings. (arXiv:2004.01079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.01079","description":"<p>The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role\nin tackling Natural Language Processing challenges for low-resource languages.\nIts dominant approaches assumed that the relationship between embeddings could\nbe represented by a linear mapping, but there has been no exploration of the\nconditions under which this assumption holds. Such a research gap becomes very\ncritical recently, as it has been evidenced that relaxing mappings to be\nnon-linear can lead to better performance in some cases. We, for the first\ntime, present a theoretical analysis that identifies the preservation of\nanalogies encoded in monolingual word embeddings as a necessary and sufficient\ncondition for the ground-truth CLWE mapping between those embeddings to be\nlinear. On a novel cross-lingual analogy dataset that covers five\nrepresentative analogy categories for twelve distinct languages, we carry out\nexperiments which provide direct empirical support for our theoretical claim.\nThese results offer additional insight into the observations of other\nresearchers and contribute inspiration for the development of more effective\ncross-lingual representation learning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Mark Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14150","description":"<p>MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Ankita De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced End-to-End Monolingual pre-training for Low-Resourced Indic Languages Code-Switching Speech Recognition. (arXiv:2106.05885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05885","description":"<p>The success in designing Code-Switching (CS) ASR often depends on the\navailability of the transcribed CS resources. Such dependency harms the\ndevelopment of ASR in low-resourced languages such as Bengali and Hindi. In\nthis paper, we exploit the transfer learning approach to design End-to-End\n(E2E) CS ASR systems for the two low-resourced language pairs using different\nmonolingual speech data and a small set of noisy CS data. We trained the\nCS-ASR, following two steps: (i) building a robust bilingual ASR system using a\nconvolution-augmented transformer (Conformer) based acoustic model and n-gram\nlanguage model, and (ii) fine-tuned the entire E2E ASR with limited noisy CS\ndata. We tested our method on MUCS 2021 challenge and achieved 3rd place in the\nCS track. We then tested the proposed method using noisy CS data released for\nHindi-English and Bengali-English pairs in Multilingual and Code-Switching ASR\nChallenges for Low Resource Indian Languages (MUCS 2021) and achieved 3rd place\nin the CS track. Unlike, the leading two systems that benefited from crawling\nYouTube and learning transliteration pairs, our proposed transfer learning\napproach focused on using only the limited CS data with no data-cleaning or\ndata re-segmentation. Our approach achieved 14.1% relative gain in word error\nrate (WER) in Hindi-English and 27.1% in Bengali-English. We provide detailed\nguidelines on the steps to finetune the self-attention based model for limited\ndata for ASR. Moreover, we release the code and recipe used in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.02473","description":"<p>In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelofske_E/0/1/0/all/0/1\">Elijah Pelofske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebrock_L/0/1/0/all/0/1\">Lorie M. Liebrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_V/0/1/0/all/0/1\">Vincent Urias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation. (arXiv:2109.05812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05812","description":"<p>With the rapid increase of multimedia data, a large body of literature has\nemerged to work on multimodal summarization, the majority of which target at\nrefining salient information from textual and visual modalities to output a\npictorial summary with the most relevant images. Existing methods mostly focus\non either extractive or abstractive summarization and rely on qualified image\ncaptions to build image references. We are the first to propose a Unified\nframework for Multimodal Summarization grounding on BART, UniMS, that\nintegrates extractive and abstractive objectives, as well as selecting the\nimage output. Specially, we adopt knowledge distillation from a vision-language\npretrained model to improve image selection, which avoids any requirement on\nthe existence and quality of image captions. Besides, we introduce a visual\nguided decoder to better integrate textual and visual modalities in guiding\nabstractive text generation. Results show that our best model achieves a new\nstate-of-the-art result on a large-scale benchmark dataset. The newly involved\nextractive objective as well as the knowledge distillation technique are proven\nto bring a noticeable improvement to the multimodal summarization task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengkun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2110.01691","description":"<p>Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by \"unit-testing\" sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie J. Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04525","description":"<p>Event Extraction bridges the gap between text and event signals. Based on the\nassumption of trigger-argument dependency, existing approaches have achieved\nstate-of-the-art performance with expert-designed templates or complicated\ndecoding constraints. In this paper, for the first time we introduce the\nprompt-based learning strategy to the domain of Event Extraction, which\nempowers the automatic exploitation of label semantics on both input and output\nsides. To validate the effectiveness of the proposed generative method, we\nconduct extensive experiments with 11 diverse baselines. Empirical results show\nthat, in terms of F1 score on Argument Extraction, our simple architecture is\nstronger than any other generative counterpart and even competitive with\nalgorithms that require template engineering. Regarding the measure of recall,\nit sets new overall records for both Argument and Trigger Extractions. We\nhereby recommend this framework to the community, with the code publicly\navailable at https://git.io/GDAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jinghui Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haotian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph informed Fake News Classification via Heterogeneous Representation Ensembles. (arXiv:2110.10457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10457","description":"<p>Increasing amounts of freely available data both in textual and relational\nform offers exploration of richer document representations, potentially\nimproving the model performance and robustness. An emerging problem in the\nmodern era is fake news detection -- many easily available pieces of\ninformation are not necessarily factually correct, and can lead to wrong\nconclusions or are used for manipulation. In this work we explore how different\ndocument representations, ranging from simple symbolic bag-of-words, to\ncontextual, neural language model-based ones can be used for efficient fake\nnews identification. One of the key contributions is a set of novel document\nrepresentation learning methods based solely on knowledge graphs, i.e.\nextensive collections of (grounded) subject-predicate-object triplets. We\ndemonstrate that knowledge graph-based representations already achieve\ncompetitive performance to conventionally accepted representation learners.\nFurthermore, when combined with existing, contextual representations, knowledge\ngraph-based document representations can achieve state-of-the-art performance.\nTo our knowledge this is the first larger-scale evaluation of how knowledge\ngraph-based representations can be systematically incorporated into the process\nof fake news classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepisnik_Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik-Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textless Speech Emotion Conversion using Discrete and Decomposed Representations. (arXiv:2111.07402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07402","description":"<p>Speech emotion conversion is the task of modifying the perceived emotion of a\nspeech utterance while preserving the lexical content and speaker identity. In\nthis study, we cast the problem of emotion conversion as a spoken language\ntranslation task. We use a decomposition of the speech signal into discrete\nlearned representations, consisting of phonetic-content units, prosodic\nfeatures, speaker, and emotion. First, we modify the speech content by\ntranslating the phonetic-content units to a target emotion, and then predict\nthe prosodic features based on these units. Finally, the speech waveform is\ngenerated by feeding the predicted representations into a neural vocoder. Such\na paradigm allows us to go beyond spectral and parametric changes of the\nsignal, and model non-verbal vocalizations, such as laughter insertion, yawning\nremoval, etc. We demonstrate objectively and subjectively that the proposed\nmethod is vastly superior to current approaches and even beats text-based\nsystems in terms of perceived emotion and audio quality. We rigorously evaluate\nall components of such a complex system and conclude with an extensive model\nanalysis and ablation study to better emphasize the architectural choices,\nstrengths and weaknesses of the proposed method. Samples are available under\nthe following link: https://speechbot.github.io/emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1\">Felix Kreuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linking-Enhanced Pre-Training for Table Semantic Parsing. (arXiv:2111.09486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09486","description":"<p>Recently pre-training models have significantly improved the performance of\nvarious NLP tasks by leveraging large-scale text corpora to improve the\ncontextual representation ability of the neural network. The large pre-training\nlanguage model has also been applied in the area of table semantic parsing.\nHowever, existing pre-training approaches have not carefully explored explicit\ninteraction relationships between a question and the corresponding database\nschema, which is a key ingredient for uncovering their semantic and structural\ncorrespondence. Furthermore, the question-aware representation learning in the\nschema grounding context has received less attention in pre-training\nobjective.To alleviate these issues, this paper designs two novel pre-training\nobjectives to impose the desired inductive bias into the learned\nrepresentations for table pre-training. We further propose a schema-aware\ncurriculum learning approach to mitigate the impact of noise and learn\neffectively from the pre-training data in an easy-to-hard manner. We evaluate\nour pre-trained framework by fine-tuning it on two benchmarks, Spider and\nSQUALL. The results demonstrate the effectiveness of our pre-training objective\nand curriculum compared to a variety of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework. (arXiv:2201.03115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03115","description":"<p>It is well known that translations of songs and poems not only break rhythm\nand rhyming patterns, but can also result in loss of semantic information. The\nBhagavad Gita is an ancient Hindu philosophical text originally written in\nSanskrit that features a conversation between Lord Krishna and Arjuna prior to\nthe Mahabharata war. The Bhagavad Gita is also one of the key sacred texts in\nHinduism and is known as the forefront of the Vedic corpus of Hinduism. In the\nlast two centuries, there has been a lot of interest in Hindu philosophy from\nwestern scholars; hence, the Bhagavad Gita has been translated in a number of\nlanguages. However, there is not much work that validates the quality of the\nEnglish translations. Recent progress of language models powered by deep\nlearning has enabled not only translations but a better understanding of\nlanguage and texts with semantic and sentiment analysis. Our work is motivated\nby the recent progress of language models powered by deep learning methods. In\nthis paper, we present a framework that compares selected translations (from\nSanskrit to English) of the Bhagavad Gita using semantic and sentiment\nanalyses. We use hand-labelled sentiment dataset for tuning state-of-art deep\nlearning-based language model known as bidirectional encoder representations\nfrom transformers (BERT). We provide sentiment and semantic analysis for\nselected chapters and verses across translations. Our results show that\nalthough the style and vocabulary in the respective translations vary widely,\nthe sentiment analysis and semantic similarity shows that the message conveyed\nare mostly similar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Venkatesh Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v2 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances. (arXiv:2202.01624v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.01624","description":"<p>The time delay neural network (TDNN) represents one of the state-of-the-art\nof neural solutions to text-independent speaker verification. However, they\nrequire a large number of filters to capture the speaker characteristics at any\nlocal frequency region. In addition, the performance of such systems may\ndegrade under short utterance scenarios. To address these issues, we propose a\nmulti-scale frequency-channel attention (MFA), where we characterize speakers\nat different scales through a novel dual-path design which consists of a\nconvolutional neural network and TDNN. We evaluate the proposed MFA on the\nVoxCeleb database and observe that the proposed framework with MFA can achieve\nstate-of-the-art performance while reducing parameters and computation\ncomplexity. Further, the MFA mechanism is found to be effective for speaker\nverification with short test utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rohan Kumar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Aspect-Based Sentiment Analysis. (arXiv:2202.01924v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01924","description":"<p>Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated\ndata for supervised training/fine-tuning. It is a big challenge to scale ABSA\nto a large number of new domains. This paper aims to train a unified model that\ncan perform zero-shot ABSA without using any annotated data for a new domain.\nWe propose a method called contrastive post-training on review Natural Language\nInference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.\nWe evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect\nsentiment classification (ASC), to end-to-end aspect-based sentiment analysis\n(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiFSMN: Binary Neural Network for Keyword Spotting. (arXiv:2202.06483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06483","description":"<p>The deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications. However, computational resources for these\nnetworks are significantly constrained since they usually run on-call on edge\ndevices. In this paper, we present BiFSMN, an accurate and extreme-efficient\nbinary neural network for KWS. We first construct a High-frequency Enhancement\nDistillation scheme for the binarization-aware training, which emphasizes the\nhigh-frequency information from the full-precision network's representation\nthat is more crucial for the optimization of the binarized network. Then, to\nallow the instant and adaptive accuracy-efficiency trade-offs at runtime, we\nalso propose a Thinnable Binarization Architecture to further liberate the\nacceleration potential of the binarized network from the topology perspective.\nMoreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8\ndevices which fully utilizes registers and increases instruction throughput to\npush the limit of deployment efficiency. Extensive experiments show that BiFSMN\noutperforms existing binarization methods by convincing margins on various\ndatasets and is even comparable with the full-precision counterpart (e.g., less\nthan 3% drop on Speech Commands V1-12). We highlight that benefiting from the\nthinnable architecture and the optimized 1-bit implementation, BiFSMN can\nachieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge\nhardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DermX: an end-to-end framework for explainable automated dermatological diagnosis. (arXiv:2202.06956v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06956","description":"<p>Dermatological diagnosis automation is essential in addressing the high\nprevalence of skin diseases and critical shortage of dermatologists. Despite\napproaching expert-level diagnosis performance, convolutional neural network\n(ConvNet) adoption in clinical practice is impeded by their limited\nexplainability, and by subjective, expensive explainability validations. We\nintroduce DermX and DermX+, an end-to-end framework for explainable automated\ndermatological diagnosis. DermX is a clinically-inspired explainable\ndermatological diagnosis ConvNet, trained using DermXDB, a 554 images dataset\nannotated by eight dermatologists with diagnoses and supporting explanations.\nDermX+ extends DermX with guided attention training for explanation attention\nmaps. Both methods achieve near-expert diagnosis performance, with DermX,\nDermX+, and dermatologist F1 scores of 0.79, 0.79, and 0.87, respectively. We\nassess the explanation plausibility in terms of identification and\nlocalization, by comparing model-selected with dermatologist-selected\nexplanations, and gradient-weighted class-activation maps with dermatologist\nexplanation maps. Both DermX and DermX+ obtain an identification F1 score of\n0.78. The localization F1 score is 0.39 for DermX and 0.35 for DermX+.\nExplanation faithfulness is assessed through contrasting samples, DermX\nobtaining 0.53 faithfulness and DermX+ 0.25. These results show that\nexplainability does not necessarily come at the expense of predictive power, as\nour high-performance models provide both plausible and faithful explanations\nfor their diagnoses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jalaboi_R/0/1/0/all/0/1\">Raluca Jalaboi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faye_F/0/1/0/all/0/1\">Frederik Faye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orbes_Arteaga_M/0/1/0/all/0/1\">Mauricio Orbes-Arteaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorgensen_D/0/1/0/all/0/1\">Dan J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galimzianova_A/0/1/0/all/0/1\">Alfiia Galimzianova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASC me to Do Anything: Multi-task Training for Embodied AI. (arXiv:2202.06987v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06987","description":"<p>Embodied AI has seen steady progress across a diverse set of independent\ntasks. While these varied tasks have different end goals, the basic skills\nrequired to complete them successfully overlap significantly. In this paper,\nour goal is to leverage these shared skills to learn to perform multiple tasks\njointly. We propose Atomic Skill Completion (ASC), an approach for multi-task\ntraining for Embodied AI, where a set of atomic skills shared across multiple\ntasks are composed together to perform the tasks. The key to the success of\nthis approach is a pre-training scheme that decouples learning of the skills\nfrom the high-level tasks making joint training effective. We use ASC to train\nagents within the AI2-THOR environment to perform four interactive tasks\njointly and find it to be remarkably effective. In a multi-task setting, ASC\nimproves success rates by a factor of 2x on Seen scenes and 4x on Unseen scenes\ncompared to no pre-training. Importantly, ASC enables us to train a multi-task\nagent that has a 52% higher Success Rate than training 4 independent single\ntask agents. Finally, our hierarchical agents are more interpretable than\ntraditional black-box architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvador_J/0/1/0/all/0/1\">Jordi Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Cross-Modality Brain Image Synthesis. (arXiv:2202.06997v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06997","description":"<p>The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical or even\nluxurious, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. A realistic solution is to\nexplore either an unsupervised learning or a semi-supervised learning to\nsynthesize the absent neuroimaging data. In this paper, we tend to approach\nmulti-modality brain image synthesis task from different perspectives, which\ninclude the level of supervision, the range of modality synthesis, and the\nsynthesis-based downstream tasks. Particularly, we provide in-depth analysis on\nhow cross-modality brain image synthesis can improve the performance of\ndifferent downstream tasks. Finally, we evaluate the challenges and provide\nseveral open directions for this community. All resources are available at\nhttps://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images. (arXiv:2202.07001v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07001","description":"<p>Diagnostic, prognostic and therapeutic decision-making of cancer in pathology\nclinics can now be carried out based on analysis of multi-gigapixel tissue\nimages, also known as whole-slide images (WSIs). Recently, deep convolutional\nneural networks (CNNs) have been proposed to derive unsupervised WSI\nrepresentations; these are attractive as they rely less on expert annotation\nwhich is cumbersome. However, a major trade-off is that higher predictive power\ngenerally comes at the cost of interpretability, posing a challenge to their\nclinical use where transparency in decision-making is generally expected. To\naddress this challenge, we present a handcrafted framework based on deep CNN\nfor constructing holistic WSI-level representations. Building on recent\nfindings about the internal working of the Transformer in the domain of natural\nlanguage processing, we break down its processes and handcraft them into a more\ntransparent framework that we term as the Handcrafted Histological Transformer\nor H2T. Based on our experiments involving various datasets consisting of a\ntotal of 5,306 WSIs, the results demonstrate that H2T based holistic WSI-level\nrepresentations offer competitive performance compared to recent\nstate-of-the-art methods and can be readily utilized for various downstream\nanalysis tasks. Finally, our results demonstrate that the H2T framework can be\nup to 14 times faster than the Transformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vu_Q/0/1/0/all/0/1\">Quoc Dang Vu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_K/0/1/0/all/0/1\">Kashif Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Visual Sensory Anomaly Detection. (arXiv:2202.07006v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07006","description":"<p>Visual sensory anomaly detection (AD) is an essential problem in computer\nvision, which is gaining momentum recently thanks to the development of AI for\ngood. Compared with semantic anomaly detection which detects anomaly at the\nlabel level (semantic shift), visual sensory AD detects the abnormal part of\nthe sample (covariate shift). However, no thorough review has been provided to\nsummarize this area for the computer vision community. In this survey, we are\nthe first one to provide a comprehensive review of visual sensory AD and\ncategory into three levels according to the form of anomalies. Furthermore, we\nclassify each kind of anomaly according to the level of supervision. Finally,\nwe summarize the challenges and provide open directions for this community. All\nresources are available at\nhttps://github.com/M-3LAB/awesome-visual-sensory-anomaly-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Inspection Toolkit: Unified Evaluation and Strong Baselines for Damage Recognition. (arXiv:2202.07012v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07012","description":"<p>In recent years, several companies and researchers have started to tackle the\nproblem of damage recognition within the scope of automated inspection of built\nstructures. While companies are neither willing to publish associated data nor\nmodels, researchers are facing the problem of data shortage on one hand and\ninconsistent dataset splitting with the absence of consistent metrics on the\nother hand. This leads to incomparable results. Therefore, we introduce the\nbuilding inspection toolkit -- bikit -- which acts as a simple to use data hub\ncontaining relevant open-source datasets in the field of damage recognition.\nThe datasets are enriched with evaluation splits and predefined metrics,\nsuiting the specific task and their data distribution. For the sake of\ncompatibility and to motivate researchers in this domain, we also provide a\nleaderboard and the possibility to share model weights with the community. As\nstarting point we provide strong baselines for multi-target classification\ntasks utilizing extensive hyperparameter search using three transfer learning\napproaches for state-of-the-art algorithms. The toolkit and the leaderboard are\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flotzinger_J/0/1/0/all/0/1\">Johannes Flotzinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosch_P/0/1/0/all/0/1\">Philipp J. R&#xf6;sch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1\">Norbert Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braml_T/0/1/0/all/0/1\">Thomas Braml</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box Supervised Video Segmentation Proposal Network. (arXiv:2202.07025v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07025","description":"<p>Video Object Segmentation (VOS) has been targeted by various fully-supervised\nand self-supervised approaches. While fully-supervised methods demonstrate\nexcellent results, self-supervised ones, which do not use pixel-level ground\ntruth, attract much attention. However, self-supervised approaches pose a\nsignificant performance gap. Box-level annotations provide a balanced\ncompromise between labeling effort and result quality for image segmentation\nbut have not been exploited for the video domain. In this work, we propose a\nbox-supervised video object segmentation proposal network, which takes\nadvantage of intrinsic video properties. Our method incorporates object motion\nin the following way: first, motion is computed using a bidirectional temporal\ndifference and a novel bounding box-guided motion compensation. Second, we\nintroduce a novel motion-aware affinity loss that encourages the network to\npredict positive pixel pairs if they share similar motion and color. The\nproposed method outperforms the state-of-the-art self-supervised benchmark by\n16.4% and 6.9% $\\mathcal{J}$ &amp;$\\mathcal{F}$ score and the majority of fully\nsupervised methods on the DAVIS and Youtube-VOS dataset without imposing\nnetwork architectural specifications. We provide extensive tests and ablations\non the datasets, demonstrating the robustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hannan_T/0/1/0/all/0/1\">Tanveer Hannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobold_J/0/1/0/all/0/1\">Jonathan Kobold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Matthias Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v1 [cs.AI])","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 45% and\n70% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07054","description":"<p>Deep neural networks have achieved great success in many important remote\nsensing tasks. Nevertheless, their vulnerability to adversarial examples should\nnot be neglected. In this study, we systematically analyze the universal\nadversarial examples in remote sensing data for the first time, without any\nknowledge from the victim model. Specifically, we propose a novel black-box\nadversarial attack method, namely Mixup-Attack, and its simple variant\nMixcut-Attack, for remote sensing data. The key idea of the proposed methods is\nto find common vulnerabilities among different networks by attacking the\nfeatures in the shallow layer of a given surrogate model. Despite their\nsimplicity, the proposed methods can generate transferable adversarial examples\nthat deceive most of the state-of-the-art deep neural networks in both scene\nclassification and semantic segmentation tasks with high success rates. We\nfurther provide the generated universal adversarial examples in the dataset\nnamed UAE-RS, which is the first dataset that provides black-box adversarial\nsamples in the remote sensing field. We hope UAE-RS may serve as a benchmark\nthat helps researchers to design deep neural networks with strong resistance\ntoward adversarial attacks in the remote sensing field. Codes and the UAE-RS\ndataset will be available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07073","description":"<p>During the training process, deep neural networks implicitly learn to\nrepresent the input data samples through a hierarchy of features, where the\nsize of the hierarchy is determined by the number of layers. In this paper, we\nfocus on enforcing the discriminative power of the high-level representations,\nthat are typically learned by the deeper layers (closer to the output). To this\nend, we introduce a new loss term inspired by the Gini impurity, which is aimed\nat minimizing the entropy (increasing the discriminative power) of individual\nhigh-level features with respect to the class labels. Although our Gini loss\ninduces highly-discriminative features, it does not ensure that the\ndistribution of the high-level features matches the distribution of the\nclasses. As such, we introduce another loss term to minimize the\nKullback-Leibler divergence between the two distributions. We conduct\nexperiments on two image classification data sets (CIFAR-100 and Caltech 101),\nconsidering multiple neural architectures ranging from convolutional networks\n(ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results\nshow that integrating our novel loss terms into the training objective\nconsistently outperforms the models trained with cross-entropy alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigore_D/0/1/0/all/0/1\">Diana-Nicoleta Grigore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaze-Guided Class Activation Mapping: Leveraging Human Attention for Network Attention in Chest X-rays Classification. (arXiv:2202.07107v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07107","description":"<p>The increased availability and accuracy of eye-gaze tracking technology has\nsparked attention-related research in psychology, neuroscience, and, more\nrecently, computer vision and artificial intelligence. The attention mechanism\nin artificial neural networks is known to improve learning tasks. However, no\nprevious research has combined the network attention and human attention. This\npaper describes a gaze-guided class activation mapping (GG-CAM) method to\ndirectly regulate the formation of network attention based on expert\nradiologists' visual attention for the chest X-ray pathology classification\nproblem, which remains challenging due to the complex and often nuanced\ndifferences among images. GG-CAM is a lightweight ($3$ additional trainable\nparameters for regulating the learning process) and generic extension that can\nbe easily applied to most classification convolutional neural networks (CNN).\nGG-CAM-modified CNNs do not require human attention as an input when fully\ntrained. Comparative experiments suggest that two standard CNNs with the GG-CAM\nextension achieve significantly greater classification performance. The median\narea under the curve (AUC) metrics for ResNet50 increases from $0.721$ to\n$0.776$. For EfficientNetv2 (s), the median AUC increases from $0.723$ to\n$0.801$. The GG-CAM also brings better interpretability of the network that\nfacilitates the weakly-supervised pathology localization and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hongzhi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salcudean_S/0/1/0/all/0/1\">Septimiu Salcudean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohling_R/0/1/0/all/0/1\">Robert Rohling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task UNet: Jointly Boosting Saliency Prediction and Disease Classification on Chest X-ray Images. (arXiv:2202.07118v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07118","description":"<p>Human visual attention has recently shown its distinct capability in boosting\nmachine learning models. However, studies that aim to facilitate medical tasks\nwith human visual attention are still scarce. To support the use of visual\nattention, this paper describes a novel deep learning model for visual saliency\nprediction on chest X-ray (CXR) images. To cope with data deficiency, we\nexploit the multi-task learning method and tackles disease classification on\nCXR simultaneously. For a more robust training process, we propose a further\noptimized multi-task learning scheme to better handle model overfitting.\nExperiments show our proposed deep learning model with our new learning scheme\ncan outperform existing methods dedicated either for saliency prediction or\nimage classification. The code used in this paper is available at\nhttps://github.com/hz-zhu/MT-UNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hongzhi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohling_R/0/1/0/all/0/1\">Robert Rohling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salcudean_S/0/1/0/all/0/1\">Septimiu Salcudean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework. (arXiv:2202.07123v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07123","description":"<p>Point cloud analysis is challenging due to irregularity and unordered data\nstructure. To capture the 3D geometries, prior works mainly rely on exploring\nsophisticated local geometric extractors using convolution, graph, or attention\nmechanisms. These methods, however, incur unfavorable latency during inference,\nand the performance saturates over the past few years. In this paper, we\npresent a novel perspective on this task. We notice that detailed local\ngeometrical information probably is not the key to point cloud analysis -- we\nintroduce a pure residual MLP network, called PointMLP, which integrates no\nsophisticated local geometrical extractors but still performs very\ncompetitively. Equipped with a proposed lightweight geometric affine module,\nPointMLP delivers the new state-of-the-art on multiple datasets. On the\nreal-world ScanObjectNN dataset, our method even surpasses the prior best\nmethod by 3.3% accuracy. We emphasize that PointMLP achieves this strong\nperformance without any sophisticated operations, hence leading to a superior\ninference speed. Compared to most recent CurveNet, PointMLP trains 2x faster,\ntests 7x faster, and is more accurate on ModelNet40 benchmark. We hope our\nPointMLP may help the community towards a better understanding of point cloud\nanalysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving. (arXiv:2202.07133v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07133","description":"<p>While supervised detection and classification frameworks in autonomous\ndriving require large labelled datasets to converge, Unsupervised Domain\nAdaptation (UDA) approaches, facilitated by synthetic data generated from\nphoto-real simulated environments, are considered low-cost and less\ntime-consuming solutions. In this paper, we propose UDA schemes using\nadversarial discriminative and generative methods for lane detection and\nclassification applications in autonomous driving. We also present Simulanes\ndataset generator to create a synthetic dataset that is naturalistic utilizing\nCARLA's vast traffic scenarios and weather conditions. The proposed UDA\nframeworks take the synthesized dataset with labels as the source domain,\nwhereas the target domain is the unlabelled real-world data. Using adversarial\ngenerative and feature discriminators, the learnt models are tuned to predict\nthe lane location and class in the target domain. The proposed techniques are\nevaluated using both real-world and our synthetic datasets. The results\nmanifest that the proposed methods have shown superiority over other baseline\nschemes in terms of detection and classification accuracy and consistency. The\nablation study reveals that the size of the simulation dataset plays important\nroles in the classification performance of the proposed methods. Our UDA\nframeworks are available at https://github.com/anita-hu/sim2real-lane-detection\nand our dataset generator is released at https://github.com/anita-hu/simulanes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_S/0/1/0/all/0/1\">Sinclair Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethier_M/0/1/0/all/0/1\">Martin Ethier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1\">Mohammad Al-Sharman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1\">Derek Rayside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1\">William Melek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07135","description":"<p>Visual scene representation learning is an important research problem in the\nfield of computer vision. The performance on vision tasks could be improved if\nmore suitable representations are learned for visual scenes. Complex visual\nscenes are the composition of relatively simple visual concepts, and have the\nproperty of combinatorial explosion. Compared with directly representing the\nentire visual scene, extracting compositional scene representations can better\ncope with the diverse combination of background and objects. Because\ncompositional scene representations abstract the concept of objects, performing\nvisual scene analysis and understanding based on these representations could be\neasier and more interpretable. Moreover, learning compositional scene\nrepresentations via reconstruction can greatly reduce the need for training\ndata annotations. Therefore, compositional scene representation learning via\nreconstruction has important research significance. In this survey, we first\ndiscuss representative methods that either learn from a single viewpoint or\nmultiple viewpoints without object-level supervision, then the applications of\ncompositional scene representations, and finally the future directions on this\ntopic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tonglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Pseudo Labeling in Self-Training. (arXiv:2202.07136v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07136","description":"<p>Deep neural networks achieve remarkable performances on a wide range of tasks\nwith the aid of large-scale labeled datasets. However, large-scale annotations\nare time-consuming and labor-exhaustive to obtain on realistic tasks. To\nmitigate the requirement for labeled data, self-training is widely used in both\nacademia and industry by pseudo labeling on readily-available unlabeled data.\nDespite its popularity, pseudo labeling is well-believed to be unreliable and\noften leads to training instability. Our experimental studies further reveal\nthat the performance of self-training is biased due to data sampling,\npre-trained models, and training strategies, especially the inappropriate\nutilization of pseudo labels. To this end, we propose Debiased, in which the\ngeneration and utilization of pseudo labels are decoupled by two independent\nheads. To further improve the quality of pseudo labels, we introduce a\nworst-case estimation of pseudo labeling and seamlessly optimize the\nrepresentations to avoid the worst-case. Extensive experiments justify that the\nproposed Debiased not only yields an average improvement of $14.4$\\% against\nstate-of-the-art algorithms on $11$ tasks (covering generic object recognition,\nfine-grained object recognition, texture classification, and scene\nclassification) but also helps stabilize training and balance performance\nacross classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-generated Faces Detection: A Survey and New Perspectives. (arXiv:2202.07145v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07145","description":"<p>Generative Adversarial Networks (GAN) have led to the generation of very\nrealistic face images, which have been used in fake social media accounts and\nother disinformation matters that can generate profound impacts. Therefore, the\ncorresponding GAN-face detection techniques are under active development that\ncan examine and expose such fake faces. In this work, we aim to provide a\ncomprehensive review of recent progress in GAN-face detection. We focus on\nmethods that can detect face images that are generated or synthesized from GAN\nmodels. We classify the existing detection works into four categories: (1) deep\nlearning-based, (2) physical-based, (3) physiological-based methods, and (4)\nevaluation and comparison against human visual performance. For each category,\nwe summarize the key ideas and connect them with method implementations. We\nalso discuss open problems and suggest future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To what extent can Plug-and-Play methods outperform neural networks alone in low-dose CT reconstruction. (arXiv:2202.07173v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07173","description":"<p>The Plug-and-Play (PnP) framework was recently introduced for low-dose CT\nreconstruction to leverage the interpretability and the flexibility of\nmodel-based methods to incorporate various plugins, such as trained deep\nlearning (DL) neural networks. However, the benefits of PnP vs.\nstate-of-the-art DL methods have not been clearly demonstrated. In this work,\nwe proposed an improved PnP framework to address the previous limitations and\ndevelop clinical-relevant segmentation metrics for quantitative result\nassessment. Compared with the DL alone methods, our proposed PnP framework was\nslightly inferior in MSE and PSNR. However, the power spectrum of the resulting\nimages better matched that of full-dose images than that of DL denoised images.\nThe resulting images supported higher accuracy in airway segmentation than DL\ndenoised images for all the ten patients in the test set, more substantially on\nthe airways with a cross-section smaller than 0.61cm$^2$, and outperformed the\nDL denoised images for 45 out of 50 lung lobes in lobar segmentation. Our PnP\nmethod proved to be significantly better at preserving the image texture, which\ntranslated to task-specific benefits in automated structure segmentation and\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qifan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qihui Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_D/0/1/0/all/0/1\">Dan Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_K/0/1/0/all/0/1\">Ke Sheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Neural Trojan Attacks and Defenses in Deep Learning. (arXiv:2202.07183v1 [cs.CR])","link":"http://arxiv.org/abs/2202.07183","description":"<p>Artificial Intelligence (AI) relies heavily on deep learning - a technology\nthat is becoming increasingly popular in real-life applications of AI, even in\nthe safety-critical and high-risk domains. However, it is recently discovered\nthat deep learning can be manipulated by embedding Trojans inside it.\nUnfortunately, pragmatic solutions to circumvent the computational requirements\nof deep learning, e.g. outsourcing model training or data annotation to third\nparties, further add to model susceptibility to the Trojan attacks. Due to the\nkey importance of the topic in deep learning, recent literature has seen many\ncontributions in this direction. We conduct a comprehensive review of the\ntechniques that devise Trojan attacks for deep learning and explore their\ndefenses. Our informative survey systematically organizes the recent literature\nand discusses the key concepts of the methods while assuming minimal knowledge\nof the domain on the readers part. It provides a comprehensible gateway to the\nbroader community to understand the recent developments in Neural Trojans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1\">Ghulam Mubashar Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Networks with Cross-Layer Ranking & k-Reciprocal Nearest Filters. (arXiv:2202.07190v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07190","description":"<p>This paper focuses on filter-level network pruning. A novel pruning method,\ntermed CLR-RNF, is proposed. We first reveal a \"long-tail\" long-tail pruning\nproblem in magnitude-based weight pruning methods, and then propose a\ncomputation-aware measurement for individual weight importance, followed by a\nCross-Layer Ranking (CLR) of weights to identify and remove the bottom-ranked\nweights. Consequently, the per-layer sparsity makes up of the pruned network\nstructure in our filter pruning. Then, we introduce a recommendation-based\nfilter selection scheme where each filter recommends a group of its closest\nfilters. To pick the preserved filters from these recommended groups, we\nfurther devise a k-Reciprocal Nearest Filter (RNF) selection scheme where the\nselected filters fall into the intersection of these recommended groups. Both\nour pruned network structure and the filter selection are non-learning\nprocesses, which thus significantly reduce the pruning complexity, and\ndifferentiate our method from existing works. We conduct image classification\non CIFAR-10 and ImageNet to demonstrate the superiority of our CLR-RNF over the\nstate-of-the-arts. For example, on CIFAR-10, CLR-RNF removes 74.1% FLOPs and\n95.0% parameters from VGGNet-16 with even 0.3\\% accuracy improvements. On\nImageNet, it removes 70.2% FLOPs and 64.8% parameters from ResNet-50 with only\n1.7% top-5 accuracy drops. Our project is at https://github.com/lmbxmu/CLR-RNF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07191","description":"<p>With rising male infertility, sperm head morphology classification becomes\ncritical for accurate and timely clinical diagnosis. Recent deep learning (DL)\nmorphology analysis methods achieve promising benchmark results, but leave\nperformance and robustness on the table by relying on limited and possibly\nnoisy class labels. To address this, we introduce a new DL training framework\nthat leverages anatomical and image priors from human sperm microscopy crops to\nextract useful features without additional labeling cost. Our core idea is to\ndistill sperm head information with reliably-generated pseudo-masks and\nunsupervised spatial prediction tasks. The predicted foreground masks from this\ndistillation step are then leveraged to regularize and reduce image and label\nnoise in the tuning stage. We evaluate our new approach on two public sperm\ndatasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy\nand 96.5% HuSHeM accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xiaomin Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunxia Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Domain Experts for Long-Tailed Camera-Trap Recognition. (arXiv:2202.07215v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07215","description":"<p>Label distributions in camera-trap images are highly imbalanced and\nlong-tailed, resulting in neural networks tending to be biased towards\nhead-classes that appear frequently. Although long-tail learning has been\nextremely explored to address data imbalances, few studies have been conducted\nto consider camera-trap characteristics, such as multi-domain and multi-frame\nsetup. Here, we propose a unified framework and introduce two datasets for\nlong-tailed camera-trap recognition. We first design domain experts, where each\nexpert learns to balance imperfect decision boundaries caused by data\nimbalances and complement each other to generate domain-balanced decision\nboundaries. Also, we propose a flow consistency loss to focus on moving\nobjects, expecting class activation maps of multi-frame matches the flow with\noptical flow maps for input images. Moreover, two long-tailed camera-trap\ndatasets, WCS-LT and DMZ-LT, are introduced to validate our methods.\nExperimental results show the effectiveness of our framework, and proposed\nmethods outperform previous methods on recessive domain samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Byeongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seungju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship to Generalize Human Pose and Mesh Reconstruction for In-the-Wild Scenes. (arXiv:2202.07228v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07228","description":"<p>We present MeshLeTemp, a powerful method for 3D human pose and mesh\nreconstruction from a single image. In terms of human body priors encoding, we\npropose using a learnable template human mesh instead of a constant template\nutilized by previous state-of-the-art methods. The proposed learnable template\nreflects not only vertex-vertex interactions but also the human pose and body\nshape, being able to adapt to diverse images. We also introduce a strategy to\nenrich the training data that contains both 2D and 3D annotations. We conduct\nextensive experiments to show the generalizability of our method and the\neffectiveness of our data strategy. As one of our ablation studies, we adapt\nMeshLeTemp to another domain which is 3D hand reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_C/0/1/0/all/0/1\">Cuong C. Than</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot semantic segmentation via mask aggregation. (arXiv:2202.07231v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07231","description":"<p>Few-shot semantic segmentation aims to recognize novel classes with only very\nfew labelled data. This challenging task requires mining of the relevant\nrelationships between the query image and the support images. Previous works\nhave typically regarded it as a pixel-wise classification problem. Therefore,\nvarious models have been designed to explore the correlation of pixels between\nthe query image and the support images. However, they focus only on pixel-wise\ncorrespondence and ignore the overall correlation of objects. In this paper, we\nintroduce a mask-based classification method for addressing this problem. The\nmask aggregation network (MANet), which is a simple mask classification model,\nis proposed to simultaneously generate a fixed number of masks and their\nprobabilities of being targets. Then, the final segmentation result is obtained\nby aggregating all the masks according to their locations. Experiments on both\nthe PASCAL-5^i and COCO-20^i datasets show that our method performs comparably\nto the state-of-the-art pixel-based methods. This competitive performance\ndemonstrates the potential of mask classification as an alternative baseline\nmethod in few-shot semantic segmentation. Our source code will be made\navailable at https://github.com/TinyAway/MANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ao_W/0/1/0/all/0/1\">Wei Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yan Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search for Dense Prediction Tasks in Computer Vision. (arXiv:2202.07242v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07242","description":"<p>The success of deep learning in recent years has lead to a rising demand for\nneural network architecture engineering. As a consequence, neural architecture\nsearch (NAS), which aims at automatically designing neural network\narchitectures in a data-driven manner rather than manually, has evolved as a\npopular field of research. With the advent of weight sharing strategies across\narchitectures, NAS has become applicable to a much wider range of problems. In\nparticular, there are now many publications for dense prediction tasks in\ncomputer vision that require pixel-level predictions, such as semantic\nsegmentation or object detection. These tasks come with novel challenges, such\nas higher memory footprints due to high-resolution data, learning multi-scale\nrepresentations, longer training times, and more complex and larger neural\narchitectures. In this manuscript, we provide an overview of NAS for dense\nprediction tasks by elaborating on these novel challenges and surveying ways to\naddress them to ease future research and application of existing methods to\nnovel problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1\">Thomas Elsken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1\">Arber Zela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1\">Benedikt Staffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval. (arXiv:2202.07247v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07247","description":"<p>We introduce CommerceMM - a multimodal model capable of providing a diverse\nand granular understanding of commerce topics associated to the given piece of\ncontent (image, text, image+text), and having the capability to generalize to a\nwide range of tasks, including Multimodal Categorization, Image-Text Retrieval,\nQuery-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the\npre-training + fine-tuning training regime and present 5 effective pre-training\ntasks on image-text pairs. To embrace more common and diverse commerce data\nwith text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal\nmapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks,\ncalled Omni-Retrieval pre-training. The pre-training is conducted in an\nefficient manner with only two forward/backward updates for the combined 14\ntasks. Extensive experiments and analysis show the effectiveness of each task.\nWhen combining all pre-training tasks, our model achieves state-of-the-art\nperformance on 7 commerce-related downstream tasks after fine-tuning.\nAdditionally, we propose a novel approach of modality randomization to\ndynamically adjust our model under different efficiency constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Animesh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao MJ Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hugo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of the Fingerprint Liveness Detection (LivDet) competition series: from 2009 to 2021. (arXiv:2202.07259v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07259","description":"<p>Fingerprint authentication systems are highly vulnerable to artificial\nreproductions of fingerprint, called fingerprint presentation attacks.\nDetecting presentation attacks is not trivial because attackers refine their\nreplication techniques from year to year. The International Fingerprint\nliveness Detection Competition (LivDet), an open and well-acknowledged meeting\npoint of academies and private companies that deal with the problem of\npresentation attack detection, has the goal to assess the performance of\nfingerprint presentation attack detection (FPAD) algorithms by using standard\nexperimental protocols and data sets. Each LivDet edition, held biannually\nsince 2009, is characterized by a different set of challenges against which\ncompetitors must be dealt with. The continuous increase of competitors and the\nnoticeable decrease in error rates across competitions demonstrate a growing\ninterest in the topic. This paper reviews the LivDet editions from 2009 to 2021\nand points out their evolution over the years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micheletto_M/0/1/0/all/0/1\">Marco Micheletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orru_G/0/1/0/all/0/1\">Giulia Orr&#xf9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casula_R/0/1/0/all/0/1\">Roberto Casula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yambay_D/0/1/0/all/0/1\">David Yambay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcialis_G/0/1/0/all/0/1\">Gian Luca Marcialis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1\">Stephanie C. Schuckers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07261","description":"<p>3D dynamic point clouds provide a discrete representation of real-world\nobjects or scenes in motion, which have been widely applied in immersive\ntelepresence, autonomous driving, surveillance, \\textit{etc}. However, point\nclouds acquired from sensors are usually perturbed by noise, which affects\ndownstream tasks such as surface reconstruction and analysis. Although many\nefforts have been made for static point cloud denoising, few works address\ndynamic point cloud denoising. In this paper, we propose a novel gradient-based\ndynamic point cloud denoising method, exploiting the temporal correspondence\nfor the estimation of gradient fields -- also a fundamental problem in dynamic\npoint cloud processing and analysis. The gradient field is the gradient of the\nlog-probability function of the noisy point cloud, based on which we perform\ngradient ascent so as to converge each point to the underlying clean surface.\nWe estimate the gradient of each surface patch by exploiting the temporal\ncorrespondence, where the temporally corresponding patches are searched\nleveraging on rigid motion in classical mechanics. In particular, we treat each\npatch as a rigid object, which moves in the gradient field of an adjacent frame\nvia force until reaching a balanced state, i.e., when the sum of gradients over\nthe patch reaches 0. Since the gradient would be smaller when the point is\ncloser to the underlying surface, the balanced patch would fit the underlying\nsurface well, thus leading to the temporal correspondence. Finally, the\nposition of each point in the patch is updated along the direction of the\ngradient averaged from corresponding patches in adjacent frames. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qianjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-relationship Learning Network for Scene Graph Generation. (arXiv:2202.07271v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07271","description":"<p>Generating informative scene graphs from images requires integrating and\nreasoning from various graph components, i.e., objects and relationships.\nHowever, current scene graph generation (SGG) methods, including the unbiased\nSGG methods, still struggle to predict informative relationships due to the\nlack of 1) high-level inference such as transitive inference between\nrelationships and 2) efficient mechanisms that can incorporate all interactions\nof graph components. To address the issues mentioned above, we devise a\nhyper-relationship learning network, termed HLN, for SGG. Specifically, the\nproposed HLN stems from hypergraphs and two graph attention networks (GATs) are\ndesigned to infer relationships: 1) the object-relationship GAT or OR-GAT to\nexplore interactions between objects and relationships, and 2) the\nhyper-relationship GAT or HR-GAT to integrate transitive inference of\nhyper-relationships, i.e., the sequential relationships between three objects\nfor transitive reasoning. As a result, HLN significantly improves the\nperformance of scene graph generation by integrating and reasoning from object\ninteractions, relationship interactions, and transitive inference of\nhyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the\nVisual Genome dataset, and the experimental results demonstrate its great\nsuperiority over recent state-of-the-art methods. For example, the proposed HLN\nimproves the recall per relationship from 11.3\\% to 13.1\\%, and maintains the\nrecall per image from 19.8\\% to 34.9\\%. We will release the source code and\npretrained models on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">BaoSheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Natural Motion: Exploring Discontinuity for Video Frame Interpolation. (arXiv:2202.07291v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07291","description":"<p>Video interpolation is the task that synthesizes the intermediate frame given\ntwo consecutive frames. Most of the previous studies have focused on\nappropriate frame warping operations and refinement modules for the warped\nframes. These studies have been conducted on natural videos having only\ncontinuous motions. However, many practical videos contain a lot of\ndiscontinuous motions, such as chat windows, watermarks, GUI elements, or\nsubtitles. We propose three techniques to expand the concept of transition\nbetween two consecutive frames to address these issues. First is a new\narchitecture that can separate continuous and discontinuous motion areas. We\nalso propose a novel data augmentation strategy called figure-text mixing (FTM)\nto make our model learn more general scenarios. Finally, we propose loss\nfunctions to give supervisions of the discontinuous motion areas with the data\naugmentation. We collected a special dataset consisting of some mobile games\nand chatting videos. We show that our method significantly improves the\ninterpolation qualities of the videos on the special dataset. Moreover, our\nmodel outperforms the state-of-the-art methods for natural video datasets\ncontaining only continuous motions, such as DAVIS and UCF101.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyeongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chajin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hanbin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. (arXiv:2202.07305v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07305","description":"<p>Image narrative generation describes the creation of stories regarding the\ncontent of image data from a subjective viewpoint. Given the importance of the\nsubjective feelings of writers, characters, and readers in storytelling, image\nnarrative generation methods must consider human emotion, which is their major\ndifference from descriptive caption generation tasks. The development of\nautomated methods to generate story-like text associated with images may be\nconsidered to be of considerable social significance, because stories serve\nessential functions both as entertainment and also for many practical purposes\nsuch as education and advertising. In this study, we propose a model called\nViNTER (Visual Narrative Transformer with Emotion arc Representation) to\ngenerate image narratives that focus on time series representing varying\nemotions as \"emotion arcs,\" to take advantage of recent advances in multimodal\nTransformer-based pre-trained models. We present experimental results of both\nmanual and automatic evaluations, which demonstrate the effectiveness of the\nproposed emotion-aware approach to image narrative generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_Y/0/1/0/all/0/1\">Yusuke Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAA4D: Few-Shot Human Atomic Action Recognition via 3D Spatio-Temporal Skeletal Alignment. (arXiv:2202.07308v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07308","description":"<p>Human actions involve complex pose variations and their 2D projections can be\nhighly ambiguous. Thus 3D spatio-temporal or 4D (i.e., 3D+T) human skeletons,\nwhich are photometric and viewpoint invariant, are an excellent alternative to\n2D+T skeletons/pixels to improve action recognition accuracy. This paper\nproposes a new 4D dataset HAA4D which consists of more than 3,300 RGB videos in\n300 human atomic action classes. HAA4D is clean, diverse, class-balanced where\neach class is viewpoint-balanced with the use of 4D skeletons, in which as few\nas one 4D skeleton per class is sufficient for training a deep recognition\nmodel. Further, the choice of atomic actions makes annotation even easier,\nbecause each video clip lasts for only a few seconds. All training and testing\n3D skeletons in HAA4D are globally aligned, using a deep alignment model to the\nsame global space, making each skeleton face the negative z-direction. Such\nalignment makes matching skeletons more stable by reducing intraclass\nvariations and thus with fewer training samples per class needed for action\nrecognition. Given the high diversity and skeletal alignment in HAA4D, we\nconstruct the first baseline few-shot 4D human atomic action recognition\nnetwork without bells and whistles, which produces comparable or higher\nperformance than relevant state-of-the-art techniques relying on embedded space\nencoding without explicit skeletal alignment, using the same small number of\ntraining samples of unseen classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_M/0/1/0/all/0/1\">Mu-Ruei Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Social Media Images for Building Function Classification. (arXiv:2202.07315v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07315","description":"<p>Urban land use on a building instance level is crucial geo-information for\nmany applications, yet difficult to obtain. An intuitive approach to close this\ngap is predicting building functions from ground level imagery. Social media\nimage platforms contain billions of images, with a large variety of motifs\nincluding but not limited to street perspectives. To cope with this issue this\nstudy proposes a filtering pipeline to yield high quality, ground level imagery\nfrom large social media image datasets. The pipeline ensures that all resulting\nimages have full and valid geotags with a compass direction to relate image\ncontent and spatial objects from maps.\n</p>\n<p>We analyze our method on a culturally diverse social media dataset from\nFlickr with more than 28 million images from 42 cities around the world. The\nobtained dataset is then evaluated in a context of 3-classes building function\nclassification task. The three building classes that are considered in this\nstudy are: commercial, residential, and other. Fine-tuned state-of-the-art\narchitectures yield F1-scores of up to 0.51 on the filtered images. Our\nanalysis shows that the performance is highly limited by the quality of the\nlabels obtained from OpenStreetMap, as the metrics increase by 0.2 if only\nhuman validated labels are considered. Therefore, we consider these labels to\nbe weak and publish the resulting images from our pipeline together with the\nbuildings they are showing as a weakly labeled dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_E/0/1/0/all/0/1\">Eike Jens Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulahhad_K/0/1/0/all/0/1\">Karam Abdulahhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Masked and Mask-Free Face Recognition via Feature Rectification. (arXiv:2202.07358v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07358","description":"<p>Face recognition under ideal conditions is now considered a well-solved\nproblem with advances in deep learning. Recognizing faces under occlusion,\nhowever, still remains a challenge. Existing techniques often fail to recognize\nfaces with both the mouth and nose covered by a mask, which is now very common\nunder the COVID-19 pandemic. Common approaches to tackle this problem include\n1) discarding information from the masked regions during recognition and 2)\nrestoring the masked regions before recognition. Very few works considered the\nconsistency between features extracted from masked faces and from their\nmask-free counterparts. This resulted in models trained for recognizing masked\nfaces often showing degraded performance on mask-free faces. In this paper, we\npropose a unified framework, named Face Feature Rectification Network\n(FFR-Net), for recognizing both masked and mask-free faces alike. We introduce\nrectification blocks to rectify features extracted by a state-of-the-art\nrecognition model, in both spatial and channel dimensions, to minimize the\ndistance between a masked face and its mask-free counterpart in the rectified\nfeature space. Experiments show that our unified framework can learn a\nrectified feature space for recognizing both masked and mask-free faces\neffectively, achieving state-of-the-art results. Project code:\nhttps://github.com/haoosz/FFR-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shaozhe Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle. (arXiv:2202.07360v1 [cs.HC])","link":"http://arxiv.org/abs/2202.07360","description":"<p>Advanced in-cabin sensing technologies, especially vision based approaches,\nhave tremendously progressed user interaction inside the vehicle, paving the\nway for new applications of natural user interaction. Just as humans use\nmultiple modes to communicate with each other, we follow an approach which is\ncharacterized by simultaneously using multiple modalities to achieve natural\nhuman-machine interaction for a specific task: pointing to or glancing towards\nobjects inside as well as outside the vehicle for deictic references. By\ntracking the movements of eye-gaze, head and finger, we design a multimodal\nfusion architecture using a deep neural network to precisely identify the\ndriver's referencing intent. Additionally, we use a speech command as a trigger\nto separate each referencing event. We observe differences in driver behavior\nin the two pointing use cases (i.e. for inside and outside objects), especially\nwhen analyzing the preciseness of the three modalities eye, head, and finger.\nWe conclude that there is no single modality that is solely optimal for all\ncases as each modality reveals certain limitations. Fusion of multiple\nmodalities exploits the relevant characteristics of each modality, hence\novercoming the case dependent limitations of each individual modality.\nUltimately, we propose a method to identity whether the driver's referenced\nobject lies inside or outside the vehicle, based on the predicted pointing\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1\">Abdul Rafey Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1\">Michael von der Beeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Anomaly Detection on X-ray Images of Fuel Cell Electrodes. (arXiv:2202.07361v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07361","description":"<p>Anomaly detection in X-ray images has been an active and lasting research\narea in the last decades, especially in the domain of medical X-ray images. For\nthis work, we created a real-world labeled anomaly dataset, consisting of\n16-bit X-ray image data of fuel cell electrodes coated with a platinum catalyst\nsolution and perform anomaly detection on the dataset using a deep learning\napproach. The dataset contains a diverse set of anomalies with 11 identified\ncommon anomalies where the electrodes contain e.g. scratches, bubbles, smudges\netc. We experiment with 16-bit image to 8-bit image conversion methods to\nutilize pre-trained Convolutional Neural Networks as feature extractors\n(transfer learning) and find that we achieve the best performance by maximizing\nthe contrasts globally across the dataset during the 16-bit to 8-bit\nconversion, through histogram equalization. We group the fuel cell electrodes\nwith anomalies into a single class called abnormal and the normal fuel cell\nelectrodes into a class called normal, thereby abstracting the anomaly\ndetection problem into a binary classification problem. We achieve a balanced\naccuracy of 85.18\\%. The anomaly detection is used by the company, Serenergy,\nfor optimizing the time spend on the quality control of the fuel cell\nelectrodes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jensen_S/0/1/0/all/0/1\">Simon B. Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreasen_S/0/1/0/all/0/1\">S&#xf8;ren J. Andreasen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SODAR: Segmenting Objects by DynamicallyAggregating Neighboring Mask Representations. (arXiv:2202.07402v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07402","description":"<p>Recent state-of-the-art one-stage instance segmentation model SOLO divides\nthe input image into a grid and directly predicts per grid cell object masks\nwith fully-convolutional networks, yielding comparably good performance as\ntraditional two-stage Mask R-CNN yet enjoying much simpler architecture and\nhigher efficiency. We observe SOLO generates similar masks for an object at\nnearby grid cells, and these neighboring predictions can complement each other\nas some may better segment certain object part, most of which are however\ndirectly discarded by non-maximum-suppression. Motivated by the observed gap,\nwe develop a novel learning-based aggregation method that improves upon SOLO by\nleveraging the rich neighboring information while maintaining the architectural\nefficiency. The resulting model is named SODAR. Unlike the original per grid\ncell object masks, SODAR is implicitly supervised to learn mask representations\nthat encode geometric structure of nearby objects and complement adjacent\nrepresentations with context. The aggregation method further includes two novel\ndesigns: 1) a mask interpolation mechanism that enables the model to generate\nmuch fewer mask representations by sharing neighboring representations among\nnearby grid cells, and thus saves computation and memory; 2) a deformable\nneighbour sampling mechanism that allows the model to adaptively adjust\nneighbor sampling locations thus gathering mask representations with more\nrelevant context and achieving higher performance. SODAR significantly improves\nthe instance segmentation performance, e.g., it outperforms a SOLO model with\nResNet-101 backbone by 2.2 AP on COCO \\texttt{test} set, with only about 3\\%\nadditional computation. We further show consistent performance gain with the\nSOLOv2 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1\">Jun Hao Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable COVID-19 Infections Identification and Delineation Using Calibrated Pseudo Labels. (arXiv:2202.07422v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07422","description":"<p>The upheaval brought by the arrival of the COVID-19 pandemic has continued to\nbring fresh challenges over the past two years. During this COVID-19 pandemic,\nthere has been a need for rapid identification of infected patients and\nspecific delineation of infection areas in computed tomography (CT) images.\nAlthough deep supervised learning methods have been established quickly, the\nscarcity of both image-level and pixellevel labels as well as the lack of\nexplainable transparency still hinder the applicability of AI. Can we identify\ninfected patients and delineate the infections with extreme minimal\nsupervision? Semi-supervised learning (SSL) has demonstrated promising\nperformance under limited labelled data and sufficient unlabelled data.\nInspired by SSL, we propose a model-agnostic calibrated pseudo-labelling\nstrategy and apply it under a consistency regularization framework to generate\nexplainable identification and delineation results. We demonstrate the\neffectiveness of our model with the combination of limited labelled data and\nsufficient unlabelled data or weakly-labelled data. Extensive experiments have\nshown that our model can efficiently utilize limited labelled data and provide\nexplainable classification and segmentation results for decision-making in\nclinical routine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Onuorah_C/0/1/0/all/0/1\">Chibudom Onuorah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1\">Simon Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A precortical module for robust CNNs to light variations. (arXiv:2202.07432v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07432","description":"<p>We present a simple mathematical model for the mammalian low visual pathway,\ntaking into account its key elements: retina, lateral geniculate nucleus (LGN),\nprimary visual cortex (V1). The analogies between the cortical level of the\nvisual system and the structure of popular CNNs, used in image classification\ntasks, suggests the introduction of an additional preliminary convolutional\nmodule inspired to precortical neuronal circuits to improve robustness with\nrespect to global light intensity and contrast variations in the input images.\nWe validate our hypothesis on the popular databases MNIST, FashionMNIST and\nSVHN, obtaining significantly more robust CNNs with respect to these\nvariations, once such extra module is added.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fioresi_R/0/1/0/all/0/1\">R. Fioresi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_J/0/1/0/all/0/1\">J. Petkovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical Cookbook for Snapshot Compressive Imaging. (arXiv:2202.07437v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07437","description":"<p>The author intends to provide you with a beautiful, elegant, user-friendly\ncookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the\ncookbook is composed of introduction and conventional optimization, using\nregularization-based optimization algorithms for SCI. The latest releases are\nstrongly recommended! For any other questions, suggestions, or comments, feel\nfree to email the author.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Analysis Framework for Trajectory Datasets. (arXiv:2202.07438v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07438","description":"<p>Trajectory datasets of road users have become more important in the last\nyears for safety validation of highly automated vehicles. Several naturalistic\ntrajectory datasets with each more than 10.000 tracks were released and others\nwill follow. Considering this amount of data, it is necessary to be able to\ncompare these datasets in-depth with ease to get an overview. By now, the\ndatasets' own provided information is mainly limited to meta-data and\nqualitative descriptions which are mostly not consistent with other datasets.\nThis is insufficient for users to differentiate the emerging datasets for\napplication-specific selection. Therefore, an automated analysis framework is\nproposed in this work. Starting with analyzing individual tracks, fourteen\nelementary characteristics, so-called detection types, are derived and used as\nthe base of this framework. To describe each traffic scenario precisely, the\ndetections are subdivided into common metrics, clustering methods and anomaly\ndetection. Those are combined using a modular approach. The detections are\ncomposed into new scores to describe three defined attributes of each track\ndata quantitatively: interaction, anomaly and relevance. These three scores are\ncalculated hierarchically for different abstract layers to provide an overview\nnot just between datasets but also for tracks, spatial regions and individual\nsituations. So, an objective comparison between datasets can be realized.\nFurthermore, it can help to get a deeper understanding of the recorded\ninfrastructure and its effect on road user behavior. To test the validity of\nthe framework, a study is conducted to compare the scores with human\nperception. Additionally, several datasets are compared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glasmacher_C/0/1/0/all/0/1\">Christoph Glasmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krajewski_R/0/1/0/all/0/1\">Robert Krajewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_L/0/1/0/all/0/1\">Lutz Eckstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Walks for Adversarial Meshes. (arXiv:2202.07453v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07453","description":"<p>A polygonal mesh is the most-commonly used representation of surfaces in\ncomputer graphics; thus, a variety of classification networks have been\nrecently proposed. However, while adversarial attacks are wildly researched in\n2D, almost no works on adversarial meshes exist. This paper proposes a novel,\nunified, and general adversarial attack, which leads to misclassification of\nnumerous state-of-the-art mesh classification neural networks. Our attack\napproach is black-box, i.e. it has access only to the network's predictions,\nbut not to the network's full architecture or gradients. The key idea is to\ntrain a network to imitate a given classification network. This is done by\nutilizing random walks along the mesh surface, which gather geometric\ninformation. These walks provide insight onto the regions of the mesh that are\nimportant for the correct prediction of the given classification network. These\nmesh regions are then modified more than other regions in order to attack the\nnetwork in a manner that is barely visible to the naked eye.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belder_A/0/1/0/all/0/1\">Amir Belder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yefet_G/0/1/0/all/0/1\">Gal Yefet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izhak_R/0/1/0/all/0/1\">Ran Ben Izhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1\">Ayellet Tal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Image Deblurring. (arXiv:2202.07456v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07456","description":"<p>With the improvement of social life quality and the real needs of daily work,\nimages are more and more all around us. Image blurring due to camera shake,\nhuman movement, etc. has become the key to affecting image quality. How to\nremove image blur and restore clear image has gradually become an important\nresearch direction in the field of computer vision. After more than half a\ncentury of unremitting efforts, the majority of scientific and technological\nworkers have made fruitful progress in image deblurring. This article reviews\nthe work of image deblurring and specifically introduces more classic image\ndeblurring methods, which is helpful to understand current research and look\nforward to future trends. This article reviews the traditional image deblurring\nmethods and depth-represented image deblurring methods, and comprehensively\nclassifies and introduces the corresponding technical methods. This review can\nprovide some guidance for researchers in the field of image deblurring, and at\nthe same time facilitate their subsequent study and research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">ChuMiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSensor: Deep Learning Testing Framework Based on Neuron Sensitivity. (arXiv:2202.07464v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07464","description":"<p>Despite impressive capabilities and outstanding performance, deep neural\nnetwork(DNN) has captured increasing public concern for its security problem,\ndue to frequent occurrence of erroneous behaviors. Therefore, it is necessary\nto conduct systematically testing before its deployment to real-world\napplications. Existing testing methods have provided fine-grained criteria\nbased on neuron coverage and reached high exploratory degree of testing. But\nthere is still a gap between the neuron coverage and model's robustness\nevaluation. To bridge the gap, we observed that neurons which change the\nactivation value dramatically due to minor perturbation are prone to trigger\nincorrect corner cases. Motivated by it, we propose neuron sensitivity and\ndevelop a novel white-box testing framework for DNN, donated as DeepSensor. The\nnumber of sensitive neurons is maximized by particle swarm optimization, thus\ndiverse corner cases could be triggered and neuron coverage be further improved\nwhen compared with baselines. Besides, considerable robustness enhancement can\nbe reached when adopting testing examples based on neuron sensitivity for\nretraining. Extensive experiments implemented on scalable datasets and models\ncan well demonstrate the testing effectiveness and robustness improvement of\nDeepSensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Contrastive Learning for Dermatological Disease Diagnosis via On-device Learning. (arXiv:2202.07470v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07470","description":"<p>Deep learning models have been deployed in an increasing number of edge and\nmobile devices to provide healthcare. These models rely on training with a\ntremendous amount of labeled data to achieve high accuracy. However, for\nmedical applications such as dermatological disease diagnosis, the private data\ncollected by mobile dermatology assistants exist on distributed mobile devices\nof patients, and each device only has a limited amount of data. Directly\nlearning from limited data greatly deteriorates the performance of learned\nmodels. Federated learning (FL) can train models by using data distributed on\ndevices while keeping the data local for privacy. Existing works on FL assume\nall the data have ground-truth labels. However, medical data often comes\nwithout any accompanying labels since labeling requires expertise and results\nin prohibitively high labor costs. The recently developed self-supervised\nlearning approach, contrastive learning (CL), can leverage the unlabeled data\nto pre-train a model, after which the model is fine-tuned on limited labeled\ndata for dermatological disease diagnosis. However, simply combining CL with FL\nas federated contrastive learning (FCL) will result in ineffective learning\nsince CL requires diverse data for learning but each device only has limited\ndata. In this work, we propose an on-device FCL framework for dermatological\ndisease diagnosis with limited labels. Features are shared in the FCL\npre-training process to provide diverse and accurate contrastive information.\nAfter that, the pre-trained model is fine-tuned with local labeled data\nindependently on each device or collaboratively with supervised federated\nlearning on all devices. Experiments on dermatological disease datasets show\nthat the proposed framework effectively improves the recall and precision of\ndermatological disease diagnosis compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Yi Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1\">Alaina J. James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Lessons from Metric Learning Generalize to Image-Caption Retrieval?. (arXiv:2202.07474v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07474","description":"<p>The triplet loss with semi-hard negatives has become the de facto choice for\nimage-caption retrieval (ICR) methods that are optimized from scratch. Recent\nprogress in metric learning has given rise to new loss functions that\noutperform the triplet loss on tasks such as image retrieval and representation\nlearning. We ask whether these findings generalize to the setting of ICR by\ncomparing three loss functions on two ICR methods. We answer this question\nnegatively: the triplet loss with semi-hard negative mining still outperforms\nnewly introduced loss functions from metric learning on the ICR task. To gain a\nbetter understanding of these outcomes, we introduce an analysis method to\ncompare loss functions by counting how many samples contribute to the gradient\nw.r.t. the query representation during optimization. We find that loss\nfunctions that result in lower evaluation scores on the ICR task, in general,\ntake too many (non-informative) samples into account when computing a gradient\nw.r.t. the query representation, which results in sub-optimal performance. The\ntriplet loss with semi-hard negatives is shown to outperform the other loss\nfunctions, as it only takes one (hard) negative into account when computing the\ngradient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-time System for Detecting Landslide Reports on Social Media using Artificial Intelligence. (arXiv:2202.07475v1 [cs.CY])","link":"http://arxiv.org/abs/2202.07475","description":"<p>This paper presents an online system that leverages social media data in real\ntime to identify landslide-related information automatically using\nstate-of-the-art artificial intelligence techniques. The designed system can\n(i) reduce the information overload by eliminating duplicate and irrelevant\ncontent, (ii) identify landslide images, (iii) infer geolocation of the images,\nand (iv) categorize the user type (organization or person) of the account\nsharing the information. The system was deployed in February 2020 online at\nhttps://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter\ndata stream and has been running continuously since then to provide\ntime-critical information to partners such as British Geological Survey and\nEuropean Mediterranean Seismological Centre. We trust this system can both\ncontribute to harvesting of global landslide data for further research and\nsupport global landslide maps to facilitate emergency response and decision\nmaking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qazi_U/0/1/0/all/0/1\">Umair Qazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roch_J/0/1/0/all/0/1\">Julien Roch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennington_C/0/1/0/all/0/1\">Catherine Pennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banks_V/0/1/0/all/0/1\">Vanessa Banks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bossu_R/0/1/0/all/0/1\">Remy Bossu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DualConv: Dual Convolutional Kernels for Lightweight Deep Neural Networks. (arXiv:2202.07481v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07481","description":"<p>CNN architectures are generally heavy on memory and computational\nrequirements which makes them infeasible for embedded systems with limited\nhardware resources. We propose dual convolutional kernels (DualConv) for\nconstructing lightweight deep neural networks. DualConv combines 3$\\times$3 and\n1$\\times$1 convolutional kernels to process the same input feature map channels\nsimultaneously and exploits the group convolution technique to efficiently\narrange convolutional filters. DualConv can be employed in any CNN model such\nas VGG-16 and ResNet-50 for image classification, YOLO and R-CNN for object\ndetection, or FCN for semantic segmentation. In this paper, we extensively test\nDualConv for classification since these network architectures form the\nbackbones for many other tasks. We also test DualConv for image detection on\nYOLO-V3. Experimental results show that, combined with our structural\ninnovations, DualConv significantly reduces the computational cost and number\nof parameters of deep neural networks while surprisingly achieving slightly\nhigher accuracy than the original models in some cases. We use DualConv to\nfurther reduce the number of parameters of the lightweight MobileNetV2 by 54%\nwith only 0.68% drop in accuracy on CIFAR-100 dataset. When the number of\nparameters is not an issue, DualConv increases the accuracy of MobileNetV1 by\n4.11% on the same dataset. Furthermore, DualConv significantly improves the\nYOLO-V3 object detection speed and improves its accuracy by 4.4% on PASCAL VOC\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jiachen Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texture Aware Autoencoder Pre-training And Pairwise Learning Refinement For Improved Iris Recognition. (arXiv:2202.07499v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07499","description":"<p>This paper presents a texture aware end-to-end trainable iris recognition\nsystem, specifically designed for datasets like iris having limited training\ndata. We build upon our previous stagewise learning framework with certain key\noptimization and architectural innovations. First, we pretrain a Stage-1\nencoder network with an unsupervised autoencoder learning optimized with an\nadditional data relation loss on top of usual reconstruction loss. The data\nrelation loss enables learning better texture representation which is pivotal\nfor a texture rich dataset such as iris. Robustness of Stage-1 feature\nrepresentation is further enhanced with an auxiliary denoising task. Such\npre-training proves beneficial for effectively training deep networks on data\nconstrained iris datasets. Next, in Stage-2 supervised refinement, we design a\npairwise learning architecture for an end-to-end trainable iris recognition\nsystem. The pairwise learning includes the task of iris matching inside the\ntraining pipeline itself and results in significant improvement in recognition\nperformance compared to usual offline matching. We validate our model across\nthree publicly available iris datasets and the proposed model consistently\noutperforms both traditional and deep learning baselines for both\nWithin-Dataset and Cross-Dataset configurations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Manashi Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Aritri Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1\">Prabir Kumar Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pabitra Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BED: A Real-Time Object Detection System for Edge Devices. (arXiv:2202.07503v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07503","description":"<p>Deploying machine learning models to edge devices has many real-world\napplications, especially for the scenarios that demand low latency, low power,\nor data privacy. However, it requires substantial research and engineering\nefforts due to the limited computational resources and memory of edge devices.\nIn this demo, we present BED, an object detection system for edge devices\npracticed on the MAX78000 DNN accelerator. BED integrates on-device DNN\ninference with a camera and a screen for image acquisition and output\nexhibition, respectively. Experiment results indicate BED can provide accurate\ndetection with an only 300KB tiny DNN model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanchu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhimeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_A/0/1/0/all/0/1\">Alfredo Costilla Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niktash_A/0/1/0/all/0/1\">Afshin Niktash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulkar_G/0/1/0/all/0/1\">Gorkem Ulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okman_E/0/1/0/all/0/1\">Erman Okman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07508","description":"<p>In this paper, we tackle the problem of blind image super-resolution(SR) with\na reformulated degradation model and two novel modules. Following the common\npractices of blind SR, our method proposes to improve both the kernel\nestimation as well as the kernel based high resolution image restoration. To be\nmore specific, we first reformulate the degradation model such that the\ndeblurring kernel estimation can be transferred into the low resolution space.\nOn top of this, we introduce a dynamic deep linear filter module. Instead of\nlearning a fixed kernel for all images, it can adaptively generate deblurring\nkernel weights conditional on the input and yields more robust kernel\nestimation. Subsequently, a deep constrained least square filtering module is\napplied to generate clean features based on the reformulation and estimated\nkernel. The deblurred feature and the low input image feature are then fed into\na dual-path structured SR network and restore the final high resolution result.\nTo evaluate our method, we further conduct evaluations on several benchmarks,\nincluding Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed\nmethod achieves better accuracy and visual improvements against\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-Training Quantization for Cross-Platform Learned Image Compression. (arXiv:2202.07513v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07513","description":"<p>It has been witnessed that learned image compression has outperformed\nconventional image coding techniques and tends to be practical in industrial\napplications. One of the most critical issues that need to be considered is the\nnon-deterministic calculation, which makes the probability prediction\ncross-platform inconsistent and frustrates successful decoding. We propose to\nsolve this problem by introducing well-developed post-training quantization and\nmaking the model inference integer-arithmetic-only, which is much simpler than\npresently existing training and fine-tuning based approaches yet still keeps\nthe superior rate-distortion performance of learned image compression. Based on\nthat, we further improve the discretization of the entropy parameters and\nextend the deterministic inference to fit Gaussian mixture models. With our\nproposed methods, the current state-of-the-art image compression models can\ninfer in a cross-platform consistent manner, which makes the further\ndevelopment and practice of learned image compression more promising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dailan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Ziming Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label fusion and training methods for reliable representation of inter-rater uncertainty. (arXiv:2202.07550v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07550","description":"<p>Medical tasks are prone to inter-rater variability due to multiple factors\nsuch as image quality, professional experience and training, or guideline\nclarity. Training deep learning networks with annotations from multiple raters\nis a common practice that mitigates the model's bias towards a single expert.\nReliable models generating calibrated outputs and reflecting the inter-rater\ndisagreement are key to the integration of artificial intelligence in clinical\npractice. Various methods exist to take into account different expert labels.\nWe focus on comparing three label fusion methods: STAPLE, average of the\nrater's segmentation, and random sampling each rater's segmentation during\ntraining. Each label fusion method is studied using the conventional training\nframework or the recently published SoftSeg framework that limits information\nloss by treating the segmentation task as a regression. Our results, across 10\ndata splittings on two public datasets, indicate that SoftSeg models,\nregardless of the ground truth fusion method, had better calibration and\npreservation of the inter-rater rater variability compared with their\nconventional counterparts without impacting the segmentation performance.\nConventional models, i.e., trained with a Dice loss, with binary inputs, and\nsigmoid/softmax final activate, were overconfident and underestimated the\nuncertainty associated with inter-rater variability. Conversely, fusing labels\nby averaging with the SoftSeg framework led to underconfident outputs and\noverestimation of the rater disagreement. In terms of segmentation performance,\nthe best label fusion method was different for the two datasets studied,\nindicating this parameter might be task-dependent. However, SoftSeg had\nsegmentation performance systematically superior or equal to the conventionally\ntrained models and had the best calibration and preservation of the inter-rater\nvariability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the repeatability of deep learning models with Monte Carlo dropout. (arXiv:2202.07562v1 [eess.IV])","link":"http://arxiv.org/abs/2202.07562","description":"<p>The integration of artificial intelligence into clinical workflows requires\nreliable and robust models. Repeatability is a key attribute of model\nrobustness. Repeatable models output predictions with low variation during\nindependent tests carried out under similar conditions. During model\ndevelopment and evaluation, much attention is given to classification\nperformance while model repeatability is rarely assessed, leading to the\ndevelopment of models that are unusable in clinical practice. In this work, we\nevaluate the repeatability of four model types (binary classification,\nmulti-class classification, ordinal classification, and regression) on images\nthat were acquired from the same patient during the same visit. We study the\nperformance of binary, multi-class, ordinal, and regression models on four\nmedical image classification tasks from public and private datasets: knee\nosteoarthritis, cervical cancer screening, breast density estimation, and\nretinopathy of prematurity. Repeatability is measured and compared on ResNet\nand DenseNet architectures. Moreover, we assess the impact of sampling Monte\nCarlo dropout predictions at test time on classification performance and\nrepeatability. Leveraging Monte Carlo predictions significantly increased\nrepeatability for all tasks on the binary, multi-class, and ordinal models\nleading to an average reduction of the 95\\% limits of agreement by 16% points\nand of the disagreement rate by 7% points. The classification accuracy improved\nin most settings along with the repeatability. Our results suggest that beyond\nabout 20 Monte Carlo iterations, there is no further gain in repeatability. In\naddition to the higher test-retest agreement, Monte Carlo predictions were\nbetter calibrated which leads to output probabilities reflecting more\naccurately the true likelihood of being correctly classified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Befano_B/0/1/0/all/0/1\">Brian Befano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanjose_S/0/1/0/all/0/1\">Silvia De Sanjos&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egemen_D/0/1/0/all/0/1\">Diden Egemen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1\">Ana Cecilia Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schiffman_M/0/1/0/all/0/1\">Mark Schiffman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_J/0/1/0/all/0/1\">John Peter Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07570","description":"<p>Progress in digital pathology is hindered by high-resolution images and the\nprohibitive cost of exhaustive localized annotations. The commonly used\nparadigm to categorize pathology images is patch-based processing, which often\nincorporates multiple instance learning (MIL) to aggregate local patch-level\nrepresentations yielding image-level prediction. Nonetheless, diagnostically\nrelevant regions may only take a small fraction of the whole tissue, and\nMIL-based aggregation operation assumes that all patch representations are\nindependent and thus mislays the contextual information from adjacent cell and\ntissue microenvironments. Consequently, the computational resources dedicated\nto a specific region are independent of its information contribution. This\npaper proposes a transformer-based architecture specifically tailored for\nhistopathological image classification, which combines fine-grained local\nattention with a coarse global attention mechanism to learn meaningful\nrepresentations of high-resolution images at an efficient computational cost.\nMore importantly, based on the observation above, we propose a novel\nmixing-based data-augmentation strategy, namely ScoreMix, by leveraging the\ndistribution of the semantic regions of images during the training and\ncarefully guiding the data mixing via sampling the locations of discriminative\nimage content. Thorough experiments and ablation studies on three challenging\nrepresentative cohorts of Haematoxylin &amp; Eosin (H&amp;E) tumour regions-of-interest\n(TRoIs) datasets have validated the superiority of our approach over existing\nstate-of-the-art methods and effectiveness of our proposed components, e.g.,\ndata augmentation in improving classification performance. We also demonstrate\nour method's interpretability, robustness, and cross-domain generalization\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1\">Thomas Stegm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spahr_A/0/1/0/all/0/1\">Antoine Spahr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Representation Learning with Feedback. (arXiv:2202.07572v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07572","description":"<p>This note complements the author's recent paper \"Robust representation\nlearning with feedback for single image deraining\" by providing heuristically\ntheoretical explanations on the mechanism of representation learning with\nfeedback, namely an essential merit of the works presented in this recent\narticle. This note facilitates understanding of key points in the mechanism of\nrepresentation learning with feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness Indicators for Systematic Assessments of Visual Feature Extractors. (arXiv:2202.07603v1 [cs.CV])","link":"http://arxiv.org/abs/2202.07603","description":"<p>Does everyone equally benefit from computer vision systems? Answers to this\nquestion become more and more important as computer vision systems are deployed\nat large scale, and can spark major concerns when they exhibit vast performance\ndiscrepancies between people from various demographic and social backgrounds.\nSystematic diagnosis of fairness, harms, and biases of computer vision systems\nis an important step towards building socially responsible systems. To initiate\nan effort towards standardized fairness audits, we propose three fairness\nindicators, which aim at quantifying harms and biases of visual systems. Our\nindicators use existing publicly available datasets collected for fairness\nevaluations, and focus on three main types of harms and bias identified in the\nliterature, namely harmful label associations, disparity in learned\nrepresentations of social and demographic traits, and biased performance on\ngeographically diverse images from across the world.We define precise\nexperimental protocols applicable to a wide range of computer vision models.\nThese indicators are part of an ever-evolving suite of fairness probes and are\nnot intended to be a substitute for a thorough analysis of the broader impact\nof the new computer vision technologies. Yet, we believe it is a necessary\nfirst step towards (1) facilitating the widespread adoption and mandate of the\nfairness assessments in computer vision research, and (2) tracking progress\ntowards building socially responsible models. To study the practical\neffectiveness and broad applicability of our proposed indicators to any visual\nsystem, we apply them to off-the-shelf models built using widely adopted model\ntraining paradigms which vary in their ability to whether they can predict\nlabels on a given image or only produce the embeddings. We also systematically\nstudy the effect of data domain and model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Priya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soriano_A/0/1/0/all/0/1\">Adriana Romero Soriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1\">Levent Sagun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lie Point Symmetry Data Augmentation for Neural PDE Solvers. (arXiv:2202.07643v1 [cs.LG])","link":"http://arxiv.org/abs/2202.07643","description":"<p>Neural networks are increasingly being used to solve partial differential\nequations (PDEs), replacing slower numerical solvers. However, a critical issue\nis that neural PDE solvers require high-quality ground truth data, which\nusually must come from the very solvers they are designed to replace. Thus, we\nare presented with a proverbial chicken-and-egg problem. In this paper, we\npresent a method, which can partially alleviate this problem, by improving\nneural PDE solver sample complexity -- Lie point symmetry data augmentation\n(LPSDA). In the context of PDEs, it turns out that we are able to\nquantitatively derive an exhaustive list of data transformations, based on the\nLie point symmetry group of the PDEs in question, something not possible in\nother application areas. We present this framework and demonstrate how it can\neasily be deployed to improve neural PDE solver sample complexity by an order\nof magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1\">Johannes Brandstetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1\">Daniel E. Worrall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclic Differentiable Architecture Search. (arXiv:2006.10724v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10724","description":"<p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in\nneural architecture search. It tries to find the optimal architecture in a\nshallow search network and then measures its performance in a deep evaluation\nnetwork. The independent optimization of the search and evaluation networks,\nhowever, leaves room for potential improvement by allowing interaction between\nthe two networks. To address the problematic optimization issue, we propose new\njoint optimization objectives and a novel Cyclic Differentiable ARchiTecture\nSearch framework, dubbed CDARTS. Considering the structure difference, CDARTS\nbuilds a cyclic feedback mechanism between the search and evaluation networks\nwith introspective distillation. First, the search network generates an initial\narchitecture for evaluation, and the weights of the evaluation network are\noptimized. Second, the architecture weights in the search network are further\noptimized by the label supervision in classification, as well as the\nregularization from the evaluation network through feature distillation.\nRepeating the above cycle results in joint optimization of the search and\nevaluation networks and thus enables the evolution of the architecture to fit\nthe final evaluation network. The experiments and analysis on CIFAR, ImageNet\nand NAS-Bench-201 demonstrate the effectiveness of the proposed approach over\nthe state-of-the-art ones. Specifically, in the DARTS search space, we achieve\n97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the\nchain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,\nwhich is 1.1% higher than EfficientNet-B0. Our code and models are publicly\navailable at https://github.com/microsoft/Cream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILTRA: Rethinking Steerable CNN by Filter Transform. (arXiv:2105.11636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11636","description":"<p>Steerable CNN imposes the prior knowledge of transformation invariance or\nequivariance in the network architecture to enhance the the network robustness\non geometry transformation of data and reduce overfitting. It has been an\nintuitive and widely used technique to construct a steerable filter by\naugmenting a filter with its transformed copies in the past decades, which is\nnamed as filter transform in this paper. Recently, the problem of steerable CNN\nhas been studied from aspect of group representation theory, which reveals the\nfunction space structure of a steerable kernel function. However, it is not yet\nclear on how this theory is related to the filter transform technique. In this\npaper, we show that kernel constructed by filter transform can also be\ninterpreted in the group representation theory. This interpretation help\ncomplete the puzzle of steerable CNN theory and provides a novel and simple\napproach to implement steerable convolution operators. Experiments are executed\non multiple datasets to verify the feasibility of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning distinct features helps, provably. (arXiv:2106.06012v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06012","description":"<p>We study the diversity of the features learned by a two-layer neural network\ntrained with the least squares loss. We measure the diversity by the average\n$L_2$-distance between the hidden-layer features and theoretically investigate\nhow learning non-redundant distinct features affects the performance of the\nnetwork. To do so, we derive novel generalization bounds depending on feature\ndiversity based on Rademacher complexity for such networks. Our analysis proves\nthat more distinct features at the network's hidden units within the\nintermediate layer lead to better generalization. We also show how to extend\nour results to deeper networks and different losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1\">Firas Laakom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images. (arXiv:2106.14465v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.14465","description":"<p>Lyme disease which is one of the most common infectious vector-borne diseases\nmanifests itself in most cases with erythema migrans (EM) skin lesions. Recent\nstudies show that convolutional neural networks (CNNs) perform well to identify\nskin lesions from images. Lightweight CNN based pre-scanner applications for\nresource-constrained mobile devices can help users with early diagnosis of Lyme\ndisease and prevent the transition to a severe late form thanks to appropriate\nantibiotic therapy. Also, resource-intensive CNN based robust computer\napplications can assist non-expert practitioners with an accurate diagnosis.\nThe main objective of this study is to extensively analyze the effectiveness of\nCNNs for diagnosing Lyme disease from images and to find out the best CNN\narchitectures considering resource constraints. First, we created an EM dataset\nwith the help of expert dermatologists from Clermont-Ferrand University\nHospital Center of France. Second, we benchmarked this dataset for twenty-three\nCNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception,\nNASNet, and EfficientNet architectures in terms of predictive performance,\ncomputational complexity, and statistical significance. Third, to improve the\nperformance of the CNNs, we used custom transfer learning from ImageNet\npre-trained models as well as pre-trained the CNNs with the skin lesion dataset\nHAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class\nActivation Mapping to visualize the regions of input that are significant to\nthe CNNs for making predictions. Fifth, we provided guidelines for model\nselection based on predictive performance and computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving. (arXiv:2109.11615v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11615","description":"<p>Sharing collective perception messages (CPM) between vehicles is investigated\nto decrease occlusions so as to improve the perception accuracy and safety of\nautonomous driving. However, highly accurate data sharing and low communication\noverhead is a big challenge for collective perception, especially when\nreal-time communication is required among connected and automated vehicles. In\nthis paper, we propose an efficient and effective keypoints-based deep feature\nfusion framework built on the 3D object detector PV-RCNN, called Fusion PV-RCNN\n(FPV-RCNN for short), for collective perception. We introduce a\nhigh-performance bounding box proposal matching module and a keypoints\nselection strategy to compress the CPM size and solve the multi-vehicle data\nfusion problem. Besides, we also propose an effective localization error\ncorrection module based on the maximum consensus principle to increase the\nrobustness of the data fusion. Compared to a bird's-eye view (BEV) keypoints\nfeature fusion, FPV-RCNN achieves improved detection accuracy by about 9% at a\nhigh evaluation criterion (IoU 0.7) on the synthetic dataset COMAP dedicated to\ncollective perception. In addition, its performance is comparable to two raw\ndata fusion baselines that have no data loss in sharing. Moreover, our method\nalso significantly decreases the CPM size to less than 0.3 KB, and is thus\nabout 50 times smaller than the BEV feature map sharing used in previous works.\nEven with further decreased CPM feature channels, i.e., from 128 to 32, the\ndetection performance does not show apparent drops. The code of our method is\navailable at https://github.com/YuanYunshuang/FPV_RCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunshuang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1\">Monika Sester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Representation Learning for Spatial Image Steganalysis. (arXiv:2110.00957v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2110.00957","description":"<p>In this paper, we introduce a graph representation learning architecture for\nspatial image steganalysis, which is motivated by the assumption that\nsteganographic modifications unavoidably distort the statistical\ncharacteristics of the hidden graph features derived from cover images. In the\ndetailed architecture, we translate each image to a graph, where nodes\nrepresent the patches of the image and edges indicate the local relationships\nbetween the patches. Each node is associated with a feature vector determined\nfrom the corresponding patch by a shallow convolutional neural network (CNN)\nstructure. By feeding the graph to an attention network, the discriminative\nfeatures can be learned for efficient steganalysis. Experiments indicate that\nthe reported architecture achieves a competitive performance compared to the\nbenchmark CNN model, which has shown the potential of graph learning for\nsteganalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v8 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Primal-Dual Deep Unrolling. (arXiv:2110.10093v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.10093","description":"<p>We propose a new type of efficient deep-unrolling networks for solving\nimaging inverse problems. Conventional deep-unrolling methods require full\nforward operator and its adjoint across each layer, and hence can be\nsignificantly more expensive computationally as compared with other end-to-end\nmethods that are based on post-processing of model-based reconstructions,\nespecially for 3D image reconstruction tasks. We develop a stochastic\n(ordered-subsets) variant of the classical learned primal-dual (LPD), which is\na state-of-the-art unrolling network for tomographic image reconstruction. The\nproposed learned stochastic primal-dual (LSPD) network only uses subsets of the\nforward and adjoint operators and offers considerable computational efficiency.\nWe provide theoretical analysis of a special case of our LSPD framework,\nsuggesting that it has the potential to achieve image reconstruction quality\ncompetitive with the full-batch LPD while requiring only a fraction of the\ncomputation. The numerical results for two different X-ray computed tomography\n(CT) imaging tasks (namely, low-dose and sparse-view CT) corroborate this\ntheoretical finding, demonstrating the promise of LSPD networks for large-scale\nimaging problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Track Boosting and Synthetic Data Aided Drone Detection. (arXiv:2111.12389v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12389","description":"<p>This is the paper for the first place winning solution of the Drone vs. Bird\nChallenge, organized by AVSS 2021. As the usage of drones increases with\nlowered costs and improved drone technology, drone detection emerges as a vital\nobject detection task. However, detecting distant drones under unfavorable\nconditions, namely weak contrast, long-range, low visibility, requires\neffective algorithms. Our method approaches the drone detection problem by\nfine-tuning a YOLOv5 model with real and synthetically generated data using a\nKalman-based object tracker to boost detection confidence. Our results indicate\nthat augmenting the real data with an optimal subset of synthetic data can\nincrease the performance. Moreover, temporal information gathered by object\ntracking methods can increase performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eryuksel_O/0/1/0/all/0/1\">Ogulcan Eryuksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfuttu_K/0/1/0/all/0/1\">Kamil Anil Ozfuttu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unity is strength: Improving the Detection of Adversarial Examples with Ensemble Approaches. (arXiv:2111.12631v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12631","description":"<p>A key challenge in computer vision and deep learning is the definition of\nrobust strategies for the detection of adversarial examples. Here, we propose\nthe adoption of ensemble approaches to leverage the effectiveness of multiple\ndetectors in exploiting distinct properties of the input data. To this end, the\nENsemble Adversarial Detector (ENAD) framework integrates scoring functions\nfrom state-of-the-art detectors based on Mahalanobis distance, Local Intrinsic\nDimensionality, and One-Class Support Vector Machines, which process the hidden\nfeatures of deep neural networks. ENAD is designed to ensure high\nstandardization and reproducibility to the computational workflow. Importantly,\nextensive tests on benchmark datasets, models and adversarial attacks show that\nENAD outperforms all competing methods in the large majority of settings. The\nimprovement over the state-of-the-art and the intrinsic generality of the\nframework, which allows one to easily extend ENAD to include any set of\ndetectors, set the foundations for the new area of ensemble adversarial\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Craighero_F/0/1/0/all/0/1\">Francesco Craighero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angaroni_F/0/1/0/all/0/1\">Fabrizio Angaroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1\">Fabio Stella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damiani_C/0/1/0/all/0/1\">Chiara Damiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniotti_M/0/1/0/all/0/1\">Marco Antoniotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graudenzi_A/0/1/0/all/0/1\">Alex Graudenzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN. (arXiv:2112.04283v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04283","description":"<p>Adverse weather image translation belongs to the unsupervised image-to-image\n(I2I) translation task which aims to transfer adverse condition domain (eg,\nrainy night) to standard domain (eg, day). It is a challenging task because\nimages from adverse domains have some artifacts and insufficient information.\nRecently, many studies employing Generative Adversarial Networks (GANs) have\nachieved notable success in I2I translation but there are still limitations in\napplying them to adverse weather enhancement. Symmetric architecture based on\nbidirectional cycle-consistency loss is adopted as a standard framework for\nunsupervised domain transfer methods. However, it can lead to inferior\ntranslation result if the two domains have imbalanced information. To address\nthis issue, we propose a novel GAN model, i.e., AU-GAN, which has an asymmetric\narchitecture for adverse domain translation. We insert a proposed feature\ntransfer network (${T}$-net) in only a normal domain generator (i.e., rainy\nnight-&gt; day) to enhance encoded features of the adverse domain image. In\naddition, we introduce asymmetric feature matching for disentanglement of\nencoded features. Finally, we propose uncertainty-aware cycle-consistency loss\nto address the regional uncertainty of a cyclic reconstructed image. We\ndemonstrate the effectiveness of our method by qualitative and quantitative\ncomparisons with state-of-the-art models. Codes are available at\nhttps://github.com/jgkwak95/AU-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Rendering Method for Light Field Display. (arXiv:2201.08266v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.08266","description":"<p>A real-time elemental image array (EIA) generation method which does not\nsacrifice accuracy nor rely on high-performance hardware is developed, through\nraytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both\noffline and online working flow, experiments will verified the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Quanzhen Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (arXiv:2201.12086v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12086","description":"<p>Vision-Language Pre-training (VLP) has advanced the performance for many\nvision-language tasks. However, most existing pre-trained models only excel in\neither understanding-based tasks or generation-based tasks. Furthermore,\nperformance improvement has been largely achieved by scaling up the dataset\nwith noisy image-text pairs collected from the web, which is a suboptimal\nsource of supervision. In this paper, we propose BLIP, a new VLP framework\nwhich transfers flexibly to both vision-language understanding and generation\ntasks. BLIP effectively utilizes the noisy web data by bootstrapping the\ncaptions, where a captioner generates synthetic captions and a filter removes\nthe noisy ones. We achieve state-of-the-art results on a wide range of\nvision-language tasks, such as image-text retrieval (+2.7% in average\nrecall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).\nBLIP also demonstrates strong generalization ability when directly transferred\nto video-language tasks in a zero-shot manner. Code, models, and datasets are\nreleased at https://github.com/salesforce/BLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting. (arXiv:2202.02543v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02543","description":"<p>Learning from unlabeled or partially labeled data to alleviate human labeling\nremains a challenging research topic in 3D modeling. Along this line,\nunsupervised representation learning is a promising direction to auto-extract\nfeatures without human intervention. This paper proposes a general unsupervised\napproach, named \\textbf{ConClu}, to perform the learning of point-wise and\nglobal features by jointly leveraging point-level clustering and instance-level\ncontrasting. Specifically, for one thing, we design an Expectation-Maximization\n(EM) like soft clustering algorithm that provides local supervision to extract\ndiscriminating local features based on optimal transport. We show that this\ncriterion extends standard cross-entropy minimization to an optimal transport\nproblem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp\nalgorithm. For another, we provide an instance-level contrasting method to\nlearn the global geometry, which is formulated by maximizing the similarity\nbetween two augmentations of one point cloud. Experimental evaluations on\ndownstream applications such as 3D object classification and semantic\nsegmentation demonstrate the effectiveness of our framework and show that it\ncan outperform state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Guofeng Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (arXiv:2202.02626v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02626","description":"<p>Deep neural network models are used today in various applications of\nartificial intelligence, the strengthening of which, in the face of adversarial\nattacks is of particular importance. An appropriate solution to adversarial\nattacks is adversarial training, which reaches a trade-off between robustness\nand generalization. This paper introduces a novel framework (Layer\nSustainability Analysis (LSA)) for the analysis of layer vulnerability in an\narbitrary neural network in the scenario of adversarial attacks. LSA can be a\nhelpful toolkit to assess deep neural networks and to extend the adversarial\ntraining approaches towards improving the sustainability of model layers via\nlayer monitoring and analysis. The LSA framework identifies a list of Most\nVulnerable Layers (MVL list) of the given network. The relative error, as a\ncomparison measure, is used to evaluate representation sustainability of each\nlayer against adversarial inputs. The proposed approach for obtaining robust\nneural networks to fend off adversarial attacks is based on a layer-wise\nregularization (LR) over LSA proposal(s) for adversarial training (AT); i.e.\nthe AT-LR procedure. AT-LR could be used with any benchmark adversarial attack\nto reduce the vulnerability of network layers and to improve conventional\nadversarial training approaches. The proposed idea performs well theoretically\nand experimentally for state-of-the-art multilayer perceptron and convolutional\nneural network architectures. Compared with the AT-LR and its corresponding\nbase adversarial training, the classification accuracy of more significant\nperturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and\nCIFAR-10 benchmark datasets, respectively. The LSA framework is available and\npublished at https://github.com/khalooei/LSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalooei_M/0/1/0/all/0/1\">Mohammad Khalooei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirmazlaghani_M/0/1/0/all/0/1\">Maryam Amirmazlaghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Deterministic Independent Component Analysis for Hyperspectral Unmixing. (arXiv:2202.02951v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02951","description":"<p>We develop a new neural network based independent component analysis (ICA)\nmethod by directly minimizing the dependence amongst all extracted components.\nUsing the matrix-based R{\\'e}nyi's $\\alpha$-order entropy functional, our\nnetwork can be directly optimized by stochastic gradient descent (SGD), without\nany variational approximation or adversarial training. As a solid application,\nwe evaluate our ICA in the problem of hyperspectral unmixing (HU) and refute a\nstatement that \"\\emph{ICA does not play a role in unmixing hyperspectral\ndata}\", which was initially suggested by \\cite{nascimento2005does}. Code and\nadditional remarks of our DDICA is available at\nhttps://github.com/hongmingli1995/DDICA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05152","description":"<p>Recent studies revealed that convolutional neural networks do not generalize\nwell to small image transformations, e.g. rotations by a few degrees or\ntranslations of a few pixels. To improve the robustness to such\ntransformations, we propose to introduce data augmentation at intermediate\nlayers of the neural architecture, in addition to the common data augmentation\napplied on the input images. By introducing small perturbations to activation\nmaps (features) at various levels, we develop the capacity of the neural\nnetwork to cope with such transformations. We conduct experiments on three\nimage classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),\nconsidering two different convolutional architectures (ResNet-18 and\nDenseNet-121). When compared with two state-of-the-art stabilization methods,\nthe empirical results show that our approach consistently attains the best\ntrade-off between accuracy and mean flip rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Super Resolution Applied to Astronomy Imaging. (arXiv:2202.05354v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05354","description":"<p>Super resolution is an essential tool in optics, especially on interstellar\nscales, due to physical laws restricting possible imaging resolution. We\npropose using optimal transport and entropy for super resolution applications.\nWe prove that the reconstruction is accurate when sparsity is known and noise\nor distortion is small enough. We prove that the optimizer is stable and robust\nto noise and perturbations. We compare this method to a state of the art\nconvolutional neural network and get similar results for much less\ncomputational cost and greater methodological flexibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rawson_M/0/1/0/all/0/1\">Michael Rawson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hultgren_J/0/1/0/all/0/1\">Jakob Hultgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment. (arXiv:2202.05972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05972","description":"<p>Motivated by their recent advances, deep learning techniques have been widely\napplied to low-light image enhancement (LIE) problem. Among which, Retinex\ntheory based ones, mostly following a decomposition-adjustment pipeline, have\ntaken an important place due to its physical interpretation and promising\nperformance. However, current investigations on Retinex based deep learning are\nstill not sufficient, ignoring many useful experiences from traditional\nmethods. Besides, the adjustment step is either performed with simple image\nprocessing techniques, or by complicated networks, both of which are\nunsatisfactory in practice. To address these issues, we propose a new deep\nlearning framework for the LIE problem. The proposed framework contains a\ndecomposition network inspired by algorithm unrolling, and adjustment networks\nconsidering both global brightness and local brightness sensitivity. By virtue\nof algorithm unrolling, both implicit priors learned from data and explicit\npriors borrowed from traditional methods can be embedded in the network,\nfacilitate to better decomposition. Meanwhile, the consideration of global and\nlocal brightness can guide designing simple yet effective network modules for\nadjustment. Besides, to avoid manually parameter tuning, we also propose a\nself-supervised fine-tuning strategy, which can always guarantee a promising\nperformance. Experiments on a series of typical LIE datasets demonstrated the\neffectiveness of the proposed method, both quantitatively and visually, as\ncompared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse facial inpainting guided by exemplars. (arXiv:2202.06358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06358","description":"<p>Facial image inpainting is a task of filling visually realistic and\nsemantically meaningful contents for missing or masked pixels in a face image.\nAlthough existing methods have made significant progress in achieving high\nvisual quality, the controllable diversity of facial image inpainting remains\nan open problem in this field. This paper introduces EXE-GAN, a novel diverse\nand interactive facial inpainting framework, which can not only preserve the\nhigh-quality visual effect of the whole image but also complete the face image\nwith exemplar-like facial attributes. The proposed facial inpainting is\nachieved based on generative adversarial networks by leveraging the global\nstyle of input image, the stochastic style, and the exemplar style of exemplar\nimage. A novel attribute similarity metric is introduced to encourage networks\nto learn the style of facial attributes from the exemplar in a self-supervised\nway. To guarantee the natural transition across the boundary of inpainted\nregions, a novel spatial variant gradient backpropagation technique is designed\nto adjust the loss gradients based on the spatial location. A variety of\nexperimental results and comparisons on public CelebA-HQ and FFHQ datasets are\npresented to demonstrate the superiority of the proposed method in terms of\nboth the quality and diversity in facial inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanglong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xianta Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiankai Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kaijie Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADeADA: Adaptive Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06484","description":"<p>In the field of domain adaptation, a trade-off exists between the model\nperformance and the number of target domain annotations. Active learning,\nmaximizing model performance with few informative labeled data, comes in handy\nfor such a scenario. In this work, we present ADeADA, a general active domain\nadaptation framework for semantic segmentation. To adapt the model to the\ntarget domain with minimum queried labels, we propose acquiring labels of the\nsamples with high probability density in the target domain yet with low\nprobability density in the source domain, complementary to the existing source\ndomain labeled data. To further facilitate the label efficiency, we design an\nadaptive budget allocation policy, which dynamically balances the labeling\nbudgets among different categories as well as between density-aware and\nuncertainty-based methods. Extensive experiments show that our method\noutperforms existing active learning and domain adaptation baselines on two\nbenchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than 5%\ntarget domain annotations, our method reaches comparable results with that of\nfull supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shao-Ji Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tung-I Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06934","description":"<p>Detection of small objects and objects far away in the scene is a major\nchallenge in surveillance applications. Such objects are represented by small\nnumber of pixels in the image and lack sufficient details, making them\ndifficult to detect using conventional detectors. In this work, an open-source\nframework called Slicing Aided Hyper Inference (SAHI) is proposed that provides\na generic slicing aided inference and fine-tuning pipeline for small object\ndetection. The proposed technique is generic in the sense that it can be\napplied on top of any available object detector without any fine-tuning.\nExperimental evaluations, using object detection baselines on the Visdrone and\nxView aerial object detection datasets show that the proposed inference method\ncan increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and\nTOOD detectors, respectively. Moreover, the detection accuracy can be further\nincreased with a slicing aided fine-tuning, resulting in a cumulative increase\nof 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been\nintegrated with Detectron2, MMDetection and YOLOv5 models and it is publicly\navailable at https://github.com/obss/sahi.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}