{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Garden-Path Traversal within GPT-2. (arXiv:2205.12302v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12302","description":"<p>In recent years, massive language models consisting exclusively of\ntransformer decoders, led by the GPT-x family, have become increasingly\npopular. While studies have examined the behavior of these models, they tend to\nonly focus on the output of the language model, avoiding analyzing their\ninternal states despite such analyses being popular tools used within BERTology\nto study transformer encoders. We present a collection of methods for analyzing\nGPT-2's hidden states, and use the model's navigation of garden path sentences\nas a case study to demonstrate the utility of studying this model's behavior\nbeyond its output alone. To support this analysis, we introduce a novel dataset\nconsisting of 3 different types of garden path sentences, along with scripts to\nmanipulate them. We find that measuring Manhattan distances and cosine\nsimilarities between hidden states shows that GPT-2 navigates these sentences\nmore intuitively than conventional methods that predict from the model's output\nalone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jurayj_W/0/1/0/all/0/1\">William Jurayj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive multilingual speech recognition with pretrained models. (arXiv:2205.12304v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12304","description":"<p>Multilingual speech recognition with supervised learning has achieved great\nresults as reflected in recent research. With the development of pretraining\nmethods on audio and text data, it is imperative to transfer the knowledge from\nunsupervised multilingual models to facilitate recognition, especially in many\nlanguages with limited data. Our work investigated the effectiveness of using\ntwo pretrained models for two modalities: wav2vec 2.0 for audio and MBART50 for\ntext, together with the adaptive weight techniques to massively improve the\nrecognition quality on the public datasets containing CommonVoice and Europarl.\nOverall, we noticed an 44% improvement over purely supervised learning, and\nmore importantly, each technique provides a different reinforcement in\ndifferent languages. We also explore other possibilities to potentially obtain\nthe best model by slightly adding either depth or relative attention to the\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prompt Tuning. (arXiv:2205.12309v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12309","description":"<p>We propose structured prompt tuning, a simple and effective method to improve\nprompt tuning. Instead of prepending a sequence of tunable embeddings to the\ninput, we generate the soft prompt embeddings through a hypernetwork. Our\napproach subsumes the standard prompt tuning, allows more flexibility in model\ndesign and can be applied to both single-task and multi-task training settings.\nEmpirically, structured prompt tuning shows a gain of +1.2$~1.5 points on the\nGLUE benchmark and is less sensitive to the change of learning rate, compared\nto standard prompt tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi-Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scoring Coreference Chains with Split-Antecedent Anaphors. (arXiv:2205.12323v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12323","description":"<p>Anaphoric reference is an aspect of language interpretation covering a\nvariety of types of interpretation beyond the simple case of identity reference\nto entities introduced via nominal expressions covered by the traditional\ncoreference task in its most recent incarnation in ONTONOTES and similar\ndatasets. One of these cases that go beyond simple coreference is anaphoric\nreference to entities that must be added to the discourse model via\naccommodation, and in particular split-antecedent references to entities\nconstructed out of other entities, as in split-antecedent plurals and in some\ncases of discourse deixis. Although this type of anaphoric reference is now\nannotated in many datasets, systems interpreting such references cannot be\nevaluated using the Reference coreference scorer Pradhan et al. (2014). As part\nof the work towards a new scorer for anaphoric reference able to evaluate all\naspects of anaphoric interpretation in the coverage of the Universal Anaphora\ninitiative, we propose in this paper a solution to the technical problem of\ngeneralizing existing metrics for identity anaphora so that they can also be\nused to score cases of split-antecedents. This is the first such proposal in\nthe literature on anaphora or coreference, and has been successfully used to\nscore both split-antecedent plural references and discourse deixis in the\nrecent CODI/CRAC anaphora resolution in dialogue shared tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paun_S/0/1/0/all/0/1\">Silviu Paun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Juntao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel sentiment analysis in arabic. (arXiv:2205.12328v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12328","description":"<p>In this study, we aimed to improve the performance results of Arabic\nsentiment analysis. This can be achieved by investigating the most successful\nmachine learning method and the most useful feature vector to classify\nsentiments in both term and document levels into two (positive or negative)\ncategories. Moreover, specification of one polarity degree for the term that\nhas more than one is investigated. Also to handle the negations and\nintensifications, some rules are developed. According to the obtained results,\nArtificial Neural Network classifier is nominated as the best classifier in\nboth term and document level sentiment analysis (SA) for Arabic Language.\nFurthermore, the average F-score achieved in the term level SA for both\npositive and negative testing classes is 0.92. In the document level SA, the\naverage F-score for positive testing classes is 0.94, while for negative\nclasses is 0.93.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_E/0/1/0/all/0/1\">Ebru Sezer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12331","description":"<p>Deep learning models have achieved great success in many fields, yet they are\nvulnerable to adversarial examples. This paper follows a causal perspective to\nlook into the adversarial vulnerability and proposes Causal Intervention by\nSemantic Smoothing (CISS), a novel framework towards robustness against natural\nlanguage attacks. Instead of merely fitting observational data, CISS learns\ncausal effects p(y|do(x)) by smoothing in the latent semantic space to make\nrobust predictions, which scales to deep architectures and avoids tedious\nconstruction of noise customized for specific attacks. CISS is provably robust\nagainst word substitution attacks, as well as empirically robust even when\nperturbations are strengthened by unknown attack algorithms. For example, on\nYELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness\nagainst word substitutions, and achieves 79.4% empirical robustness when\nsyntactic attacks are integrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiteng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-12BERT: BERT for K-12 education. (arXiv:2205.12335v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12335","description":"<p>Online education platforms are powered by various NLP pipelines, which\nutilize models like BERT to aid in content curation. Since the inception of the\npre-trained language models like BERT, there have also been many efforts toward\nadapting these pre-trained models to specific domains. However, there has not\nbeen a model specifically adapted for the education domain (particularly K-12)\nacross subjects to the best of our knowledge. In this work, we propose to train\na language model on a corpus of data curated by us across multiple subjects\nfrom various sources for K-12 education. We also evaluate our model, K12-BERT,\non downstream tasks like hierarchical taxonomy tagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1\">Vasu Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahnan_D/0/1/0/all/0/1\">Dhruv Sahnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_D/0/1/0/all/0/1\">Deep Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint. (arXiv:2205.12368v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12368","description":"<p>Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Heng-Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabari_N/0/1/0/all/0/1\">Narges Tabari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Model Editing Processes. (arXiv:2205.12374v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12374","description":"<p>Most existing sequence generation models produce outputs in one pass, usually\nleft-to-right. However, this is in contrast with a more natural approach that\nhumans use in generating content; iterative refinement and editing. Recent work\nhas introduced edit-based models for various tasks (such as neural machine\ntranslation and text style transfer), but these generally model a single edit\nstep. In this work, we propose modeling editing processes, modeling the whole\nprocess of iteratively generating sequences. We form a conceptual framework to\ndescribe the likelihood of multi-step edits, and describe neural models that\ncan learn a generative model of sequences based on these multistep edits. We\nintroduce baseline results and metrics on this task, finding that modeling\nediting processes improves performance on a variety of axes on both our\nproposed task and related downstream tasks compared to previous single-step\nmodels of edits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoynaSlov: A Data Set of Russian Social Media Activity during the 2022 Ukraine-Russia War. (arXiv:2205.12382v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12382","description":"<p>In this report, we describe a new data set called VoynaSlov which contains\n21M+ Russian-language social media activities (i.e. tweets, posts, comments)\nmade by Russian media outlets and by the general public during the time of war\nbetween Ukraine and Russia. We scraped the data from two major platforms that\nare widely used in Russia: Twitter and VKontakte (VK), a Russian social media\nplatform based in Saint Petersburg commonly referred to as \"Russian Facebook\".\nWe provide descriptions of our data collection process and data statistics that\ncompare state-affiliated and independent Russian media, and also the two\nplatforms, VK and Twitter. The main differences that distinguish our data from\npreviously released data related to the ongoing war are its focus on Russian\nmedia and consideration of state-affiliation as well as the inclusion of data\nfrom VK, which is more suitable than Twitter for understanding Russian public\nsentiment considering its wide use within Russia. We hope our data set can\nfacilitate future research on information warfare and ultimately enable the\nreduction and prevention of disinformation and opinion manipulation campaigns.\nThe data set is available at https://github.com/chan0park/VoynaSlov and will be\nregularly updated as we continuously collect more data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1\">Julia Mendelsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12386","description":"<p>Recently, neural models have been leveraged to significantly improve the\nperformance of information extraction from semi-structured websites. However, a\nbarrier for continued progress is the small number of datasets large enough to\ntrain these models. In this work, we introduce the PLAtE (Pages of Lists\nAttribute Extraction) dataset as a challenging new web extraction task. PLAtE\nfocuses on shopping data, specifically extractions from product review pages\nwith multiple items. PLAtE encompasses both the tasks of: (1) finding\nproduct-list segmentation boundaries and (2) extracting attributes for each\nproduct. PLAtE is composed of 53, 905 items from 6, 810 pages, making it the\nfirst large-scale list page web extraction dataset. We construct PLAtE by\ncollecting list pages from Common Crawl, then annotating them on Mechanical\nTurk. Quantitative and qualitative analyses are performed to demonstrate PLAtE\nhas high-quality annotations. We establish strong baseline performance on PLAtE\nwith a SOTA model achieving an F1-score of 0.750 for attribute classification\nand 0.915 for segmentation, indicating opportunities for future research\ninnovations in web extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_A/0/1/0/all/0/1\">Aidan San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakus_J/0/1/0/all/0/1\">Jan Bakus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lockard_C/0/1/0/all/0/1\">Colin Lockard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciemiewicz_D/0/1/0/all/0/1\">David Ciemiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elfardy_H/0/1/0/all/0/1\">Heba Elfardy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toxicity Detection with Generative Prompt-based Inference. (arXiv:2205.12390v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12390","description":"<p>Due to the subtleness, implicity, and different possible interpretations\nperceived by different people, detecting undesirable content from text is a\nnuanced difficulty. It is a long-known risk that language models (LMs), once\ntrained on corpus containing undesirable content, have the power to manifest\nbiases and toxicity. However, recent studies imply that, as a remedy, LMs are\nalso capable of identifying toxic content without additional fine-tuning.\nPrompt-methods have been shown to effectively harvest this surprising\nself-diagnosing capability. However, existing prompt-based methods usually\nspecify an instruction to a language model in a discriminative way. In this\nwork, we explore the generative variant of zero-shot prompt-based toxicity\ndetection with comprehensive trials on prompt engineering. We evaluate on three\ndatasets with toxicity labels annotated on social media posts. Our analysis\nhighlights the strengths of our generative classification approach both\nquantitatively and qualitatively. Interesting aspects of self-diagnosis and its\nethical implications are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yau-Shian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Understanding Bias Correlations for Mitigation in NLP. (arXiv:2205.12391v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12391","description":"<p>Natural Language Processing (NLP) models have been found discriminative\nagainst groups of different social identities such as gender and race. With the\nnegative consequences of these undesired biases, researchers have responded\nwith unprecedented effort and proposed promising approaches for bias\nmitigation. In spite of considerable practical importance, current algorithmic\nfairness literature lacks an in-depth understanding of the relations between\ndifferent forms of biases. Social bias is complex by nature. Numerous studies\nin social psychology identify the \"generalized prejudice\", i.e., generalized\ndevaluing sentiments across different groups. For example, people who devalue\nethnic minorities are also likely to devalue women and gays. Therefore, this\nwork aims to provide a first systematic study toward understanding bias\ncorrelations in mitigation. In particular, we examine bias mitigation in two\ncommon NLP tasks -- toxicity detection and word embeddings -- on three social\nidentities, i.e., race, gender, and religion. Our findings suggest that biases\nare correlated and present scenarios in which independent debiasing approaches\ndominant in current literature may be insufficient. We further investigate\nwhether jointly mitigating correlated biases is more desired than independent\nand individual debiasing. Lastly, we shed light on the inherent issue of\ndebiasing-accuracy trade-off in bias mitigation. This study serves to motivate\nfuture research on joint bias mitigation that accounts for correlated biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models. (arXiv:2205.12392v1 [cs.AI])","link":"http://arxiv.org/abs/2205.12392","description":"<p>Emergent communication, also known as symbol emergence, seeks to investigate\ncomputational models that can better explain human language evolution and the\ncreation of symbol systems. This study aims to provide a new model for emergent\ncommunication, which is based on a probabilistic generative model. We define\nthe Metropolis-Hastings (MH) naming game by generalizing a model proposed by\nHagiwara et al. \\cite{hagiwara2019symbol}. The MH naming game is a sort of MH\nalgorithm for an integrative probabilistic generative model that combines two\nagents playing the naming game. From this viewpoint, symbol emergence is\nregarded as decentralized Bayesian inference, and semiotic communication is\nregarded as inter-personal cross-modal inference. We also offer Inter-GMM+VAE,\na deep generative model for simulating emergent communication, in which two\nagents create internal representations and categories and share signs (i.e.,\nnames of objects) from raw visual images observed from different viewpoints.\nThe model has been validated on MNIST and Fruits 360 datasets. Experiment\nfindings show that categories are formed from real images observed by agents,\nand signs are correctly shared across agents by successfully utilizing both of\nthe agents' views via the MH naming game. Furthermore, it has been verified\nthat the visual images were recalled from the signs uttered by the agents.\nNotably, emergent communication without supervision and reward feedback\nimproved the performance of unsupervised representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_Y/0/1/0/all/0/1\">Yuto Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual-T0: Progressively Instructing 50+ Tasks to Language Models Without Forgetting. (arXiv:2205.12393v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12393","description":"<p>Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskEval: Weighted MLM-Based Evaluation for Text Summarization and Simplification. (arXiv:2205.12394v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12394","description":"<p>In text summarization and simplification, system outputs must be evaluated\nalong multiple dimensions such as relevance, factual consistency, fluency, and\ngrammaticality, and a wide range of possible outputs could be of high quality.\nThese properties make the development of an adaptable, reference-less\nevaluation metric both necessary and challenging. We introduce MaskEval, a\nreference-less metric for text summarization and simplification that operates\nby performing masked language modeling (MLM) on the concatenation of the\ncandidate and the source texts. It features an attention-like weighting\nmechanism to modulate the relative importance of each MLM step, which crucially\nallows MaskEval to be adapted to evaluate different quality dimensions. We\ndemonstrate its effectiveness on English summarization and on multilingual text\nsimplification in terms of correlations with human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaliom_T/0/1/0/all/0/1\">Thomas Scaliom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks. (arXiv:2205.12396v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12396","description":"<p>Learning effective recipe representations is essential in food studies.\nUnlike what has been developed for image-based recipe retrieval or learning\nstructural text embeddings, the combined effect of multi-modal information\n(i.e., recipe images, text, and relation data) receives less attention. In this\npaper, we formalize the problem of multi-modal recipe representation learning\nto integrate the visual, textual, and relational information into recipe\nembeddings. In particular, we first present Large-RG, a new recipe graph data\nwith over half a million nodes, making it the largest recipe graph to date. We\nthen propose Recipe2Vec, a novel graph neural network based recipe embedding\nmodel to capture multi-modal information. Additionally, we introduce an\nadversarial attack strategy to ensure stable learning and improve performance.\nFinally, we design a joint objective function of node classification and\nadversarial learning to optimize the model. Extensive experiments demonstrate\nthat Recipe2Vec outperforms state-of-the-art baselines on two classic food\nstudy tasks, i.e., cuisine category classification and region prediction.\nDataset and codes are available at https://github.com/meettyj/Recipe2Vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yijun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metoyer_R/0/1/0/all/0/1\">Ronald Metoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT. (arXiv:2205.12399v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12399","description":"<p>We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. The Sparse Mixer slightly outperforms (&lt;1%) BERT on GLUE\nand SuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms (&lt;0.2%) BERT on SuperGLUE, but trains and runs\nnearly twice as fast: 89% faster training and 98% faster inference. We justify\nthe design of these two models by carefully ablating through various mixing\nmechanisms, MoE configurations and model hyperparameters. The Sparse Mixer\novercomes many of the latency and stability concerns of MoE models and offers\nthe prospect of serving sparse student models, without resorting to distilling\nthem to dense variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLUTE: Figurative Language Understanding and Textual Explanations. (arXiv:2205.12404v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12404","description":"<p>In spite of the prevalence of figurative language, transformer-based models\nstruggle to demonstrate an understanding of it. Meanwhile, even classical\nnatural language inference (NLI) tasks have been plagued by spurious\ncorrelations and annotation artifacts. Datasets like eSNLI have been released,\nallowing to probe whether language models are right for the right reasons. Yet\nno such data exists for figurative language, making it harder to asses genuine\nunderstanding of such expressions. In light of the above, we release FLUTE, a\ndataset of 8,000 figurative NLI instances with explanations, spanning three\ncategories: Sarcasm, Simile, and Metaphor. We collect the data through the\nHuman-AI collaboration framework based on GPT-3, crowdworkers, and expert\nannotation. We show how utilizing GPT-3 in conjunction with human experts can\naid in scaling up the creation of datasets even for such complex linguistic\nphenomena as figurative language. Baseline performance of the T5 model shows\nour dataset is a challenging testbed for figurative language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models. (arXiv:2205.12410v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12410","description":"<p>Fine-tuning large-scale pre-trained language models to downstream tasks\nrequire updating hundreds of millions of parameters. This not only increases\nthe serving cost to store a large copy of the model weights for every task, but\nalso exhibits instability during few-shot task adaptation. Parameter-efficient\ntechniques have been developed that tune small trainable components (e.g.,\nadapters) injected in the large model while keeping most of the model weights\nfrozen. The prevalent mechanism to increase adapter capacity is to increase the\nbottleneck dimension which increases the adapter parameters. In this work, we\nintroduce a new mechanism to improve adapter capacity without increasing\nparameters or computational cost by two key techniques. (i) We introduce\nmultiple shared adapter components in each layer of the Transformer\narchitecture. We leverage sparse learning via random routing to update the\nadapter parameters (encoder is kept frozen) resulting in the same amount of\ncomputational cost (FLOPs) as that of training a single adapter. (ii) We\npropose a simple merging mechanism to average the weights of multiple adapter\ncomponents to collapse to a single adapter in each Transformer layer, thereby,\nkeeping the overall parameters also the same but with significant performance\nimprovement. We demonstrate these techniques to work well across multiple task\nsettings including fully supervised and few-shot Natural Language Understanding\ntasks. By only tuning 0.23% of a pre-trained language model's parameters, our\nmodel outperforms the full model fine-tuning performance and several competing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12411","description":"<p>It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Data Augmentation improves Factuality of Abstractive Summarization. (arXiv:2205.12416v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12416","description":"<p>Abstractive summarization systems based on pretrained language models often\ngenerate coherent but factually inconsistent sentences. In this paper, we\npresent a counterfactual data augmentation approach where we augment data with\nperturbed summaries that increase the training data diversity. Specifically, we\npresent three augmentation approaches based on replacing (i) entities from\nother and the same category and (ii) nouns with their corresponding WordNet\nhypernyms. We show that augmenting the training data with our approach improves\nthe factual correctness of summaries without significantly affecting the ROUGE\nscore. We show that in two commonly used summarization datasets (CNN/Dailymail\nand XSum), we improve the factual correctness by about 2.5 points on average\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Cicero Nogueira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Action Conditions from Instructional Manuals for Instruction Understanding. (arXiv:2205.12420v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12420","description":"<p>The ability to infer pre- and postconditions of an action is vital for\ncomprehending complex instructions, and is essential for applications such as\nautonomous instruction-guided agents and assistive AI that supports humans to\nperform physical tasks. In this work, we propose a task dubbed action condition\ninference, and collecting a high-quality, human annotated dataset of\npreconditions and postconditions of actions in instructional manuals. We\npropose a weakly supervised approach to automatically construct large-scale\ntraining instances from online instructional manuals, and curate a densely\nhuman-annotated and validated dataset to study how well the current NLP models\ncan infer action-condition dependencies in the instruction texts. We design two\ntypes of models differ by whether contextualized and global information is\nleveraged, as well as various combinations of heuristics to construct the weak\nsupervisions. Our experimental results show a &gt;20% F1-score improvement with\nconsidering the entire instruction contexts and a &gt;6% F1-score benefit with the\nproposed heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Caiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Programming by Example with a Natural Language Prior. (arXiv:2205.12422v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12422","description":"<p>We introduce APEL, a new framework that enables non-programmers to indirectly\nannotate natural language utterances with executable meaning representations,\nsuch as SQL programs. Based on a natural language utterance, we first run a\nseed semantic parser to generate a prior over a list of candidate programs. To\nobtain information about which candidate is correct, we synthesize an input on\nwhich the more likely programs tend to produce different outputs, and ask an\nannotator which output is appropriate for the utterance. Hence, the annotator\ndoes not have to directly inspect the programs. To further reduce effort\nrequired from annotators, we aim to synthesize simple input databases that\nnonetheless have high information gain. With human annotators and Bayesian\ninference to handle annotation errors, we outperform Codex's top-1 performance\n(59%) and achieve the same accuracy as the original expert annotators (75%), by\nsoliciting answers for each utterance on only 2 databases with an average of 9\nrecords each. In contrast, it would be impractical to solicit outputs on the\noriginal 30K-record databases provided by SPIDER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Label Regularization for Fine-tuning Pre-trained Language Models. (arXiv:2205.12428v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12428","description":"<p>Knowledge Distillation (KD) is a prominent neural model compression technique\nwhich heavily relies on teacher network predictions to guide the training of a\nstudent model. Considering the ever-growing size of pre-trained language models\n(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is\nevident that in KD, deploying the teacher network during training adds to the\nmemory and computational requirements of training. In the computer vision\nliterature, the necessity of the teacher network is put under scrutiny by\nshowing that KD is a label regularization technique that can be replaced with\nlighter teacher-free variants such as the label-smoothing technique. However,\nto the best of our knowledge, this issue is not investigated in NLP. Therefore,\nthis work concerns studying different label regularization techniques and\nwhether we actually need the teacher labels to fine-tune smaller PLM student\nnetworks on downstream tasks. In this regard, we did a comprehensive set of\nexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600\ndistinct trials and ran each configuration five times. This investigation led\nto a surprising observation that KD and other label regularization techniques\ndo not play any meaningful role over regular fine-tuning when the student model\nis pre-trained. We further explore this phenomenon in different settings of NLP\nand computer vision tasks and demonstrate that pre-training itself acts as a\nkind of regularization, and additional label regularization is unnecessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Aref Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Omri_A/0/1/0/all/0/1\">Alan Do-Omri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Natural Language Proofs with Verifier-Guided Search. (arXiv:2205.12443v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12443","description":"<p>Deductive reasoning (drawing conclusions from assumptions) is a challenging\nproblem in NLP. In this work, we focus on proof generation: given a hypothesis\nand a set of supporting facts in natural language, the model generates a proof\ntree indicating how to deduce the hypothesis from supporting facts. Instead of\ngenerating the entire proof in one shot, prior work has demonstrated the\npromise of stepwise generation but achieved limited success on real-world data.\nExisting stepwise methods struggle to generate proof steps that are both valid\nand relevant. In this paper, we present a novel stepwise method NLProofS\n(Natural Language Proof Search), which learns to generate relevant steps\nconditioning on the hypothesis. At the core of our approach, we train an\nindependent verifier to check the validity of proof steps. Instead of\ngenerating steps greedily, we search for proofs maximizing a global proof score\njudged by the verifier. NLProofS achieves state-of-the-art performance on\nEntailmentBank and RuleTaker. For example, it improves the percentage of\ncorrectly predicted proofs from 20.9% to 33.3% in the distractor setting of\nEntailmentBank. This is the first time stepwise methods have led to better\ngeneration of challenging human-authored proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech. (arXiv:2205.12446v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12446","description":"<p>We introduce FLEURS, the Few-shot Learning Evaluation of Universal\nRepresentations of Speech benchmark. FLEURS is an n-way parallel speech dataset\nin 102 languages built on top of the machine translation FLoRes-101 benchmark,\nwith approximately 12 hours of speech supervision per language. FLEURS can be\nused for a variety of speech tasks, including Automatic Speech Recognition\n(ASR), Speech Language Identification (Speech LangID), Translation and\nRetrieval. In this paper, we provide baselines for the tasks based on\nmultilingual pre-trained models like mSLAM. The goal of FLEURS is to enable\nspeech technology in more languages and catalyze research in low-resource\nspeech understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse*BERT: Sparse Models are Robust. (arXiv:2205.12452v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12452","description":"<p>Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make the usage of these models less costly recent work has\nexplored leveraging structured and unstructured pruning, quantization, and\ndistillation as ways to improve inference speed and decrease size. This paper\nstudies how models pruned using Gradual Unstructured Magnitude Pruning can\ntransfer between domains and tasks. Our experimentation shows that models that\nare pruned during pretraining using general domain masked language models can\ntransfer to novel domains and tasks without extensive hyperparameter\nexploration or specialized approaches. We demonstrate that our general sparse\nmodel Sparse*BERT can become SparseBioBERT simply by pretraining the compressed\narchitecture on unstructured biomedical text. Moreover, we show that\nSparseBioBERT can match the quality of BioBERT with only 10\\% of the\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-tuning. (arXiv:2205.12453v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12453","description":"<p>A recent family of techniques, dubbed as lightweight fine-tuning methods,\nfacilitates parameter-efficient transfer learning by updating only a small set\nof additional parameters while keeping the parameters of the pretrained\nlanguage model frozen. While proven to be an effective method, there are no\nexisting studies on if and how such knowledge of the downstream fine-tuning\napproach should affect the pretraining stage. In this work, we show that taking\nthe ultimate choice of fine-tuning method into consideration boosts the\nperformance of parameter-efficient fine-tuning. By relying on\noptimization-based meta-learning using MAML with certain modifications for our\ndistinct purpose, we prime the pretrained model specifically for\nparameter-efficient fine-tuning, resulting in gains of up to 1.7 points on\ncross-lingual NER fine-tuning. Our ablation settings and analyses further\nreveal that the tweaks we introduce in MAML are crucial for the attained gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Information Inconsistency in Multilingual Open-Domain Question Answering. (arXiv:2205.12456v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12456","description":"<p>Retrieval based open-domain QA systems use retrieved documents and\nanswer-span selection over retrieved documents to find best-answer candidates.\nWe hypothesize that multilingual Question Answering (QA) systems are prone to\ninformation inconsistency when it comes to documents written in different\nlanguages, because these documents tend to provide a model with varying\ninformation about the same topic. To understand the effects of the biased\navailability of information and cultural influence, we analyze the behavior of\nmultilingual open-domain question answering models with a focus on retrieval\nbias. We analyze if different retriever models present different passages given\nthe same question in different languages on TyDi QA and XOR-TyDi QA, two\nmultilingualQA datasets. We speculate that the content differences in documents\nacross languages might reflect cultural divergences and/or social biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palta_S/0/1/0/all/0/1\">Shramay Palta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haozhe An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shuaiyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving CTC-based ASR Models with Gated Interlayer Collaboration. (arXiv:2205.12462v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12462","description":"<p>For Automatic Speech Recognition (ASR), the CTC-based methods have become a\ndominant paradigm due to its simple architecture and efficient\nnon-autoregressive inference manner. However, these methods without external\nlanguage models usually lack the capacity of modeling the conditional\ndependencies and the textual interaction. In this work, we present a Gated\nInterlayer Collaboration (GIC) mechanism which introduces the contextual\ninformation into the models and relaxes the conditional independence assumption\nof the CTC-based models. Specifically, we train the model with intermediate CTC\nlosses calculated by the interlayer outputs of the model, in which the\nprobability distributions of the intermediate layers naturally serve as soft\nlabel sequences. The GIC block consists of an embedding layer to obtain the\ntextual embedding of the soft label at each position, and a gate unit to fuse\nthe textual embedding and the acoustic features. Experiments on AISHELL-1 and\nAIDATATANG benchmarks show that the proposed method outperforms the recently\npublished CTC-based ASR models. Specifically, our method achieves CER of\n4.0%/4.4% on AISHELL-1 dev/test sets and CER of 3.8%/4.4% on AIDATATANG\ndev/test sets using CTC greedy search decoding without external language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Binbin Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2D2: Robust Data-to-Text with Replacement Detection. (arXiv:2205.12467v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12467","description":"<p>Unfaithful text generation is a common problem for text generation systems.\nIn the case of Data-to-Text (D2T) systems, the factuality of the generated text\nis particularly crucial for any real-world applications. We introduce R2D2, a\ntraining framework that addresses unfaithful Data-to-Text generation by\ntraining a system both as a generator and a faithfulness discriminator with\nadditional replacement detection and unlikelihood learning tasks. To facilitate\nsuch training, we propose two methods for sampling unfaithful sentences. We\nargue that the poor entity retrieval capability of D2T systems is one of the\nprimary sources of unfaithfulness, so in addition to the existing metrics, we\nfurther propose NER-based metrics to evaluate the fidelity of D2T generations.\nOur experimental results show that R2D2 systems could effectively mitigate the\nunfaithful text generation, and they achieve new state-of-the-art results on\nFeTaQA, LogicNLG, and ToTTo, all with significant improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1\">Lorenzo Jaime Yu Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_L/0/1/0/all/0/1\">Luke Benson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Weijin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI. (arXiv:2205.12469v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12469","description":"<p>Evaluating an explanation's faithfulness is desired for many reasons such as\ntrust, interpretability and diagnosing the sources of model's errors. In this\nwork, which focuses on the NLI task, we introduce the methodology of\nFaithfulness-through-Counterfactuals, which first generates a counterfactual\nhypothesis based on the logical predicates expressed in the explanation, and\nthen evaluates if the model's prediction on the counterfactual is consistent\nwith that expressed logic (i.e. if the new formula is \\textit{logically\nsatisfiable}). In contrast to existing approaches, this does not require any\nexplanations for training a separate verification model. We first validate the\nefficacy of automatic counterfactual hypothesis generation, leveraging on the\nfew-shot priming paradigm. Next, we show that our proposed metric distinguishes\nbetween human-model agreement and disagreement on new counterfactual input. In\naddition, we conduct a sensitivity analysis to validate that our metric is\nsensitive to unfaithful explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sia_S/0/1/0/all/0/1\">Suzanna Sia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1\">Anton Belyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1\">Amjad Almahairi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Better Initialization for Soft Prompts via Meta-Learning. (arXiv:2205.12471v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12471","description":"<p>Prompt tuning (PT) is an effective approach to adapting pre-trained language\nmodels to downstream tasks. Without a good initialization, prompt tuning\ndoesn't perform well under few-shot settings. So pre-trained prompt tuning\n(PPT) is proposed to initialize prompts by leveraging pre-training data. We\npropose MetaPT (Meta-learned Prompt Tuning) to further improve PPT's\ninitialization by considering latent structure within the pre-training data.\nSpecifically, we introduce the structure by first clustering pre-training data\ninto different auxiliary tasks with unsupervised methods. Then we use these\ntasks to pre-train prompts with a meta-learning algorithm. Such a process can\nmake prompts learn a better initialization by discovering commonalities among\nthese auxiliary tasks. We evaluate our method on seven downstream tasks. Our\nMetaPT achieves better and more stable performance than the state-of-the-art\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yukun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Resource Style Transfer via Domain Adaptive Meta Learning. (arXiv:2205.12475v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12475","description":"<p>Text style transfer (TST) without parallel data has achieved some practical\nsuccess. However, most of the existing unsupervised text style transfer methods\nsuffer from (i) requiring massive amounts of non-parallel data to guide\ntransferring different text styles. (ii) colossal performance degradation when\nfine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain\nAdaptive Meta-Learning with Adversarial Transfer Model), which consists of two\nparts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn\ngeneral knowledge in multiple heterogeneous source domains, capable of adapting\nto new unseen domains with a small amount of data. Moreover, we propose a new\nunsupervised TST approach Adversarial Transfer Model (ATM), composed of a\nsequence-to-sequence pre-trained language model and uses adversarial style\ntraining for better content preservation and style transfer. Results on\nmulti-domain datasets demonstrate that our approach generalizes well on unseen\nlow-resource domains, achieving state-of-the-art results against ten strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Locality in Abstractive Text Summarization. (arXiv:2205.12476v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12476","description":"<p>Despite the successes of neural attention models for natural language\ngeneration tasks, the quadratic memory complexity of the self-attention module\nwith respect to the input length hinders their applications in long text\nsummarization. Instead of designing more efficient attention modules, we\napproach this problem by investigating if models with a restricted context can\nhave competitive performance compared with the memory-efficient attention\nmodels that maintain a global context by treating the input as an entire\nsequence. Our model is applied to individual pages, which contain parts of\ninputs grouped by the principle of locality, during both encoding and decoding\nstages. We empirically investigated three kinds of localities in text\nsummarization at different levels, ranging from sentences to documents. Our\nexperimental results show that our model can have better performance compared\nwith strong baseline models with efficient attention modules, and our analysis\nprovides further insights of our locality-aware modeling strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Linyong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GisPy: A Tool for Measuring Gist Inference Score in Text. (arXiv:2205.12484v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12484","description":"<p>Decision making theories such as Fuzzy-Trace Theory (FTT) suggest that\nindividuals tend to rely on gist, or bottom-line meaning, in the text when\nmaking decisions. In this work, we delineate the process of developing GisPy,\nan open-source tool in Python for measuring the Gist Inference Score (GIS) in\ntext. Evaluation of GisPy on documents in three benchmarks from the news and\nscientific text domains demonstrates that scores generated by our tool\nsignificantly distinguish low vs. high gist documents. Our tool is publicly\navailable to use at: https://github.com/phosseini/GisPy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Pedram Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Christopher R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broniatowski_D/0/1/0/all/0/1\">David A. Broniatowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional set generation using Seq2seq models. (arXiv:2205.12485v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12485","description":"<p>Conditional set generation learns a mapping from an input sequence of tokens\nto a set. Several NLP tasks, such as entity typing and dialogue emotion\ntagging, are instances of set generation. Sequence-to-sequence~(Seq2seq) models\nare a popular choice to model set generation, but they treat a set as a\nsequence and do not fully leverage its key properties, namely order-invariance\nand cardinality. We propose a novel algorithm for effectively sampling\ninformative orders over the combinatorial space of label orders. Further, we\njointly model the set cardinality and output by adding the set size as the\nfirst element and taking advantage of the autoregressive factorization used by\nSeq2seq models. Our method is a model-independent data augmentation approach\nthat endows any Seq2seq model with the signals of order-invariance and\ncardinality. Training a Seq2seq model on this new augmented data~(without any\nadditional annotations) gets an average relative improvement of 20% for four\nbenchmarks datasets across models spanning from BART-base, T5-xxl, and GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents by Sampling Summary Views. (arXiv:2205.12486v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12486","description":"<p>We argue that disentangling content selection from the budget used to cover\nsalient content improves the performance and applicability of abstractive\nsummarizers. Our method, FactorSum, does this disentanglement by factorizing\nsummarization into two steps through an energy function: (1) generation of\nabstractive summary views; (2) combination of these views into a final summary,\nfollowing a budget and content guidance. This guidance may come from different\nsources, including from an advisor model such as BART or BigBird, or in oracle\nmode -- from the reference. This factorization achieves significantly higher\nROUGE scores on multiple benchmarks for long document summarization, namely\nPubMed, arXiv, and GovReport. Most notably, our model is effective for domain\nadaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1\nscore on arXiv, which indicates a strong performance due to more flexible\nbudget adaptation and content selection less dependent on domain-specific\ntextual structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_M/0/1/0/all/0/1\">Marcio Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models. (arXiv:2205.12487v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12487","description":"<p>We propose the end-to-end multimodal fact-checking and explanation\ngeneration, where the input is a claim and a large collection of web sources,\nincluding articles, images, videos, and tweets, and the goal is to assess the\ntruthfulness of the claim by retrieving relevant evidence and predicting a\ntruthfulness label (i.e., support, refute and not enough information), and\ngenerate a rationalization statement to explain the reasoning and ruling\nprocess. To support this research, we construct Mocheg, a large-scale dataset\nthat consists of 21,184 claims where each claim is assigned with a truthfulness\nlabel and ruling statement, with 58,523 evidence in the form of text and\nimages. To establish baseline performances on Mocheg, we experiment with\nseveral state-of-the-art neural architectures on the three pipelined subtasks:\nmultimodal evidence retrieval, claim verification, and explanation generation,\nand demonstrate the current state-of-the-art performance of end-to-end\nmultimodal fact-checking is still far from satisfying. To the best of our\nknowledge, we are the first to build the benchmark dataset and solutions for\nend-to-end multimodal fact-checking and justification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Barry Menglong Yao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jin-Hee Cho</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a> (2) ((1) University at Buffalo, (2) Virginia Tech, (3) Lehigh University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Event Extraction via Self-Training with Gradient Guidance. (arXiv:2205.12490v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12490","description":"<p>Data scarcity and imbalance have been the main factors that hinder the\nprogress of event extraction (EE). In this work, we propose a self-training\nwith gradient guidance (STGG) framework which consists of (1) a base event\nextraction model which is firstly trained on existing event annotations and\nthen applied to large-scale unlabeled corpora to predict new event mentions,\nand (2) a scoring model that takes in each predicted event trigger and argument\nas well as their path in the Abstract Meaning Representation (AMR) graph to\nestimate a probability score indicating the correctness of the event\nprediction. The new event predictions along with their correctness scores are\nthen used as pseudo labeled examples to improve the base event extraction model\nwhile the magnitude and direction of its gradients are guided by the\ncorrectness scores. Experimental results on three benchmark datasets, including\nACE05-E, ACE05-E+ and ERE-EN, demonstrate the effectiveness of the STGG\nframework on event extraction task with up to 1.9 F-score improvement over the\nbase event extraction models. Our experimental analysis further shows that STGG\nis a general framework as it can be applied to any base event extraction models\nand improve their performance by leveraging broad unlabeled data, even when the\nhigh-quality AMR graph annotations are not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Contrastive Learning for Relation Extraction. (arXiv:2205.12491v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12491","description":"<p>Recent relation extraction (RE) works have shown encouraging improvements by\nconducting contrastive learning on silver labels generated by distant\nsupervision before fine-tuning on gold labels. Existing methods typically\nassume all these silver labels are accurate and therefore treat them equally in\ncontrastive learning; however, distant supervision is inevitably noisy -- some\nsilver labels are more reliable than others. In this paper, we first assess the\nquality of silver labels via a simple and automatic approach we call \"learning\norder denoising,\" where we train a language model to learn these relations and\nrecord the order of learned training instances. We show that learning order\nlargely corresponds to label accuracy -- early learned silver labels have, on\naverage, more accurate labels compared to later learned silver labels. We then\npropose a novel fine-grained contrastive learning (FineCL) for RE, which\nleverages this additional, fine-grained information about which silver labels\nare and are not noisy to improve the quality of learned relationship\nrepresentations for RE. Experiments on many RE benchmarks show consistent,\nsignificant performance gains of FineCL over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection. (arXiv:2205.12495v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12495","description":"<p>Hate speech detection is complex; it relies on commonsense reasoning,\nknowledge of stereotypes, and an understanding of social nuance that differs\nfrom one culture to the next. It is also difficult to collect a large-scale\nhate speech annotated dataset. In this work, we frame this problem as a\nfew-shot learning task, and show significant gains with decomposing the task\ninto its \"constituent\" parts. In addition, we see that infusing knowledge from\nreasoning datasets (e.g. Atomic2020) improves the performance even further.\nMoreover, we observe that the trained models generalize to out-of-distribution\ndatasets, showing the superiority of task decomposition and knowledge infusion\ncompared to previously used methods. Concretely, our method outperforms the\nbaseline by 17.83% absolute gain in the 16-shot case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1\">Badr AlKhamissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Broad Reasoning Skills via Decomposition-Guided Contexts. (arXiv:2205.12496v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12496","description":"<p>Question-answering datasets require a broad set of reasoning skills. We show\nhow to use question decompositions to teach language models these broad\nreasoning skills in a robust fashion. Specifically, we use widely available\nQDMR representations to programmatically create synthetic contexts for real\nquestions in six multihop reasoning datasets. These contexts are carefully\ndesigned to avoid common reasoning shortcuts prevalent in real contexts that\nprevent models from learning the right skills. This results in a pretraining\ndataset, named TeaBReaC, containing 525K multihop questions (with associated\nformal programs) covering about 900 reasoning patterns. We show that\npretraining standard language models (LMs) on TeaBReaC before fine-tuning them\non target datasets improves their performance by up to 13 EM points across 3\nmultihop QA datasets, with a 30 point gain on more complex questions. The\nresulting models also demonstrate higher robustness, with a 6-11 point\nimprovement on two contrast sets. Furthermore, TeaBReaC pretraining\nsubstantially improves model performance and robustness even when starting with\nnumeracy-aware LMs pretrained using recent methods (e.g., PReasM). Our work\nthus shows how one can effectively use decomposition-guided contexts to\nrobustly teach multihop reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training. (arXiv:2205.12502v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12502","description":"<p>Visual dialog (VisDial) is a task of answering a sequence of questions\ngrounded in an image, using the dialog history as context. Prior work has\ntrained the dialog agents solely on VisDial data via supervised learning or\nleveraged pre-training on related vision-and-language datasets. This paper\npresents a semi-supervised learning approach for visually-grounded dialog,\ncalled Generative Self-Training (GST), to leverage unlabeled images on the Web.\nSpecifically, GST first retrieves in-domain images through out-of-distribution\ndetection and generates synthetic dialogs regarding the images via multimodal\nconditional text generation. GST then trains a dialog agent on the synthetic\nand the original VisDial data. As a result, GST scales the amount of training\ndata up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For\nrobust training of the generated dialogs, we also propose perplexity-based data\nselection and multimodal consistency regularization. Evaluation on VisDial v1.0\nand v0.9 datasets shows that GST achieves new state-of-the-art results on both\ndatasets. We further observe strong performance gains in the low-data regime\n(up to 9.35 absolute points on NDCG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENEVA: Pushing the Limit of Generalizability for Event Argument Extraction with 100+ Event Types. (arXiv:2205.12505v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12505","description":"<p>Numerous events occur worldwide and are documented in the news, social media,\nand various online platforms in raw text. Extracting useful and succinct\ninformation about these events is crucial to various downstream applications.\nEvent Argument Extraction (EAE) deals with the task of extracting\nevent-specific information from natural language text. In order to cater to new\nevents and domains in a realistic low-data setting, there is a growing urgency\nfor EAE models to be generalizable. Consequentially, there is a necessity for\nbenchmarking setups to evaluate the generalizability of EAE models. But most\nexisting benchmarking datasets like ACE and ERE have limited coverage in terms\nof events and cannot adequately evaluate the generalizability of EAE models. To\nalleviate this issue, we introduce a new dataset GENEVA covering a diverse\nrange of 115 events and 187 argument roles. Using this dataset, we create four\nbenchmarking test suites to assess the model's generalization capability from\ndifferent perspectives. We benchmark various representative models on these\ntest suites and compare their generalizability relatively. Finally, we propose\na new model SCAD that outperforms the previous models and serves as a strong\nbenchmark for these test suites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_T/0/1/0/all/0/1\">Tanmay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization in NLP Fine-tuning Methods. (arXiv:2205.12506v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12506","description":"<p>Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1\">Archit Uniyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Calibration for Question Answering. (arXiv:2205.12507v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12507","description":"<p>Model calibration aims to adjust (calibrate) models' confidence so that they\nmatch expected accuracy. We argue that the traditional evaluation of\ncalibration (expected calibration error; ECE) does not reflect usefulness of\nthe model confidence. For example, after conventional temperature scaling,\nconfidence scores become similar for all predictions, which makes it hard for\nusers to distinguish correct predictions from wrong ones, even though it\nachieves low ECE. Building on those observations, we propose a new calibration\nmetric, MacroCE, that better captures whether the model assigns low confidence\nto wrong predictions and high confidence to correct predictions. We examine\nvarious conventional calibration methods including temperature scaling,\nfeature-based classifier, neural answer reranking, and label smoothing, all of\nwhich do not bring significant gains under our new MacroCE metric. Towards more\neffective calibration, we propose a new calibration method based on the model's\nprediction consistency along the training trajectory. This new method, which we\nname as consistency calibration, shows promise for better calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Robustness to Natural Asemantic Variation. (arXiv:2205.12514v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12514","description":"<p>We introduce and formalize an under-studied linguistic phenomenon we call\nNatural Asemantic Variation (NAV) and investigate it in the context of Machine\nTranslation (MT) robustness. Standard MT models are shown to be less robust to\nrarer, nuanced language forms, and current robustness techniques do not account\nfor this kind of perturbation despite their prevalence in \"real world\" data.\nExperiment results provide more insight into the nature of NAV and we\ndemonstrate strategies to improve performance on NAV. We also show that NAV\nrobustness can be transferred across languages and fine that synthetic\nperturbations can achieve some but not all of the benefits of human-generated\nNAV data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bremerman_J/0/1/0/all/0/1\">Jacob Bremerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. (arXiv:2205.12522v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12522","description":"<p>Research in massively multilingual image captioning has been severely\nhampered by a lack of high-quality evaluation datasets. In this paper we\npresent the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse\nset of 3600 images annotated with human-generated reference captions in 36\nlanguages. The images were selected from across the world, covering regions\nwhere the 36 languages are spoken, and annotated with captions that achieve\nconsistency in terms of style across all languages, while avoiding annotation\nartifacts due to direct translation. We apply this benchmark to model selection\nfor massively multilingual image captioning models, and show superior\ncorrelation results with human evaluations when using XM3600 as golden\nreferences for automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish V. Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation. (arXiv:2205.12523v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12523","description":"<p>Direct speech-to-speech translation (S2ST) systems leverage recent progress\nin speech representation learning, where a sequence of discrete representations\n(units) derived in a self-supervised manner, are predicted from the model and\npassed to a vocoder for speech synthesis, still facing the following\nchallenges: 1) Acoustic multimodality: the discrete units derived from speech\nwith same content could be indeterministic due to the acoustic property (e.g.,\nrhythm, pitch, and energy), which causes deterioration of translation accuracy;\n2) high latency: current S2ST systems utilize autoregressive models which\npredict each unit conditioned on the sequence previously generated, failing to\ntake full advantage of parallelism. In this work, we propose TranSpeech, a\nspeech-to-speech translation model with bilateral perturbation. To alleviate\nthe acoustic multimodal problem, we propose bilateral perturbation, which\nconsists of the style normalization and information enhancement stages, to\nlearn only the linguistic information from speech samples and generate more\ndeterministic representations. With reduced multimodality, we step forward and\nbecome the first to establish a non-autoregressive S2ST technique, which\nrepeatedly masks and predicts unit choices and produces high-accuracy results\nin just a few cycles. Experimental results on three language pairs demonstrate\nthe state-of-the-art results by up to 2.5 BLEU points over the best\npublicly-available textless S2ST baseline. Moreover, TranSpeech shows a\nsignificant improvement in inference latency, enabling speedup up to 21.4x than\nautoregressive technique. Audio samples are available at\n\\url{https://TranSpeech.github.io/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinzheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting Numerical Substitution Ciphers. (arXiv:2205.12527v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12527","description":"<p>Deciphering historical substitution ciphers is a challenging problem. Example\nproblems that have been previously studied include detecting cipher type,\ndetecting plaintext language, and acquiring the substitution key for segmented\nciphers. However, attacking unsegmented, space-free ciphers is still a\nchallenging task. Segmentation (i.e. finding substitution units) is the first\nstep towards cracking those ciphers. In this work, we propose the first\nautomatic methods to segment those ciphers using Byte Pair Encoding (BPE) and\nunigram language models. Our methods achieve an average segmentation error of\n2\\% on 100 randomly-generated monoalphabetic ciphers and 27\\% on 3 real\nhomophonic ciphers. We also propose a method for solving non-deterministic\nciphers with existing keys using a lattice and a pretrained language model. Our\nmethod leads to the full solution of the IA cipher; a real historical cipher\nthat has not been fully solved until this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldarrab_N/0/1/0/all/0/1\">Nada Aldarrab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification. (arXiv:2205.12528v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12528","description":"<p>Weakly supervised text classification methods typically train a deep neural\nclassifier based on pseudo-labels. The quality of pseudo-labels is crucial to\nfinal performance but they are inevitably noisy due to their heuristic nature,\nso selecting the correct ones has a huge potential for performance boost. One\nstraightforward solution is to select samples based on the softmax probability\nscores in the neural classifier corresponding to their pseudo-labels. However,\nwe show through our experiments that such solutions are ineffective and\nunstable due to the erroneously high-confidence predictions from poorly\ncalibrated models. Recent studies on the memorization effects of deep neural\nmodels suggest that these models first memorize training samples with clean\nlabels and then those with noisy labels. Inspired by this observation, we\npropose a novel pseudo-label selection method LOPS that takes learning order of\nsamples into consideration. We hypothesize that the learning order reflects the\nprobability of wrong annotation in terms of ranking, and therefore, propose to\nselect the samples that are learnt earlier. LOPS can be viewed as a strong\nperformance-boost plug-in to most of existing weakly-supervised text\nclassification methods, as confirmed in extensive experiments on four\nreal-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a Question Decomposition Unit All We Need?. (arXiv:2205.12538v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12538","description":"<p>Large Language Models (LMs) have achieved state-of-the-art performance on\nmany Natural Language Processing (NLP) benchmarks. With the growing number of\nnew benchmarks, we build bigger and more complex LMs. However, building new LMs\nmay not be an ideal option owing to the cost, time and environmental impact\nassociated with it. We explore an alternative route: can we modify data by\nexpressing it in terms of the model's strengths, so that a question becomes\neasier for models to answer? We investigate if humans can decompose a hard\nquestion into a set of simpler questions that are relatively easier for models\nto solve. We analyze a range of datasets involving various forms of reasoning\nand find that it is indeed possible to significantly improve model performance\n(24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via\ndecomposition. Our approach provides a viable option to involve people in NLP\nresearch in a meaningful way. Our findings indicate that Human-in-the-loop\nQuestion Decomposition (HQD) can potentially provide an alternate path to\nbuilding large LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pruthvi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ER-TEST: Evaluating Explanation Regularization Methods for NLP Models. (arXiv:2205.12542v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12542","description":"<p>Neural language models' (NLMs') reasoning processes are notoriously hard to\nexplain. Recently, there has been much progress in automatically generating\nmachine rationales of NLM behavior, but less in utilizing the rationales to\nimprove NLM behavior. For the latter, explanation regularization (ER) aims to\nimprove NLM generalization by pushing the machine rationales to align with\nhuman rationales. Whereas prior works primarily evaluate such ER models via\nin-distribution (ID) generalization, ER's impact on out-of-distribution (OOD)\nis largely underexplored. Plus, little is understood about how ER model\nperformance is affected by the choice of ER criteria or by the number/choice of\ntraining instances with human rationales. In light of this, we propose ER-TEST,\na protocol for evaluating ER models' OOD generalization along three dimensions:\n(1) unseen datasets, (2) contrast set tests, and (3) functional tests. Using\nER-TEST, we study three key questions: (A) Which ER criteria are most effective\nfor the given OOD setting? (B) How is ER affected by the number/choice of\ntraining instances with human rationales? (C) Is ER effective with distantly\nsupervised human rationales? ER-TEST enables comprehensive analysis of these\nquestions by considering a diverse range of tasks and datasets. Through\nER-TEST, we show that ER has little impact on ID performance, but can yield\nlarge gains on OOD performance w.r.t. (1)-(3). Also, we find that the best ER\ncriterion is task-dependent, while ER can improve OOD performance even with\nlimited and distantly-supervised human rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1\">Brihi Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RLPrompt: Optimizing Discrete Text Prompts With Reinforcement Learning. (arXiv:2205.12548v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12548","description":"<p>Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by \"enumeration\n(e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Mingkai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cheng-Ping Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Meng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12554","description":"<p>Task-oriented dialogue systems aim to answer questions from users and provide\nimmediate help. Therefore, how humans perceive their helpfulness is important.\nHowever, neither the human-perceived helpfulness of task-oriented dialogue\nsystems nor its fairness implication has been studied yet. In this paper, we\ndefine a dialogue response as helpful if it is relevant &amp; coherent, useful, and\ninformative to a query and study computational measurements of helpfulness.\nThen, we propose utilizing the helpfulness level of different groups to gauge\nthe fairness of a dialogue system. To study this, we collect human annotations\nfor the helpfulness of dialogue responses and build a classifier that can\nautomatically determine the helpfulness of a response. We design experiments\nunder 3 information-seeking scenarios and collect instances for each from\nWikipedia. With collected instances, we use carefully-constructed questions to\nquery the state-of-the-art dialogue systems. Through analysis, we find that\ndialogue systems tend to be more helpful for highly-developed countries than\nless-developed countries, uncovering a fairness issue underlying these dialogue\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Sampling from Language Models via Langevin Dynamics in Embedding Spaces. (arXiv:2205.12558v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12558","description":"<p>Large pre-trained language models are well-established for their ability to\ngenerate text seemingly indistinguishable from humans. In this work, we study\nthe problem of constrained sampling from such language models. That is,\ngenerating text that satisfies user-defined constraints. Typical decoding\nstrategies which generate samples left-to-right are not always conducive to\nimposing such constraints globally. Instead, we propose MuCoLa -- a sampling\nprocedure that combines the log-likelihood of the language model with arbitrary\ndifferentiable constraints into a single energy function; and generates samples\nby initializing the entire output sequence with noise and following a Markov\nchain defined by Langevin Dynamics using the gradients of this energy. We\nevaluate our approach on different text generation tasks with soft and hard\nconstraints as well as their combinations with competitive results for toxicity\navoidance, sentiment control, and keyword-guided generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paria_B/0/1/0/all/0/1\">Biswajit Paria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing. (arXiv:2205.12570v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12570","description":"<p>Existing work on Entity Linking mostly assumes that the reference knowledge\nbase is complete, and therefore all mentions can be linked. In practice this is\nhardly ever the case, as knowledge bases are incomplete and because novel\nconcepts arise constantly. This paper created the Unknown Entity Discovery and\nIndexing (EDIN) benchmark where unknown entities, that is entities without a\ndescription in the knowledge base and labeled mentions, have to be integrated\ninto an existing entity linking system. By contrasting EDIN with zero-shot\nentity linking, we provide insight on the additional challenges it poses.\nBuilding on dense-retrieval based entity linking, we introduce the end-to-end\nEDIN pipeline that detects, clusters, and indexes mentions of unknown entities\nin context. Experiments show that indexing a single embedding per entity\nunifying the information of multiple mentions works better than indexing\nmentions independently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plekhanov_M/0/1/0/all/0/1\">Mikhail Plekhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cancedda_N/0/1/0/all/0/1\">Nicola Cancedda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Unified Tagging Model with Priming for Relational Structure Predictions. (arXiv:2205.12585v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12585","description":"<p>Relational structure extraction covers a wide range of tasks and plays an\nimportant role in natural language processing. Recently, many approaches tend\nto design sophisticated graphical models to capture the complex relations\nbetween objects that are described in a sentence. In this work, we demonstrate\nthat simple tagging models can surprisingly achieve competitive performances\nwith a small trick -- priming. Tagging models with priming append information\nabout the operated objects to the input sequence of pretrained language model.\nMaking use of the contextualized nature of pretrained language model, the\npriming approach help the contextualized representation of the sentence better\nembed the information about the operated objects, hence, becomes more suitable\nfor addressing relational structure extraction. We conduct extensive\nexperiments on three different tasks that span ten datasets across five\ndifferent languages, and show that our model is a general and effective model,\ndespite its simplicity. We further carry out comprehensive analysis to\nunderstand our model and propose an efficient approximation to our method,\nwhich can perform almost the same performance but with faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Premkumar Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbation Augmentation for Fairer NLP. (arXiv:2205.12586v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12586","description":"<p>Unwanted and often harmful social biases are becoming ever more salient in\nNLP research, affecting both models and datasets. In this work, we ask: does\ntraining on demographically perturbed data lead to more fair language models?\nWe collect a large dataset of human annotated text perturbations and train an\nautomatic perturber on it, which we show to outperform heuristic alternatives.\nWe find: (i) Language models (LMs) pre-trained on demographically perturbed\ncorpora are more fair, at least, according to our current best metrics for\nmeasuring model fairness, and (ii) LMs finetuned on perturbed GLUE datasets\nexhibit less demographic bias on downstream tasks. We find that improved\nfairness does not come at the expense of accuracy. Although our findings appear\npromising, there are still some limitations, as well as outstanding questions\nabout how best to evaluate the (un)fairness of large language models. We hope\nthat this initial exploration of neural demographic perturbation will help\ndrive more improvement towards fairer NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rebecca Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Jude Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText Generators. (arXiv:2205.12590v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12590","description":"<p>In this paper, we study the task of improving the cohesion and coherence of\nlong-form text generated by language models. To this end, we propose RSTGen, a\nframework that utilises Rhetorical Structure Theory (RST), a classical language\ntheory, to control the discourse structure, semantics and topics of generated\ntext. Firstly, we demonstrate our model's ability to control structural\ndiscourse and semantic features of generated text in open generation\nevaluation. Then we experiment on the two challenging long-form text tasks of\nargument generation and story generation. Evaluation using automated metrics\nand a metric with high correlation to human evaluation, shows that our model\nperforms competitively against existing models, while offering significantly\nmore controls over generated text than alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewoyin_R/0/1/0/all/0/1\">Rilwan A. Adewoyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_R/0/1/0/all/0/1\">Ritabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12593","description":"<p>Many recent works indicate that the deep neural networks tend to take dataset\nbiases as shortcuts to make decision, rather than understand the tasks, which\nresults in failures on the real-world applications. In this work, we focus on\nthe spurious correlation between feature and label, which derive from the\nbiased data distribution in the training data, and analyze it concretely. In\nparticular, we define the word highly co-occurring with a specific label as\nbiased word, and the example containing biased word as biased example. Our\nanalysis reveals that the biased examples with spurious correlations are easier\nfor models to learn, and when predicting, the biased words make significantly\nhigher contributions to models' predictions than other words, and the models\ntend to assign the labels over-relying on the spurious correlation between\nwords and labels. To mitigate the model's over-reliance on the shortcut, we\npropose a training strategy Less-Learn-Shortcut (LLS): we quantify the biased\ndegree of the biased examples, and down-weight them with the biased degree.\nExperimental results on QM and NLI tasks show that the models improve the\nperformances both on in-domain and adversarial data (1.57% on DuQM and 2.12% on\nHANS) with our LLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yanrui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning. (arXiv:2205.12598v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12598","description":"<p>Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in English natural\nlanguage. While the progress is promising, it is currently unclear if these\nmodels indeed perform logical reasoning by understanding the underlying logical\nsemantics in the language. To this end, we propose RobustLR, a suite of\nevaluation datasets that evaluate the robustness of these models to minimal\nlogical edits in rulebases and some standard logical equivalence conditions. In\nour experiments with RoBERTa and T5, we find that the models trained in prior\nworks do not perform consistently on the different perturbations in RobustLR,\nthus showing that the models are not robust to the proposed logical\nperturbations. Further, we find that the models find it especially hard to\nlearn logical negation and disjunction operators. Overall, using our evaluation\nsets, we demonstrate some shortcomings of the deductive reasoning-based\nlanguage models, which can eventually help towards designing better models for\nlogical reasoning over natural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zeyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data. (arXiv:2205.12600v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12600","description":"<p>Large pretrained language models have been performing increasingly well in a\nvariety of downstream tasks via prompting. However, it remains unclear from\nwhere the model learns the task-specific knowledge, especially in a zero-shot\nsetup. In this work, we want to find evidence of the model's task-specific\ncompetence from pretraining and are specifically interested in locating a very\nsmall subset of pretraining data that directly supports the model in the task.\nWe call such a subset supporting data evidence and propose a novel method ORCA\nto effectively identify it, by iteratively using gradient information related\nto the downstream task. This supporting data evidence offers interesting\ninsights about the prompted language models: in the tasks of sentiment analysis\nand textual entailment, BERT shows a substantial reliance on BookCorpus, the\nsmaller corpus of BERT's two pretraining corpora, as well as on pretraining\nexamples that mask out synonyms to the task verbalizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intermediate Training on Question Answering Datasets Improves Generative Data Augmentation. (arXiv:2205.12604v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12604","description":"<p>Manually annotating datasets requires domain experts to read through many\ndocuments and carefully label them, which is often expensive. Recently,\npre-trained generative language models (GLMs) have demonstrated exceptional\nabilities in generating text which motivates to leverage them for generative\ndata augmentation. We improve generative data augmentation by formulating the\ndata generation as context generation task and use question answering (QA)\ndatasets for intermediate training. Specifically, we view QA to be more as a\nformat than of a task and train GLMs as context generators for a given question\nand its respective answer. Then, we cast downstream tasks into question\nanswering format and adapt the fine-tuned context generators to the target task\ndomain. Finally, we use the fine-tuned GLM to generate relevant contexts, which\nis further used as synthetic training data for their corresponding tasks. We\nperform extensive experiments, case studies, and ablation studies on multiple\nsentiment and topic classification datasets and demonstrate substantial\nimprovements in performance in few-shot, zero-shot settings. Remarkably, on the\nSST-2 dataset, intermediate training on SocialIQA dataset achieves an\nimprovement of 40% on Macro-F1 score. Through thorough analyses, we observe\nthat QA datasets that requires high-level reasoning abilities (e.g.,\nabstractive and common-sense QA datasets) tend to give the best boost in\nperformance in both few-shot and zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Realistic Generation of Information-Seeking Conversations. (arXiv:2205.12609v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12609","description":"<p>In this paper, we introduce a novel framework SimSeek (simulating\ninformation-seeking conversation from unlabeled documents) and compare two\nvariants of it to provide a deeper perspective into the information-seeking\nbehavior. We first introduce a strong simulator for information-symmetric\nconversation, SimSeek-sym, where questioner and answerer share all knowledge\nwhen conversing with one another. Although it simulates reasonable\nconversations, we take a further step toward more realistic information-seeking\nconversation. Hence, we propose SimSeek-asym that assumes information asymmetry\nbetween two agents, which encourages the questioner to seek new information\nfrom an inaccessible document. In our experiments, we demonstrate that\nSimSeek-asym successfully generates information-seeking conversations for two\ndownstream tasks, CQA and conversational search. In particular, SimSeek-asym\nimproves baseline models by 1.1-1.9 F1 score in QuAC, and by 1.1 of MRR in\nOR-QuAC. Moreover, we thoroughly analyze our synthetic datasets to identify\ncrucial factors for realistic information-seeking conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisinfoMeme: A Multimodal Dataset for Detecting Meme Intentionally Spreading Out Disinformation. (arXiv:2205.12617v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12617","description":"<p>Disinformation has become a serious problem on social media. In particular,\ngiven their short format, visual attraction, and humorous nature, memes have a\nsignificant advantage in dissemination among online communities, making them an\neffective vehicle for the spread of disinformation. We present DisinfoMeme to\nhelp detect disinformation memes. The dataset contains memes mined from Reddit\ncovering three current topics: the COVID-19 pandemic, the Black Lives Matter\nmovement, and veganism/vegetarianism. The dataset poses multiple unique\nchallenges: limited data and label imbalance, reliance on external knowledge,\nmultimodal reasoning, layout dependency, and noise from OCR. We test multiple\nwidely-used unimodal and multimodal models on this dataset. The experiments\nshow that the room for improvement is still huge for current models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jingnong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased and Efficient Sampling of Dependency Trees. (arXiv:2205.12621v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12621","description":"<p>Distributions over spanning trees are the most common way of computational\nmodeling of dependency syntax. However, most treebanks require that every valid\ndependency tree has a single edge coming out of the ROOT node, a constraint\nthat is not part of the definition of spanning trees. For this reason all\nstandard inference algorithms for spanning trees are sub-optimal for modeling\ndependency trees.\n</p>\n<p>Zmigrod et al. (2021b) have recently proposed algorithms for sampling with\nand without replacement from the single-root dependency tree distribution. In\nthis paper we show that their fastest algorithm for sampling with replacement,\nWilson-RC, is in fact producing biased samples and we provide two alternatives\nthat are unbiased. Additionally, we propose two algorithms (one incremental,\none parallel) that reduce the asymptotic runtime of their algorithm for\nsampling $k$ trees without replacement to $\\mathcal{O}(kn^3)$. These algorithms\nare both asymptotically and practically more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Pre-Trained Language Models Leaking Your Personal Information?. (arXiv:2205.12628v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12628","description":"<p>Large Pre-Trained Language Models (PLMs) have facilitated and dominated many\nNLP tasks in recent years. However, despite the great success of PLMs, there\nare also privacy concerns brought with PLMs. For example, recent studies show\nthat PLMs memorize a lot of training data, including sensitive information,\nwhile the information may be leaked unintentionally and be utilized by\nmalicious attackers.\n</p>\n<p>In this paper, we propose to measure whether PLMs are prone to leaking\npersonal information. Specifically, we attempt to query PLMs for email\naddresses with contexts of the email address or prompts containing the owner's\nname. We find that PLMs do leak personal information due to memorization.\nHowever, the risk of specific personal information being extracted by attackers\nis low because the models are weak at associating the personal information with\nits owner. We hope this work could help the community to better understand the\nprivacy risk of PLMs and bring new insights to make PLMs safe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hanyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Knowledge Alignment with Reinforcement Learning. (arXiv:2205.12630v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12630","description":"<p>Large language models readily adapt to novel settings, even without\ntask-specific training data. Can their zero-shot capacity be extended to\nmultimodal inputs? In this work, we propose ESPER which extends language-only\nzero-shot models to unseen multimodal tasks, like image and audio captioning.\nOur key novelty is to use reinforcement learning to align multimodal inputs to\nlanguage model generations without direct supervision: for example, in the\nimage case our reward optimization relies only on cosine similarity derived\nfrom CLIP, and thus requires no additional explicitly paired (image, caption)\ndata. Because the parameters of the language model are left unchanged, the\nmodel maintains its capacity for zero-shot generalization. Experiments\ndemonstrate that ESPER outperforms baselines and prior work on a variety of\nzero-shot tasks; these include a new benchmark we collect+release, ESP dataset,\nwhich tasks models with generating several diversely-styled captions for each\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jiwan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Heeseung Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">JaeSung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring industrial safety knowledge via Zipf law. (arXiv:2205.12636v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12636","description":"<p>The hazard and operability analysis (HAZOP) report contains precious\nindustrial safety knowledge (ISK) with expert experience and process nature,\nwhich is of great significance to the development of industrial intelligence.\nSubject to the attributes of ISK, existing researches mine them through\nsequence labeling in deep learning. Yet, there are two thorny issues: (1)\nUneven distribution of ISK and (2) Consistent importance of ISK: for safety\nreview. In this study, we propose a novel generative mining strategy called\nCRGM to explore ISK. Inspired Zipf law in linguistics, CRGM consists of\ncommon-rare discriminator, induction-extension generator and ISK extractor.\nFirstly, the common-rare discriminator divides HAZOP descriptions into common\nwords and rare words, and obtains the common description and the rare\ndescription, where the latter contains more industrial substances. Then, they\nare operated by the induction-extension generator in the way of deep text\ngeneration, the common description is induced and the rare description is\nextended, the material knowledge and the equipment knowledge can be enriched.\nFinally, the ISK extractor processes the material knowledge and equipment\nknowledge from the generated description through the rule template method, the\nadditional ISK is regarded as the supplement of the training set to train the\nproposed sequence labeling model. We conduct multiple evaluation experiments on\ntwo industrial safety datasets. The results show that CRGM has promising and\ngratifying aptitudes, greatly improves the performance of the model, and is\nefficient and generalized. Our sequence labeling model also shows the expected\nperformance, which is better than the existing research. Our research provides\na new perspective for exploring ISK, we hope it can contribute support for the\nintelligent progress of industrial safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Ming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing. (arXiv:2205.12640v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12640","description":"<p>The entity typing task aims at predicting one or more words or phrases that\ndescribe the type(s) of a specific mention in a sentence. Due to shortcuts from\nsurface patterns to annotated entity labels and biased training, existing\nentity typing models are subject to the problem of spurious correlations. To\ncomprehensively investigate the faithfulness and reliability of entity typing\nmethods, we first systematically define distinct kinds of model biases that are\nreflected mainly from spurious correlations. Particularly, we identify six\ntypes of existing model biases, including mention-context bias, lexical\noverlapping bias, named entity bias, pronoun bias, dependency bias, and\novergeneralization bias. To mitigate these model biases, we then introduce a\ncounterfactual data augmentation method. By augmenting the original training\nset with their bias-free counterparts, models are forced to fully comprehend\nthe sentences and discover the fundamental cues for entity typing, rather than\nrelying on spurious correlations for shortcuts. Experimental results on the\nUFET dataset show that our counterfactual data augmentation approach helps\nimprove generalization of different entity typing models with consistently\nbetter performance on both in- and out-of-distribution test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bangzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingtao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking the Right Questions in Low Resource Template Extraction. (arXiv:2205.12643v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12643","description":"<p>Information Extraction (IE) researchers are mapping tasks to Question\nAnswering (QA) in order to leverage existing large QA resources, and thereby\nimprove data efficiency. Especially in template extraction (TE), mapping an\nontology to a set of questions can be more time-efficient than collecting\nlabeled examples. We ask whether end users of TE systems can design these\nquestions, and whether it is beneficial to involve an NLP practitioner in the\nprocess. We compare questions to other ways of phrasing natural language\nprompts for TE. We propose a novel model to perform TE with prompts, and find\nit benefits from questions over other styles of prompts, and that they do not\nrequire an NLP background to author.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution. (arXiv:2205.12644v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12644","description":"<p>While coreference resolution typically involves various linguistic\nchallenges, recent models are based on a single pairwise scorer for all types\nof pairs. We present LingMess, a new coreference model that defines different\ncategories of coreference cases and optimize multiple pairwise scorers, where\neach scorer learns a specific set of linguistic challenges. Our model\nsubstantially improves pairwise scores for most categories and outperforms\ncluster-level performance on Ontonotes. Our model is available in\nhttps://github.com/shon-otmazgin/lingmess-coref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otmazgin_S/0/1/0/all/0/1\">Shon Otmazgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation. (arXiv:2205.12647v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12647","description":"<p>In this paper, we explore the challenging problem of performing a generative\ntask (i.e., summarization) in a target language when labeled data is only\navailable in English. We assume a strict setting with no access to parallel\ndata or machine translation. Prior work has shown, and we confirm, that\nstandard transfer learning techniques struggle in this setting, as a generative\nmultilingual model fine-tuned purely on English catastrophically forgets how to\ngenerate non-English. Given the recent rise of parameter-efficient adaptation\ntechniques (e.g., prompt tuning), we conduct the first investigation into how\nwell these methods can overcome catastrophic forgetting to enable zero-shot\ncross-lingual generation. We find that parameter-efficient adaptation provides\ngains over standard fine-tuning when transferring between less-related\nlanguages, e.g., from English to Thai. However, a significant gap still remains\nbetween these methods and fully-supervised baselines. To improve cross-lingual\ntransfer further, we explore three approaches: (1) mixing in unlabeled\nmultilingual data, (2) pre-training prompts on target language data, and (3)\nexplicitly factoring prompts into recombinable language and task components.\nOur methods can provide further quality gains, suggesting that robust zero-shot\ncross-lingual generation is within reach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1\">Aditya Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation. (arXiv:2205.12649v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12649","description":"<p>Code-switching (CS) poses several challenges to NLP tasks, where data\nsparsity is a main problem hindering the development of CS NLP systems. In this\npaper, we investigate data augmentation techniques for synthesizing Dialectal\nArabic-English CS text. We perform lexical replacements using parallel corpora\nand alignments where CS points are either randomly chosen or learnt using a\nsequence-to-sequence model. We evaluate the effectiveness of data augmentation\non language modeling (LM), machine translation (MT), and automatic speech\nrecognition (ASR) tasks. Results show that in the case of using 1-1 alignments,\nusing trained predictive models produces more natural CS sentences, as\nreflected in perplexity. By relying on grow-diag-final alignments, we then\nidentify aligning segments and perform replacements accordingly. By replacing\nsegments instead of words, the quality of synthesized data is greatly improved.\nWith this improvement, random-based approach outperforms using trained\npredictive models on all extrinsic tasks. Our best models achieve 33.6%\nimprovement in perplexity, +3.2-5.6 BLEU points on MT task, and 7% relative\nimprovement on WER for ASR task. We also contribute in filling the gap in\nresources by collecting and publishing the first Arabic English CS-English\nparallel corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA. (arXiv:2205.12650v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12650","description":"<p>We study unsupervised multi-hop reranking for multi-hop QA (MQA) with\nopen-domain questions. Since MQA requires piecing information from multiple\ndocuments, the main challenge thus resides in retrieving and reranking chains\nof passages that support the reasoning process. Our approach relies on LargE\nmodels with Prompt-Utilizing reranking Strategy (LEPUS): we construct an\ninstruction-like prompt based on a candidate document path and compute a\nrelevance score of the path as the probability of generating a given question,\naccording to a pre-trained language model. Though unsupervised, LEPUS yields\ncompetitive reranking performance against state-of-the-art methods that are\ntrained on thousands of examples. Adding a small number of samples (e.g., $2$),\nwe demonstrate further performance gain using in-context learning. Finally, we\nshow that when integrated with a reader module, LEPUS can obtain competitive\nmulti-hop QA performance, e.g., outperforming fully-supervised QA systems.\n</p>\n<p>Code will be released at https://github.com/mukhal/LEPUS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages. (arXiv:2205.12654v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12654","description":"<p>Scaling multilingual representation learning beyond the hundred most frequent\nlanguages is challenging, in particular to cover the long tail of low-resource\nlanguages. A promising approach has been to train one-for-all multilingual\nmodels capable of cross-lingual transfer, but these models often suffer from\ninsufficient capacity and interference between unrelated languages. Instead, we\nmove away from this approach and focus on training multiple language (family)\nspecific representations, but most prominently enable all languages to still be\nencoded in the same representational space. To achieve this, we focus on\nteacher-student training, allowing all encoders to be mutually compatible for\nbitext mining, and enabling fast learning of new languages. We introduce a new\nteacher-student training scheme which combines supervised and self-supervised\ntraining, allowing encoders to take advantage of monolingual training data,\nwhich is valuable in the low-resource setting.\n</p>\n<p>Our approach significantly outperforms the original LASER encoder. We study\nvery low-resource languages and handle 50 African languages, many of which are\nnot covered by any other model. For these languages, we train sentence\nencoders, mine bitexts, and validate the bitexts by training NMT systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celebi_O/0/1/0/all/0/1\">Onur &#xc7;elebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogZoo: Large-Scale Dialog-Oriented Task Learning. (arXiv:2205.12662v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12662","description":"<p>Building unified conversational agents has been a long-standing goal of the\ndialogue research community. Most previous works only focus on a subset of\nvarious dialogue tasks. In this work, we aim to build a unified foundation\nmodel which can solve massive diverse dialogue tasks. To achieve this goal, we\nfirst collect a large-scale well-labeled dialogue dataset from 73 publicly\navailable datasets. In addition to this dataset, we further propose two\ndialogue-oriented self-supervised tasks, and finally use the mixture of\nsupervised and self-supervised datasets to train our foundation model. The\nsupervised examples make the model learn task-specific skills, while the\nself-supervised examples make the model learn more general skills. We evaluate\nour model on various downstream dialogue tasks. The experimental results show\nthat our method not only improves the ability of dialogue generation and\nknowledge distillation, but also the representation ability of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jijia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuncong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Su Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12665","description":"<p>Existing benchmarks for open-domain question answering (ODQA) typically focus\non questions whose answers can be extracted from a single paragraph. By\ncontrast, many natural questions, such as \"What players were drafted by the\nBrooklyn Nets?\" have a list of answers. Answering such questions requires\nretrieving and reading from many passages, in a large corpus. We introduce\nQAMPARI, an ODQA benchmark, where question answers are lists of entities,\nspread across many paragraphs. We created QAMPARI by (a) generating questions\nwith multiple answers from Wikipedia's knowledge graph and tables, (b)\nautomatically pairing answers with supporting evidence in Wikipedia paragraphs,\nand (c) manually paraphrasing questions and validating each answer. We train\nODQA models from the retrieve-and-read family and find that QAMPARI is\nchallenging in terms of both passage retrieval and answer generation, reaching\nan F1 score of 26.6 at best. Our results highlight the need for developing ODQA\nmodels that handle a broad range of question types, including single and\nmulti-answer questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rubin_S/0/1/0/all/0/1\">Samuel Joseph Amouyal Ohad Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Language-neutral Sub-networks in Multilingual Language Models. (arXiv:2205.12672v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12672","description":"<p>Multilingual pre-trained language models perform remarkably well on\ncross-lingual transfer for downstream tasks. Despite their impressive\nperformance, our understanding of their language neutrality (i.e., the extent\nto which they use shared representations to encode similar phenomena across\nlanguages) and its role in achieving such performance remain open questions. In\nthis work, we conceptualize language neutrality of multilingual models as a\nfunction of the overlap between language-encoding sub-networks of these models.\nUsing mBERT as a foundation, we employ the lottery ticket hypothesis to\ndiscover sub-networks that are individually optimized for various languages and\ntasks. Using three distinct tasks and eleven typologically-diverse languages in\nour evaluation, we show that the sub-networks found for different languages are\nin fact quite similar, supporting the idea that mBERT jointly encodes multiple\nlanguages in shared parameters. We conclude that mBERT is comprised of a\nlanguage-neutral sub-network shared among many languages, along with multiple\nancillary language-specific sub-networks, with the former playing a more\nprominent role in mBERT's impressive cross-lingual performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foroutan_N/0/1/0/all/0/1\">Negar Foroutan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1\">Mohammadreza Banaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">Remi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning. (arXiv:2205.12673v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12673","description":"<p>Instruction tuning is an emergent paradigm in NLP wherein natural language\ninstructions are leveraged with language models to induce zero-shot performance\non unseen tasks. Instructions have been shown to enable good performance on\nunseen tasks and datasets in both large and small language models. Dialogue is\nan especially interesting area to explore instruction tuning because dialogue\nsystems perform multiple kinds of tasks related to language (e.g., natural\nlanguage understanding and generation, domain-specific interaction), yet\ninstruction tuning has not been systematically explored for dialogue-related\ntasks. We introduce InstructDial, an instruction tuning framework for dialogue,\nwhich consists of a repository of 48 diverse dialogue tasks in a unified\ntext-to-text format created from 59 openly available dialogue datasets. Next,\nwe explore cross-task generalization ability on models tuned on InstructDial\nacross diverse dialogue tasks. Our analysis reveals that InstructDial enables\ngood zero-shot performance on unseen datasets and tasks such as dialogue\nevaluation and intent detection, and even better performance in a few-shot\nsetting. To ensure that models adhere to instructions, we introduce novel\nmeta-tasks. We establish benchmark zero-shot and few-shot performance of models\ntrained using the proposed framework on multiple dialogue tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_C/0/1/0/all/0/1\">Cathy Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Memory Augmentation. (arXiv:2205.12674v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12674","description":"<p>Recent work has improved language models remarkably by equipping them with a\nnon-parametric memory component. However, most existing approaches only\nintroduce memories at testing time, or represent them using a separately\ntrained encoder -- resulting in sub-optimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining language models with memory augmentation. Our approach uses a training\nobjective that directly takes in-batch examples as accessible memory. We also\npresent new methods for memory construction and data batching, which are used\nfor adapting to different sets of memories -- local, long-term, and external\nmemory -- at testing time. We evaluate our approach on multiple language\nmodeling and machine translation benchmarks. We find that simply replacing the\nvanilla language modeling objective by ours greatly reduces the perplexity,\nwithout modifying the model architecture or incorporating extra context (e.g.,\n18.70 $\\to$ 17.76 on WikiText-103). We further augment language models with\nlong-range contexts and external knowledge and demonstrate significant gains\nover previous memory-augmented approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages. (arXiv:2205.12676v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12676","description":"<p>In order for NLP technology to be widely applicable and useful, it needs to\nbe inclusive of users across the world's languages, equitable, i.e., not unduly\nbiased towards any particular language, and accessible to users, particularly\nin low-resource settings where compute constraints are common. In this paper,\nwe propose an evaluation paradigm that assesses NLP technologies across all\nthree dimensions, hence quantifying the diversity of users they can serve.\nWhile inclusion and accessibility have received attention in recent literature,\nequity is currently unexplored. We propose to address this gap using the Gini\ncoefficient, a well-established metric used for estimating societal wealth\ninequality. Using our paradigm, we highlight the distressed state of diversity\nof current technologies for Indian (IN) languages, motivated by their\nlinguistic diversity and large, varied speaker population. To improve upon\nthese metrics, we demonstrate the importance of region-specific choices in\nmodel building and dataset creation and also propose a novel approach to\noptimal resource allocation during fine-tuning. Finally, we discuss steps that\nmust be taken to mitigate these biases and call upon the community to\nincorporate our evaluation paradigm when building linguistically diverse\ntechnologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12677","description":"<p>Pre-trained language models learn large amounts of knowledge from their\ntraining corpus, while the memorized facts could become outdated over a few\nyears. Model editing aims to make post-hoc updates on specific facts in a model\nwhile leaving irrelevant knowledge unchanged. However, existing work studies\nonly the monolingual scenario. In this paper, we focus on cross-lingual model\nediting. Firstly, we propose the definition and metrics of the cross-lingual\nmodel editing, where updates in a single language should take effect in the\nothers as well. Next, we propose a simple framework to convert a monolingual\nmodel editing approach to its cross-lingual variant using the parallel corpus.\nExperiments show that such an approach outperforms monolingual baselines by a\nlarge margin. Furthermore, we propose language anisotropic editing to improve\ncross-lingual editing by estimating parameter importance for each language.\nExperiments reveal that language anisotropic editing decreases the editing\nfailing rate by another $26\\%$ relatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroGen$^+$: Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning. (arXiv:2205.12679v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12679","description":"<p>Nowadays, owing to the superior capacity of the large pre-trained language\nmodels (PLM), the PLM-based zero-shot learning has shown promising performances\non various natural language processing tasks. There are emerging interests in\nfurther exploring the zero-shot learning potential of PLMs. Among them, ZeroGen\nattempts to purely use PLM to generate data and train a tiny model without\nrelying on any task-specific annotation. Despite its remarkable results, we\nobserve that the synthesized data from PLM contains a significant portion of\nsamples with low quality, overfitting on such data greatly hampers the\nperformance of the trained model and makes it unreliable for deployment.Since\nno gold data is accessible in zero-shot scenario, it is hard to perform\nmodel/data selection to prevent overfitting to the low-quality data. To address\nthis problem, we propose a noise-robust bi-level re-weighting framework which\nis able to learn the per-sample weights measuring the data quality without\nrequiring any gold data. With the learnt weights, clean subsets of different\nsizes can then be sampled to train the task model. We theoretically and\nempirically verify our method is able to construct synthetic dataset with good\nquality. Our method yeilds a 7.1% relative improvement than ZeroGen on average\naccuracy across five different established text classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining Query Representations for Dense Retrieval at Test Time. (arXiv:2205.12680v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12680","description":"<p>Dense retrieval uses a contrastive learning framework to learn dense\nrepresentations of queries and contexts. Trained encoders are directly used for\neach test query, but they often fail to accurately represent out-of-domain\nqueries. In this paper, we introduce a framework that refines instance-level\nquery representations at test time, with only the signals coming from the\nintermediate retrieval results. We optimize the query representation based on\nthe retrieval result similar to pseudo relevance feedback (PRF) in information\nretrieval. Specifically, we adopt a cross-encoder labeler to provide pseudo\nlabels over the retrieval result and iteratively refine the query\nrepresentation with a gradient descent method, treating each test query as a\nsingle data point to train on. Our theoretical analysis reveals that our\nframework can be viewed as a generalization of the classical Rocchio's\nalgorithm for PRF, which leads us to propose interesting variants of our\nmethod. We show that our test-time query refinement strategy improves the\nperformance of phrase retrieval (+8.1% Acc@1) and passage retrieval (+3.7%\nAcc@20) for open-domain QA with large improvements on out-of-domain queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations. (arXiv:2205.12685v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12685","description":"<p>Despite recent explosion in research interests, in-context learning and the\nprecise impact of the quality of demonstrations remain elusive. While, based on\ncurrent literature, it is expected that in-context learning shares a similar\nmechanism to supervised learning, Min et al. (2022) recently reported that,\nsurprisingly, input-label correspondence is less important than other aspects\nof prompt demonstrations. Inspired by this counter-intuitive observation, we\nre-examine the importance of ground truth labels on in-context learning from\ndiverse and statistical points of view. With the aid of the newly introduced\nmetrics, i.e., Ground-truth Label Effect Ratio (GLER), demo-gain, and label\nsensitivity, we find that the impact of the correct input-label matching can\nvary according to different configurations. Expanding upon the previous key\nfinding on the role of demonstrations, the complementary and contrastive\nresults suggest that one might need to take more care when estimating the\nimpact of each component in in-context learning demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Hwiyeol Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsocialDialog: A Prosocial Backbone for Conversational Agents. (arXiv:2205.12688v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12688","description":"<p>Most existing dialogue systems fail to respond properly to potentially unsafe\nuser utterances by either ignoring or passively agreeing with them. To address\nthis issue, we introduce ProsocialDialog, the first large-scale multi-turn\ndialogue dataset to teach conversational agents to respond to problematic\ncontent following social norms. Covering diverse unethical, problematic,\nbiased, and toxic situations, ProsocialDialog contains responses that encourage\nprosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,\nRoTs). Created via a human-AI collaborative framework, ProsocialDialog consists\nof 58K dialogues, with 331K utterances, 160K RoTs, and 497K dialogue safety\nlabels accompanied by free-form rationales.\n</p>\n<p>With this dataset, we introduce a dialogue safety detection module, Canary,\ncapable of generating RoTs given conversational context, and a\nsocially-informed dialogue agent, Prost. Empirical results show that Prost\ngenerates more socially acceptable dialogues compared to other state-of-the-art\nlanguage and dialogue models in both in-domain and out-of-domain settings.\nAdditionally, Canary effectively guides conversational agents and off-the-shelf\nlanguage models to generate significantly more prosocial responses. Our work\nhighlights the promise and importance of creating and steering conversational\nAI to be socially responsible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Zero-Shot Clinical Information Extractors. (arXiv:2205.12689v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12689","description":"<p>We show that large language models, such as GPT-3, perform well at zero-shot\ninformation extraction from clinical text despite not being trained\nspecifically for the clinical domain. We present several examples showing how\nto use these models as tools for the diverse tasks of (i) concept\ndisambiguation, (ii) evidence extraction, (iii) coreference resolution, and\n(iv) concept extraction, all on clinical text. The key to good performance is\nthe use of simple task-specific programs that map from the language model\noutputs to the label space of the task. We refer to these programs as\nresolvers, a generalization of the verbalizer, which defines a mapping between\noutput tokens and a discrete label space. We show in our examples that good\nresolvers share common components (e.g., \"safety checks\" that ensure the\nlanguage model outputs faithfully match the input data), and that the common\npatterns across tasks make resolvers lightweight and easy to create. To better\nevaluate these systems, we also introduce two new datasets for benchmarking\nzero-shot clinical information extraction based on manual relabeling of the\nCASI dataset (Moon et al., 2014) with labels for new tasks. On the clinical\nextraction tasks we studied, the GPT-3 + resolver systems significantly\noutperform existing zero- and few-shot baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Monica Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegselmann_S/0/1/0/all/0/1\">Stefan Hegselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Natural Language in Context. (arXiv:2205.12691v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12691","description":"<p>Recent years have seen an increasing number of applications that have a\nnatural language interface, either in the form of chatbots or via personal\nassistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana\n(Microsoft). To use these applications, a basic dialog between the robot and\nthe human is required.\n</p>\n<p>While this kind of dialog exists today mainly within \"static\" robots that do\nnot make any movement in the household space, the challenge of reasoning about\nthe information conveyed by the environment increases significantly when\ndealing with robots that can move and manipulate objects in our home\nenvironment.\n</p>\n<p>In this paper, we focus on cognitive robots, which have some knowledge-based\nmodels of the world and operate by reasoning and planning with this model.\nThus, when the robot and the human communicate, there is already some formalism\nthey can use - the robot's knowledge representation formalism.\n</p>\n<p>Our goal in this research is to translate natural language utterances into\nthis robot's formalism, allowing much more complicated household tasks to be\ncompleted. We do so by combining off-the-shelf SOTA language models, planning\ntools, and the robot's knowledge-base for better communication. In addition, we\nanalyze different directive types and illustrate the contribution of the\nworld's context to the translation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1\">Avichai Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpas_E/0/1/0/all/0/1\">Erez Karpas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models. (arXiv:2205.12694v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12694","description":"<p>Model compression by way of parameter pruning, quantization, or distillation\nhas recently gained popularity as an approach for reducing the computational\nrequirements of modern deep neural network models for NLP. Pruning unnecessary\nparameters has emerged as a simple and effective method for compressing large\nmodels that is compatible with a wide variety of contemporary off-the-shelf\nhardware (unlike quantization), and that requires little additional training\n(unlike distillation). Pruning approaches typically take a large, accurate\nmodel as input, then attempt to discover a smaller subnetwork of that model\ncapable of achieving end-task accuracy comparable to the full model. Inspired\nby previous work suggesting a connection between simpler, more generalizable\nmodels and those that lie within flat basins in the loss landscape, we propose\nto directly optimize for flat minima while performing task-specific pruning,\nwhich we hypothesize should lead to simpler parameterizations and thus more\ncompressible models. In experiments combining sharpness-aware minimization with\nboth iterative magnitude pruning and structured pruning approaches, we show\nthat optimizing for flat minima consistently leads to greater compressibility\nof parameters compared to standard Adam optimization when fine-tuning BERT\nmodels, leading to higher rates of compression with little to no loss in\naccuracy on the GLUE classification benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Na_C/0/1/0/all/0/1\">Clara Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting DocRED -- Addressing the Overlooked False Negative Problem in Relation Extraction. (arXiv:2205.12696v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12696","description":"<p>The DocRED dataset is one of the most popular and widely used benchmarks for\ndocument-level relation extraction (RE). It adopts a recommend-revise\nannotation scheme so as to have a large-scale annotated dataset. However, we\nfind that the annotation of DocRED is incomplete, i.e., the false negative\nsamples are prevalent. We analyze the causes and effects of the overwhelming\nfalse negative problem in the DocRED dataset. To address the shortcoming, we\nre-annotate 4,053 documents in the DocRED dataset by adding the missed relation\ntriples back to the original DocRED. We name our revised DocRED dataset\nRe-DocRED. We conduct extensive experiments with state-of-the-art neural models\non both datasets, and the experimental results show that the models trained and\nevaluated on our Re-DocRED achieve performance improvements of around 13 F1\npoints. Moreover, we propose different metrics to comprehensively evaluate the\ndocument-level RE task. We make our data publicly available at\nhttps://github.com/tonytan48/Re-DocRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation. (arXiv:2205.12697v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12697","description":"<p>Logical table-to-text generation is a task that involves generating logically\nfaithful sentences from tables, which requires models to derive logical level\nfacts from table records via logical inference. It raises a new challenge on\nthe logical-level content planning of table-to-text models. However, directly\nlearning the logical inference knowledge from table-text pairs is very\ndifficult for neural models because of the ambiguity of natural language and\nthe scarcity of parallel data. Hence even large-scale pre-trained language\nmodels present low logical fidelity on logical table-to-text. In this work, we\npropose a PLOG (Pretrained Logical Form Generator) framework to improve the\ngeneration fidelity. Specifically, PLOG is first pretrained on a\ntable-to-logic-form generation (table-to-logic) task, then finetuned on\ndownstream table-to-text tasks. The formal definition of logical forms enables\nus to collect large amount of accurate logical forms from tables without human\nannotation. In addition, PLOG can learn logical inference from table-logic\npairs much more definitely than from table-text pairs. To evaluate our model,\nwe further collect a controlled logical table-to-text dataset CONTLOG based on\nan existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms\nstrong baselines by a large margin on the logical fidelity, demonstrating the\neffectiveness of table-to-logic pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathic Conversations: A Multi-level Dataset of Contextualized Conversations. (arXiv:2205.12698v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12698","description":"<p>Empathy is a cognitive and emotional reaction to an observed situation of\nothers. Empathy has recently attracted interest because it has numerous\napplications in psychology and AI, but it is unclear how different forms of\nempathy (e.g., self-report vs counterpart other-report, concern vs. distress)\ninteract with other affective phenomena or demographics like gender and age. To\nbetter understand this, we created the {\\it Empathic Conversations} dataset of\nannotated negative, empathy-eliciting dialogues in which pairs of participants\nconverse about news articles. People differ in their perception of the empathy\nof others. These differences are associated with certain characteristics such\nas personality and demographics. Hence, we collected detailed characterization\nof the participants' traits, their self-reported empathetic response to news\narticles, their conversational partner other-report, and turn-by-turn\nthird-party assessments of the level of self-disclosure, emotion, and empathy\nexpressed. This dataset is the first to present empathy in multiple forms along\nwith personal distress, emotion, personality characteristics, and person-level\ndemographic information. We present baseline models for predicting some of\nthese features from conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omitaomu_D/0/1/0/all/0/1\">Damilola Omitaomu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafreshi_S/0/1/0/all/0/1\">Shabnam Tafreshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buechel_S/0/1/0/all/0/1\">Sven Buechel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichstaedt_J/0/1/0/all/0/1\">Johannes Eichstaedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Backdoor Attacks with Iterative Trigger Injection. (arXiv:2205.12700v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12700","description":"<p>The backdoor attack has become an emerging threat for Natural Language\nProcessing (NLP) systems. A victim model trained on poisoned data can be\nembedded with a \"backdoor\", making it predict the adversary-specified output\n(e.g., the positive sentiment label) on inputs satisfying the trigger pattern\n(e.g., containing a certain keyword). In this paper, we demonstrate that it's\npossible to design an effective and stealthy backdoor attack by iteratively\ninjecting \"triggers\" into a small set of training data. While all triggers are\ncommon words that fit into the context, our poisoning process strongly\nassociates them with the target label, forming the model backdoor. Experiments\non sentiment analysis and hate speech detection show that our proposed attack\nis both stealthy and effective, raising alarm on the usage of untrusted\ntraining data. We further propose a defense method to combat this threat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vansh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts. (arXiv:2205.12701v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12701","description":"<p>Recent work suggests that transformer models are capable of multi-task\nlearning on diverse NLP tasks. However, the potential of these models may be\nlimited as they use the same set of parameters for all tasks. In contrast,\nhumans tackle tasks in a more flexible way, by making proper presumptions on\nwhat skills and knowledge are relevant and executing only the necessary\ncomputations. Inspired by this, we propose to use task-level mixture-of-expert\nmodels, which has a collection of transformer layers (i.e., experts) and a\nrouter component to choose among these experts dynamically and flexibly. We\nshow that the learned routing decisions and experts partially rediscover human\ncategorization of NLP tasks -- certain experts are strongly associated with\nextractive tasks, some with classification tasks, and some with tasks requiring\nworld knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Juan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Label Errors using Pre-Trained Language Models. (arXiv:2205.12702v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12702","description":"<p>We show that large pre-trained language models are extremely capable of\nidentifying label errors in datasets: simply verifying data points in\ndescending order of out-of-distribution loss significantly outperforms more\ncomplex mechanisms for detecting label errors on natural language datasets. We\ncontribute a novel method to produce highly realistic, human-originated label\nnoise from crowdsourced data, and demonstrate the effectiveness of this method\non TweetNLP, providing an otherwise difficult to obtain measure of realistic\nrecall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Derek Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenny Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Would You Ask it that Way? Measuring and Improving Question Naturalness for Knowledge Graph Question Answering. (arXiv:2205.12768v1 [cs.IR])","link":"http://arxiv.org/abs/2205.12768","description":"<p>Knowledge graph question answering (KGQA) facilitates information access by\nleveraging structured data without requiring formal query language expertise\nfrom the user. Instead, users can express their information needs by simply\nasking their questions in natural language (NL). Datasets used to train KGQA\nmodels that would provide such a service are expensive to construct, both in\nterms of expert and crowdsourced labor. Typically, crowdsourced labor is used\nto improve template-based pseudo-natural questions generated from formal\nqueries. However, the resulting datasets often fall short of representing\ngenuinely natural and fluent language. In the present work, we investigate ways\nto characterize and remedy these shortcomings. We create the IQN-KGQA test\ncollection by sampling questions from existing KGQA datasets and evaluating\nthem with regards to five different aspects of naturalness. Then, the questions\nare rewritten to improve their fluency. Finally, the performance of existing\nKGQA models is compared on the original and rewritten versions of the NL\nquestions. We find that some KGQA systems fare worse when presented with more\nrealistic formulations of NL questions. The IQN-KGQA test collection is a\nresource to help evaluate KGQA systems in a more realistic setting. The\nconstruction of this test collection also sheds light on the challenges of\nconstructing large-scale KGQA datasets with genuinely NL questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Linjordet_T/0/1/0/all/0/1\">Trond Linjordet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy. (arXiv:2205.12771v1 [cs.CY])","link":"http://arxiv.org/abs/2205.12771","description":"<p>In an effort to guarantee that machine learning model outputs conform with\nhuman moral values, recent work has begun exploring the possibility of\nexplicitly training models to learn the difference between right and wrong.\nThis is typically done in a bottom-up fashion, by exposing the model to\ndifferent scenarios, annotated with human moral judgements. One question,\nhowever, is whether the trained models actually learn any consistent,\nhigher-level ethical principles from these datasets -- and if so, what? Here,\nwe probe the Allen AI Delphi model with a set of standardized morality\nquestionnaires, and find that, despite some inconsistencies, Delphi tends to\nmirror the moral principles associated with the demographic groups involved in\nthe annotation process. We question whether this is desirable and discuss how\nwe might move forward with this knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balkir_E/0/1/0/all/0/1\">Esma Balkir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic question generation based on sentence structure analysis using machine learning approach. (arXiv:2205.12811v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12811","description":"<p>Automatic question generation is one of the most challenging tasks of Natural\nLanguage Processing. It requires \"bidirectional\" language processing: firstly,\nthe system has to understand the input text (Natural Language Understanding)\nand it then has to generate questions also in the form of text (Natural\nLanguage Generation). In this article, we introduce our framework for\ngenerating the factual questions from unstructured text in the English\nlanguage. It uses a combination of traditional linguistic approaches based on\nsentence patterns with several machine learning methods. We firstly obtain\nlexical, syntactic and semantic information from an input text and we then\nconstruct a hierarchical set of patterns for each sentence. The set of features\nis extracted from the patterns and it is then used for automated learning of\nnew transformation rules. Our learning process is totally data-driven because\nthe transformation rules are obtained from a set of initial sentence-question\npairs. The advantages of this approach lie in a simple expansion of new\ntransformation rules which allows us to generate various types of questions and\nalso in the continuous improvement of the system by reinforcement learning. The\nframework also includes a question evaluation module which estimates the\nquality of generated questions. It serves as a filter for selecting the best\nquestions and eliminating incorrect ones or duplicates. We have performed\nseveral experiments to evaluate the correctness of generated questions and we\nhave also compared our system with several state-of-the-art systems. Our\nresults indicate that the quality of generated questions outperforms the\nstate-of-the-art systems and our questions are also comparable to questions\ncreated by humans. We have also created and published an interface with all\ncreated datasets and evaluated questions, so it is possible to follow up on our\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blstak_M/0/1/0/all/0/1\">Miroslav Bl&#x161;t&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozinajova_V/0/1/0/all/0/1\">Viera Rozinajov&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Building Spoken Language Understanding Systems for Low Resourced Languages. (arXiv:2205.12818v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12818","description":"<p>Spoken dialog systems are slowly becoming and integral part of the human\nexperience due to their various advantages over textual interfaces. Spoken\nlanguage understanding (SLU) systems are fundamental building blocks of spoken\ndialog systems. But creating SLU systems for low resourced languages is still a\nchallenge. In a large number of low resourced language, we don't have access to\nenough data to build automatic speech recognition (ASR) technologies, which are\nfundamental to any SLU system. Also, ASR based SLU systems do not generalize to\nunwritten languages. In this paper, we present a series of experiments to\nexplore extremely low-resourced settings where we perform intent classification\nwith systems trained on as low as one data-point per intent and with only one\nspeaker in the dataset. We also work in a low-resourced setting where we do not\nuse language specific ASR systems to transcribe input speech, which compounds\nthe challenge of building SLU systems to simulate a true low-resourced setting.\nWe test our system on Belgian Dutch (Flemish) and English and find that using\nphonetic transcriptions to make intent classification systems in such\nlow-resourced setting performs significantly better than using speech features.\nSpecifically, when using a phonetic transcription based system over a feature\nbased system, we see average improvements of 12.37% and 13.08% for binary and\nfour-class classification problems respectively, when averaged over 49\ndifferent experimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Paradigm Change for Formal Syntax: Computational Algorithms in the Grammar of English. (arXiv:2205.12825v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12825","description":"<p>Language sciences rely less and less on formal syntax as their base. The\nreason is probably its lack of psychological reality, knowingly avoided.\nPhilosophers of science call for a paradigm shift in which explanations are by\nmechanisms, as in biology. We turned to programming languages as heuristic\nmodels for a process-based syntax of English. The combination of a functional\nword and a content word was chosen as the topic of modeling. Such combinations\nare very frequent, and their output is the important immediate constituents of\nsentences. We found their parallel in Object Oriented Programming where an\nall-methods element serves as an interface, and the content-full element serves\nas its implementation, defining computational objects. The fit of the model was\ntested by deriving three functional characteristics crucial for the algorithm\nand checking their presence in English grammar. We tested the reality of the\ninterface-implementation mechanism on psycholinguistic and neurolinguistic\nevidence concerning processing, development and loss of syntax. The close fit\nand psychological reality of the mechanism suggests that a paradigm shift to an\nalgorithmic theory of syntax is a possibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ninio_A/0/1/0/all/0/1\">Anat Ninio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12854","description":"<p>The propensity of abstractive summarization systems to make factual errors\nhas been the subject of significant study, including work on models to detect\nfactual errors and annotation of errors in current systems' outputs. However,\nthe ever-evolving nature of summarization systems, error detectors, and\nannotated benchmarks make factuality evaluation a moving target; it is hard to\nget a clear picture of how techniques compare. In this work, we collect labeled\nfactuality errors from across nine datasets of annotated summary outputs and\nstratify them in a new way, focusing on what kind of base summarization model\nwas used. To support finer-grained analysis, we unify the labeled error types\ninto a single taxonomy and project each of the datasets' errors into this\nshared labeled space. We then contrast five state-of-the-art error detection\nmethods on this benchmark. Our findings show that benchmarks built on modern\nsummary outputs (those from pre-trained models) show significantly different\nresults than benchmarks using pre-Transformer models. Furthermore, no one\nfactuality technique is superior in all settings or for all error types,\nsuggesting that system developers should take care to choose the right system\nfor their task at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahvuz_S/0/1/0/all/0/1\">Semih Yahvuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1\">Justin F. Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Sign Language Translation Learned from Online Video. (arXiv:2205.12870v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12870","description":"<p>Existing work on sign language translation--that is, translation from sign\nlanguage videos into sentences in a written language--has focused mainly on (1)\ndata collected in a controlled environment or (2) data in a specific domain,\nwhich limits the applicability to real-world settings. In this paper, we\nintroduce OpenASL, a large-scale ASL-English dataset collected from online\nvideo sites (e.g., YouTube). OpenASL contains 288 hours of ASL videos in\nvarious domains (news, VLOGs, etc.) from over 200 signers and is the largest\npublicly available ASL translation dataset to date. To tackle the challenges of\nsign language translation in realistic settings and without glosses, we propose\na set of techniques including sign search as a pretext task for pre-training\nand fusion of mouthing and handshape features. The proposed techniques produce\nconsistent and large improvements in translation quality, over baseline models\nbased on prior work. Our data, code and model will be publicly available at\nhttps://github.com/chevalierNoir/OpenASL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brentari_D/0/1/0/all/0/1\">Diane Brentari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Logically Interacted Conditions for Question Answering. (arXiv:2205.12898v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12898","description":"<p>Some questions have multiple answers that are not equally correct, i.e.\nanswers are different under different conditions. Conditions are used to\ndistinguish answers as well as to provide additional information to support\nthem. In this paper, we study a more challenging task where answers are\nconstrained by a list of conditions that logically interact, which requires\nperforming logical reasoning over the conditions to determine the correctness\nof the answers. Even more challenging, we only provide evidences for a subset\nof the conditions, so some questions may not have deterministic answers. In\nsuch cases, models are asked to find probable answers and identify conditions\nthat need to be satisfied to make the answers correct. We propose a new model,\nTReasoner, for this challenging reasoning task. TReasoner consists of an\nentailment module, a reasoning module, and a generation module (if the answers\nare free-form text spans). TReasoner achieves state-of-the-art performance on\ntwo benchmark conditional QA datasets, outperforming the previous\nstate-of-the-art by 3-10 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models. (arXiv:2205.12910v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12910","description":"<p>Theorem proving in natural mathematical language - the mixture of symbolic\nand natural language used by humans - plays a central role in mathematical\nadvances and education, and tests aspects of reasoning that are core to\nintelligence. Yet it has remained underexplored with modern generative models.\nWe study large-scale language models on two new generation tasks: suggesting\nthe next step in a mathematical proof, and full proof generation. Naively\napplying language models to these problems yields proofs riddled with\nhallucinations and logical incoherence. We develop NaturalProver, a language\nmodel that generates proofs by conditioning on background references (e.g.\ntheorems and definitions that are either retrieved or human-provided), and\noptionally enforces their presence with constrained decoding. On theorems from\nthe NaturalProofs benchmark, NaturalProver improves the quality of next-step\nsuggestions and generated proofs over fine-tuned GPT-3, according to human\nevaluations from university-level mathematics students. NaturalProver is\ncapable of proving some theorems that require short (2-6 step) proofs, and\nproviding next-step suggestions that are rated as correct and useful over 40%\nof the time, which is to our knowledge the first demonstration of these\ncapabilities using neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Intent Discovery with Pre-training and Contrastive Learning. (arXiv:2205.12914v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12914","description":"<p>New intent discovery aims to uncover novel intent categories from user\nutterances to expand the set of supported intent classes. It is a critical task\nfor the development and service expansion of a practical dialogue system.\nDespite its importance, this problem remains under-explored in the literature.\nExisting approaches typically rely on a large amount of labeled utterances and\nemploy pseudo-labeling methods for representation learning and clustering,\nwhich are label-intensive, inefficient, and inaccurate. In this paper, we\nprovide new solutions to two important research questions for new intent\ndiscovery: (1) how to learn semantic utterance representations and (2) how to\nbetter cluster utterances. Particularly, we first propose a multi-task\npre-training strategy to leverage rich unlabeled data along with external\nlabeled data for representation learning. Then, we design a new contrastive\nloss to exploit self-supervisory signals in unlabeled data for clustering.\nExtensive experiments on three intent recognition benchmarks demonstrate the\nhigh effectiveness of our proposed method, which outperforms state-of-the-art\nmethods by a large margin in both unsupervised and semi-supervised scenarios.\nThe source code will be available at\n\\url{https://github.com/zhang-yu-wei/MTP-CLNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Li-Ming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Heterogeneous Features in Sequence to Sequence Tasks: Latent Enhanced Multi-filter Seq2Seq Model. (arXiv:2105.08840v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08840","description":"<p>In language processing, training data with extremely large variance may lead\nto difficulty in the language model's convergence. It is difficult for the\nnetwork parameters to adapt sentences with largely varied semantics or\ngrammatical structures. To resolve this problem, we introduce a model that\nconcentrates the each of the heterogeneous features in the input sentences.\nBuilding upon the encoder-decoder architecture, we design a latent-enhanced\nmulti-filter seq2seq model (LEMS) that analyzes the input representations by\nintroducing a latent space transformation and clustering. The representations\nare extracted from the final hidden state of the encoder and lie in the latent\nspace. A latent space transformation is applied for enhancing the quality of\nthe representations. Thus the clustering algorithm can easily separate samples\nbased on the features of these representations. Multiple filters are trained by\nthe features from their corresponding clusters, and the heterogeneity of the\ntraining data can be resolved accordingly. We conduct two sets of comparative\nexperiments on semantic parsing and machine translation, using the Geo-query\ndataset and Multi30k English-French to demonstrate the enhancement our model\nhas made respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhaokun Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01226","description":"<p>More predictable words are easier to process - they are read faster and\nelicit smaller neural signals associated with processing difficulty, most\nnotably, the N400 component of the event-related brain potential. Thus, it has\nbeen argued that prediction of upcoming words is a key component of language\ncomprehension, and that studying the amplitude of the N400 is a valuable way to\ninvestigate the predictions we make. In this study, we investigate whether the\nlinguistic predictions of computational language models or humans better\nreflect the way in which natural language stimuli modulate the amplitude of the\nN400. One important difference in the linguistic predictions of humans versus\ncomputational language models is that while language models base their\npredictions exclusively on the preceding linguistic context, humans may rely on\nother factors. We find that the predictions of three top-of-the-line\ncontemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more\nclosely than human predictions. This suggests that the predictive processes\nunderlying the N400 may be more sensitive to the surface-level statistics of\nlanguage than previously thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation. (arXiv:2109.06604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06604","description":"<p>Recently, $k$NN-MT has shown the promising capability of directly\nincorporating the pre-trained neural machine translation (NMT) model with\ndomain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve\ndomain adaptation without retraining. Despite being conceptually attractive, it\nheavily relies on high-quality in-domain parallel corpora, limiting its\ncapability on unsupervised domain adaptation, where in-domain parallel corpora\nare scarce or nonexistent. In this paper, we propose a novel framework that\ndirectly uses in-domain monolingual sentences in the target language to\nconstruct an effective datastore for $k$-nearest-neighbor retrieval. To this\nend, we first introduce an autoencoder task based on the target language, and\nthen insert lightweight adapters into the original NMT model to map the\ntoken-level representation of this task to the ideal representation of\ntranslation task. Experiments on multi-domain datasets demonstrate that our\nproposed approach significantly improves the translation accuracy with\ntarget-side monolingual data, while achieving comparable performance with\nback-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models. (arXiv:2110.07736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07736","description":"<p>Recently, NLP models have achieved remarkable progress across a variety of\ntasks; however, they have also been criticized for being not robust. Many\nrobustness problems can be attributed to models exploiting spurious\ncorrelations, or shortcuts between the training data and the task labels. Most\nexisting work identifies a limited set of task-specific shortcuts via human\npriors or error analyses, which requires extensive expertise and efforts. In\nthis paper, we aim to automatically identify such spurious correlations in NLP\nmodels at scale. We first leverage existing interpretability methods to extract\ntokens that significantly affect model's decision process from the input text.\nWe then distinguish \"genuine\" tokens and \"spurious\" tokens by analyzing model\npredictions across multiple corpora and further verify them through\nknowledge-aware perturbations. We show that our proposed method can effectively\nand efficiently identify a scalable set of \"shortcuts\", and mitigating these\nleads to more robust models in multiple applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_R/0/1/0/all/0/1\">Rohit Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models. (arXiv:2112.08609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08609","description":"<p>In this paper, we focus on studying robustness evaluation of Chinese question\nmatching. Most of the previous work on analyzing robustness issue focus on just\none or a few types of artificial adversarial examples. Instead, we argue that\nit is necessary to formulate a comprehensive evaluation about the linguistic\ncapabilities of models on natural texts. For this purpose, we create a Chinese\ndataset namely DuQM which contains natural questions with linguistic\nperturbations to evaluate the robustness of question matching models. DuQM\ncontains 3 categories and 13 subcategories with 32 linguistic perturbations.\nThe extensive experiments demonstrate that DuQM has a better ability to\ndistinguish different models. Importantly, the detailed breakdown of evaluation\nby linguistic phenomenon in DuQM helps us easily diagnose the strength and\nweakness of different models. Additionally, our experiment results show that\nthe effect of artificial adversarial examples does not work on the natural\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Minimization with Dynamic Reweighting. (arXiv:2112.08772v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.08772","description":"<p>Deep neural networks are often overparameterized and may not easily achieve\nmodel generalization. Adversarial training has shown effectiveness in improving\ngeneralization by regularizing the change of loss on top of adversarially\nchosen perturbations. The recently proposed sharpness-aware minimization (SAM)\nalgorithm conducts adversarial weight perturbation, encouraging the model to\nconverge to a flat minima. SAM finds a common adversarial weight perturbation\nper-batch. Although per-instance adversarial weight perturbations are stronger\nadversaries and they can potentially lead to better generalization performance,\ntheir computational cost is very high and thus it is impossible to use\nper-instance perturbations efficiently in SAM. In this paper, we tackle this\nefficiency bottleneck and propose sharpness-aware minimization with dynamic\nreweighting ({\\delta}-SAM). Our theoretical analysis motivates that it is\npossible to approach the stronger, per-instance adversarial weight\nperturbations using reweighted per-batch weight perturbations. {\\delta}-SAM\ndynamically reweights perturbation within each batch according to the\ntheoretically principled weighting factors, serving as a good approximation to\nper-instance perturbation. Experiments on various natural language\nunderstanding tasks demonstrate the effectiveness of {\\delta}-SAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets. (arXiv:2112.09237v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09237","description":"<p>Many believe human-level natural language inference (NLI) has already been\nachieved. In reality, modern NLI benchmarks have serious flaws, rendering\nprogress questionable. Chief among them is the problem of single sentence label\nleakage, where spurious correlations and biases in datasets enable the accurate\nprediction of a sentence pair relation from only a single sentence, something\nthat should in principle be impossible. This leakage enables models to cheat\nrather than learn the desired reasoning capabilities, and hasn't gone away\nsince its 2018 discovery. We analyze this problem across 10 modern NLI\ndatasets, and find that new datasets have a single sentence accuracy of 8% over\nchance at best and 19% on average. We examine how regular NLI models cheat on\nthis data and discuss how to ameliorate this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus. (arXiv:2112.09924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09924","description":"<p>In order to address increasing demands of real-world applications, the\nresearch for knowledge-intensive NLP (KI-NLP) should advance by capturing the\nchallenges of a truly open-domain environment: web-scale knowledge, lack of\nstructure, inconsistent quality and noise. To this end, we propose a new setup\nfor evaluating existing knowledge intensive tasks in which we generalize the\nbackground corpus to a universal web snapshot. We investigate a slate of NLP\ntasks which rely on knowledge - either factual or common sense, and ask systems\nto use a subset of CCNet - the Sphere corpus - as a knowledge source. In\ncontrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere\nis orders of magnitude larger and better reflects the full diversity of\nknowledge on the web. Despite potential gaps in coverage, challenges of scale,\nlack of structure and lower quality, we find that retrieval from Sphere enables\na state of the art system to match and even outperform Wikipedia-based models\non several tasks. We also observe that while a dense index can outperform a\nsparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To\nfacilitate further research and minimise the community's reliance on\nproprietary, black-box search engines, we share our indices, evaluation metrics\nand infrastructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broscheit_S/0/1/0/all/0/1\">Samuel Broscheit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement. (arXiv:2112.10991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10991","description":"<p>End-to-end speech-to-text translation (E2E-ST) is becoming increasingly\npopular due to the potential of its less error propagation, lower latency, and\nfewer parameters. Given the triplet training corpus $\\langle speech,\ntranscription, translation\\rangle$, the conventional high-quality E2E-ST system\nleverages the $\\langle speech, transcription\\rangle$ pair to pre-train the\nmodel and then utilizes the $\\langle speech, translation\\rangle$ pair to\noptimize it further. However, this process only involves two-tuple data at each\nstage, and this loose coupling fails to fully exploit the association between\ntriplet data. In this paper, we attempt to model the joint probability of\ntranscription and translation based on the speech input to directly leverage\nsuch triplet data. Based on that, we propose a novel regularization method for\nmodel training to improve the agreement of dual-path decomposition within\ntriplet data, which should be equal in theory. To achieve this goal, we\nintroduce two Kullback-Leibler divergence regularization terms into the model\ntraining objective to reduce the mismatch between output probabilities of\ndual-path. Then the well-trained model can be naturally transformed as the\nE2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C\nbenchmark demonstrate that our proposed approach significantly outperforms\nstate-of-the-art E2E-ST baselines on all 8 language pairs, while achieving\nbetter performance in the automatic speech recognition task. Our code is\nopen-sourced at https://github.com/duyichao/E2E-ST-TDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should You Mask 15% in Masked Language Modeling?. (arXiv:2202.08005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08005","description":"<p>Masked language models conventionally use a masking rate of 15% due to the\nbelief that more masking would provide insufficient context to learn good\nrepresentations, and less masking would make training too expensive.\nSurprisingly, we find that masking up to 40% of input tokens can outperform the\n15% baseline, and even masking 80% can preserve most of the performance, as\nmeasured by finetuning on downstream tasks. Increasing the masking rates has\ntwo distinct effects, which we investigate through careful ablations: (1) A\nlarger proportion of input tokens are corrupted, reducing the context size and\ncreating a harder task, and (2) models perform more predictions, which benefits\ntraining. We observe that larger models with more capacity to tackle harder\ntasks in particular favor higher masking rates. We also find that even more\nsophisticated masking schemes such as span masking or PMI masking can benefit\nfrom higher masking rates, albeit to a smaller extent. Our results contribute\nto a better understanding of masked language modeling and shed light on more\nefficient language pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Processing the structure of documents: Logical Layout Analysis of historical newspapers in French. (arXiv:2202.08125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08125","description":"<p>Background. In recent years, libraries and archives led important\ndigitisation campaigns that opened the access to vast collections of historical\ndocuments. While such documents are often available as XML ALTO documents, they\nlack information about their logical structure. In this paper, we address the\nproblem of Logical Layout Analysis applied to historical documents in French.\nWe propose a rule-based method, that we evaluate and compare with two\nMachine-Learning models, namely RIPPER and Gradient Boosting. Our data set\ncontains French newspapers, periodicals and magazines, published in the first\nhalf of the twentieth century in the Franche-Comt\\'e Region. Results. Our\nrule-based system outperforms the two other models in nearly all evaluations.\nIt has especially better Recall results, indicating that our system covers more\ntypes of every logical label than the other two models. When comparing RIPPER\nwith Gradient Boosting, we can observe that Gradient Boosting has better\nPrecision scores but RIPPER has better Recall scores. Conclusions. The\nevaluation shows that our system outperforms the two Machine Learning models,\nand provides significantly higher Recall. It also confirms that our system can\nbe used to produce annotated data sets that are large enough to envisage\nMachine Learning or Deep Learning approaches for the task of Logical Layout\nAnalysis. Combining rules and Machine Learning models into hybrid systems could\npotentially provide even better performances. Furthermore, as the layout in\nhistorical documents evolves rapidly, one possible solution to overcome this\nproblem would be to apply Rule Learning algorithms to bootstrap rule sets\nadapted to different publication periods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutehrle_N/0/1/0/all/0/1\">Nicolas Gutehrl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanassova_I/0/1/0/all/0/1\">Iana Atanassova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07648","description":"<p>Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric. (arXiv:2203.08299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08299","description":"<p>Syntax is a fundamental component of language, yet few metrics have been\nemployed to capture syntactic similarity or coherence at the utterance- and\ndocument-level. The existing standard document-level syntactic similarity\nmetric is computationally expensive and performs inconsistently when faced with\nsyntactically dissimilar documents. To address these challenges, we present\nFastKASSIM, a metric for utterance- and document-level syntactic similarity\nwhich pairs and averages the most similar dependency parse trees between a pair\nof documents based on tree kernels. FastKASSIM is more robust to syntactic\ndissimilarities and runs up to to 5.32 times faster than the baseline method\nover the documents in the r/ChangeMyView corpus. These improvements allow us to\nexamine hypotheses in two settings with large documents: persuasive online\narguments on r/ChangeMyView, and authorship attribution in the Australian High\nCourt Judgment corpus. With FastKASSIM, we are able to show that more\nsyntactically similar arguments tend to be more persuasive, and that syntax\nprovides a key indicator of writing style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caitlyn Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperdecoders: Instance-specific decoders for multi-task NLP. (arXiv:2203.08304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08304","description":"<p>We investigate input-conditioned hypernetworks for multi-tasking in NLP,\ngenerating parameter-efficient adaptations for a decoder using a hypernetwork\nconditioned on the output of an encoder. This approach produces a unique\ndecoder for every input instance, allowing the network a larger degree of\nflexibility than prior work that specializes the decoder for each task. We\napply our method to sequence classification tasks, extractive QA, and\nsummarisation and find that it surpasses previous parameter efficient\nfine-tuning methods and often outperforms fully finetuning the underlying\nmodel. An analysis of the embeddings used by our hypernetwork shows that they\nare sensitive to output label and type, suggesting that our approach better\nmaps from encoder representations to output labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10232","description":"<p>In this paper, we present DuReader-retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader-retrieval contains more than 90K queries and\nover 8M unique passages from Baidu search. To ensure the quality of our\nbenchmark and address the shortcomings in other existing datasets, we (1)\nreduce the false negatives in development and testing sets by pooling the\nresults from multiple retrievers with human annotations, (2) and remove the\ntraining queries that are semantically similar to the development and testing\nqueries. Additionally, we provide two out-of-domain testing sets for\ncross-domain evaluation, as well as a cross-lingual set that has been manually\ntranslated for cross-lingual retrieval. The experiments demonstrate that\nDuReader-retrieval is challenging and there is still plenty of room for\nimprovement, e.g. salient phrase and syntax mismatch between query and\nparagraph. These experimental results show that the dense retriever does not\ngeneralize well across domains, and cross-lingual retrieval is essentially\nchallenging. DuReader-retrieval will be publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yifu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM. (arXiv:2204.02821v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02821","description":"<p>This paper describes our system for SemEval-2022 Task 2 Multilingual\nIdiomaticity Detection and Sentence Embedding sub-task B. We modify a standard\nBERT sentence transformer by adding embeddings for each idioms, which are\ncreated using BERTRAM and a small number of contexts. We show that this\ntechnique increases the quality of idiom representations and leads to better\nperformance on the task. We also perform analysis on our final results and show\nthat the quality of the produced idiom embeddings is highly sensitive to the\nquality of the input contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phelps_D/0/1/0/all/0/1\">Dylan Phelps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks. (arXiv:2204.02892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02892","description":"<p>The field of Natural Language Processing has experienced a dramatic leap in\ncapabilities with the recent introduction of huge Language Models. Despite this\nsuccess, natural language problems that involve several compounded steps are\nstill practically unlearnable, even by the largest LMs. This complies with\nexperimental failures for end-to-end learning of composite problems that were\ndemonstrated in a variety of domains. An effective mitigation is to introduce\nintermediate supervision for solving sub-tasks of the compounded problem.\nRecently, several works have demonstrated high gains by taking a\nstraightforward approach for incorporating intermediate supervision in\ncompounded natural language problems: the sequence-to-sequence LM is fed with\nan augmented input, in which the decomposed tasks' labels are simply\nconcatenated to the original input. In this paper, we prove a positive learning\nresult that motivates these recent efforts. We show that when concatenating\nintermediate supervision to the input and training a sequence-to-sequence model\non this modified input, unlearnable composite problems can become learnable. We\nshow that this is true for any family of tasks which on the one hand, are\nunlearnable, and on the other hand, can be decomposed into a polynomial number\nof simple sub-tasks, each of which depends only on O(1) previous sub-task\nresults. Beyond motivating contemporary empirical efforts for incorporating\nintermediate supervision in sequence-to-sequence language models, our positive\ntheoretical result is the first of its kind in the landscape of results on the\nbenefits of intermediate supervision for neural-network learning: Until now,\nall theoretical results on the subject are negative, i.e., show cases where\nlearning is impossible without intermediate supervision, while our result is\npositive, showing that learning is facilitated in the presence of intermediate\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment. (arXiv:2204.09148v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09148","description":"<p>The instruction learning paradigm -- where a model learns to perform new\ntasks from task descriptions alone -- has become popular in general-purpose\nmodel research. The capabilities of large transformer models as instruction\nlearners, however, remain poorly understood. We use a controlled synthetic\nenvironment to characterize such capabilities. Specifically, we use the task of\ndeciding whether a given string matches a regular expression (viewed as an\ninstruction) to identify properties of tasks, instructions, and instances that\nmake instruction learning challenging. For instance, we find that our model, a\nfine-tuned T5-based text2text transformer, struggles with large regular\nlanguages, suggesting that less precise instructions are challenging for\nmodels. Additionally, instruction executions that require tracking longer\ncontexts of prior steps are also more difficult. We use our findings to\nsystematically construct a challenging instruction learning dataset, which we\ncall Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to\ncorrectly interpret only 65.6% of test instructions (with at least 90%\naccuracy), and 11%-24% of the instructions in out-of-distribution\ngeneralization settings. We propose Hard RegSet as a challenging instruction\nlearning task, and a controlled environment for studying instruction learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering. (arXiv:2204.10629v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10629","description":"<p>Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG\nembedding contains concise data used in NLP tasks requiring implicit\ninformation about the real world. Furthermore, the size of KGs that may be\nuseful in actual NLP assignments is enormous, and creating embedding over it\nhas memory cost issues. We represent KG as a 3rd-order binary tensor and move\nbeyond the standard CP decomposition by using a data-specific generalized\nversion of it. The generalization of the standard CP-ALS algorithm allows\nobtaining optimization gradients without a backpropagation mechanism. It\nreduces the memory needed in training while providing computational benefits.\nWe propose a MEKER, a memory-efficient KG embedding model, which yields\nSOTA-comparable performance on link prediction tasks and KG-based Question\nAnswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chekalina_V/0/1/0/all/0/1\">Viktoriia Chekalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayapin_A/0/1/0/all/0/1\">Albert Sayapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frolov_E/0/1/0/all/0/1\">Evgeny Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Language Variation Acoustically with Few Resources. (arXiv:2205.02694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02694","description":"<p>Deep acoustic models represent linguistic information based on massive\namounts of data. Unfortunately, for regional languages and dialects such\nresources are mostly not available. However, deep acoustic models might have\nlearned linguistic information that transfers to low-resource languages. In\nthis study, we evaluate whether this is the case through the task of\ndistinguishing low-resource (Dutch) regional varieties. By extracting\nembeddings from the hidden layers of various wav2vec 2.0 models (including new\nmodels which are pre-trained and/or fine-tuned on Dutch) and using dynamic time\nwarping, we compute pairwise pronunciation differences averaged over 10 words\nfor over 100 individual dialects from four (regional) languages. We then\ncluster the resulting difference matrix in four groups and compare these to a\ngold standard, and a partitioning on the basis of comparing phonetic\ntranscriptions. Our results show that acoustic models outperform the\n(traditional) transcription-based approach without requiring phonetic\ntranscriptions, with the best performance achieved by the multilingual XLSR-53\nmodel fine-tuned on Dutch. On the basis of only six seconds of speech, the\nresulting clustering closely matches the gold standard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11211","description":"<p>End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Paradox of Learning to Reason from Data. (arXiv:2205.11502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11502","description":"<p>Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be\ntrained end-to-end to solve logical reasoning problems presented in natural\nlanguage? We attempt to answer this question in a confined problem space where\nthere exists a set of parameters that perfectly simulates logical reasoning. We\nmake observations that seem to contradict each other: BERT attains near-perfect\naccuracy on in-distribution test examples while failing to generalize to other\ndata distributions over the exact same problem space. Our study provides an\nexplanation for this paradox: instead of learning to emulate the correct\nreasoning function, BERT has in fact learned statistical features that\ninherently exist in logical reasoning problems. We also show that it is\ninfeasible to jointly remove statistical features from data, illustrating the\ndifficulty of learning to reason in general. Our result naturally extends to\nother neural models and unveils the fundamental difference between learning to\nreason and learning to achieve high performance on NLP benchmarks using\nstatistical features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition. (arXiv:2205.11998v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11998","description":"<p>The choice of modeling units affects the performance of the acoustic modeling\nand plays an important role in automatic speech recognition (ASR). In mandarin\nscenarios, the Chinese characters represent meaning but are not directly\nrelated to the pronunciation. Thus only considering the writing of Chinese\ncharacters as modeling units is insufficient to capture speech features. In\nthis paper, we present a novel method involves with multi-level modeling units,\nwhich integrates multi-level information for mandarin speech recognition.\nSpecifically, the encoder block considers syllables as modeling units, and the\ndecoder block deals with character modeling units. During inference, the input\nfeature sequences are converted into syllable sequences by the encoder block\nand then converted into Chinese characters by the decoder block. This process\nis conducted by a unified end-to-end model without introducing additional\nconversion models. By introducing InterCE auxiliary task, our method achieves\ncompetitive results with CER of 4.1%/4.6% and 4.6%/5.2% on the widely used\nAISHELL-1 benchmark without a language model, using the Conformer and the\nTransformer backbones respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Binbin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12005","description":"<p>Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Action Recognition for American Sign Language. (arXiv:2205.12261v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12261","description":"<p>In this research, we present our findings to recognize American Sign Language\nfrom series of hand gestures. While most researches in literature focus only on\nstatic handshapes, our work target dynamic hand gestures. Since dynamic signs\ndataset are very few, we collect an initial dataset of 150 videos for 10 signs\nand an extension of 225 videos for 15 signs. We apply transfer learning models\nin combination with deep neural networks and background subtraction for videos\nin different temporal settings. Our primarily results show that we can get an\naccuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12\nframes accordingly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phong_N/0/1/0/all/0/1\">Nguyen Huu Phong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bernardete Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12268","description":"<p>Convolutional Neural Networks (CNNs) are known for requiring extensive\ncomputational resources, and quantization is among the best and most common\nmethods for compressing them. While aggressive quantization (i.e., less than\n4-bits) performs well for classification, it may cause severe performance\ndegradation in image-to-image tasks such as semantic segmentation and depth\nestimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a\nnovel approach for high-resolution activation maps compression integrated with\npoint-wise convolutions, which are the main computational cost of modern\narchitectures. To this end, we use an efficient and hardware-friendly\nHaar-wavelet transform, known for its effectiveness in image compression, and\ndefine the convolution on the compressed activation map. We experiment on\nvarious tasks, that benefit from high-resolution input, and by combining WCC\nwith light quantization, we achieve compression rates equivalent to 1-4bit\nactivation quantization with relatively small and much more graceful\ndegradation in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finder_S/0/1/0/all/0/1\">Shahaf E. Finder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohav_Y/0/1/0/all/0/1\">Yair Zohav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashkenazi_M/0/1/0/all/0/1\">Maor Ashkenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Optimization for Physics-Based Reconstruction of 3d Human Pose from Monocular Video. (arXiv:2205.12292v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12292","description":"<p>We focus on the task of estimating a physically plausible articulated human\nmotion from monocular video. Existing approaches that do not consider physics\noften produce temporally inconsistent output with motion artifacts, while\nstate-of-the-art physics-based approaches have either been shown to work only\nin controlled laboratory conditions or consider simplified body-ground contact\nlimited to feet. This paper explores how these shortcomings can be addressed by\ndirectly incorporating a fully-featured physics engine into the pose estimation\nprocess. Given an uncontrolled, real-world scene as input, our approach\nestimates the ground-plane location and the dimensions of the physical body\nmodel. It then recovers the physical motion by performing trajectory\noptimization. The advantage of our formulation is that it readily generalizes\nto a variety of scenes that might have diverse ground properties and supports\nany form of self-contact and contact between the articulated body and scene\ngeometry. We show that our approach achieves competitive results with respect\nto existing physics-based methods on the Human3.6M benchmark, while being\ndirectly applicable without re-training to more complex dynamic motions from\nthe AIST benchmark and to uncontrolled internet videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gartner_E/0/1/0/all/0/1\">Erik G&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriluka_M/0/1/0/all/0/1\">Mykhaylo Andriluka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face2Text revisited: Improved data set and baseline results. (arXiv:2205.12342v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12342","description":"<p>Current image description generation models do not transfer well to the task\nof describing human faces. To encourage the development of more human-focused\ndescriptions, we developed a new data set of facial descriptions based on the\nCelebA image data set. We describe the properties of this data set, and present\nresults from a face description generator trained on it, which explores the\nfeasibility of using transfer learning from VGGFace/ResNet CNNs. Comparisons\nare drawn through both automated metrics and human evaluation by 76\nEnglish-speaking participants. The descriptions generated by the VGGFace-LSTM +\nAttention model are closest to the ground truth according to human evaluation\nwhilst the ResNet-LSTM + Attention model obtained the highest CIDEr and CIDEr-D\nresults (1.252 and 0.686 respectively). Together, the new data set and these\nexperimental results provide data and baselines for future work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdilla_S/0/1/0/all/0/1\">Shaun Abdilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscat_A/0/1/0/all/0/1\">Adrian Muscat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrugia_R/0/1/0/all/0/1\">Reuben A. Farrugia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark and Asymmetrical-Similarity Learning for Practical Image Copy Detection. (arXiv:2205.12358v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12358","description":"<p>Image copy detection (ICD) aims to determine whether a query image is an\nedited copy of any image from a reference set. Currently, there are very\nlimited public benchmarks for ICD, while all overlook a critical challenge in\nreal-world applications, i.e., the distraction from hard negative queries.\nSpecifically, some queries are not edited copies but are inherently similar to\nsome reference images. These hard negative queries are easily false recognized\nas edited copies, significantly compromising the ICD accuracy. This observation\nmotivates us to build the first ICD benchmark featuring this characteristic.\nBased on existing ICD datasets, this paper constructs a new dataset by\nadditionally adding 100, 000 and 24, 252 hard negative pairs into the training\nand test set, respectively. Moreover, this paper further reveals a unique\ndifficulty for solving the hard negative problem in ICD, i.e., there is a\nfundamental conflict between current metric learning and ICD. This conflict is:\nthe metric learning adopts symmetric distance while the edited copy is an\nasymmetric (unidirectional) process, e.g., a partial crop is close to its\nholistic reference image and is an edited copy, while the latter cannot be the\nedited copy of the former (in spite the distance is equally small). This\ninsight results in an Asymmetrical-Similarity Learning (ASL) method, which\nallows the similarity in two directions (the query &lt;-&gt; the reference image) to\nbe different from each other. Experimental results show that ASL outperforms\nstate-of-the-art methods by a clear margin, confirming that solving the\nsymmetric-asymmetric conflict is critical for ICD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Optimizing Color Rendition and In-Camera Backgrounds in an RGB Virtual Production Stage. (arXiv:2205.12403v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12403","description":"<p>While the LED panels used in virtual production systems can display vibrant\nimagery with a wide color gamut, they produce problematic color shifts when\nused as lighting due to their peaky spectral output from narrow-band red,\ngreen, and blue LEDs. In this work, we present an improved color calibration\nprocess for virtual production stages which ameliorates this color rendition\nproblem while also passing through accurate in-camera background colors. We do\nthis by optimizing linear color correction transformations for 1) the LED panel\npixels visible in the field of view of the camera, 2) the pixels outside the\nfield of view of the camera illuminating the subjects, and, as a post-process,\n3) the pixel values recorded by the camera. The result is that footage shot in\nan RGB LED panel virtual production stage can exhibit more accurate skin tones\nand costume colors while still reproducing the desired colors of the in-camera\nbackground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepicovsky_L/0/1/0/all/0/1\">Lukas Lepicovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debevec_P/0/1/0/all/0/1\">Paul Debevec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Processes for Inpainting Satellite Images. (arXiv:2205.12407v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12407","description":"<p>The widespread availability of satellite images has allowed researchers to\nmodel complex systems such as disease dynamics. However, many satellite images\nhave missing values due to measurement defects, which render them unusable\nwithout data imputation. For example, the scanline corrector for the LANDSAT 7\nsatellite broke down in 2003, resulting in a loss of around 20\\% of its data.\nInpainting involves predicting what is missing based on the known pixels and is\nan old problem in image processing, classically based on PDEs or interpolation\nmethods, but recent deep learning approaches have shown promise. However, many\nof these methods do not explicitly take into account the inherent\nspatiotemporal structure of satellite images. In this work, we cast satellite\nimage inpainting as a natural meta-learning problem, and propose using\nconvolutional neural processes (ConvNPs) where we frame each satellite image as\nits own task or 2D regression problem. We show ConvNPs can outperform classical\nmethods and state-of-the-art deep learning inpainting models on a scanline\ninpainting problem for LANDSAT 7 satellite images, assessed on a variety of in\nand out-of-distribution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pondaven_A/0/1/0/all/0/1\">Alexander Pondaven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakler_M/0/1/0/all/0/1\">M&#xe4;rt Bakler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Donghu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashim_H/0/1/0/all/0/1\">Hamzah Hashim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatov_M/0/1/0/all/0/1\">Martin Ignatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Harrison Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interaction of a priori Anatomic Knowledge with Self-Supervised Contrastive Learning in Cardiac Magnetic Resonance Imaging. (arXiv:2205.12429v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12429","description":"<p>Training deep learning models on cardiac magnetic resonance imaging (CMR) can\nbe a challenge due to the small amount of expert generated labels and inherent\ncomplexity of data source. Self-supervised contrastive learning (SSCL) has\nrecently been shown to boost performance in several medical imaging tasks.\nHowever, it is unclear how much the pre-trained representation reflects the\nprimary organ of interest compared to spurious surrounding tissue. In this\nwork, we evaluate the optimal method of incorporating prior knowledge of\nanatomy into a SSCL training paradigm. Specifically, we evaluate using a\nsegmentation network to explicitly local the heart in CMR images, followed by\nSSCL pretraining in multiple diagnostic tasks. We find that using a priori\nknowledge of anatomy can greatly improve the downstream diagnostic performance.\nFurthermore, SSCL pre-training with in-domain data generally improved\ndownstream performance and more human-like saliency compared to end-to-end\ntraining and ImageNet pre-trained networks. However, introducing anatomic\nknowledge to pre-training generally does not have significant impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nakashima_M/0/1/0/all/0/1\">Makiya Nakashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_I/0/1/0/all/0/1\">Inyeop Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basnet_R/0/1/0/all/0/1\">Ramesh Basnet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benovoy_M/0/1/0/all/0/1\">Mitchel Benovoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1\">W.H. Wilson Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1\">Christopher Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwon_D/0/1/0/all/0/1\">Deborah Kwon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_T/0/1/0/all/0/1\">Tae Hyun Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">David Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Cancer Diagnostics with an All-Inclusive Smartphone Application. (arXiv:2205.12438v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12438","description":"<p>Among the different types of skin cancer, melanoma is considered to be the\ndeadliest and is difficult to treat at advanced stages. Detection of melanoma\nat earlier stages can lead to reduced mortality rates. Desktop-based\ncomputer-aided systems have been developed to assist dermatologists with early\ndiagnosis. However, there is significant interest in developing portable,\nat-home melanoma diagnostic systems which can assess the risk of cancerous skin\nlesions. Here, we present a smartphone application that combines image capture\ncapabilities with preprocessing and segmentation to extract the Asymmetry,\nBorder irregularity, Color variegation, and Diameter (ABCD) features of a skin\nlesion. Using the feature sets, classification of malignancy is achieved\nthrough support vector machine classifiers. By using adaptive algorithms in the\nindividual data-processing stages, our approach is made computationally light,\nuser friendly, and reliable in discriminating melanoma cases from benign ones.\nImages of skin lesions are either captured with the smartphone camera or\nimported from public datasets. The entire process from image capture to\nclassification runs on an Android smartphone equipped with a detachable 10x\nlens, and processes an image in less than a second. The overall performance\nmetrics are evaluated on a public database of 200 images with Synthetic\nMinority Over-sampling Technique (SMOTE) (80% sensitivity, 90% specificity, 88%\naccuracy, and 0.85 area under curve (AUC)) and without SMOTE (55% sensitivity,\n95% specificity, 90% accuracy, and 0.75 AUC). The evaluated performance metrics\nand computation times are comparable or better than previous methods. This\nall-inclusive smartphone application is designed to be easy-to-download and\neasy-to-navigate for the end user, which is imperative for the eventual\ndemocratization of such medical diagnostic systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kalwa_U/0/1/0/all/0/1\">Upender Kalwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Legner_C/0/1/0/all/0/1\">Christopher Legner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_T/0/1/0/all/0/1\">Taejoon Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandey_S/0/1/0/all/0/1\">Santosh Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Style Mixing for Face Cartoonization. (arXiv:2205.12450v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12450","description":"<p>Cartoon domain has recently gained increasing popularity. Previous studies\nhave attempted quality portrait stylization into the cartoon domain; however,\nthis poses a great challenge since they have not properly addressed the\ncritical constraints, such as requiring a large number of training images or\nthe lack of support for abstract cartoon faces. Recently, a layer swapping\nmethod has been used for stylization requiring only a limited number of\ntraining images; however, its use cases are still narrow as it inherits the\nremaining issues. In this paper, we propose a novel method called Cross-domain\nStyle mixing, which combines two latent codes from two different domains. Our\nmethod effectively stylizes faces into multiple cartoon characters at various\nface abstraction levels using only a single generator without even using a\nlarge number of training images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungkwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_C/0/1/0/all/0/1\">Chaeheon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dohyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kwangho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_J/0/1/0/all/0/1\">Jihye Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1\">Namhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daesik Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-aware Knowledge Distillation for Efficient Image-to-Image Translation. (arXiv:2205.12451v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12451","description":"<p>Recent progress in image-to-image translation has witnessed the success of\ngenerative adversarial networks (GANs). However, GANs usually contain a huge\nnumber of parameters, which lead to intolerant memory and computation\nconsumption and limit their deployment on edge devices. To address this issue,\nknowledge distillation is proposed to transfer the knowledge from a cumbersome\nteacher model to an efficient student model. However, most previous knowledge\ndistillation methods are designed for image classification and lead to limited\nperformance in image-to-image translation. In this paper, we propose\nRegion-aware Knowledge Distillation ReKo to compress image-to-image translation\nmodels. Firstly, ReKo adaptively finds the crucial regions in the images with\nan attention module. Then, patch-wise contrastive learning is adopted to\nmaximize the mutual information between students and teachers in these crucial\nregions. Experiments with eight comparison methods on nine datasets demonstrate\nthe substantial effectiveness of ReKo on both paired and unpaired\nimage-to-image translation. For instance, our 7.08X compressed and 6.80X\naccelerated CycleGAN student outperforms its teacher by 1.33 and 1.04 FID\nscores on Horse to Zebra and Zebra to Horse, respectively. Codes will be\nreleased on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lightweight NMS-free Framework for Real-time Visual Fault Detection System of Freight Trains. (arXiv:2205.12458v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12458","description":"<p>Real-time vision-based system of fault detection (RVBS-FD) for freight trains\nis an essential part of ensuring railway transportation safety. Most existing\nvision-based methods still have high computational costs based on convolutional\nneural networks. The computational cost is mainly reflected in the backbone,\nneck, and post-processing, i.e., non-maximum suppression (NMS). In this paper,\nwe propose a lightweight NMS-free framework to achieve real-time detection and\nhigh accuracy simultaneously. First, we use a lightweight backbone for feature\nextraction and design a fault detection pyramid to process features. This fault\ndetection pyramid includes three novel individual modules using attention\nmechanism, bottleneck, and dilated convolution for feature enhancement and\ncomputation reduction. Instead of using NMS, we calculate different loss\nfunctions, including classification and location costs in the detection head,\nto further reduce computation. Experimental results show that our framework\nachieves over 83 frames per second speed with a smaller model size and higher\naccuracy than the state-of-the-art detectors. Meanwhile, the hardware resource\nrequirements of our method are low during the training and testing process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huilin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Ye Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CNN with Noise Inclined Module and Denoise Framework for Hyperspectral Image Classification. (arXiv:2205.12459v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12459","description":"<p>Deep Neural Networks have been successfully applied in hyperspectral image\nclassification. However, most of prior works adopt general deep architectures\nwhile ignore the intrinsic structure of the hyperspectral image, such as the\nphysical noise generation. This would make these deep models unable to generate\ndiscriminative features and provide impressive classification performance. To\nleverage such intrinsic information, this work develops a novel deep learning\nframework with the noise inclined module and denoise framework for\nhyperspectral image classification. First, we model the spectral signature of\nhyperspectral image with the physical noise model to describe the high\nintraclass variance of each class and great overlapping between different\nclasses in the image. Then, a noise inclined module is developed to capture the\nphysical noise within each object and a denoise framework is then followed to\nremove such noise from the object. Finally, the CNN with noise inclined module\nand the denoise framework is developed to obtain discriminative features and\nprovides good classification performance of hyperspectral image. Experiments\nare conducted over two commonly used real-world datasets and the experimental\nresults show the effectiveness of the proposed method. The implementation of\nthe proposed method and other compared methods could be accessed at\nhttps://github.com/shendu-sw/noise-physical-framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1\">Ping Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiahao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Panhe Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"sat2pc: Estimating Point Cloud of Building Roofs from 2D Satellite Images. (arXiv:2205.12464v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12464","description":"<p>Three-dimensional (3D) urban models have gained interest because of their\napplications in many use-cases such as urban planning and virtual reality.\nHowever, generating these 3D representations requires LiDAR data, which are not\nalways readily available. Thus, the applicability of automated 3D model\ngeneration algorithms is limited to a few locations. In this paper, we propose\nsat2pc, a deep learning architecture that predicts the point cloud of a\nbuilding roof from a single 2D satellite image. Our architecture combines\nChamfer distance and EMD loss, resulting in better 2D to 3D performance. We\nextensively evaluate our model and perform ablation studies on a building roof\ndataset. Our results show that sat2pc was able to outperform existing baselines\nby at least 18.6%. Further, we show that the predicted point cloud captures\nmore detail and geometric characteristics than other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_Y/0/1/0/all/0/1\">Yoones Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stephen Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning. (arXiv:2205.12466v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12466","description":"<p>Learning harmful shortcuts such as spurious correlations and biases prevents\ndeep neural networks from learning the meaningful and useful representations,\nthus jeopardizing the generalizability and interpretability of the learned\nrepresentation. The situation becomes even more serious in medical imaging,\nwhere the clinical data (e.g., MR images with pathology) are limited and scarce\nwhile the reliability, generalizability and transparency of the learned model\nare highly required. To address this problem, we propose to infuse human\nexperts' intelligence and domain knowledge into the training of deep neural\nnetworks. The core idea is that we infuse the visual attention information from\nexpert radiologists to proactively guide the deep model to focus on regions\nwith potential pathology and avoid being trapped in learning harmful shortcuts.\nTo do so, we propose a novel eye-gaze-guided vision transformer (EG-ViT) for\ndiagnosis with limited medical image data. We mask the input image patches that\nare out of the radiologists' interest and add an additional residual connection\nin the last encoder layer of EG-ViT to maintain the correlations of all\npatches. The experiments on two public datasets of INbreast and SIIM-ACR\ndemonstrate our EG-ViT model can effectively learn/transfer experts' domain\nknowledge and achieve much better performance than baselines. Meanwhile, it\nsuccessfully rectifies the harmful shortcut learning and significantly improves\nthe EG-ViT model's interpretability. In general, EG-ViT takes the advantages of\nboth human expert's prior knowledge and the power of deep neural networks. This\nwork opens new avenues for advancing current artificial intelligence paradigms\nby infusing human intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenxiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">David Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Textured Mesh Recovery from Multiple Views with Differentiable Rendering. (arXiv:2205.12468v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12468","description":"<p>Despite of the promising results on shape and color recovery using\nself-supervision, the multi-layer perceptrons-based methods usually costs hours\nto train the deep neural network due to the implicit surface representation.\nMoreover, it is quite computational intensive to render a single image, since a\nforward network inference is required for each pixel. To tackle these\nchallenges, in this paper, we propose an efficient coarse-to-fine approach to\nrecover the textured mesh from multi-view images. Specifically, we take\nadvantage of a differentiable Poisson Solver to represent the shape, which is\nable to produce topology-agnostic and watertight surfaces. To account for the\ndepth information, we optimize the shape geometry by minimizing the difference\nbetween the rendered mesh with the depth predicted by the learning-based\nmulti-view stereo algorithm. In contrast to the implicit neural representation\non shape and color, we introduce a physically based inverse rendering scheme to\njointly estimate the lighting and reflectance of the objects, which is able to\nrender the high resolution image at real-time. Additionally, we fine-tune the\nextracted mesh by inverse rendering to obtain the mesh with fine details and\nhigh fidelity image. We have conducted the extensive experiments on several\nmulti-view stereo datasets, whose promising results demonstrate the efficacy of\nour proposed approach. We will make our full implementation publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yisu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training. (arXiv:2205.12502v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12502","description":"<p>Visual dialog (VisDial) is a task of answering a sequence of questions\ngrounded in an image, using the dialog history as context. Prior work has\ntrained the dialog agents solely on VisDial data via supervised learning or\nleveraged pre-training on related vision-and-language datasets. This paper\npresents a semi-supervised learning approach for visually-grounded dialog,\ncalled Generative Self-Training (GST), to leverage unlabeled images on the Web.\nSpecifically, GST first retrieves in-domain images through out-of-distribution\ndetection and generates synthetic dialogs regarding the images via multimodal\nconditional text generation. GST then trains a dialog agent on the synthetic\nand the original VisDial data. As a result, GST scales the amount of training\ndata up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For\nrobust training of the generated dialogs, we also propose perplexity-based data\nselection and multimodal consistency regularization. Evaluation on VisDial v1.0\nand v0.9 datasets shows that GST achieves new state-of-the-art results on both\ndatasets. We further observe strong performance gains in the low-data regime\n(up to 9.35 absolute points on NDCG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Face Generation with StyleGAN2. (arXiv:2205.12512v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12512","description":"<p>Synthesizing images from text descriptions has become an active research area\nwith the advent of Generative Adversarial Networks. The main goal here is to\ngenerate photo-realistic images that are aligned with the input descriptions.\nText-to-Face generation (T2F) is a sub-domain of Text-to-Image generation (T2I)\nthat is more challenging due to the complexity and variation of facial\nattributes. It has a number of applications mainly in the domain of public\nsafety. Even though several models are available for T2F, there is still the\nneed to improve the image quality and the semantic alignment. In this research,\nwe propose a novel framework, to generate facial images that are well-aligned\nwith the input descriptions. Our framework utilizes the high-resolution face\ngenerator, StyleGAN2, and explores the possibility of using it in T2F. Here, we\nembed text in the input latent space of StyleGAN2 using BERT embeddings and\noversee the generation of facial images using text descriptions. We trained our\nframework on attribute-based descriptions to generate images of 1024x1024 in\nresolution. The images generated exhibit a 57% similarity to the ground truth\nimages, with a face semantic distance of 0.92, outperforming\nstate-of-the-artwork. The generated images have a FID score of 118.097 and the\nexperimental results show that our model generates promising images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayanthi_D/0/1/0/all/0/1\">D. M. A. Ayanthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munasinghe_S/0/1/0/all/0/1\">Sarasi Munasinghe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Aware and Class Balanced 3D Object Detection on nuScenes Dataset. (arXiv:2205.12519v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12519","description":"<p>3-D object detection is pivotal for autonomous driving. Point cloud based\nmethods have become increasingly popular for 3-D object detection, owing to\ntheir accurate depth information. NuTonomy's nuScenes dataset greatly extends\ncommonly used datasets such as KITTI in size, sensor modalities, categories,\nand annotation numbers. However, it suffers from severe class imbalance. The\nClass-balanced Grouping and Sampling paper addresses this issue and suggests\naugmentation and sampling strategy. However, the localization precision of this\nmodel is affected by the loss of spatial information in the downscaled feature\nmaps. We propose to enhance the performance of the CBGS model by designing an\nauxiliary network, that makes full use of the structure information of the 3D\npoint cloud, in order to improve the localization accuracy. The detachable\nauxiliary network is jointly optimized by two point-level supervisions, namely\nforeground segmentation and center estimation. The auxiliary network does not\nintroduce any extra computation during inference, since it can be detached at\ntest time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagesh_S/0/1/0/all/0/1\">Sushruth Nagesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_A/0/1/0/all/0/1\">Asfiya Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Savitha Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. (arXiv:2205.12522v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12522","description":"<p>Research in massively multilingual image captioning has been severely\nhampered by a lack of high-quality evaluation datasets. In this paper we\npresent the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse\nset of 3600 images annotated with human-generated reference captions in 36\nlanguages. The images were selected from across the world, covering regions\nwhere the 36 languages are spoken, and annotated with captions that achieve\nconsistency in terms of style across all languages, while avoiding annotation\nartifacts due to direct translation. We apply this benchmark to model selection\nfor massively multilingual image captioning models, and show superior\ncorrelation results with human evaluations when using XM3600 as golden\nreferences for automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish V. Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Diffusion Models via Early Stop of the Diffusion Process. (arXiv:2205.12524v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12524","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive\nperformance on various generation tasks. By modeling the reverse process of\ngradually diffusing the data distribution into a Gaussian distribution,\ngenerating a sample in DDPMs can be regarded as iteratively denoising a\nrandomly sampled Gaussian noise. However, in practice DDPMs often need hundreds\neven thousands of denoising steps to obtain a high-quality sample from the\nGaussian noise, leading to extremely low inference efficiency. In this work, we\npropose a principled acceleration strategy, referred to as Early-Stopped DDPM\n(ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where\nonly the few initial diffusing steps are considered and the reverse denoising\nprocess starts from a non-Gaussian distribution. By further adopting a powerful\npre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from\nthe target non-Gaussian distribution can be efficiently achieved by diffusing\nsamples obtained from the pre-trained generative model. In this way, the number\nof required denoising steps is significantly reduced. In the meantime, the\nsample quality of ES-DDPM also improves substantially, outperforming both the\nvanilla DDPM and the adopted pre-trained generative model. On extensive\nexperiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat,\nES-DDPM obtains promising acceleration effect and performance improvement over\nrepresentative baseline methods. Moreover, ES-DDPM also demonstrates several\nattractive properties, including being orthogonal to existing acceleration\nmethods, as well as simultaneously enabling both global semantic and local\npixel-level control in image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XU_X/0/1/0/all/0/1\">Xudong XU</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Uncertainty in the Observation Space of Variational Autoencoders. (arXiv:2205.12533v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12533","description":"<p>Variational autoencoders (VAEs) are a popular class of deep generative models\nwith many variants and a wide range of applications. Improvements upon the\nstandard VAE mostly focus on the modelling of the posterior distribution over\nthe latent space and the properties of the neural network decoder. In contrast,\nimproving the model for the observational distribution is rarely considered and\ntypically defaults to a pixel-wise independent categorical or normal\ndistribution. In image synthesis, sampling from such distributions produces\nspatially-incoherent results with uncorrelated pixel noise, resulting in only\nthe sample mean being somewhat useful as an output prediction. In this paper,\nwe aim to stay true to VAE theory by improving the samples from the\nobservational distribution. We propose an alternative model for the observation\nspace, encoding spatial dependencies via a low-rank parameterisation. We\ndemonstrate that this new observational distribution has the ability to capture\nrelevant covariance between pixels, resulting in spatially-coherent samples. In\ncontrast to pixel-wise independent distributions, our samples seem to contain\nsemantically meaningful variations from the mean allowing the prediction of\nmultiple plausible outputs with a single forward pass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Langley_J/0/1/0/all/0/1\">James Langley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">Miguel Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Charles Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1\">Nick Pawlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misleading Deep-Fake Detection with GAN Fingerprints. (arXiv:2205.12543v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12543","description":"<p>Generative adversarial networks (GANs) have made remarkable progress in\nsynthesizing realistic-looking images that effectively outsmart even humans.\nAlthough several detection methods can recognize these deep fakes by checking\nfor image artifacts from the generation process, multiple counterattacks have\ndemonstrated their limitations. These attacks, however, still require certain\nconditions to hold, such as interacting with the detection method or adjusting\nthe GAN directly. In this paper, we introduce a novel class of simple\ncounterattacks that overcomes these limitations. In particular, we show that an\nadversary can remove indicative artifacts, the GAN fingerprint, directly from\nthe frequency spectrum of a generated image. We explore different realizations\nof this removal, ranging from filtering high frequencies to more nuanced\nfrequency-peak cleansing. We evaluate the performance of our attack with\ndifferent detection methods, GAN architectures, and datasets. Our results show\nthat an adversary can often remove GAN fingerprints and thus evade the\ndetection of generated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wesselkamp_V/0/1/0/all/0/1\">Vera Wesselkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_K/0/1/0/all/0/1\">Konrad Rieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arp_D/0/1/0/all/0/1\">Daniel Arp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quiring_E/0/1/0/all/0/1\">Erwin Quiring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dense Local Feature Matching and Vehicle Removal for Indoor Visual Localization. (arXiv:2205.12544v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12544","description":"<p>Visual localization is an essential component of intelligent transportation\nsystems, enabling broad applications that require understanding one's self\nlocation when other sensors are not available. It is mostly tackled by image\nretrieval such that the location of a query image is determined by its closest\nmatch in the previously collected images. Existing approaches focus on large\nscale localization where landmarks are helpful in finding the location.\nHowever, visual localization becomes challenging in small scale environments\nwhere objects are hardly recognizable. In this paper, we propose a visual\nlocalization framework that robustly finds the match for a query among the\nimages collected from indoor parking lots. It is a challenging problem when the\nvehicles in the images share similar appearances and are frequently replaced\nsuch as parking lots. We propose to employ a deep dense local feature matching\nthat resembles human perception to find correspondences and eliminating matches\nfrom vehicles automatically with a vehicle detector. The proposed solution is\nrobust to the scenes with low textures and invariant to false matches caused by\nvehicles. We compare our framework with alternatives to validate our\nsuperiority on a benchmark dataset containing 267 pre-collected images and 99\nquery images taken from 34 sections of a parking lot. Our method achieves 86.9\npercent accuracy, outperforming the alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyung Ho Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some equivalence relation between persistent homology and morphological dynamics. (arXiv:2205.12546v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12546","description":"<p>In Mathematical Morphology (MM), connected filters based on dynamics are used\nto filter the extrema of an image. Similarly, persistence is a concept coming\nfrom Persistent Homology (PH) and Morse Theory (MT) that represents the\nstability of the extrema of a Morse function. Since these two concepts seem to\nbe closely related, in this paper we examine their relationship, and we prove\nthat they are equal on n-D Morse functions, n $\\ge$ 1. More exactly, pairing a\nminimum with a 1-saddle by dynamics or pairing the same 1-saddle with a minimum\nby persistence leads exactly to the same pairing, assuming that the critical\nvalues of the studied Morse function are unique. This result is a step further\nto show how much topological data analysis and mathematical morphology are\nrelated, paving the way for a more in-depth study of the relations between\nthese two research fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1\">Nicolas Boutry</a> (LRDE), <a href=\"http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1\">Laurent Najman</a> (LIGM), <a href=\"http://arxiv.org/find/cs/1/au:+Geraud_T/0/1/0/all/0/1\">Thierry G&#xe9;raud</a> (LRDE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking the Chain of Gradient Leakage in Vision Transformers. (arXiv:2205.12551v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12551","description":"<p>User privacy is of great concern in Federated Learning, while Vision\nTransformers (ViTs) have been revealed to be vulnerable to gradient-based\ninversion attacks. We show that the learned low-dimensional spatial prior in\nposition embeddings (PEs) accelerates the training of ViTs. As a side effect,\nit makes the ViTs tend to be position sensitive and at high risk of privacy\nleakage. We observe that enhancing the position-insensitive property of a ViT\nmodel is a promising way to protect data privacy against these gradient\nattacks. However, simply removing the PEs may not only harm the convergence and\naccuracy of ViTs but also places the model at more severe privacy risk. To deal\nwith the aforementioned contradiction, we propose a simple yet efficient Masked\nJigsaw Puzzle (MJP) method to break the chain of gradient leakage in ViTs. MJP\ncan be easily plugged into existing ViTs and their derived variants. Extensive\nexperiments demonstrate that our proposed MJP method not only boosts the\nperformance on large-scale datasets (i.e., ImageNet-1K), but can also improve\nthe privacy preservation capacity in the typical gradient attacks by a large\nmargin. Our code is available at: https://github.com/yhlleo/MJP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spotlights: Probing Shapes from Spherical Viewpoints. (arXiv:2205.12564v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12564","description":"<p>Recent years have witnessed the surge of learned representations that\ndirectly build upon point clouds. Though becoming increasingly expressive, most\nexisting representations still struggle to generate ordered point sets.\nInspired by spherical multi-view scanners, we propose a novel sampling model\ncalled Spotlights to represent a 3D shape as a compact 1D array of depth\nvalues. It simulates the configuration of cameras evenly distributed on a\nsphere, where each virtual camera casts light rays from its principal point\nthrough sample points on a small concentric spherical cap to probe for the\npossible intersections with the object surrounded by the sphere. The structured\npoint cloud is hence given implicitly as a function of depths. We provide a\ndetailed geometric analysis of this new sampling scheme and prove its\neffectiveness in the context of the point cloud completion task. Experimental\nresults on both synthetic and real data demonstrate that our method achieves\ncompetitive accuracy and consistency while having a significantly reduced\ncomputational cost. Furthermore, we show superior performance on the downstream\npoint cloud registration task over state-of-the-art completion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaxin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lige Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">Soren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pedestrian Detection to Crosswalk Estimation: An EM Algorithm and Analysis on Diverse Datasets. (arXiv:2205.12579v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12579","description":"<p>In this work, we contribute an EM algorithm for estimation of corner points\nand linear crossing segments for both marked and unmarked pedestrian crosswalks\nusing the detections of pedestrians from processed LiDAR point clouds or camera\nimages. We demonstrate the algorithmic performance by analyzing three\nreal-world datasets containing multiple periods of data collection for\nfour-corner and two-corner intersections with marked and unmarked crosswalks.\nAdditionally, we include a Python video tool to visualize the crossing\nparameter estimation, pedestrian trajectories, and phase intervals in our\npublic source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan Trivedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose. (arXiv:2205.12583v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12583","description":"<p>Reconstructing multi-human body mesh from a single monocular image is an\nimportant but challenging computer vision problem. In addition to the\nindividual body mesh models, we need to estimate relative 3D positions among\nsubjects to generate a coherent representation. In this work, through a single\ngraph neural network, named MUG (Multi-hUman Graph network), we construct\ncoherent multi-human meshes using only multi-human 2D pose as input. Compared\nwith existing methods, which adopt a detection-style pipeline (i.e., extracting\nimage features and then locating human instances and recovering body meshes\nfrom that) and suffer from the significant domain gap between lab-collected\ntraining datasets and in-the-wild testing datasets, our method benefits from\nthe 2D pose which has a relatively consistent geometric property across\ndatasets. Our method works like the following: First, to model the multi-human\nenvironment, it processes multi-human 2D poses and builds a novel heterogeneous\ngraph, where nodes from different people and within one person are connected to\ncapture inter-human interactions and draw the body geometry (i.e., skeleton and\nmesh structure). Second, it employs a dual-branch graph neural network\nstructure -- one for predicting inter-human depth relation and the other one\nfor predicting root-joint-relative mesh coordinates. Finally, the entire\nmulti-human 3D meshes are constructed by combining the output from both\nbranches. Extensive experiments demonstrate that MUG outperforms previous\nmulti-human mesh estimation methods on standard 3D human benchmarks --\nPanoptic, MuPoTS-3D and 3DPW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yandong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deniable Steganography. (arXiv:2205.12587v1 [cs.CR])","link":"http://arxiv.org/abs/2205.12587","description":"<p>Steganography conceals the secret message into the cover media, generating a\nstego media which can be transmitted on public channels without drawing\nsuspicion. As its countermeasure, steganalysis mainly aims to detect whether\nthe secret message is hidden in a given media. Although the steganography\ntechniques are improving constantly, the sophisticated steganalysis can always\nbreak a known steganographic method to some extent. With a stego media\ndiscovered, the adversary could find out the sender or receiver and coerce them\nto disclose the secret message, which we name as coercive attack in this paper.\nInspired by the idea of deniable encryption, we build up the concepts of\ndeniable steganography for the first time and discuss the feasible\nconstructions for it. As an example, we propose a receiver-deniable\nsteganographic scheme to deal with the receiver-side coercive attack using deep\nneural networks (DNN). Specifically, besides the real secret message, a piece\nof fake message is also embedded into the cover. On the receiver side, the real\nmessage can be extracted with an extraction module; while once the receiver has\nto surrender a piece of secret message under coercive attack, he can extract\nthe fake message to deceive the adversary with another extraction module.\nExperiments demonstrate the scalability and sensitivity of the DNN-based\nreceiver-deniable steganographic scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhihua Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jian Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation. (arXiv:2205.12602v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12602","description":"<p>This paper presents Volumetric Transformer Pose estimator (VTP), the first 3D\nvolumetric transformer framework for multi-view multi-person 3D human pose\nestimation. VTP aggregates features from 2D keypoints in all camera views and\ndirectly learns the spatial relationships in the 3D voxel space in an\nend-to-end fashion. The aggregated 3D features are passed through 3D\nconvolutions before being flattened into sequential embeddings and fed into a\ntransformer. A residual structure is designed to further improve the\nperformance. In addition, the sparse Sinkhorn attention is empowered to reduce\nthe memory cost, which is a major bottleneck for volumetric representations,\nwhile also achieving excellent performance. The output of the transformer is\nagain concatenated with 3D convolutional features by a residual design. The\nproposed VTP framework integrates the high performance of the transformer with\nvolumetric representations, which can be used as a good alternative to the\nconvolutional backbones. Experiments on the Shelf, Campus and CMU Panoptic\nbenchmarks show promising results in terms of both Mean Per Joint Position\nError (MPJPE) and Percentage of Correctly estimated Parts (PCP). Our code will\nbe available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_O/0/1/0/all/0/1\">Ouhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReSmooth: Detecting and Utilizing OOD Samples when Training with Data Augmentation. (arXiv:2205.12606v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12606","description":"<p>Data augmentation (DA) is a widely used technique for enhancing the training\nof deep neural networks. Recent DA techniques which achieve state-of-the-art\nperformance always meet the need for diversity in augmented training samples.\nHowever, an augmentation strategy that has a high diversity usually introduces\nout-of-distribution (OOD) augmented samples and these samples consequently\nimpair the performance. To alleviate this issue, we propose ReSmooth, a\nframework that firstly detects OOD samples in augmented samples and then\nleverages them. To be specific, we first use a Gaussian mixture model to fit\nthe loss distribution of both the original and augmented samples and\naccordingly split these samples into in-distribution (ID) samples and OOD\nsamples. Then we start a new training where ID and OOD samples are incorporated\nwith different smooth labels. By treating ID samples and OOD samples unequally,\nwe can make better use of the diverse augmented data. Further, we incorporate\nour ReSmooth framework with negative data augmentation strategies. By properly\nhandling their intentionally created ODD samples, the classification\nperformance of negative data augmentations is largely ameliorated. Experiments\non several classification benchmarks show that ReSmooth can be easily extended\nto existing augmentation strategies (such as RandAugment, rotate, and jigsaw)\nand improve on them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Aesthetic Assessment and Retrieval of Breast Cancer Treatment Outcomes. (arXiv:2205.12611v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12611","description":"<p>Treatments for breast cancer have continued to evolve and improve in recent\nyears, resulting in a substantial increase in survival rates, with\napproximately 80\\% of patients having a 10-year survival period. Given the\nserious impact that breast cancer treatments can have on a patient's body\nimage, consequently affecting her self-confidence and sexual and intimate\nrelationships, it is paramount to ensure that women receive the treatment that\noptimizes both survival and aesthetic outcomes. Currently, there is no gold\nstandard for evaluating the aesthetic outcome of breast cancer treatment. In\naddition, there is no standard way to show patients the potential outcome of\nsurgery. The presentation of similar cases from the past would be extremely\nimportant to manage women's expectations of the possible outcome. In this work,\nwe propose a deep neural network to perform the aesthetic evaluation. As a\nproof-of-concept, we focus on a binary aesthetic evaluation. Besides its use\nfor classification, this deep neural network can also be used to find the most\nsimilar past cases by searching for nearest neighbours in the highly semantic\nspace before classification. We performed the experiments on a dataset\nconsisting of 143 photos of women after conservative treatment for breast\ncancer. The results for accuracy and balanced accuracy showed the superior\nperformance of our proposed model compared to the state of the art in aesthetic\nevaluation of breast cancer treatments. In addition, the model showed a good\nability to retrieve similar previous cases, with the retrieved cases having the\nsame or adjacent class (in the 4-class setting) and having similar types of\nasymmetry. Finally, a qualitative interpretability assessment was also\nperformed to analyse the robustness and trustworthiness of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1\">Wilson Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1\">Maria Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavioso_C/0/1/0/all/0/1\">Carlos Mavioso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">Maria J. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Visual Question Answering with Attention Priors. (arXiv:2205.12616v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12616","description":"<p>The current success of modern visual reasoning systems is arguably attributed\nto cross-modality attention mechanisms. However, in deliberative reasoning such\nas in VQA, attention is unconstrained at each step, and thus may serve as a\nstatistical pooling mechanism rather than a semantic operation intended to\nselect information relevant to inference. This is because at training time,\nattention is only guided by a very sparse signal (i.e. the answer label) at the\nend of the inference chain. This causes the cross-modality attention weights to\ndeviate from the desired visual-language bindings. To rectify this deviation,\nwe propose to guide the attention mechanism using explicit linguistic-visual\ngrounding. This grounding is derived by connecting structured linguistic\nconcepts in the query to their referents among the visual objects. Here we\nlearn the grounding from the pairing of questions and images alone, without the\nneed for answer annotation or external grounding supervision. This grounding\nguides the attention mechanism inside VQA models through a duality of\nmechanisms: pre-training attention weight calculation and directly guiding the\nweights at inference time on a case-by-case basis. The resultant algorithm is\ncapable of probing attention-based reasoning models, injecting relevant\nassociative knowledge, and regulating the core reasoning process. This scalable\nenhancement improves the performance of VQA models, fortifies their robustness\nto limited access to supervised data, and increases interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisinfoMeme: A Multimodal Dataset for Detecting Meme Intentionally Spreading Out Disinformation. (arXiv:2205.12617v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12617","description":"<p>Disinformation has become a serious problem on social media. In particular,\ngiven their short format, visual attraction, and humorous nature, memes have a\nsignificant advantage in dissemination among online communities, making them an\neffective vehicle for the spread of disinformation. We present DisinfoMeme to\nhelp detect disinformation memes. The dataset contains memes mined from Reddit\ncovering three current topics: the COVID-19 pandemic, the Black Lives Matter\nmovement, and veganism/vegetarianism. The dataset poses multiple unique\nchallenges: limited data and label imbalance, reliance on external knowledge,\nmultimodal reasoning, layout dependency, and noise from OCR. We test multiple\nwidely-used unimodal and multimodal models on this dataset. The experiments\nshow that the room for improvement is still huge for current models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jingnong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Location-free Human Pose Estimation. (arXiv:2205.12619v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12619","description":"<p>Human pose estimation (HPE) usually requires large-scale training data to\nreach high performance. However, it is rather time-consuming to collect\nhigh-quality and fine-grained annotations for human body. To alleviate this\nissue, we revisit HPE and propose a location-free framework without supervision\nof keypoint locations. We reformulate the regression-based HPE from the\nperspective of classification. Inspired by the CAM-based weakly-supervised\nobject localization, we observe that the coarse keypoint locations can be\nacquired through the part-aware CAMs but unsatisfactory due to the gap between\nthe fine-grained HPE and the object-level localization. To this end, we propose\na customized transformer framework to mine the fine-grained representation of\nhuman context, equipped with the structural relation to capture subtle\ndifferences among keypoints. Concretely, we design a Multi-scale Spatial-guided\nContext Encoder to fully capture the global human context while focusing on the\npart-aware regions and a Relation-encoded Pose Prototype Generation module to\nencode the structural relations. All these works together for strengthening the\nweak supervision from image-level category labels on locations. Our model\nachieves competitive performance on three datasets when only supervised at a\ncategory-level and importantly, it can achieve comparable results with\nfully-supervised methods with only 25\\% location labels on MS-COCO and MPII.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xixia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qi Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primitive3D: 3D Object Dataset Synthesis from Randomly Assembled Primitives. (arXiv:2205.12627v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12627","description":"<p>Numerous advancements in deep learning can be attributed to the access to\nlarge-scale and well-annotated datasets. However, such a dataset is\nprohibitively expensive in 3D computer vision due to the substantial collection\ncost. To alleviate this issue, we propose a cost-effective method for\nautomatically generating a large amount of 3D objects with annotations. In\nparticular, we synthesize objects simply by assembling multiple random\nprimitives. These objects are thus auto-annotated with part labels originating\nfrom primitives. This allows us to perform multi-task learning by combining the\nsupervised segmentation with unsupervised reconstruction. Considering the large\noverhead of learning on the generated dataset, we further propose a dataset\ndistillation strategy to remove redundant samples regarding a target dataset.\nWe conduct extensive experiments for the downstream tasks of 3D object\nclassification. The results indicate that our dataset, together with multi-task\npretraining on its annotations, achieves the best performance compared to other\ncommonly used datasets. Further study suggests that our strategy can improve\nthe model performance by pretraining and fine-tuning scheme, especially for the\ndataset with a small scale. In addition, pretraining with the proposed dataset\ndistillation method can save 86\\% of the pretraining time with negligible\nperformance degradation. We expect that our attempt provides a new data-centric\nperspective for training 3D deep models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chee_Y/0/1/0/all/0/1\">Yeow Meng Chee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Knowledge Alignment with Reinforcement Learning. (arXiv:2205.12630v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12630","description":"<p>Large language models readily adapt to novel settings, even without\ntask-specific training data. Can their zero-shot capacity be extended to\nmultimodal inputs? In this work, we propose ESPER which extends language-only\nzero-shot models to unseen multimodal tasks, like image and audio captioning.\nOur key novelty is to use reinforcement learning to align multimodal inputs to\nlanguage model generations without direct supervision: for example, in the\nimage case our reward optimization relies only on cosine similarity derived\nfrom CLIP, and thus requires no additional explicitly paired (image, caption)\ndata. Because the parameters of the language model are left unchanged, the\nmodel maintains its capacity for zero-shot generalization. Experiments\ndemonstrate that ESPER outperforms baselines and prior work on a variety of\nzero-shot tasks; these include a new benchmark we collect+release, ESP dataset,\nwhich tasks models with generating several diversely-styled captions for each\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jiwan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Heeseung Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">JaeSung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on High Dynamic Range Imaging: Methods and Results. (arXiv:2205.12633v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12633","description":"<p>This paper reviews the challenge on constrained high dynamic range (HDR)\nimaging that was part of the New Trends in Image Restoration and Enhancement\n(NTIRE) workshop, held in conjunction with CVPR 2022. This manuscript focuses\non the competition set-up, datasets, the proposed methods and their results.\nThe challenge aims at estimating an HDR image from multiple respective low\ndynamic range (LDR) observations, which might suffer from under- or\nover-exposed regions and different sources of noise. The challenge is composed\nof two tracks with an emphasis on fidelity and complexity constraints: In Track\n1, participants are asked to optimize objective fidelity scores while imposing\na low-complexity constraint (i.e. solutions can not exceed a given number of\noperations). In Track 2, participants are asked to minimize the complexity of\ntheir solutions while imposing a constraint on fidelity scores (i.e. solutions\nare required to obtain a higher fidelity score than the prescribed baseline).\nBoth tracks use the same data and metrics: Fidelity is measured by means of\nPSNR with respect to a ground-truth HDR image (computed both directly and with\na canonical tonemapping operation), while complexity metrics include the number\nof Multiply-Accumulate (MAC) operations and runtime (in seconds).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catley_Chandar_S/0/1/0/all/0/1\">Sibi Catley-Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Richard Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yunbo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gaocheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Javen Qinfeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengdi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guannan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Baozhu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenjie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chengzhi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_Vega_J/0/1/0/all/0/1\">Juan Mar&#xed;n-Vega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloth_M/0/1/0/all/0/1\">Michael Sloth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_R/0/1/0/all/0/1\">Richard R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Long Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1\">Gen Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xing Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_R/0/1/0/all/0/1\">Ruipeng Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fei Lei</a>, et al. (31 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12634","description":"<p>While motion compensation greatly improves video deblurring quality,\nseparately performing motion compensation and video deblurring demands huge\ncomputational overhead. This paper proposes a real-time video deblurring\nframework consisting of a lightweight multi-task unit that supports both video\ndeblurring and motion compensation in an efficient way. The multi-task unit is\nspecifically designed to handle large portions of the two tasks using a single\nshared network, and consists of a multi-task detail network and simple networks\nfor deblurring and motion compensation. The multi-task unit minimizes the cost\nof incorporating motion compensation into video deblurring and enables\nreal-time deblurring. Moreover, by stacking multiple multi-task units, our\nframework provides flexible control between the cost and deblurring quality. We\nexperimentally validate the state-of-the-art deblurring quality of our\napproach, which runs at a much faster speed compared to previous methods, and\nshow practical real-time performance (30.99dB@30fps measured in the DVD\ndataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoViT: Mobile Convolutional Vision Transformer. (arXiv:2205.12635v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12635","description":"<p>Recently, Transformer networks have achieved impressive results on a variety\nof vision tasks. However, most of them are computationally expensive and not\nsuitable for real-world mobile applications. In this work, we present Mobile\nConvolutional Vision Transformer (MoCoViT), which improves in performance and\nefficiency by introducing transformer into mobile convolutional networks to\nleverage the benefits of both architectures. Different from recent works on\nvision transformer, the mobile transformer block in MoCoViT is carefully\ndesigned for mobile devices and is very lightweight, accomplished through two\nprimary modifications: the Mobile Self-Attention (MoSA) module and the Mobile\nFeed Forward Network (MoFFN). MoSA simplifies the calculation of the attention\nmap through Branch Sharing scheme while MoFFN serves as a mobile version of MLP\nin the transformer, further reducing the computation by a large margin.\nComprehensive experiments verify that our proposed MoCoViT family outperform\nstate-of-the-art portable CNNs and transformer neural architectures on various\nvision tasks. On ImageNet classification, it achieves 74.5% top-1 accuracy at\n147M FLOPs, gaining 1.2% over MobileNetV3 with less computations. And on the\nCOCO object detection task, MoCoViT outperforms GhostNet by 2.1 AP in RetinaNet\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreEnhance: An Automatic Tree-Search Based Method for Low-Light Image Enhancement. (arXiv:2205.12639v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12639","description":"<p>In this paper we present TreEnhance, an automatic method for low-light image\nenhancement capable of improving the quality of digital images. The method\ncombines tree search theory, and in particular the Monte Carlo Tree Search\n(MCTS) algorithm, with deep reinforcement learning. Given as input a low-light\nimage, TreEnhance produces as output its enhanced version together with the\nsequence of image editing operations used to obtain it. The method repeatedly\nalternates two main phases. In the generation phase a modified version of MCTS\nexplores the space of image editing operations and selects the most promising\nsequence. In the optimization phase the parameters of a neural network,\nimplementing the enhancement policy, are updated. After training, two different\ninference solutions are proposed for the enhancement of new images: one is\nbased on MCTS and is more accurate but more time and memory consuming; the\nother directly applies the learned policy and is faster but slightly less\nprecise. Unlike other methods from the state of the art, TreEnhance does not\npose any constraint on the image resolution and can be used in a variety of\nscenarios with minimal tuning. We tested the method on two datasets: the\nLow-Light dataset and the Adobe Five-K dataset obtaining good results from both\na qualitative and a quantitative point of view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotogni_M/0/1/0/all/0/1\">Marco Cotogni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cusano_C/0/1/0/all/0/1\">Claudio Cusano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniInst: Unique Representation for End-to-End Instance Segmentation. (arXiv:2205.12646v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12646","description":"<p>Existing instance segmentation methods have achieved impressive performance\nbut still suffer from a common dilemma: redundant representations (e.g.,\nmultiple boxes, grids, and anchor points) are inferred for one instance, which\nleads to multiple duplicated predictions. Thus, mainstream methods usually rely\non a hand-designed non-maximum suppression (NMS) post-processing to select the\noptimal prediction result, which hinders end-to-end training. To address this\nissue, we propose a box-free and NMS-free end-to-end instance segmentation\nframework, termed UniInst, that yields only one unique representation for each\ninstance. Specifically, we design an instance-aware one-to-one assignment\nscheme, namely Only Yield One Representation (OYOR), which dynamically assigns\none unique representation to one instance according to the matching quality\nbetween predictions and ground truths. Then, a novel prediction re-ranking\nstrategy is elegantly integrated into the framework to address the misalignment\nbetween the classification score and the mask quality, enabling the learned\nrepresentation to be more discriminative. With these techniques, our UniInst,\nthe first FCN-based end-to-end instance segmentation framework, achieves\ncompetitive performance, e.g., 39.0 mask AP with ResNet-50-FPN and 40.2 mask AP\nwith ResNet-101-FPN, against mainstream methods on the COCO benchmark.\nMoreover, the proposed instance-aware method is robust to occlusion scenes,\noutperforming common baselines by remarkable mask AP on the heavily-occluded\nOCHuman benchmark. Our codes will be available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yimin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lufan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Severity Classification on Chest X-ray Images. (arXiv:2205.12705v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12705","description":"<p>Biomedical imaging analysis combined with artificial intelligence (AI)\nmethods has proven to be quite valuable in order to diagnose COVID-19. So far,\nvarious classification models have been used for diagnosing COVID-19. However,\nclassification of patients based on their severity level is not yet analyzed.\nIn this work, we classify covid images based on the severity of the infection.\nFirst, we pre-process the X-ray images using a median filter and histogram\nequalization. Enhanced X-ray images are then augmented using SMOTE technique\nfor achieving a balanced dataset. Pre-trained Resnet50, VGG16 model and SVM\nclassifier are then used for feature extraction and classification. The result\nof the classification model confirms that compared with the alternatives, with\nchest X-Ray images, the ResNet-50 model produced remarkable classification\nresults in terms of accuracy (95%), recall (0.94), and F1-Score (0.92), and\nprecision (0.91).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Aditi Sagar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swaraj_A/0/1/0/all/0/1\">Aman Swaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verma_K/0/1/0/all/0/1\">Karan Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIoU Loss: More Powerful Learning for Bounding Box Regression. (arXiv:2205.12740v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12740","description":"<p>The effectiveness of Object Detection, one of the central problems in\ncomputer vision tasks, highly depends on the definition of the loss function -\na measure of how accurately your ML model can predict the expected outcome.\nConventional object detection loss functions depend on aggregation of metrics\nof bounding box regression such as the distance, overlap area and aspect ratio\nof the predicted and ground truth boxes (i.e. GIoU, CIoU, ICIoU etc). However,\nnone of the methods proposed and used to date considers the direction of the\nmismatch between the desired ground box and the predicted, \"experimental\" box.\nThis shortage results in slower and less effective convergence as the predicted\nbox can \"wander around\" during the training process and eventually end up\nproducing a worse model. In this paper a new loss function SIoU was suggested,\nwhere penalty metrics were redefined considering the angle of the vector\nbetween the desired regression. Applied to conventional Neural Networks and\ndatasets it is shown that SIoU improves both the speed of training and the\naccuracy of the inference. The effectiveness of the proposed loss function was\nrevealed in a number of simulations and tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gevorgyan_Z/0/1/0/all/0/1\">Zhora Gevorgyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Distribution Shift Robustness From the Perspective of Pre-Training and Data Augmentation. (arXiv:2205.12753v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12753","description":"<p>The performance of machine learning models under distribution shift has been\nthe focus of the community in recent years. Most of current methods have been\nproposed to improve the robustness to distribution shift from the algorithmic\nperspective, i.e., designing better training algorithms to help the\ngeneralization in shifted test distributions. This paper studies the\ndistribution shift problem from the perspective of pre-training and data\naugmentation, two important factors in the practice of deep learning that have\nnot been systematically investigated by existing work. By evaluating seven\npre-trained models, including ResNets and ViT's with self-supervision and\nsupervision mode, on five important distribution-shift datasets, from WILDS and\nDomainBed benchmarks, with five different learning algorithms, we provide the\nfirst comprehensive empirical study focusing on pre-training and data\naugmentation. With our empirical result obtained from 1,330 models, we provide\nthe following main observations: 1) ERM combined with data augmentation can\nachieve state-of-the-art performance if we choose a proper pre-trained model\nrespecting the data property; 2) specialized algorithms further improve the\nrobustness on top of ERM when handling a specific type of distribution shift,\ne.g., GroupDRO for spurious correlation and CORAL for large-scale\nout-of-distribution data; 3) Comparing different pre-training modes,\narchitectures and data sizes, we provide novel observations about pre-training\non distribution shift, which sheds light on designing or selecting pre-training\nstrategy for different kinds of distribution shifts. In summary, our empirical\nstudy provides a comprehensive baseline for a wide range of pre-training models\nfine-tuned with data augmentation, which potentially inspires research\nexploiting the power of pre-training and data augmentation in the future of\ndistribution shift study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12755","description":"<p>Multitask learning assumes that models capable of learning from multiple\ntasks can achieve better quality and efficiency via knowledge transfer, a key\nfeature of human learning. Though, state of the art ML models rely on high\ncustomization for each task and leverage size and data scale rather than\nscaling the number of tasks. Also, continual learning, that adds the temporal\naspect to multitask, is often focused to the study of common pitfalls such as\ncatastrophic forgetting instead of being studied at a large scale as a critical\ncomponent to build the next generation artificial intelligence. We propose an\nevolutionary method that can generate a large scale multitask model, and can\nsupport the dynamic and continuous addition of new tasks. The generated\nmultitask model is sparsely activated and integrates a task-based routing that\nguarantees bounded compute cost and fewer added parameters per task as the\nmodel expands. The proposed method relies on a knowledge compartmentalization\ntechnique to achieve immunity against catastrophic forgetting and other common\npitfalls such as gradient interference and negative transfer. We empirically\nshow that the proposed method can jointly solve and achieve competitive results\non 69image classification tasks, for example achieving the best test accuracy\nreported fora model trained only on public data for competitive tasks such as\ncifar10: 99.43%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesmundo_A/0/1/0/all/0/1\">Andrea Gesmundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AO2-DETR: Arbitrary-Oriented Object Detection Transformer. (arXiv:2205.12785v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12785","description":"<p>Arbitrary-oriented object detection (AOOD) is a challenging task to detect\nobjects in the wild with arbitrary orientations and cluttered arrangements.\nExisting approaches are mainly based on anchor-based boxes or dense points,\nwhich rely on complicated hand-designed processing steps and inductive bias,\nsuch as anchor generation, transformation, and non-maximum suppression\nreasoning. Recently, the emerging transformer-based approaches view object\ndetection as a direct set prediction problem that effectively removes the need\nfor hand-designed components and inductive biases. In this paper, we propose an\nArbitrary-Oriented Object DEtection TRansformer framework, termed AO2-DETR,\nwhich comprises three dedicated components. More precisely, an oriented\nproposal generation mechanism is proposed to explicitly generate oriented\nproposals, which provides better positional priors for pooling features to\nmodulate the cross-attention in the transformer decoder. An adaptive oriented\nproposal refinement module is introduced to extract rotation-invariant region\nfeatures and eliminate the misalignment between region features and objects.\nAnd a rotation-aware set matching loss is used to ensure the one-to-one\nmatching process for direct set prediction without duplicate predictions. Our\nmethod considerably simplifies the overall pipeline and presents a new AOOD\nparadigm. Comprehensive experiments on several challenging datasets show that\nour method achieves superior performance on the AOOD task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Linhui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-rigid Point Cloud Registration with Neural Deformation Pyramid. (arXiv:2205.12796v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12796","description":"<p>Non-rigid point cloud registration is a key component in many computer vision\nand computer graphics applications. The high complexity of the unknown\nnon-rigid motion make this task a challenging problem. In this paper, we break\ndown this problem via hierarchical motion decomposition. Our method called\nNeural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid\narchitecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),\ntakes as input a sinusoidally encoded 3D point and outputs its motion\nincrements from the previous level. The sinusoidal function starts with a low\ninput frequency and gradually increases when the pyramid level goes down. This\nallows a multi-level rigid to nonrigid motion decomposition and also speeds up\nthe solving by 50 times compared to the existing MLP-based approach. Our method\nachieves advanced partialto-partial non-rigid point cloud registration results\non the 4DMatch/4DLoMatch benchmark under both no-learned and supervised\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistillAdapt: Source-Free Active Visual Domain Adaptation. (arXiv:2205.12840v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12840","description":"<p>We present a novel method, DistillAdapt, for the challenging problem of\nSource-Free Active Domain Adaptation (SF-ADA). The problem requires adapting a\npretrained source domain network to a target domain, within a provided budget\nfor acquiring labels in the target domain, while assuming that the source data\nis not available for adaptation due to privacy concerns or otherwise.\nDistillAdapt is one of the first approaches for SF-ADA, and holistically\naddresses the challenges of SF-ADA via a novel Guided Attention Transfer\nNetwork (GATN) and an active learning heuristic, H_AL. The GATN enables\nselective distillation of features from the pre-trained network to the target\nnetwork using a small subset of annotated target samples mined by H_AL. H_AL\nacquires samples at batch-level and balances transfer-ability from the\npre-trained network and uncertainty of the target network. DistillAdapt is\ntask-agnostic, and can be applied across visual tasks such as classification,\nsegmentation and detection. Moreover, DistillAdapt can handle shifts in output\nlabel space. We conduct experiments and extensive ablation studies across 3\nvisual tasks, viz. digits classification (MNIST, SVHN), synthetic (GTA5) to\nreal (CityScapes) image segmentation, and document layout detection (PubLayNet\nto DSSE). We show that our source-free approach, DistillAdapt, results in an\nimprovement of 0.5% - 31.3% (across datasets and tasks) over prior adaptation\nmethods that assume access to large amounts of annotated source data for\nadaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghuhan_M/0/1/0/all/0/1\">Manoj Ghuhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_T/0/1/0/all/0/1\">Tripti Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Gastric Histopathology Sub-size Image Classification: from Linear Regression to Visual Transformer. (arXiv:2205.12843v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12843","description":"<p>Gastric cancer is the fifth most common cancer in the world. At the same\ntime, it is also the fourth most deadly cancer. Early detection of cancer\nexists as a guide for the treatment of gastric cancer. Nowadays, computer\ntechnology has advanced rapidly to assist physicians in the diagnosis of\npathological pictures of gastric cancer. Ensemble learning is a way to improve\nthe accuracy of algorithms, and finding multiple learning models with\ncomplementarity types is the basis of ensemble learning. The complementarity of\nsub-size pathology image classifiers when machine performance is insufficient\nis explored in this experimental platform. We choose seven classical machine\nlearning classifiers and four deep learning classifiers for classification\nexperiments on the GasHisSDB database. Among them, classical machine learning\nalgorithms extract five different image virtual features to match multiple\nclassifier algorithms. For deep learning, we choose three convolutional neural\nnetwork classifiers. In addition, we also choose a novel Transformer-based\nclassifier. The experimental platform, in which a large number of classical\nmachine learning and deep learning methods are performed, demonstrates that\nthere are differences in the performance of different classifiers on GasHisSDB.\nClassical machine learning models exist for classifiers that classify Abnormal\ncategories very well, while classifiers that excel in classifying Normal\ncategories also exist. Deep learning models also exist with multiple models\nthat can be complementarity. Suitable classifiers are selected for ensemble\nlearning, when machine performance is insufficient. This experimental platform\ndemonstrates that multiple classifiers are indeed complementarity and can\nimprove the efficiency of ensemble learning. This can better assist doctors in\ndiagnosis, improve the detection of gastric cancer, and increase the cure rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Gradient Learning for Efficient Camouflaged Object Detection. (arXiv:2205.12853v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12853","description":"<p>This paper introduces DGNet, a novel deep framework that exploits object\ngradient supervision for camouflaged object detection (COD). It decouples the\ntask into two connected branches, i.e., a context and a texture encoder. The\nessential connection is the gradient-induced transition, representing a soft\ngrouping between context and texture features. Benefiting from the simple but\nefficient framework, DGNet outperforms existing state-of-the-art COD models by\na large margin. Notably, our efficient version, DGNet-S, runs in real-time (80\nfps) and achieves comparable results to the cutting-edge model\nJCSOD-CVPR$_{21}$ with only 6.82% parameters. Application results also show\nthat the proposed DGNet performs well in polyp segmentation, defect detection,\nand transparent object segmentation tasks. Codes will be made available at\nhttps://github.com/GewelsJI/DGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yu-Cheng Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Unbiased Adversarial Model for Medical Image Segmentation. (arXiv:2205.12857v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12857","description":"<p>Generative models have been widely proposed in image recognition to generate\nmore images where the distribution is similar to that of the real images. It\noften introduces a discriminator network to discriminate original real data and\ngenerated data.\n</p>\n<p>However, such discriminator often considers the distribution of the data and\ndid not pay enough attention to the intrinsic gap due to structure.\n</p>\n<p>In this paper, we reformulate a new image to image translation problem to\nreduce structural gap, in addition to the typical intensity distribution gap.\nWe further propose a simple yet important Structure Unbiased Adversarial Model\nfor Medical Image Segmentation (SUAM) with learnable inverse structural\ndeformation for medical image segmentation. It consists of a structure\nextractor, an attention diffeomorphic registration and a structure \\&amp; intensity\ndistribution rendering module. The structure extractor aims to extract the\ndominant structure of the input image. The attention diffeomorphic registration\nis proposed to reduce the structure gap with an inverse deformation field to\nwarp the prediction masks back to their original form. The structure rendering\nmodule is to render the deformed structure to an image with targeted intensity\ndistribution. We apply the proposed SUAM on both optical coherence tomography\n(OCT), magnetic resonance imaging (MRI) and computerized tomography (CT) data.\nExperimental results show that the proposed method has the capability to\ntransfer both intensity and structure distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shaoming Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bartlett_J/0/1/0/all/0/1\">Joseph Bartlett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaowen Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jinming Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Colorization using U-Net with Skip Connections and Fusion Layer on Landscape Images. (arXiv:2205.12867v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12867","description":"<p>We present a novel technique to automatically colorize grayscale images that\ncombine the U-Net model and Fusion Layer features. This approach allows the\nmodel to learn the colorization of images from pre-trained U-Net. Moreover, the\nFusion layer is applied to merge local information results dependent on small\nimage patches with global priors of an entire image on each class, forming\nvisually more compelling colorization results. Finally, we validate our\napproach with a user study evaluation and compare it against state-of-the-art,\nresulting in improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zayd_M/0/1/0/all/0/1\">Muhammad Hisyam Zayd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wihandika_R/0/1/0/all/0/1\">Randy Cahya Wihandika</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Sign Language Translation Learned from Online Video. (arXiv:2205.12870v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12870","description":"<p>Existing work on sign language translation--that is, translation from sign\nlanguage videos into sentences in a written language--has focused mainly on (1)\ndata collected in a controlled environment or (2) data in a specific domain,\nwhich limits the applicability to real-world settings. In this paper, we\nintroduce OpenASL, a large-scale ASL-English dataset collected from online\nvideo sites (e.g., YouTube). OpenASL contains 288 hours of ASL videos in\nvarious domains (news, VLOGs, etc.) from over 200 signers and is the largest\npublicly available ASL translation dataset to date. To tackle the challenges of\nsign language translation in realistic settings and without glosses, we propose\na set of techniques including sign search as a pretext task for pre-training\nand fusion of mouthing and handshape features. The proposed techniques produce\nconsistent and large improvements in translation quality, over baseline models\nbased on prior work. Our data, code and model will be publicly available at\nhttps://github.com/chevalierNoir/OpenASL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brentari_D/0/1/0/all/0/1\">Diane Brentari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos. (arXiv:2205.12886v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12886","description":"<p>Moment retrieval in videos is a challenging task that aims to retrieve the\nmost relevant video moment in an untrimmed video given a sentence description.\nPrevious methods tend to perform self-modal learning and cross-modal\ninteraction in a coarse manner, which neglect fine-grained clues contained in\nvideo content, query context, and their alignment. To this end, we propose a\nnovel Multi-Granularity Perception Network (MGPN) that perceives intra-modality\nand inter-modality information at a multi-granularity level. Specifically, we\nformulate moment retrieval as a multi-choice reading comprehension task and\nintegrate human reading strategies into our framework. A coarse-grained feature\nencoder and a co-attention mechanism are utilized to obtain a preliminary\nperception of intra-modality and inter-modality information. Then a\nfine-grained feature encoder and a conditioned interaction module are\nintroduced to enhance the initial perception inspired by how humans address\nreading comprehension problems. Moreover, to alleviate the huge computation\nburden of some existing methods, we further design an efficient choice\ncomparison module and reduce the hidden size with imperceptible quality loss.\nExtensive experiments on Charades-STA, TACoS, and ActivityNet Captions datasets\ndemonstrate that our solution outperforms existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v1 [eess.IV])","link":"http://arxiv.org/abs/2205.12902","description":"<p>Glaucoma is one of the most severe eye diseases, characterized by rapid\nprogression and leading to irreversible blindness. It is often the case that\npathology diagnostics is carried out when the one's sight has already\nsignificantly degraded due to the lack of noticeable symptoms at early stage of\nthe disease. Regular glaucoma screenings of the population shall improve\nearly-stage detection, however the desirable frequency of etymological checkups\nis often not feasible due to excessive load imposed by manual diagnostics on\nlimited number of specialists. Considering the basic methodology to detect\nglaucoma is to analyze fundus images for the \\textit{optic-disc-to-optic-cup\nratio}, Machine Learning domain can offer sophisticated tooling for image\nprocessing and classification. In our work, we propose an advanced image\npre-processing technique combined with an ensemble of deep classification\nnetworks. Our \\textit{Retinal Auto Detection (RADNet)} model has been\nsuccessfully tested on Rotterdam EyePACS AIROGS train dataset with AUC of 0.92,\nand then additionally finetuned and tested on a fraction of RIM-ONE DL dataset\nwith AUC of 0.91.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Medvedev_D/0/1/0/all/0/1\">Dmitrii Medvedev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhtaseb_R/0/1/0/all/0/1\">Rand Muhtaseb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahrooqi_A/0/1/0/all/0/1\">Ahmed Al Mahrooqi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Video Reconstruction for Rolling Shutter Cameras. (arXiv:2205.12912v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12912","description":"<p>With the ubiquity of rolling shutter (RS) cameras, it is becoming\nincreasingly attractive to recover the latent global shutter (GS) video from\ntwo consecutive RS frames, which also places a higher demand on realism.\nExisting solutions, using deep neural networks or optimization, achieve\npromising performance. However, these methods generate intermediate GS frames\nthrough image warping based on the RS model, which inevitably result in black\nholes and noticeable motion artifacts. In this paper, we alleviate these issues\nby proposing a context-aware GS video reconstruction architecture. It\nfacilitates the advantages such as occlusion reasoning, motion compensation,\nand temporal abstraction. Specifically, we first estimate the bilateral motion\nfield so that the pixels of the two RS frames are warped to a common GS frame\naccordingly. Then, a refinement scheme is proposed to guide the GS frame\nsynthesis along with bilateral occlusion masks to produce high-fidelity GS\nvideo frames at arbitrary times. Furthermore, we derive an approximated\nbilateral motion field model, which can serve as an alternative to provide a\nsimple but effective GS frame initialization for related tasks. Experiments on\nsynthetic and real data show that our approach achieves superior performance\nover state-of-the-art methods in terms of objective metrics and subjective\nvisual quality. Code is available at \\url{https://github.com/GitCVfb/CVR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Low Memory Footprint Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth Maps. (arXiv:2205.12918v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12918","description":"<p>Sparse active illumination enables precise time-of-flight depth sensing as it\nmaximizes signal-to-noise ratio for low power budgets. However, depth\ncompletion is required to produce dense depth maps for 3D perception. We\naddress this task with realistic illumination and sensor resolution constraints\nby simulating ToF datasets for indoor 3D perception with challenging sparsity\nlevels. We propose a quantized convolutional encoder-decoder network for this\ntask. Our model achieves optimal depth map quality by means of input\npre-processing and carefully tuned training with a geometry-preserving loss\nfunction. We also achieve low memory footprint for weights and activations by\nmeans of mixed precision quantization-at-training techniques. The resulting\nquantized models are comparable to the state of the art in terms of quality,\nbut they require very low GPU times and achieve up to 14-fold memory size\nreduction for the weights w.r.t. their floating point counterpart with minimal\nimpact on quality metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaowen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambareri_V/0/1/0/all/0/1\">Valerio Cambareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agresti_G/0/1/0/all/0/1\">Gianluca Agresti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugwu_C/0/1/0/all/0/1\">Cynthia Ifeyinwa Ugwu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonetto_A/0/1/0/all/0/1\">Adriano Simonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1\">Fabien Cardinaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DH-GAN: A Physics-driven Untrained Generative Adversarial Network for 3D Microscopic Imaging using Digital Holography. (arXiv:2205.12920v1 [cs.IR])","link":"http://arxiv.org/abs/2205.12920","description":"<p>Digital holography is a 3D imaging technique by emitting a laser beam with a\nplane wavefront to an object and measuring the intensity of the diffracted\nwaveform, called holograms. The object's 3D shape can be obtained by numerical\nanalysis of the captured holograms and recovering the incurred phase. Recently,\ndeep learning (DL) methods have been used for more accurate holographic\nprocessing. However, most supervised methods require large datasets to train\nthe model, which is rarely available in most DH applications due to the\nscarcity of samples or privacy concerns. A few one-shot DL-based recovery\nmethods exist with no reliance on large datasets of paired images. Still, most\nof these methods often neglect the underlying physics law that governs wave\npropagation. These methods offer a black-box operation, which is not\nexplainable, generalizable, and transferrable to other samples and\napplications. In this work, we propose a new DL architecture based on\ngenerative adversarial networks that uses a discriminative network for\nrealizing a semantic measure for reconstruction quality while using a\ngenerative network as a function approximator to model the inverse of hologram\nformation. We impose smoothness on the background part of the recovered image\nusing a progressive masking module powered by simulated annealing to enhance\nthe reconstruction quality. The proposed method is one of its kind that\nexhibits high transferability to similar samples, which facilitates its fast\ndeployment in time-sensitive applications without the need for retraining the\nnetwork. The results show a considerable improvement to competitor methods in\nreconstruction quality (about 5 dB PSNR gain) and robustness to noise (about\n50% reduction in PSNR vs noise increase rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1\">Abofazl Razi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozicki_M/0/1/0/all/0/1\">Michael Kozicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1\">Christopher Mann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Object Detection using SE Adaptors and Center Loss. (arXiv:2205.12923v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12923","description":"<p>Despite growing interest in object detection, very few works address the\nextremely practical problem of cross-domain robustness especially for\nautomative applications. In order to prevent drops in performance due to domain\nshift, we introduce an unsupervised domain adaptation method built on the\nfoundation of faster-RCNN with two domain adaptation components addressing the\nshift at the instance and image levels respectively and apply a consistency\nregularization between them. We also introduce a family of adaptation layers\nthat leverage the squeeze excitation mechanism called SE Adaptors to improve\ndomain attention and thus improves performance without any prior requirement of\nknowledge of the new target domain. Finally, we incorporate a center loss in\nthe instance and image level representations to improve the intra-class\nvariance. We report all results with Cityscapes as our source domain and Foggy\nCityscapes as the target domain outperforming previous baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagesh_S/0/1/0/all/0/1\">Sushruth Nagesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_S/0/1/0/all/0/1\">Shreyas Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_A/0/1/0/all/0/1\">Asfiya Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Savitha Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining is All You Need for Image-to-Image Translation. (arXiv:2205.12952v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12952","description":"<p>We propose to use pretraining to boost general image-to-image translation.\nPrior image-to-image translation methods usually need dedicated architectural\ndesign and train individual translation models from scratch, struggling for\nhigh-quality generation of complex scenes, especially when paired training data\nare not abundant. In this paper, we regard each image-to-image translation\nproblem as a downstream task and introduce a simple and generic framework that\nadapts a pretrained diffusion model to accommodate various kinds of\nimage-to-image translation. We also propose adversarial training to enhance the\ntexture synthesis in the diffusion model training, in conjunction with\nnormalized guidance sampling to improve the generation quality. We present\nextensive empirical comparison across various tasks on challenging benchmarks\nsuch as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based\nimage-to-image translation (PITI) is capable of synthesizing images of\nunprecedented realism and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural 3D Reconstruction in the Wild. (arXiv:2205.12955v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12955","description":"<p>We are witnessing an explosion of neural implicit representations in computer\nvision and graphics. Their applicability has recently expanded beyond tasks\nsuch as shape generation and image-based rendering to the fundamental problem\nof image-based 3D reconstruction. However, existing methods typically assume\nconstrained 3D environments with constant illumination captured by a small set\nof roughly uniformly distributed cameras. We introduce a new method that\nenables efficient and accurate surface reconstruction from Internet photo\ncollections in the presence of varying illumination. To achieve this, we\npropose a hybrid voxel- and surface-guided sampling technique that allows for\nmore efficient ray sampling around surfaces and leads to significant\nimprovements in reconstruction quality. Further, we present a new benchmark and\nprotocol for evaluating reconstruction performance on such in-the-wild scenes.\nWe perform extensive experiments, demonstrating that our approach surpasses\nboth classical and neural reconstruction methods on a wide variety of metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inception Transformer. (arXiv:2205.12956v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12956","description":"<p>Recent studies show that Transformer has strong capability of building\nlong-range dependencies, yet is incompetent in capturing high frequencies that\npredominantly convey local information. To tackle this issue, we present a\nnovel and general-purpose Inception Transformer, or iFormer for short, that\neffectively learns comprehensive features with both high- and low-frequency\ninformation in visual data. Specifically, we design an Inception mixer to\nexplicitly graft the advantages of convolution and max-pooling for capturing\nthe high-frequency information to Transformers. Different from recent hybrid\nframeworks, the Inception mixer brings greater efficiency through a channel\nsplitting mechanism to adopt parallel convolution/max-pooling path and\nself-attention path as high- and low-frequency mixers, while having the\nflexibility to model discriminative information scattered within a wide\nfrequency range. Considering that bottom layers play more roles in capturing\nhigh-frequency details while top layers more in modeling low-frequency global\ninformation, we further introduce a frequency ramp structure, i.e. gradually\ndecreasing the dimensions fed to the high-frequency mixer and increasing those\nto the low-frequency mixer, which can effectively trade-off high- and\nlow-frequency components across different layers. We benchmark the iFormer on a\nseries of vision tasks, and showcase that it achieves impressive performance on\nimage classification, COCO detection and ADE20K segmentation. For example, our\niFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than\nDeiT-S by 3.6%, and even slightly better than much bigger model Swin-B (83.3%)\nwith only 1/4 parameters and 1/3 FLOPs. Code and models will be released at\nhttps://github.com/sail-sg/iFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation Function for Neural Networks. (arXiv:1901.05894v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1901.05894","description":"<p>The activation function in neural network is one of the important aspects\nwhich facilitates the deep training by introducing the non-linearity into the\nlearning process. However, because of zero-hard rectification, some of the\nexisting activation functions such as ReLU and Swish miss to utilize the large\nnegative input values and may suffer from the dying gradient problem. Thus, it\nis important to look for a better activation function which is free from such\nproblems. As a remedy, this paper proposes a new non-parametric function,\ncalled Linearly Scaled Hyperbolic Tangent (LiSHT) for Neural Networks (NNs).\nThe proposed LiSHT activation function is an attempt to scale the non-linear\nHyperbolic Tangent (Tanh) function by a linear function and tackle the dying\ngradient problem. The training and classification experiments are performed\nover benchmark Iris, MNIST, CIFAR10, CIFAR100 and twitter140 datasets to show\nthat the proposed activation achieves faster convergence and higher\nperformance. A very promising performance improvement is observed on three\ndifferent type of neural networks including Multi-layer Perceptron (MLP),\nConvolutional Neural Network (CNN) and Recurrent neural network like Long-short\nterm memory (LSTM). The advantages of proposed activation function are also\nvisualized in terms of the feature activation maps, weight distribution and\nloss landscape. The code is available at https://github.com/swalpa/lisht.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1\">Suvojit Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1\">Bidyut Baran Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.06255","description":"<p>Aerial tracking, which has exhibited its omnipresent dedication and splendid\nperformance, is one of the most active applications in the remote sensing\nfield. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,\nequipped with a visual tracking approach, has been widely used in aviation,\nnavigation, agriculture,transportation, and public security, etc. As is\nmentioned above, the UAV-based aerial tracking platform has been gradually\ndeveloped from research to practical application stage, reaching one of the\nmain aerial remote sensing technologies in the future. However, due to the\nreal-world onerous situations, e.g., harsh external challenges, the vibration\nof the UAV mechanical structure (especially under strong wind conditions), the\nmaneuvering flight in complex environment, and the limited computation\nresources onboard, accuracy, robustness, and high efficiency are all crucial\nfor the onboard tracking methods. Recently, the discriminative correlation\nfilter (DCF)-based trackers have stood out for their high computational\nefficiency and appealing robustness on a single CPU, and have flourished in the\nUAV visual tracking community. In this work, the basic framework of the\nDCF-based trackers is firstly generalized, based on which, 23 state-of-the-art\nDCF-based trackers are orderly summarized according to their innovations for\nsolving various issues. Besides, exhaustive and quantitative experiments have\nbeen extended on various prevailing UAV tracking benchmarks, i.e., UAV123,\nUAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903\nframes in total. The experiments show the performance, verify the feasibility,\nand demonstrate the current challenges of DCF-based trackers onboard UAV\ntracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fuling Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Geng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RELLIS-3D Dataset: Data, Benchmarks and Analysis. (arXiv:2011.12954v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12954","description":"<p>Semantic scene understanding is crucial for robust and safe autonomous\nnavigation, particularly so in off-road environments. Recent deep learning\nadvances for 3D semantic segmentation rely heavily on large sets of training\ndata, however existing autonomy datasets either represent urban environments or\nlack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal\ndataset collected in an off-road environment, which contains annotations for\n13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis\nCampus of Texas A\\&amp;M University and presents challenges to existing algorithms\nrelated to class imbalance and environmental topography. Additionally, we\nevaluate the current state-of-the-art deep learning semantic segmentation\nmodels on this dataset. Experimental results show that RELLIS-3D presents\nchallenges for algorithms designed for segmentation in urban environments. This\nnovel dataset provides the resources needed by researchers to continue to\ndevelop more advanced algorithms and investigate new research directions to\nenhance autonomous navigation in off-road environments. RELLIS-3D is available\nat https://github.com/unmannedlab/RELLIS-3D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osteen_P/0/1/0/all/0/1\">Philip Osteen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wigness_M/0/1/0/all/0/1\">Maggie Wigness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1\">Srikanth Saripalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Object Detection for Autonomous Driving: A Survey. (arXiv:2106.10823v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10823","description":"<p>Autonomous driving is regarded as one of the most promising remedies to\nshield human beings from severe crashes. To this end, 3D object detection\nserves as the core basis of perception stack especially for the sake of path\nplanning, motion prediction, and collision avoidance etc. Taking a quick glance\nat the progress we have made, we attribute challenges to visual appearance\nrecovery in the absence of depth information from images, representation\nlearning from partially occluded unstructured point clouds, and semantic\nalignments over heterogeneous features from cross modalities. Despite existing\nefforts, 3D object detection for autonomous driving is still in its infancy.\nRecently, a large body of literature have been investigated to address this 3D\nvision task. Nevertheless, few investigations have looked into collecting and\nstructuring this growing knowledge. We therefore aim to fill this gap in a\ncomprehensive survey, encompassing all the main concerns including sensors,\ndatasets, performance metrics and the recent state-of-the-art detection\nmethods, together with their pros and cons. Furthermore, we provide\nquantitative comparisons with the state of the art. A case study on fifteen\nselected representative methods is presented, involved with runtime analysis,\nerror analysis, and robustness analysis. Finally, we provide concluding remarks\nafter an in-depth analysis of the surveyed works and identify promising\ndirections for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03375","description":"<p>Temporal Action Localization (TAL) task which is to predict the start and end\nof each action in a video along with the class label of the action has numerous\napplications in the real world. But due to the complexity of this task,\nacceptable accuracy rates have not been achieved yet, whereas this is not the\ncase regarding the action recognition task. In this paper, we propose a new\nnetwork based on Gated Recurrent Unit (GRU) and two novel post-processing\nmethods for TAL task. Specifically, we propose a new design for the output\nlayer of the conventionally GRU resulting in the so-called GRU-Split network.\nMoreover, linear interpolation is used to generate the action proposals with\nprecise start and end times. Finally, to rank the generated proposals\nappropriately, we use a Learn to Rank (LTR) approach. We evaluated the\nperformance of the proposed method on Thumos14 and ActivityNet-1.3 datasets.\nResults show the superiority of the performance of the proposed method compared\nto state-of-the-art. Specifically in the mean Average Precision (mAP) metric at\nIntersection over Union (IoU) of 0.7 on Thumos14, we get 27.52% accuracy which\nis 5.12% better than that of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keshvarikhojasteh_H/0/1/0/all/0/1\">Hassan Keshvarikhojasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1\">Hoda Mohammadzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1\">Hamid Behroozi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Foveated Reconstruction to Preserve Perceived Image Statistics. (arXiv:2108.03499v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2108.03499","description":"<p>Foveated image reconstruction recovers full image from a sparse set of\nsamples distributed according to the human visual system's retinal sensitivity\nthat rapidly drops with eccentricity. Recently, the use of Generative\nAdversarial Networks was shown to be a promising solution for such a task as\nthey can successfully hallucinate missing image information. Like for other\nsupervised learning approaches, also for this one, the definition of the loss\nfunction and training strategy heavily influences the output quality. In this\nwork, we pose the question of how to efficiently guide the training of foveated\nreconstruction techniques such that they are fully aware of the human visual\nsystem's capabilities and limitations, and therefore, reconstruct visually\nimportant image features. Our primary goal is to make training procedure less\nsensitive to the distortions that humans cannot detect and focus on penalizing\nperceptually important artifacts. Due to the nature of GAN-based solutions, we\nconcentrate on humans' sensitivity to hallucination for different input sample\ndensities. We present new psychophysical experiments, a dataset, and a\nprocedure for training foveated image reconstruction. The strategy provides\nflexibility to the generator network by penalizing only perceptually important\ndeviations in the output. As a result, the method aims to preserve perceived\nimage statistics rather than natural image statistics. We evaluate our strategy\nand compare it to alternative solutions using a newly trained objective metric,\na recent foveated video quality metric, and user experiments. Our evaluations\nshow significant improvements in perceived image reconstruction quality\ncompared to standard GAN training approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Surace_L/0/1/0/all/0/1\">Luca Surace</a> (Universit&#xe0; della Svizzera italiana), <a href=\"http://arxiv.org/find/cs/1/au:+Wernikowski_M/0/1/0/all/0/1\">Marek Wernikowski</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_C/0/1/0/all/0/1\">Cara Tursun</a> (Universit&#xe0; della Svizzera italiana and University of Groningen), <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a> (Max Planck Institute for Informatics), <a href=\"http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rados&#x142;aw Mantiuk</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a> (Universit&#xe0; della Svizzera italiana)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVT: Point-Voxel Transformer for Point Cloud Learning. (arXiv:2108.06076v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06076","description":"<p>The recently developed pure Transformer architectures have attained promising\naccuracy on point cloud learning benchmarks compared to convolutional neural\nnetworks. However, existing point cloud Transformers are computationally\nexpensive since they waste a significant amount of time on structuring the\nirregular data. To solve this shortcoming, we present Sparse Window Attention\n(SWA) module to gather coarse-grained local features from non-empty voxels,\nwhich not only bypasses the expensive irregular data structuring and invalid\nempty voxel computation, but also obtains linear computational complexity with\nrespect to voxel resolution. Meanwhile, to gather fine-grained features about\nthe global shape, we introduce relative attention (RA) module, a more robust\nself-attention variant for rigid transformations of objects. Equipped with the\nSWA and RA, we construct our neural architecture called PVT that integrates\nboth modules into a joint framework for point cloud learning. Compared with\nprevious Transformer-based and attention-based models, our method attains top\naccuracy of 94.0% on classification benchmark and 10x inference speedup on\naverage. Extensive experiments also valid the effectiveness of PVT on part and\nsemantic segmentation benchmarks (86.6% and 69.2% mIoU, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning JPEG Compression Artifacts for Image Manipulation Detection and Localization. (arXiv:2108.12947v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.12947","description":"<p>Detecting and localizing image manipulation are necessary to counter\nmalicious use of image editing techniques. Accordingly, it is essential to\ndistinguish between authentic and tampered regions by analyzing intrinsic\nstatistics in an image. We focus on JPEG compression artifacts left during\nimage acquisition and editing. We propose a convolutional neural network (CNN)\nthat uses discrete cosine transform (DCT) coefficients, where compression\nartifacts remain, to localize image manipulation. Standard CNNs cannot learn\nthe distribution of DCT coefficients because the convolution throws away the\nspatial coordinates, which are essential for DCT coefficients. We illustrate\nhow to design and train a neural network that can learn the distribution of DCT\ncoefficients. Furthermore, we introduce Compression Artifact Tracing Network\n(CAT-Net) that jointly uses image acquisition artifacts and compression\nartifacts. It significantly outperforms traditional and deep neural\nnetwork-based methods in detecting and localizing tampered regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kwon_M/0/1/0/all/0/1\">Myung-Joon Kwon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_S/0/1/0/all/0/1\">Seung-Hun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_I/0/1/0/all/0/1\">In-Jae Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Heung-Kyu Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01174","description":"<p>Image reconstruction for positron emission tomography (PET) is challenging\nbecause of the ill-conditioned tomographic problem and low counting statistics.\nKernel methods address this challenge by using kernel representation to\nincorporate image prior information in the forward model of iterative PET image\nreconstruction. Existing kernel methods construct the kernels commonly using an\nempirical process, which may lead to unsatisfactory performance. In this paper,\nwe describe the equivalence between the kernel representation and a trainable\nneural network model. A deep kernel method is then proposed by exploiting a\ndeep neural network to enable automated learning of an improved kernel model\nand is directly applicable to single subjects in dynamic PET. The training\nprocess utilizes available image prior data to form a set of robust kernels in\nan optimized way rather than empirically. The results from computer simulations\nand a real patient dataset demonstrate that the proposed deep kernel method can\noutperform the existing kernel method and neural network method for dynamic PET\nimage reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guobao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do we still need ImageNet pre-training in remote sensing scene classification?. (arXiv:2111.03690v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03690","description":"<p>Due to the scarcity of labeled data, using supervised models pre-trained on\nImageNet is a de facto standard in remote sensing scene classification.\nRecently, the availability of larger high resolution remote sensing (HRRS)\nimage datasets and progress in self-supervised learning have brought up the\nquestions of whether supervised ImageNet pre-training is still necessary for\nremote sensing scene classification and would supervised pre-training on HRRS\nimage datasets or self-supervised pre-training on ImageNet achieve better\nresults on target remote sensing scene classification tasks. To answer these\nquestions, in this paper we both train models from scratch and fine-tune\nsupervised and self-supervised ImageNet models on several HRRS image datasets.\nWe also evaluate the transferability of learned representations to HRRS scene\nclassification tasks and show that self-supervised pre-training outperforms the\nsupervised one, while the performance of HRRS pre-training is similar to\nself-supervised pre-training or slightly lower. Finally, we propose using an\nImageNet pre-trained model combined with a second round of pre-training using\nin-domain HRRS images, i.e. domain-adaptive pre-training. The experimental\nresults show that domain-adaptive pre-training results in models that achieve\nstate-of-the-art results on HRRS scene classification benchmarks. The source\ncode and pre-trained models are available at\n\\url{https://github.com/risojevicv/RSSC-transfer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risojevic_V/0/1/0/all/0/1\">Vladimir Risojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojnic_V/0/1/0/all/0/1\">Vladan Stojni&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Images Meet Crowdsourced Trajectories: A New Approach to Robust Road Extraction. (arXiv:2111.15119v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15119","description":"<p>Land remote sensing analysis is a crucial research in earth science. In this\nwork, we focus on a challenging task of land analysis, i.e., automatic\nextraction of traffic roads from remote sensing data, which has widespread\napplications in urban development and expansion estimation. Nevertheless,\nconventional methods either only utilized the limited information of aerial\nimages, or simply fused multimodal information (e.g., vehicle trajectories),\nthus cannot well recognize unconstrained roads. To facilitate this problem, we\nintroduce a novel neural network framework termed Cross-Modal Message\nPropagation Network (CMMPNet), which fully benefits the complementary different\nmodal data (i.e., aerial images and crowdsourced trajectories). Specifically,\nCMMPNet is composed of two deep Auto-Encoders for modality-specific\nrepresentation learning and a tailor-designed Dual Enhancement Module for\ncross-modal representation refinement. In particular, the complementary\ninformation of each modality is comprehensively extracted and dynamically\npropagated to enhance the representation of another modality. Extensive\nexperiments on three real-world benchmarks demonstrate the effectiveness of our\nCMMPNet for robust road extraction benefiting from blending different modal\ndata, either using image and trajectory data or image and Lidar data. From the\nexperimental results, we observe that the proposed approach outperforms current\nstate-of-the-art methods by large margins.Our source code is resealed on the\nproject page <a href=\"http://lingboliu.com/multimodal_road_extraction.html.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zewei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Resolution Decrement: Utilizing the Shape Consistency for Higher Computational Efficiency. (arXiv:2112.00954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00954","description":"<p>Image resolution that has close relations with accuracy and computational\ncost plays a pivotal role in network training. In this paper, we observe that\nthe reduced image retains relatively complete shape semantics but loses\nextensive texture information. Inspired by the consistency of the shape\nsemantics as well as the fragility of the texture information, we propose a\nnovel training strategy named Temporally Resolution Decrement. Wherein, we\nrandomly reduce the training images to a smaller resolution in the time domain.\nDuring the alternate training with the reduced images and the original images,\nthe unstable texture information in the images results in a weaker correlation\nbetween the texture-related patterns and the correct label, naturally enforcing\nthe model to rely more on shape properties that are robust and conform to the\nhuman decision rule. Surprisingly, our approach greatly improves both the\ntraining and inference efficiency of convolutional neural networks. On ImageNet\nclassification, using only 33\\% calculation quantity (randomly reducing the\ntraining image to 112$\\times$112 within 90\\% epochs) can still improve\nResNet-50 from 76.32\\% to 77.71\\%. Superimposed with the strong training\nprocedure of ResNet-50 on ImageNet, our method achieves 80.42\\% top-1 accuracy\nwith saving 37.5\\% calculation overhead. To the best of our knowledge this is\nthe highest ImageNet single-crop accuracy on ResNet-50 under 224$\\times$224\nwithout extra data or distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the usefulness of Quantum Blur. (arXiv:2112.01646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01646","description":"<p>Though some years remain before quantum computation can outperform\nconventional computation, it already provides resources that can be used for\nexploratory purposes in various fields. This includes certain tasks for\nprocedural generation in computer games, music and art. The so-called `Quantum\nBlur' method represents the first step on this journey, providing a simple\nproof-of-principle example of how quantum software can be useful in these areas\ntoday. Here we analyse the `Quantum Blur' method and compare it to conventional\nblur effects. This investigation was guided by discussions with the most\nprominent user of the method, to determine which features were found most\nuseful. In particular we determine how these features depend on the quantum\nphenomena of superposition and entanglement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wootton_J/0/1/0/all/0/1\">James R. Wootton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfaffhauser_M/0/1/0/all/0/1\">Marcel Pfaffhauser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), e.g. deep regression forests and deep\ndecision forests, have been extensively studied recently to solve problems such\nas facial age estimation, head pose estimation, etc.. Due to a shortage of\nwell-labeled data that does not have noise and imbalanced distribution\nproblems, learning DDMs is always challenging. Existing methods usually tackle\nthese challenges through learning more discriminative features or re-weighting\nsamples. We argue that learning DDMs gradually, from easy to hard, is more\nreasonable, for two reasons. First, this is more consistent with the cognitive\nprocess of human beings. Second, noisy as well as underrepresented examples can\nbe distinguished by virtue of previously learned knowledge. Thus, we resort to\na gradual learning strategy -- self-paced learning (SPL). Then, a natural\nquestion arises: can SPL lead DDMs to achieve more robust and less biased\nsolutions? To answer this question, this paper proposes a new SPL method: easy\nand underrepresented examples first, for learning DDMs. This tackles the\nfundamental ranking and selection problem in SPL from a new perspective:\nfairness. Our idea is fundamental and can be easily combined with a variety of\nDDMs. Extensive experimental results on three computer vision tasks, i.e.,\nfacial age estimation, head pose estimation, and gaze estimation, show our new\nmethod gains considerable performance improvement in both accuracy and\nfairness. Source code is available at https://github.com/learninginvision/SPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Proposals Refinement for 3D Object Detection. (arXiv:2201.07070v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07070","description":"<p>Recent advances in 3D object detection are made by developing the refinement\nstage for voxel-based Region Proposal Networks (RPN) to better strike the\nbalance between accuracy and efficiency. A popular approach among\nstate-of-the-art frameworks is to divide proposals, or Regions of Interest\n(ROI), into grids and extract features for each grid location before\nsynthesizing them to form ROI features. While achieving impressive\nperformances, such an approach involves several hand-crafted components (e.g.\ngrid sampling, set abstraction) which requires expert knowledge to be tuned\ncorrectly. This paper proposes a data-driven approach to ROI feature computing\nnamed APRO3D-Net which consists of a voxel-based RPN and a refinement stage\nmade of Vector Attention. Unlike the original multi-head attention, Vector\nAttention assigns different weights to different channels within a point\nfeature, thus being able to capture a more sophisticated relation between\npooled points and ROI. Our method achieves a competitive performance of 84.85\nAP for class Car at moderate difficulty on the validation set of KITTI and\n47.03 mAP (average over 10 classes) on NuScenes while having the least\nparameters compared to closely related methods and attaining an inference speed\nat 15 FPS on NVIDIA V100 GPU. The code is released at\nhttps://github.com/quan-dao/APRO3D-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1\">Minh-Quan Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hery_E/0/1/0/all/0/1\">Elwan H&#xe9;ry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fremont_V/0/1/0/all/0/1\">Vincent Fr&#xe9;mont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Scene Flow Estimation with 4D Automotive Radar. (arXiv:2203.01137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01137","description":"<p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of\nmultiple independent objects which is the key to long-term mobile autonomy.\nWhile estimating the scene flow from LiDAR has progressed recently, it remains\nlargely unknown how to estimate the scene flow from a 4D radar - an\nincreasingly popular automotive sensor for its robustness against adverse\nweather and lighting conditions. Compared with the LiDAR point clouds, radar\ndata are drastically sparser, noisier and in much lower resolution. Annotated\ndatasets for radar scene flow are also in absence and costly to acquire in the\nreal world. These factors jointly pose the radar scene flow estimation as a\nchallenging problem. This work aims to address the above challenges and\nestimate scene flow from 4D radar point clouds by leveraging self-supervised\nlearning. A robust scene flow estimation architecture and three novel losses\nare bespoken designed to cope with intractable radar data. Real-world\nexperimental results validate that our method is able to robustly estimate the\nradar scene flow in the wild and effectively supports the downstream task of\nmotion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhijun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianning Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tree-Structured Multi-Task Model Recommender. (arXiv:2203.05092v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05092","description":"<p>Tree-structured multi-task architectures have been employed to jointly tackle\nmultiple vision tasks in the context of multi-task learning (MTL). The major\nchallenge is to determine where to branch out for each task given a backbone\nmodel to optimize for both task accuracy and computation efficiency. To address\nthe challenge, this paper proposes a recommender that, given a set of tasks and\na convolutional neural network-based backbone model, automatically suggests\ntree-structured multi-task architectures that could achieve a high task\nperformance while meeting a user-specified computation budget without\nperforming model training. Extensive evaluations on popular MTL benchmarks show\nthat the recommended architectures could achieve competitive task accuracy and\ncomputation efficiency compared with state-of-the-art MTL methods. Our\ntree-structured multi-task model recommender is open-sourced and available at\nhttps://github.com/zhanglijun95/TreeMTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Hui Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2LIVE: Text-Driven Layered Image and Video Editing. (arXiv:2204.02491v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02491","description":"<p>We present a method for zero-shot, text-driven appearance manipulation in\nnatural images and videos. Given an input image or video and a target text\nprompt, our goal is to edit the appearance of existing objects (e.g., object's\ntexture) or augment the scene with visual effects (e.g., smoke, fire) in a\nsemantically meaningful manner. We train a generator using an internal dataset\nof training examples, extracted from a single input (image or video and target\ntext prompt), while leveraging an external pre-trained CLIP model to establish\nour losses. Rather than directly generating the edited output, our key idea is\nto generate an edit layer (color+opacity) that is composited over the original\ninput. This allows us to constrain the generation process and maintain high\nfidelity to the original input via novel text-driven losses that are applied\ndirectly to the edit layer. Our method neither relies on a pre-trained\ngenerator nor requires user-provided edit masks. We demonstrate localized,\nsemantic edits on high-resolution natural images and videos across a variety of\nobjects and scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1\">Omer Bar-Tal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofri_Amar_D/0/1/0/all/0/1\">Dolev Ofri-Amar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fridman_R/0/1/0/all/0/1\">Rafail Fridman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.06929","description":"<p>Ultrasound (US) imaging is widely used for anatomical structure inspection in\nclinical diagnosis. The training of new sonographers and deep learning based\nalgorithms for US image analysis usually requires a large amount of data.\nHowever, obtaining and labeling large-scale US imaging data are not easy tasks,\nespecially for diseases with low incidence. Realistic US image synthesis can\nalleviate this problem to a great extent. In this paper, we propose a\ngenerative adversarial network (GAN) based image synthesis framework. Our main\ncontributions include: 1) we present the first work that can synthesize\nrealistic B-mode US images with high-resolution and customized texture editing\nfeatures; 2) to enhance structural details of generated images, we propose to\nintroduce auxiliary sketch guidance into a conditional GAN. We superpose the\nedge sketch onto the object mask and use the composite mask as the network\ninput; 3) to generate high-resolution US images, we adopt a progressive\ntraining strategy to gradually generate high-resolution images from\nlow-resolution images. In addition, a feature loss is proposed to minimize the\ndifference of high-level features between the generated and real images, which\nfurther improves the quality of generated images; 4) the proposed US image\nsynthesis method is quite universal and can also be generalized to the US\nimages of other anatomical structures besides the three ones tested in our\nstudy (lung, hip joint, and ovary); 5) extensive experiments on three large US\nimage datasets are conducted to validate our method. Ablation studies,\ncustomized texture editing, user studies, and segmentation tests demonstrate\npromising results of our method in synthesizing realistic US images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jiamin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zejian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Image Captioning. (arXiv:2204.13324v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13324","description":"<p>State-of-the-art image captioners can generate accurate sentences to describe\nimages in a sequence to sequence manner without considering the controllability\nand interpretability. This, however, is far from making image captioning widely\nused as an image can be interpreted in infinite ways depending on the target\nand the context at hand. Achieving controllability is important especially when\nthe image captioner is used by different people with different way of\ninterpreting the images. In this paper, we introduce a novel framework for\nimage captioning which can generate diverse descriptions by capturing the\nco-dependence between Part-Of-Speech tags and semantics. Our model decouples\ndirect dependence between successive variables. In this way, it allows the\ndecoder to exhaustively search through the latent Part-Of-Speech choices, while\nkeeping decoding speed proportional to the size of the POS vocabulary. Given a\ncontrol signal in the form of a sequence of Part-Of-Speech tags, we propose a\nmethod to generate captions through a Transformer network, which predicts words\nbased on the input Part-Of-Speech tag sequences. Experiments on publicly\navailable datasets show that our model significantly outperforms\nstate-of-the-art methods on generating diverse image captions with high\nqualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_L/0/1/0/all/0/1\">Luka Maxwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupling Deep Imputation with Multitask Learning for Downstream Tasks on Genomics Data. (arXiv:2204.13705v2 [q-bio.GN] UPDATED)","link":"http://arxiv.org/abs/2204.13705","description":"<p>Genomics data such as RNA gene expression, methylation and micro RNA\nexpression are valuable sources of information for various clinical predictive\ntasks. For example, predicting survival outcomes, cancer histology type and\nother patients' related information is possible using not only clinical data\nbut molecular data as well. Moreover, using these data sources together, for\nexample in multitask learning, can boost the performance. However, in practice,\nthere are many missing data points which leads to significantly lower patient\nnumbers when analysing full cases, which in our setting refers to all\nmodalities being present.\n</p>\n<p>In this paper we investigate how imputing data with missing values using deep\nlearning coupled with multitask learning can help to reach state-of-the-art\nperformance results using combined genomics modalities, RNA, micro RNA and\nmethylation. We propose a generalised deep imputation method to impute values\nwhere a patient has all modalities present except one. Interestingly enough,\ndeep imputation alone outperforms multitask learning alone for the\nclassification and regression tasks across most combinations of modalities. In\ncontrast, when using all modalities for survival prediction we observe that\nmultitask learning alone outperforms deep imputation alone with statistical\nsignificance (adjusted p-value 0.03). Thus, both approaches are complementary\nwhen optimising performance for downstream predictive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Peacock_S/0/1/0/all/0/1\">Sophie Peacock</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jacob_E/0/1/0/all/0/1\">Etai Jacob</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Burlutskiy_N/0/1/0/all/0/1\">Nikolay Burlutskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation with Simplified Transformer. (arXiv:2204.13791v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13791","description":"<p>Transformer and its variants have shown state-of-the-art results in many\nvision tasks recently, ranging from image classification to dense prediction.\nDespite of their success, limited work has been reported on improving the model\nefficiency for deployment in latency-critical applications, such as autonomous\ndriving and robotic navigation. In this paper, we aim at improving upon the\nexisting transformers in vision, and propose a method for self-supervised\nmonocular Depth Estimation with Simplified Transformer (DEST), which is\nefficient and particularly suitable for deployment on GPU-based platforms.\nThrough strategic design choices, our model leads to significant reduction in\nmodel size, complexity, as well as inference latency, while achieving superior\naccuracy as compared to state-of-the-art. We also show that our design\ngeneralize well to other dense prediction task without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_A/0/1/0/all/0/1\">Anurag Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jinkyu Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Su Inn Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.00690","description":"<p>Noisy labels are inevitable yet problematic in machine learning society. It\nruins the generalization power of a classifier by making the classifier be\ntrained to be overfitted to wrong labels. Existing methods on noisy label have\nfocused on modifying classifier training procedure. It results in two possible\nproblems. First, these methods are not applicable to a pre-trained classifier\nwithout further access into training. Second, it is not easy to train a\nclassifier and remove all of negative effects from noisy labels simultaneously.\nFrom these problems, we suggests a new branch of approach, Noisy Prediction\nCalibration (NPC) in learning with noisy labels. Through the introduction and\nestimation of a new type of transition matrix via generative model, NPC\ncorrects the noisy prediction from the pre-trained classifier to the true label\nas a post-processing scheme. We prove that NPC theoretically aligns with the\ntransition matrix based methods. Yet, NPC provides more accurate pathway to\nestimate true label, even without involvement in classifier learning. Also, NPC\nis applicable to any classifier trained with noisy label methods, if training\ninstances and its predictions are available. Our method, NPC, boosts the\nclassification performances of all baseline models on both synthetic and\nreal-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1\">HeeSun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungjae Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byeonghu Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">JoonHo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1\">Il-Chul Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning. (arXiv:2205.07246v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.07246","description":"<p>Pseudo labeling and consistency regularization approaches based on confidence\nthresholding have made great progress in semi-supervised learning (SSL).\nHowever, we argue that existing methods might fail to adopt suitable thresholds\nsince they either use a pre-defined / fixed threshold or an ad-hoc threshold\nadjusting scheme, resulting in inferior performance and slow convergence. We\nfirst analyze a motivating example to achieve some intuitions on the\nrelationship between the desirable threshold and model's learning status. Based\non the analysis, we hence propose FreeMatch to define and adjust the confidence\nthreshold in a self-adaptive manner according to the model's learning status.\nWe further introduce a self-adaptive class fairness regularization penalty that\nencourages the model to produce diverse predictions during the early stages of\ntraining. Extensive experimental results indicate the superiority of FreeMatch\nespecially when the labeled data are extremely rare. FreeMatch achieves 5.78%,\n13.59%, and 1.28% error rate reduction over the latest state-of-the-art method\nFlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class,\nand ImageNet with 100 labels per class, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_Q/0/1/0/all/0/1\">Qiang Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network. (arXiv:2205.09612v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.09612","description":"<p>In this paper, we propose a Classification Confidence Network (CLCNet) that\ncan determine whether the classification model classifies input samples\ncorrectly. It can take a classification result in the form of vector in any\ndimension, and return a confidence score as output, which represents the\nprobability of an instance being classified correctly. We can utilize CLCNet in\na simple cascade structure system consisting of several SOTA (state-of-the-art)\nclassification models, and our experiments show that the system can achieve the\nfollowing advantages: 1. The system can customize the average computation\nrequirement (FLOPs) per image while inference. 2. Under the same computation\nrequirement, the performance of the system can exceed any model that has\nidentical structure with the model in the system, but different in size. In\nfact, this is a new type of ensemble modeling. Like general ensemble modeling,\nit can achieve higher performance than single classification model, yet our\nsystem requires much less computation than general ensemble modeling. We have\nuploaded our code to a github repository:\nhttps://github.com/yaoching0/CLCNet-Rethinking-of-Ensemble-Modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao-Ching Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Shi-Jinn Horng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VNT-Net: Rotational Invariant Vector Neuron Transformers. (arXiv:2205.09690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09690","description":"<p>Learning 3D point sets with rotational invariance is an important and\nchallenging problem in machine learning. Through rotational invariant\narchitectures, 3D point cloud neural networks are relieved from requiring a\ncanonical global pose and from exhaustive data augmentation with all possible\nrotations. In this work, we introduce a rotational invariant neural network by\ncombining recently introduced vector neurons with self-attention layers to\nbuild a point cloud vector neuron transformer network (VNT-Net). Vector neurons\nare known for their simplicity and versatility in representing SO(3) actions\nand are thereby incorporated in common neural operations. Similarly,\nTransformer architectures have gained popularity and recently were shown\nsuccessful for images by applying directly on sequences of image patches and\nachieving superior performance and convergence. In order to benefit from both\nworlds, we combine the two structures by mainly showing how to adapt the\nmulti-headed attention layers to comply with vector neurons operations. Through\nthis adaptation attention layers become SO(3) and the overall network becomes\nrotational invariant. Experiments demonstrate that our network efficiently\nhandles 3D point cloud objects in arbitrary poses. We also show that our\nnetwork achieves higher accuracy when compared to related state-of-the-art\nmethods and requires less training due to a smaller number of hyperparameters\nin common classification and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zisling_H/0/1/0/all/0/1\">Hedi Zisling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharf_A/0/1/0/all/0/1\">Andrei Sharf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People Tracking and Re-Identifying in Distributed Contexts: Extension Study of PoseTReID. (arXiv:2205.10086v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10086","description":"<p>In our previous paper, we introduced PoseTReID which is a generic framework\nfor real-time 2D multi-person tracking in distributed interaction spaces where\nlong-term people's identities are important for other studies such as behavior\nanalysis, etc. In this paper, we introduce a further study of PoseTReID\nframework in order to give a more complete comprehension of the framework. We\nuse a well-known bounding box detector YOLO (v4) for the detection to compare\nto OpenPose which was used in our last paper, and we use SORT and DeepSORT to\ncompare to centroid which was also used previously, and most importantly for\nthe re-identification, we use a bunch of deep leaning methods such as MLFN,\nOSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet\nwhich was also used earlier in our last paper. By evaluating on our PoseTReID\ndatasets, even though those deep learning re-identification methods are\ndesigned for only short-term re-identification across multiple cameras or\nvideos, it is worth showing that they give impressive results which boost the\noverall tracking performance of PoseTReID framework regardless the type of\ntracking method. At the same time, we also introduce our research-friendly and\nopen source Python toolbox pyppbox, which is purely written in Python and\ncontains all sub-modules which are used in this study along with real-time\nonline and offline evaluations for our PoseTReID datasets. This pyppbox is\navailable on GitHub https://github.com/rathaumons/pyppbox .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siv_R/0/1/0/all/0/1\">Ratha Siv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valy_D/0/1/0/all/0/1\">Dona Valy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreng_S/0/1/0/all/0/1\">Sokchenda Sreng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems. (arXiv:2205.10937v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10937","description":"<p>Most uses of machine learning today involve training a model from scratch for\na particular task, or sometimes starting with a model pretrained on a related\ntask and then fine-tuning on a downstream task. Both approaches offer limited\nknowledge transfer between different tasks, time-consuming human-driven\ncustomization to individual tasks and high computational costs especially when\nstarting from randomly initialized models. We propose a method that uses the\nlayers of a pretrained deep neural network as building blocks to construct an\nML system that can jointly solve an arbitrary number of tasks. The resulting\nsystem can leverage cross tasks knowledge transfer, while being immune from\ncommon drawbacks of multitask approaches such as catastrophic forgetting,\ngradients interference and negative transfer. We define an evolutionary\napproach designed to jointly select the prior knowledge relevant for each task,\nchoose the subset of the model parameters to train and dynamically auto-tune\nits hyperparameters. Furthermore, a novel scale control method is employed to\nachieve quality/size trade-offs that outperform common fine-tuning techniques.\nCompared with standard fine-tuning on a benchmark of 10 diverse image\nclassification tasks, the proposed model improves the average accuracy by 2.39%\nwhile using 47% less parameters per task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesmundo_A/0/1/0/all/0/1\">Andrea Gesmundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPU-BOLT: A Dataset for Bolt Object Detection in Natural Scene Images. (arXiv:2205.11191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11191","description":"<p>Bolt joints are very common and important in engineering structures. Due to\nextreme service environment and load factors, bolts often get loose or even\ndisengaged. To real-time or timely detect the loosed or disengaged bolts is an\nurgent need in practical engineering, which is critical to keep structural\nsafety and service life. In recent years, many bolt loosening detection methods\nusing deep learning and machine learning techniques have been proposed and are\nattracting more and more attention. However, most of these studies use bolt\nimages captured in laboratory for deep leaning model training. The images are\nobtained in a well-controlled light, distance, and view angle conditions. Also,\nthe bolted structures are well designed experimental structures with brand new\nbolts and the bolts are exposed without any shelter nearby. It is noted that in\npractical engineering, the above well controlled lab conditions are not easy\nrealized and the real bolt images often have blur edges, oblique perspective,\npartial occlusion and indistinguishable colors etc., which make the trained\nmodels obtained in laboratory conditions loss their accuracy or fails.\nTherefore, the aim of this study is to develop a dataset named NPU-BOLT for\nbolt object detection in natural scene images and open it to researchers for\npublic use and further development. In the first version of the dataset, it\ncontains 337 samples of bolt joints images mainly in the natural environment,\nwith image data sizes ranging from 400*400 to 6000*4000, totaling approximately\n1275 bolt targets. The bolt targets are annotated into four categories named\nblur bolt, bolt head, bolt nut and bolt side. The dataset is tested with\nadvanced object detection models including yolov5, Faster-RCNN and CenterNet.\nThe effectiveness of the dataset is validated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yadian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Domain Adaptation with Multi-level Contrastive Units for Semantic Segmentation. (arXiv:2205.11192v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11192","description":"<p>To further reduce the cost of semi-supervised domain adaptation (SSDA)\nlabeling, a more effective way is to use active learning (AL) to annotate a\nselected subset with specific properties. However, domain adaptation tasks are\nalways addressed in two interactive aspects: domain transfer and the\nenhancement of discrimination, which requires the selected data to be both\nuncertain under the model and diverse in feature space. Contrary to active\nlearning in classification tasks, it is usually challenging to select pixels\nthat contain both the above properties in segmentation tasks, leading to the\ncomplex design of pixel selection strategy. To address such an issue, we\npropose a novel Active Domain Adaptation scheme with Multi-level Contrastive\nUnits (ADA-MCU) for semantic image segmentation. A simple pixel selection\nstrategy followed with the construction of multi-level contrastive units is\nintroduced to optimize the model for both domain adaptation and active\nsupervised learning. In practice, MCUs are constructed from intra-image,\ncross-image, and cross-domain levels by using both labeled and unlabeled\npixels. At each level, we define contrastive losses from center-to-center and\npixel-to-pixel manners, with the aim of jointly aligning the category centers\nand reducing outliers near the decision boundaries. In addition, we also\nintroduce a categories correlation matrix to implicitly describe the\nrelationship between categories, which are used to adjust the weights of the\nlosses for MCUs. Extensive experimental results on standard benchmarks show\nthat the proposed method achieves competitive performance against\nstate-of-the-art SSDA methods with 50% fewer labeled pixels and significantly\noutperforms state-of-the-art with a large margin by using the same level of\nannotation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhanglin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yanqing Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Feature Learning through Feature Distance Loss. (arXiv:2205.11606v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11606","description":"<p>Convolutional neural networks have shown remarkable ability to learn\ndiscriminative semantic features in image recognition tasks. Though, for\nclassification they often concentrate on specific regions in images. This work\nproposes a novel method that combines variant rich base models to concentrate\non different important image regions for classification. A feature distance\nloss is implemented while training an ensemble of base models to force them to\nlearn discriminative feature concepts. The experiments on benchmark\nconvolutional neural networks (VGG16, ResNet, AlexNet), popular datasets\n(Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training\nsamples (3, 5, 10, 20, 50, 100 per class) show our methods effectiveness and\ngeneralization ability. Our method outperforms ensemble versions of the base\nmodels without feature distance loss, and the Class Activation Maps explicitly\nproves the ability to learn different discriminative feature concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiwen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noack_B/0/1/0/all/0/1\">Benjamin Noack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffuse Map Guiding Unsupervised Generative Adversarial Network for SVBRDF Estimation. (arXiv:2205.11951v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11951","description":"<p>Reconstructing materials in the real world has always been a difficult\nproblem in computer graphics. Accurately reconstructing the material in the\nreal world is critical in the field of realistic rendering. Traditionally,\nmaterials in computer graphics are mapped by an artist, then mapped onto a\ngeometric model by coordinate transformation, and finally rendered with a\nrendering engine to get realistic materials. For opaque objects, the industry\ncommonly uses physical-based bidirectional reflectance distribution function\n(BRDF) rendering models for material modeling. The commonly used physical-based\nrendering models are Cook-Torrance BRDF, Disney BRDF. In this paper, we use the\nCook-Torrance model to reconstruct the materials. The SVBRDF material\nparameters include Normal, Diffuse, Specular and Roughness. This paper presents\na Diffuse map guiding material estimation method based on the Generative\nAdversarial Network(GAN). This method can predict plausible SVBRDF maps with\nglobal features using only a few pictures taken by the mobile phone. The main\ncontributions of this paper are: 1) We preprocess a small number of input\npictures to produce a large number of non-repeating pictures for training to\nreduce over-fitting. 2) We use a novel method to directly obtain the guessed\ndiffuse map with global characteristics, which provides more prior information\nfor the training process. 3) We improve the network architecture of the\ngenerator so that it can generate fine details of normal maps and reduce the\npossibility to generate over-flat normal maps. The method used in this paper\ncan obtain prior knowledge without using dataset training, which greatly\nreduces the difficulty of material reconstruction and saves a lot of time to\ngenerate and calibrate datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongnan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12005","description":"<p>Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning. (arXiv:2205.12183v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2205.12183","description":"<p>3D scene stylization aims at generating stylized images of the scene from\narbitrary novel views following a given set of style examples, while ensuring\nconsistency when rendered from different views. Directly applying methods for\nimage or video stylization to 3D scenes cannot achieve such consistency. Thanks\nto recently proposed neural radiance fields (NeRF), we are able to represent a\n3D scene in a consistent way. Consistent 3D scene stylization can be\neffectively achieved by stylizing the corresponding NeRF. However, there is a\nsignificant domain gap between style examples which are 2D images and NeRF\nwhich is an implicit volumetric representation. To address this problem, we\npropose a novel mutual learning framework for 3D scene stylization that\ncombines a 2D image stylization network and NeRF to fuse the stylization\nability of 2D stylization network with the 3D consistency of NeRF. We first\npre-train a standard NeRF of the 3D scene to be stylized and replace its color\nprediction module with a style network to obtain a stylized NeRF. It is\nfollowed by distilling the prior knowledge of spatial consistency from NeRF to\nthe 2D stylization network through an introduced consistency loss. We also\nintroduce a mimic loss to supervise the mutual learning of the NeRF style\nmodule and fine-tune the 2D stylization decoder. In order to further make our\nmodel handle ambiguities of 2D stylization results, we introduce learnable\nlatent codes that obey the probability distributions conditioned on the style.\nThey are attached to training samples as conditional inputs to better learn the\nstyle module in our novel stylized NeRF. Experimental results demonstrate that\nour method is superior to existing approaches in both visual quality and\nlong-range consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi-Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yu-Jie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}