{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models. (arXiv:2206.00052v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00052","description":"<p>Pre-trained programming language (PL) models (such as CodeT5, CodeBERT,\nGraphCodeBERT, etc.,) have the potential to automate software engineering tasks\ninvolving code understanding and code generation. However, these models are not\nrobust to changes in the input and thus, are potentially susceptible to\nadversarial attacks. We propose, CodeAttack, a simple yet effective black-box\nattack model that uses code structure to generate imperceptible, effective, and\nminimally perturbed adversarial code samples. We demonstrate the\nvulnerabilities of the state-of-the-art PL models to code-specific adversarial\nattacks. We evaluate the transferability of CodeAttack on several code-code\n(translation and repair) and code-NL (summarization) tasks across different\nprogramming languages. CodeAttack outperforms state-of-the-art adversarial NLP\nattack models to achieve the best overall performance while being more\nefficient and imperceptible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Mixture-of-Expert Approach to RL-based Dialogue Management. (arXiv:2206.00059v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00059","description":"<p>Despite recent advancements in language models (LMs), their application to\ndialogue management (DM) problems and ability to carry on rich conversations\nremain a challenge. We use reinforcement learning (RL) to develop a dialogue\nagent that avoids being short-sighted (outputting generic utterances) and\nmaximizes overall user satisfaction. Most existing RL approaches to DM train\nthe agent at the word-level, and thus, have to deal with a combinatorially\ncomplex action space even for a medium-size vocabulary. As a result, they\nstruggle to produce a successful and engaging dialogue even if they are\nwarm-started with a pre-trained LM. To address this issue, we develop a\nRL-based DM using a novel mixture of expert language model (MoE-LM) that\nconsists of (i) a LM capable of learning diverse semantics for conversation\nhistories, (ii) a number of {\\em specialized} LMs (or experts) capable of\ngenerating utterances corresponding to a particular attribute or personality,\nand (iii) a RL-based DM that performs dialogue planning with the utterances\ngenerated by the experts. Our MoE approach provides greater flexibility to\ngenerate sensible utterances with different intents and allows RL to focus on\nconversational-level DM. We compare it with SOTA baselines on open-domain\ndialogues and demonstrate its effectiveness both in terms of the diversity and\nsensibility of the generated utterances and the overall DM performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1\">Yinlam Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulepbergenov_A/0/1/0/all/0/1\">Aza Tulepbergenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_M/0/1/0/all/0/1\">MoonKyung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1\">Craig Boutilier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VALHALLA: Visual Hallucination for Machine Translation. (arXiv:2206.00100v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00100","description":"<p>Designing better machine translation systems by considering auxiliary inputs\nsuch as images has attracted much attention in recent years. While existing\nmethods show promising performance over the conventional text-only translation\nsystems, they typically require paired text and image as input during\ninference, which limits their applicability to real-world scenarios. In this\npaper, we introduce a visual hallucination framework, called VALHALLA, which\nrequires only source sentences at inference time and instead uses hallucinated\nvisual representations for multimodal machine translation. In particular, given\na source sentence an autoregressive hallucination transformer is used to\npredict a discrete visual representation from the input text, and the combined\ntext and hallucinated representations are utilized to obtain the target\ntranslation. We train the hallucination transformer jointly with the\ntranslation transformer using standard backpropagation with cross-entropy\nlosses while being guided by an additional loss that encourages consistency\nbetween predictions using either ground-truth or hallucinated visual\nrepresentations. Extensive experiments on three standard translation datasets\nwith a diverse set of language pairs demonstrate the effectiveness of our\napproach over both text-only baselines and state-of-the-art methods. Project\npage: <a href=\"http://www.svcl.ucsd.edu/projects/valhalla.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents. (arXiv:2206.00142v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00142","description":"<p>We present the IGLU Gridworld: a reinforcement learning environment for\nbuilding and evaluating language conditioned embodied agents in a scalable way.\nThe environment features visual agent embodiment, interactive learning through\ncollaboration, language conditioned RL, and combinatorically hard task (3d\nblocks building) space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volovikova_Z/0/1/0/all/0/1\">Zoya Volovikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Artur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre Cot&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr I. Panov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding How People Rate Their Conversations. (arXiv:2206.00167v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00167","description":"<p>User ratings play a significant role in spoken dialogue systems. Typically,\nsuch ratings tend to be averaged across all users and then utilized as feedback\nto improve the system or personalize its behavior. While this method can be\nuseful to understand broad, general issues with the system and its behavior, it\ndoes not take into account differences between users that affect their ratings.\nIn this work, we conduct a study to better understand how people rate their\ninteractions with conversational agents. One macro-level characteristic that\nhas been shown to correlate with how people perceive their inter-personal\ncommunication is personality. We specifically focus on agreeableness and\nextraversion as variables that may explain variation in ratings and therefore\nprovide a more meaningful signal for training or personalization. In order to\nelicit those personality traits during an interaction with a conversational\nagent, we designed and validated a fictional story, grounded in prior work in\npsychology. We then implemented the story into an experimental conversational\nagent that allowed users to opt-in to hearing the story. Our results suggest\nthat for human-conversational agent interactions, extraversion may play a role\nin user ratings, but more data is needed to determine if the relationship is\nsignificant. Agreeableness, on the other hand, plays a statistically\nsignificant role in conversation ratings: users who are more agreeable are more\nlikely to provide a higher rating for their interaction. In addition, we found\nthat users who opted to hear the story were, in general, more likely to rate\ntheir conversational experience higher than those who did not.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chartier_N/0/1/0/all/0/1\">Nicole Chartier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_P/0/1/0/all/0/1\">Pankaj Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1\">Julia Hirschberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering the Hidden Vocabulary of DALLE-2. (arXiv:2206.00169v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00169","description":"<p>We discover that DALLE-2 seems to have a hidden vocabulary that can be used\nto generate images with absurd prompts. For example, it seems that\n\\texttt{Apoploe vesrreaitais} means birds and \\texttt{Contarra ccetnxniams\nluryca tanniounons} (sometimes) means bugs or pests. We find that these prompts\nare often consistent in isolation but also sometimes in combinations. We\npresent our black-box method to discover words that seem random but have some\ncorrespondence to visual concepts. This creates important security and\ninterpretability challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models. (arXiv:2206.00192v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00192","description":"<p>Previous works show that deep NLP models are not always conceptually sound:\nthey do not always learn the correct linguistic concepts. Specifically, they\ncan be insensitive to word order. In order to systematically evaluate models\nfor their conceptual soundness with respect to word order, we introduce a new\nexplanation method for sequential data: Order-sensitive Shapley Values (OSV).\nWe conduct an extensive empirical evaluation to validate the method and surface\nhow well various deep NLP models learn word order. Using synthetic data, we\nfirst show that OSV is more faithful in explaining model behavior than\ngradient-based methods. Second, applying to the HANS dataset, we discover that\nthe BERT-based NLI model uses only the word occurrences without word orders.\nAlthough simple data augmentation improves accuracy on HANS, OSV shows that the\naugmented model does not fundamentally improve the model's learning of order.\nThird, we discover that not all sentiment analysis models learn negation\nproperly: some fail to capture the correct syntax of the negation construct.\nFinally, we show that pretrained language models such as BERT may rely on the\nabsolute positions of subject words to learn long-range Subject-Verb Agreement.\nWith each NLP task, we also demonstrate how OSV can be leveraged to generate\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaiji Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption. (arXiv:2206.00216v1 [cs.CR])","link":"http://arxiv.org/abs/2206.00216","description":"<p>As more and more pre-trained language models adopt on-cloud deployment, the\nprivacy issues grow quickly, mainly for the exposure of plain-text user data\n(e.g., search history, medical record, bank account). Privacy-preserving\ninference of transformer models is on the demand of cloud service users. To\nprotect privacy, it is an attractive choice to compute only with ciphertext in\nhomomorphic encryption (HE). However, enabling pre-trained models inference on\nciphertext data is difficult due to the complex computations in transformer\nblocks, which are not supported by current HE tools yet. In this work, we\nintroduce $\\textit{THE-X}$, an approximation approach for transformers, which\nenables privacy-preserving inference of pre-trained models developed by popular\nframeworks. $\\textit{THE-X}$ proposes a workflow to deal with complex\ncomputation in transformer networks, including all the non-polynomial functions\nlike GELU, softmax, and LayerNorm. Experiments reveal our proposed\n$\\textit{THE-X}$ can enable transformer inference on encrypted data for\ndifferent downstream tasks, all with negligible performance drop but enjoying\nthe theory-guaranteed privacy-preserving advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback. (arXiv:2206.00234v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00234","description":"<p>Although approximately 50% of medical school graduates today are women,\nfemale physicians tend to be underrepresented in senior positions, make less\nmoney than their male counterparts and receive fewer promotions. There is a\ngrowing body of literature demonstrating gender bias in various forms of\nevaluation in medicine, but this work was mainly conducted by looking for\nspecific words using fixed dictionaries such as LIWC and focused on\nrecommendation letters. We use a dataset of written and quantitative\nassessments of medical student performance on individual shifts of work,\ncollected across multiple institutions, to investigate the extent to which\ngender bias exists in a day-to-day context for medical students. We investigate\ndifferences in the narrative comments given to male and female students by both\nmale or female faculty assessors, using a fine-tuned BERT model. This allows us\nto examine whether groups are written about in systematically different ways,\nwithout relying on hand-crafted wordlists or topic models. We compare these\nresults to results from the traditional LIWC method and find that, although we\nfind no evidence of group-level gender bias in this dataset, terms related to\nfamily and children are used more in feedback given to women.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubosh_N/0/1/0/all/0/1\">Nicole Dubosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiller_K/0/1/0/all/0/1\">Katherine Mosher Hiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDANI: Inference-time Domain Adaptation via Neuron-level Interventions. (arXiv:2206.00259v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00259","description":"<p>Large pre-trained models are usually fine-tuned on downstream task data, and\ntested on unseen data. When the train and test data come from different\ndomains, the model is likely to struggle, as it is not adapted to the test\ndomain. We propose a new approach for domain adaptation (DA), using\nneuron-level interventions: We modify the representation of each test example\nin specific neurons, resulting in a counterfactual example from the source\ndomain, which the model is more familiar with. The modified example is then fed\nback into the model. While most other DA methods are applied during training\ntime, ours is applied during inference only, making it more efficient and\napplicable. Our experiments show that our method improves performance on unseen\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antverg_O/0/1/0/all/0/1\">Omer Antverg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InducT-GCN: Inductive Graph Convolutional Networks for Text Classification. (arXiv:2206.00265v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00265","description":"<p>Text classification aims to assign labels to textual units by making use of\nglobal information. Recent studies have applied graph neural network (GNN) to\ncapture the global word co-occurrence in a corpus. Existing approaches require\nthat all the nodes (training and test) in a graph are present during training,\nwhich are transductive and do not naturally generalise to unseen nodes. To make\nthose models inductive, they use extra resources, like pretrained word\nembedding. However, high-quality resource is not always available and hard to\ntrain. Under the extreme settings with no extra resource and limited amount of\ntraining set, can we still learn an inductive graph-based text classification\nmodel? In this paper, we introduce a novel inductive graph-based text\nclassification framework, InducT-GCN (InducTive Graph Convolutional Networks\nfor Text classification). Compared to transductive models that require test\ndocuments in training, we construct a graph based on the statistics of training\ndocuments only and represent document vectors with a weighted sum of word\nvectors. We then conduct one-directional GCN propagation during testing. Across\nfive text classification benchmarks, our InducT-GCN outperformed\nstate-of-the-art methods that are either transductive in nature or pre-trained\nadditional resources. We also conducted scalability testing by gradually\nincreasing the data size and revealed that our InducT-GCN can reduce the time\nand space complexity. The code is available on:\nhttps://github.com/usydnlp/InductTGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORE: A Metric Learning Based Framework for Open-domain Relation Extraction. (arXiv:2206.00289v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00289","description":"<p>Open relation extraction (OpenRE) is the task of extracting relation schemes\nfrom open-domain corpora. Most existing OpenRE methods either do not fully\nbenefit from high-quality labeled corpora or can not learn semantic\nrepresentation directly, affecting downstream clustering efficiency. To address\nthese problems, in this work, we propose a novel learning framework named MORE\n(Metric learning-based Open Relation Extraction). The framework utilizes deep\nmetric learning to obtain rich supervision signals from labeled data and drive\nthe neural model to learn semantic relational representation directly.\nExperiments result in two real-world datasets show that our method outperforms\nother state-of-the-art baselines. Our source code is available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yutong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">MaoYan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Layer Normalizations and Residual Connections in Transformers. (arXiv:2206.00330v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00330","description":"<p>In the perspective of a layer normalization (LN) position, the architecture\nof Transformers can be categorized into two types: Post-LN and Pre-LN. Recent\nTransformers prefer to select Pre-LN because the training in Post-LN with deep\nTransformers, e.g., ten or more layers, often becomes unstable, resulting in\nuseless models. However, in contrast, Post-LN has also consistently achieved\nbetter performance than Pre-LN in relatively shallow Transformers, e.g., six or\nfewer layers. This study first investigates the reason for these discrepant\nobservations empirically and theoretically and discovers 1, the LN in Post-LN\nis the source of the vanishing gradient problem that mainly leads the unstable\ntraining whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger\ngradient norms in higher layers during the back-propagation that may lead an\neffective training. Exploiting the new findings, we propose a method that can\nequip both higher stability and effective training by a simple modification\nfrom Post-LN. We conduct experiments on a wide range of text generation tasks\nand demonstrate that our method outperforms Pre-LN, and stable training\nregardless of the shallow or deep layer settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical character recognition quality affects perceived usefulness of historical newspaper clippings. (arXiv:2206.00369v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00369","description":"<p>Introduction. We study effect of different quality optical character\nrecognition in interactive information retrieval with a collection of one\ndigitized historical Finnish newspaper. Method. This study is based on the\nsimulated interactive information retrieval work task model. Thirty-two users\nmade searches to an article collection of Finnish newspaper Uusi Suometar\n1869-1918 with ca. 1.45 million auto segmented articles. Our article search\ndatabase had two versions of each article with different quality optical\ncharacter recognition. Each user performed six pre-formulated and six\nself-formulated short queries and evaluated subjectively the top-10 results\nusing graded relevance scale of 0-3 without knowing about the optical character\nrecognition quality differences of the otherwise identical articles. Analysis.\nAnalysis of the user evaluations was performed by comparing mean averages of\nevaluations scores in user sessions. Differences of query results were detected\nby analysing lengths of returned articles in pre-formulated and self-formulated\nqueries and number of different documents retrieved overall in these two\nsessions. Results. The main result of the study is that improved optical\ncharacter recognition quality affects perceived usefulness of historical\nnewspaper articles positively. Conclusions. We were able to show that\nimprovement in optical character recognition quality of documents leads to\nhigher mean relevance evaluation scores of query results in our historical\nnewspaper collection. To the best of our knowledge this simulated interactive\nuser-task is the first one showing empirically that users' subjective relevance\nassessments are affected by a change in the quality of optically read text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kettunen_K/0/1/0/all/0/1\">Kimmo Kettunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskustalo_H/0/1/0/all/0/1\">Heikki Keskustalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumpulainen_S/0/1/0/all/0/1\">Sanna Kumpulainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paakkonen_T/0/1/0/all/0/1\">Tuula P&#xe4;&#xe4;kk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rautiainen_J/0/1/0/all/0/1\">Juha Rautiainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts. (arXiv:2206.00372v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00372","description":"<p>Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romim_N/0/1/0/all/0/1\">Nauros Romim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Mosahed Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arnab Sen Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukder_H/0/1/0/all/0/1\">Hriteshwar Talukder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Mohammad Ruhul Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Use of NLP-Based Text Representation Techniques to Support Requirement Engineering Tasks: A Systematic Mapping Review. (arXiv:2206.00421v1 [cs.SE])","link":"http://arxiv.org/abs/2206.00421","description":"<p>Natural Language Processing (NLP) is widely used to support the automation of\ndifferent Requirements Engineering (RE) tasks. Most of the proposed approaches\nstart with various NLP steps that analyze requirements statements, extract\ntheir linguistic information, and convert them to easy-to-process\nrepresentations, such as lists of features or embedding-based vector\nrepresentations. These NLP-based representations are usually used at a later\nstage as inputs for machine learning techniques or rule-based methods. Thus,\nrequirements representations play a major role in determining the accuracy of\ndifferent approaches. In this paper, we conducted a survey in the form of a\nsystematic literature mapping (classification) to find out (1) what are the\nrepresentations used in RE tasks literature, (2) what is the main focus of\nthese works, (3) what are the main research directions in this domain, and (4)\nwhat are the gaps and potential future directions. After compiling an initial\npool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we\nobtained a final pool containing 104 relevant papers. Our survey shows that the\nresearch direction has changed from the use of lexical and syntactic features\nto the use of advanced embedding techniques, especially in the last two years.\nUsing advanced embedding representations has proved its effectiveness in most\nRE tasks (such as requirement analysis, extracting requirements from reviews\nand forums, and semantic-level quality tasks). However, representations that\nare based on lexical and syntactic features are still more appropriate for\nother RE tasks (such as modeling and syntax-level quality tasks) since they\nprovide the required information for the rules and regular expressions used\nwhen handling these tasks. In addition, we identify four gaps in the existing\nliterature, why they matter, and how future research can begin to address them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sonbol_R/0/1/0/all/0/1\">Riad Sonbol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebdawi_G/0/1/0/all/0/1\">Ghaida Rebdawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghneim_N/0/1/0/all/0/1\">Nada Ghneim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What a Creole Wants, What a Creole Needs. (arXiv:2206.00437v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00437","description":"<p>In recent years, the natural language processing (NLP) community has given\nincreased attention to the disparity of efforts directed towards high-resource\nlanguages over low-resource ones. Efforts to remedy this delta often begin with\ntranslations of existing English datasets into other languages. However, this\napproach ignores that different language communities have different needs. We\nconsider a group of low-resource languages, Creole languages. Creoles are both\nlargely absent from the NLP literature, and also often ignored by society at\nlarge due to stigma, despite these languages having sizable and vibrant\ncommunities. We demonstrate, through conversations with Creole experts and\nsurveys of Creole-speaking communities, how the things needed from language\ntechnology can change dramatically from one language to another, even when the\nlanguages are considered to be very similar to each other, as with Creoles. We\ndiscuss the prominent themes arising from these conversations, and ultimately\ndemonstrate that useful language technology cannot be built without involving\nthe relevant community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vietnamese Hate and Offensive Detection using PhoBERT-CNN and Social Media Streaming Data. (arXiv:2206.00524v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00524","description":"<p>Society needs to develop a system to detect hate and offense to build a\nhealthy and safe environment. However, current research in this field still\nfaces four major shortcomings, including deficient pre-processing techniques,\nindifference to data imbalance issues, modest performance models, and lacking\npractical applications. This paper focused on developing an intelligent system\ncapable of addressing these shortcomings. Firstly, we proposed an efficient\npre-processing technique to clean comments collected from Vietnamese social\nmedia. Secondly, a novel hate speech detection (HSD) model, which is the\ncombination of a pre-trained PhoBERT model and a Text-CNN model, was proposed\nfor solving tasks in Vietnamese. Thirdly, EDA techniques are applied to deal\nwith imbalanced data to improve the performance of classification models.\nBesides, various experiments were conducted as baselines to compare and\ninvestigate the proposed model's performance against state-of-the-art methods.\nThe experiment results show that the proposed PhoBERT-CNN model outperforms\nSOTA methods and achieves an F1-score of 67,46% and 98,45% on two benchmark\ndatasets, ViHSD and HSD-VLSP, respectively. Finally, we also built a streaming\nHSD application to demonstrate the practicality of our proposed system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Phu Gia Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_C/0/1/0/all/0/1\">Canh Duc Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Trong-Hop Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Diversity in Back Translation for Low-Resource Machine Translation. (arXiv:2206.00564v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00564","description":"<p>Back translation is one of the most widely used methods for improving the\nperformance of neural machine translation systems. Recent research has sought\nto enhance the effectiveness of this method by increasing the 'diversity' of\nthe generated translations. We argue that the definitions and metrics used to\nquantify 'diversity' in previous work have been insufficient. This work puts\nforward a more nuanced framework for understanding diversity in training data,\nsplitting it into lexical diversity and syntactic diversity. We present novel\nmetrics for measuring these different aspects of diversity and carry out\nempirical analysis into the effect of these types of diversity on final neural\nmachine translation model performance for low-resource\nEnglish$\\leftrightarrow$Turkish and mid-resource\nEnglish$\\leftrightarrow$Icelandic. Our findings show that generating back\ntranslation using nucleus sampling results in higher final model performance,\nand that this method of generation has high levels of both lexical and\nsyntactic diversity. We also find evidence that lexical diversity is more\nimportant than syntactic for back translation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burchell_L/0/1/0/all/0/1\">Laurie Burchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00621","description":"<p>In this paper, we introduce Cross-View Language Modeling, a simple and\neffective language model pre-training framework that unifies cross-lingual\ncross-modal pre-training with shared architectures and objectives. Our approach\nis motivated by a key observation that cross-lingual and cross-modal\npre-training share the same goal of aligning two different views of the same\nobject into a common semantic space. To this end, the cross-view language\nmodeling framework considers both multi-modal data (i.e., image-caption pairs)\nand multi-lingual data (i.e., parallel sentence pairs) as two different views\nof the same object, and trains the model to align the two views by maximizing\nthe mutual information between them with conditional masked language modeling\nand contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal\nLanguage Model, with the cross-view language modeling framework. Empirical\nresults on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual\nimage-text retrieval datasets show that while conceptually simpler, CCLM\nsignificantly outperforms the prior state-of-the-art with an average absolute\nimprovement of over 10%. Notably, CCLM is the first multi-lingual multi-modal\nmodel that surpasses the translate-test performance of representative English\nvision-language models by zero-shot cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Ao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v1 [q-fin.ST])","link":"http://arxiv.org/abs/2206.00648","description":"<p>Bitcoin, with its ever-growing popularity, has demonstrated extreme price\nvolatility since its origin. This volatility, together with its decentralised\nnature, make Bitcoin highly subjective to speculative trading as compared to\nmore traditional assets. In this paper, we propose a multimodal model for\npredicting extreme price fluctuations. This model takes as input a variety of\ncorrelated assets, technical indicators, as well as Twitter content. In an\nin-depth study, we explore whether social media discussions from the general\npublic on Bitcoin have predictive power for extreme price movements. A dataset\nof 5,000 tweets per day containing the keyword `Bitcoin' was collected from\n2015 to 2021. This dataset, called PreBit, is made available online. In our\nhybrid model, we use sentence-level FinBERT embeddings, pretrained on financial\nlexicons, so as to capture the full contents of the tweets and feed it to the\nmodel in an understandable way. By combining these embeddings with a\nConvolutional Neural Network, we built a predictive model for significant\nmarket movements. The final multimodal ensemble model includes this NLP model\ntogether with a model based on candlestick data, technical indicators and\ncorrelated asset prices. In an ablation study, we explore the contribution of\nthe individual modalities. Finally, we propose and backtest a trading strategy\nbased on the predictions of our models with varying prediction threshold and\nshow that it can used to build a profitable trading strategy with a reduced\nrisk over a `hold' or moving average strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Zou_Y/0/1/0/all/0/1\">Yanzhao Zou</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sense Language Modelling. (arXiv:2012.05776v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.05776","description":"<p>The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekkas_A/0/1/0/all/0/1\">Andrea Lekkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'Just because you are right, doesn't mean I am wrong': Overcoming a Bottleneck in the Development and Evaluation of Open-Ended Visual Question Answering (VQA) Tasks. (arXiv:2103.15022v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.15022","description":"<p>GQA~\\citep{hudson2019gqa} is a dataset for real-world visual reasoning and\ncompositional question answering. We found that many answers predicted by the\nbest vision-language models on the GQA dataset do not match the ground-truth\nanswer but still are semantically meaningful and correct in the given context.\nIn fact, this is the case with most existing visual question answering (VQA)\ndatasets where they assume only one ground-truth answer for each question. We\npropose Alternative Answer Sets (AAS) of ground-truth answers to address this\nlimitation, which is created automatically using off-the-shelf NLP tools. We\nintroduce a semantic metric based on AAS and modify top VQA solvers to support\nmultiple plausible answers for a question. We implement this approach on the\nGQA dataset and show the performance improvements. Code and data are available\nin this link \\url{https://github.com/luomancs/alternative_answer_set.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampat_S/0/1/0/all/0/1\">Shailaja Keyur Sampat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallman_R/0/1/0/all/0/1\">Riley Tallman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yankai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vancha_M/0/1/0/all/0/1\">Manuha Vancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajja_A/0/1/0/all/0/1\">Akarshan Sajja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00356","description":"<p>Masked language models (MLMs) are pre-trained with a denoising objective that\nis in a mismatch with the objective of downstream fine-tuning. We propose\npragmatic masking and surrogate fine-tuning as two complementing strategies\nthat exploit social cues to drive pre-trained representations toward a broad\nset of concepts useful for a wide class of social meaning tasks. We test our\nmodels on $15$ different Twitter datasets for social meaning detection. Our\nmethods achieve $2.34\\%$ $F_1$ over a competitive baseline, while outperforming\ndomain-specific language models pre-trained on large datasets. Our methods also\nexcel in few-shot learning: with only $5\\%$ of training data (severely\nfew-shot), our methods enable an impressive $68.54\\%$ average $F_1$. The\nmethods are also language agnostic, as we show in a zero-shot setting involving\nsix datasets from three different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain. (arXiv:2109.02555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02555","description":"<p>Deep neural language models have set new breakthroughs in many tasks of\nNatural Language Processing (NLP). Recent work has shown that deep transformer\nlanguage models (pretrained on large amounts of texts) can achieve high levels\nof task-specific few-shot performance comparable to state-of-the-art models.\nHowever, the ability of these large language models in few-shot transfer\nlearning has not yet been explored in the biomedical domain. We investigated\nthe performance of two powerful transformer language models, i.e. GPT-3 and\nBioBERT, in few-shot settings on various biomedical NLP tasks. The experimental\nresults showed that, to a great extent, both the models underperform a language\nmodel fine-tuned on the full training data. Although GPT-3 had already achieved\nnear state-of-the-art results in few-shot knowledge transfer on open-domain NLP\ntasks, it could not perform as effectively as BioBERT, which is orders of\nmagnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on\nlarge biomedical text corpora, our study suggests that language models may\nlargely benefit from in-domain pretraining in task-specific few-shot learning.\nHowever, in-domain pretraining seems not to be sufficient; novel pretraining\nand few-shot learning strategies are required in the biomedical NLP domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haberl_F/0/1/0/all/0/1\">Florian Haberl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03333","description":"<p>Conversational speech normally is embodied with loose syntactic structures at\nthe utterance level but simultaneously exhibits topical coherence relations\nacross consecutive utterances. Prior work has shown that capturing longer\ncontext information with a recurrent neural network or long short-term memory\nlanguage model (LM) may suffer from the recent bias while excluding the\nlong-range context. In order to capture the long-term semantic interactions\namong words and across utterances, we put forward disparate conversation\nhistory fusion methods for language modeling in automatic speech recognition\n(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is\nintroduced, which manages to fuse and utilize the acoustic embeddings of a\ncurrent utterance and the semantic content of its corresponding conversation\nhistory in a cooperative way. To flesh out our ideas, we frame the ASR N-best\nhypothesis rescoring task as a prediction problem, leveraging BERT, an iconic\npre-trained LM, as the ingredient vehicle to facilitate selection of the oracle\nhypothesis from a given N-best hypothesis list. Empirical experiments conducted\non the AMI benchmark dataset seem to demonstrate the feasibility and efficacy\nof our methods in relation to some current top-of-line methods. The proposed\nmethods not only achieve significant inference time reduction but also improve\nthe ASR performance for conversational speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bi-Cheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsuan-Sheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08276","description":"<p>Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection and make fine-grained alignments\nbetween the extracted features and texts. It is challenging for these methods\nto learn relations among multiple objects. To this end, we propose a new method\ncalled X-VLM to perform `multi-grained vision language pre-training.' The key\nto learning multi-grained alignments is to locate visual concepts in the image\ngiven the associated texts, and in the meantime align the texts with the visual\nconcepts, where the alignments are in multi-granularity. Experimental results\nshow that X-VLM effectively leverages the learned multi-grained alignments to\nmany downstream vision language tasks and consistently outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.15011","description":"<p>In clinics, a radiology report is crucial for guiding a patient's treatment.\nHowever, writing radiology reports is a heavy burden for radiologists. To this\nend, we present an automatic, multi-modal approach for report generation from a\nchest x-ray. Our approach, motivated by the observation that the descriptions\nin radiology reports are highly correlated with specific information of the\nx-ray images, features two distinct modules: (i) Learned knowledge base: To\nabsorb the knowledge embedded in the radiology reports, we build a knowledge\nbase that can automatically distil and restore medical knowledge from textual\nembedding without manual labour; (ii) Multi-modal alignment: to promote the\nsemantic alignment among reports, disease labels, and images, we explicitly\nutilize textual embedding to guide the learning of the visual feature space. We\nevaluate the performance of the proposed model using metrics from both natural\nlanguage generation and clinic efficacy on the public IU-Xray and MIMIC-CXR\ndatasets. Our ablation study shows that each module contributes to improving\nthe quality of generated reports. Furthermore, with the assistance of both\nmodules, our approach outperforms state-of-the-art methods over almost all the\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S.Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2201.09555","description":"<p>Scholarly data is growing continuously containing information about the\narticles from a plethora of venues including conferences, journals, etc. Many\ninitiatives have been taken to make scholarly data available as Knowledge\nGraphs (KGs). These efforts to standardize these data and make them accessible\nhave also led to many challenges such as exploration of scholarly articles,\nambiguous authors, etc. This study more specifically targets the problem of\nAuthor Name Disambiguation (AND) on Scholarly KGs and presents a novel\nframework, Literally Author Name Disambiguation (LAND), which utilizes\nKnowledge Graph Embeddings (KGEs) using multimodal literal information\ngenerated from these KGs. This framework is based on three components: 1)\nMultimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical\nAgglomerative Clustering. Extensive experiments have been conducted on two\nnewly created KGs: (i) KG containing information from Scientometrics Journal\nfrom 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known\nbenchmark for AND provided by AMiner (AMiner-534K). The results show that our\nproposed architecture outperforms our baselines of 8-14% in terms of the F1\nscore and shows competitive performances on a challenging benchmark such as\nAMiner. The code and the datasets are publicly available through Github:\nhttps://github.com/sntcristian/and-kge and\nZenodo:https://doi.org/10.5281/zenodo.6309855 respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santini_C/0/1/0/all/0/1\">Cristian Santini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesese_G/0/1/0/all/0/1\">Genet Asefa Gesese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1\">Aldo Gangemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11903","description":"<p>We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. (arXiv:2202.03052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03052","description":"<p>In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &amp;\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Language Modeling with Sparse all-MLP. (arXiv:2203.06850v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06850","description":"<p>All-MLP architectures have attracted increasing interest as an alternative to\nattention-based models. In NLP, recent work like gMLP shows that all-MLPs can\nmatch Transformers in language modeling, but still lag behind in downstream\ntasks. In this work, we analyze the limitations of MLPs in expressiveness, and\npropose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature\nand input (token) dimensions. Such sparse all-MLPs significantly increase model\ncapacity and expressiveness while keeping the compute constant. We address\ncritical challenges in incorporating conditional computation with two routing\nstrategies. The proposed sparse all-MLP improves language modeling perplexity\nand obtains up to 2$\\times$ improvement in training efficiency compared to both\nTransformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH\nLayers) as well as dense Transformers and all-MLPs. Finally, we evaluate its\nzero-shot in-context learning performance on six downstream tasks, and find\nthat it surpasses Transformer-based MoEs and dense Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06925","description":"<p>Named Entity Recognition task is one of the core tasks of information\nextraction.Word ambiguity and word abbreviation are important reasons for the\nlow recognition rate of named entities. In this paper, we propose a novel named\nentity recognition model WCL-BBCD (Word Contrastive Learning with\nBERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The\nmodel first trains the sentence pairs in the text, calculate similarity between\nwords in sentence pairs by cosine similarity, and fine-tunes the BERT model\nused for the named entity recognition task through the similarity, so as to\nalleviate word ambiguity. Then, the fine-tuned BERT model is combined with the\nBiLSTM-CRF model to perform the named entity recognition task. Finally, the\nrecognition results are corrected in combination with prior knowledge such as\nknowledge graphs, so as to alleviate the recognition caused by word\nabbreviations low-rate problem. Experimental results show that our model\noutperforms other similar model methods on the CoNLL-2003 English dataset and\nOntoNotes V5 English dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Renjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jian Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianjun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14260","description":"<p>Understanding realistic visual scene images together with language\ndescriptions is a fundamental task towards generic visual understanding.\nPrevious works have shown compelling comprehensive results by building\nhierarchical structures for visual scenes (e.g., scene graphs) and natural\nlanguages (e.g., dependency trees), individually. However, how to construct a\njoint vision-language (VL) structure has barely been investigated. More\nchallenging but worthwhile, we introduce a new task that targets on inducing\nsuch a joint VL structure in an unsupervised manner. Our goal is to bridge the\nvisual scene graphs and linguistic dependency trees seamlessly. Due to the lack\nof VL structural data, we start by building a new dataset VLParse. Rather than\nusing labor-intensive labeling from scratch, we propose an automatic alignment\nprocedure to produce coarse structures followed by human refinement to produce\nhigh-quality ones. Moreover, we benchmark our dataset by proposing a\ncontrastive learning (CL)-based framework VLGAE, short for Vision-Language\nGraph Autoencoder. Our model obtains superior performance on two derived tasks,\ni.e., language grammar induction and VL phrase grounding. Ablations show the\neffectiveness of both visual cues and dependency relationships on fine-grained\nVL structure construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16027","description":"<p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and\nprovides performance lift by adapting unlabelled data to downstream task.\nUnfortunately, existing adaptations mainly involve deterministic rules that\ncannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze\nanswer extraction method used in TAPT that is extendable for adaptation on any\ncloze-style machine reading comprehension (MRC) downstream tasks. We experiment\non multiple-choice cloze-style MRC tasks, and show that Clozer performs\nsignificantly better compared to the oracle and state-of-the-art in escalating\nTAPT effectiveness in lifting model performance, and prove that Clozer is able\nto recognize the gold answers independently of any heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Su Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Continuous Posteriors for Latent Variational Dialogue Policies. (arXiv:2205.07633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07633","description":"<p>Utilizing amortized variational inference for latent-action reinforcement\nlearning (RL) has been shown to be an effective approach in Task-oriented\nDialogue (ToD) systems for optimizing dialogue success. Until now, categorical\nposteriors have been argued to be one of the main drivers of performance. In\nthis work we revisit Gaussian variational posteriors for latent-action RL and\nshow that they can yield even better performance than categoricals. We achieve\nthis by simplifying the training procedure and propose ways to regularize the\nlatent dialogue policy to retain good response coherence. Using continuous\nlatent representations our model achieves state of the art dialogue success\nrate on the MultiWOZ benchmark, and also compares well to categorical latent\nmethods in response coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1\">Marin Vlastelica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_P/0/1/0/all/0/1\">Patrick Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szarvas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Szarvas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialog Inpainting: Turning Documents into Dialogs. (arXiv:2205.09073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09073","description":"<p>Many important questions (e.g. \"How to eat healthier?\") require conversation\nto establish context and explore in depth. However, conversational question\nanswering (ConvQA) systems have long been stymied by scarce training data that\nis expensive to collect. To address this problem, we propose a new technique\nfor synthetically generating diverse and high-quality dialog data: dialog\ninpainting. Our approach takes the text of any document and transforms it into\na two-person dialog between the writer and an imagined reader: we treat\nsentences from the article as utterances spoken by the writer, and then use a\ndialog inpainter to predict what the imagined reader asked or said in between\neach of the writer's utterances. By applying this approach to passages from\nWikipedia and the web, we produce WikiDialog and WebDialog, two datasets\ntotalling 19 million diverse information-seeking dialogs -- 1,000x larger than\nthe largest existing ConvQA dataset. Furthermore, human raters judge the answer\nadequacy and conversationality of WikiDialog to be as good or better than\nexisting manually-collected datasets. Using our inpainted data to pre-train\nConvQA retrieval systems, we significantly advance state-of-the-art across\nthree benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains\non standard evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Aida Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_Q/0/1/0/all/0/1\">Qazi Mamunur Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Mike Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing. (arXiv:2205.09607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09607","description":"<p>Semantic parsing is the task of producing structured meaning representations\nfor natural language sentences. Recent research has pointed out that the\ncommonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to\ngeneralize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation directly\nas a graph and not as a sequence. To this end we propose LAGr (Label Aligned\nGraphs), a general framework to produce semantic parses by independently\npredicting node and edge labels for a complete multi-layer input-aligned graph.\nThe strongly-supervised LAGr algorithm requires aligned graphs as inputs,\nwhereas weakly-supervised LAGr infers alignments for originally unaligned\ntarget graphs using approximate maximum-a-posteriori inference. Experiments\ndemonstrate that LAGr achieves significant improvements in systematic\ngeneralization upon the baseline seq2seq parsers in both strongly- and\nweakly-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jambor_D/0/1/0/all/0/1\">Dora Jambor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training. (arXiv:2205.10471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10471","description":"<p>Keyphrase generation is the task of automatically predicting keyphrases given\na piece of long text. Despite its recent flourishing, keyphrase generation on\nnon-English languages haven't been vastly investigated. In this paper, we call\nattention to a new setting named multilingual keyphrase generation and we\ncontribute two new datasets, EcommerceMKP and AcademicMKP, covering six\nlanguages. Technically, we propose a retrieval-augmented method for\nmultilingual keyphrase generation to mitigate the data shortage problem in\nnon-English languages. The retrieval-augmented model leverages keyphrase\nannotations in English datasets to facilitate generating keyphrases in\nlow-resource languages. Given a non-English passage, a cross-lingual dense\npassage retrieval module finds relevant English passages. Then the associated\nEnglish keyphrases serve as external knowledge for keyphrase generation in the\ncurrent language. Moreover, we develop a retriever-generator iterative training\nalgorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual\npassage retriever. Comprehensive experiments and ablations show that the\nproposed approach outperforms all baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Light-Weight Answer Text Retrieval in Dialogue Systems. (arXiv:2205.14226v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.14226","description":"<p>Dialogue systems can benefit from being able to search through a corpus of\ntext to find information relevant to user requests, especially when\nencountering a request for which no manually curated response is available. The\nstate-of-the-art technology for neural dense retrieval or re-ranking involves\ndeep learning models with hundreds of millions of parameters. However, it is\ndifficult and expensive to get such models to operate at an industrial scale,\nespecially for cloud services that often need to support a big number of\nindividually customized dialogue systems, each with its own text corpus. We\nreport our work on enabling advanced neural dense retrieval systems to operate\neffectively at scale on relatively inexpensive hardware. We compare with\nleading alternative industrial solutions and show that we can provide a\nsolution that is effective, fast, and cost-efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_J/0/1/0/all/0/1\">J. William Murdock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1\">Saloni Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor Prediction: A Topic Modeling Approach. (arXiv:2205.14631v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14631","description":"<p>Networks of documents connected by hyperlinks, such as Wikipedia, are\nubiquitous. Hyperlinks are inserted by the authors to enrich the text and\nfacilitate the navigation through the network. However, authors tend to insert\nonly a fraction of the relevant hyperlinks, mainly because this is a time\nconsuming task. In this paper we address an annotation, which we refer to as\nanchor prediction. Even though it is conceptually close to link prediction or\nentity linking, it is a different task that require developing a specific\nmethod to solve it. Given a source document and a target document, this task\nconsists in automatically identifying anchors in the source document, i.e words\nor terms that should carry a hyperlink pointing towards the target document. We\npropose a contextualized relational topic model, CRTM, that models directed\nlinks between documents as a function of the local context of the anchor in the\nsource document and the whole content of the target document. The model can be\nused to predict anchors in a source document, given the target document,\nwithout relying on a dictionary of previously seen mention or title, nor any\nexternal knowledge graph. Authors can benefit from CRTM, by letting it\nautomatically suggest hyperlinks, given a new document and the set of target\ndocument to connect to. It can also benefit to readers, by dynamically\ninserting hyperlinks between the documents they're reading. Experiments\nconducted on several Wikipedia corpora (in English, Italian and German)\nhighlight the practical usefulness of anchor prediction and demonstrate the\nrelevancy of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_J/0/1/0/all/0/1\">Jean Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guille_A/0/1/0/all/0/1\">Adrien Guille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacques_J/0/1/0/all/0/1\">Julien Jacques</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking. (arXiv:2205.15503v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15503","description":"<p>Current natural language interaction for self-tracking tools largely depends\non bespoke implementation optimized for a specific tracking theme and data\nformat, which is neither generalizable nor scalable to a tremendous design\nspace of self-tracking. However, training machine learning models in the\ncontext of self-tracking is challenging due to the wide variety of tracking\ntopics and data formats. In this paper, we propose a novel NLP task for\nself-tracking that extracts close- and open-ended information from a\nretrospective activity log described as a plain text, and a domain-agnostic,\nGPT-3-based NLU framework that performs this task. The framework augments the\nprompt using synthetic samples to transform the task into 10-shot learning, to\naddress a cold-start problem in bootstrapping a new tracking topic. Our\npreliminary evaluation suggests that our approach significantly outperforms the\nbaseline QA models. Going further, we discuss future application domains toward\nwhich the NLP and HCI researchers can collaborate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Transformers for Product Matching -- Experiments and a New Benchmark in Polish. (arXiv:2205.15712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15712","description":"<p>Product matching corresponds to the task of matching identical products\nacross different data sources. It typically employs available product features\nwhich, apart from being multimodal, i.e., comprised of various data types,\nmight be non-homogeneous and incomplete. The paper shows that pre-trained,\nmultilingual Transformer models, after fine-tuning, are suitable for solving\nthe product matching problem using textual features both in English and Polish\nlanguages. We tested multilingual mBERT and XLM-RoBERTa models in English on\nWeb Data Commons - training dataset and gold standard for large-scale product\nmatching. The obtained results show that these models perform similarly to the\nlatest solutions tested on this set, and in some cases, the results were even\nbetter.\n</p>\n<p>Additionally, we prepared a new dataset entirely in Polish and based on\noffers in selected categories obtained from several online stores for the\nresearch purpose. It is the first open dataset for product matching tasks in\nPolish, which allows comparing the effectiveness of the pre-trained models.\nThus, we also showed the baseline results obtained by the fine-tuned mBERT and\nXLM-RoBERTa models on the Polish datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozdzonek_M/0/1/0/all/0/1\">Micha&#x142; Mo&#x17c;d&#x17c;onek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tkachuk_S/0/1/0/all/0/1\">Sergiy Tkachuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1\">Szymon &#x141;ukasik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues. (arXiv:2205.15951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15951","description":"<p>Movies reflect society and also hold power to transform opinions. Social\nbiases and stereotypes present in movies can cause extensive damage due to\ntheir reach. These biases are not always found to be the need of storyline but\ncan creep in as the author's bias. Movie production houses would prefer to\nascertain that the bias present in a script is the story's demand. Today, when\ndeep learning models can give human-level accuracy in multiple tasks, having an\nAI solution to identify the biases present in the script at the writing stage\ncan help them avoid the inconvenience of stalled release, lawsuits, etc. Since\nAI solutions are data intensive and there exists no domain specific data to\naddress the problem of biases in scripts, we introduce a new dataset of movie\nscripts that are annotated for identity bias. The dataset contains dialogue\nturns annotated for (i) bias labels for seven categories, viz., gender,\nrace/ethnicity, religion, age, occupation, LGBTQ, and other, which contains\nbiases like body shaming, personality bias, etc. (ii) labels for sensitivity,\nstereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated\nwith context awareness, (iv) target groups and reason for bias labels and (v)\nexpert-driven group-validation process for high quality annotations. We also\nreport various baseline performances for bias identification and category\ndetection on our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sandhya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prapti Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_N/0/1/0/all/0/1\">Nihar Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallela_N/0/1/0/all/0/1\">Niteesh Mallela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savagaonkar_M/0/1/0/all/0/1\">Milind Savagaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nidhi/0/1/0/all/0/1\">Nidhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramnani_R/0/1/0/all/0/1\">Roshni Ramnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maitra_A/0/1/0/all/0/1\">Anutosh Maitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubhashis Sengupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Evaluation of Automatically Generated Text. (arXiv:2205.16001v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.16001","description":"<p>While probabilistic language generators have improved dramatically over the\nlast few years, the automatic evaluation metrics used to assess them have not\nkept pace with this progress. In the domain of language generation, a good\nmetric must correlate highly with human judgements. Yet, with few exceptions,\nthere is a lack of such metrics in the literature. In this work, we analyse the\ngeneral paradigm of language generator evaluation. We first discuss the\ncomputational and qualitative issues with using automatic evaluation metrics\nthat operate on probability distributions over strings, the backbone of most\nlanguage generators. We then propose the use of distributions over clusters\ninstead, where we cluster strings based on their text embeddings (obtained from\na pretrained language model). While we find the biases introduced by this\nsubstitution to be quite strong, we observe that, empirically, this methodology\nleads to metric estimators with higher correlation with human judgements, while\nsimultaneously reducing estimator variance. We finish the paper with a probing\nanalysis, which leads us to conclude that -- by encoding syntactic- and\ncoherence-level features of text, while ignoring surface-level features --\nthese clusters may simply be better equipped to evaluate state-of-the-art\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Calibrated Bagging Deep Learning for Image Semantic Segmentation: A Case Study on COVID-19 Chest X-ray Image. (arXiv:2206.00002v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00002","description":"<p>Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes\ncoronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR)\nand computed tomography (CT) can provide useful information to clinical staff\nfor facilitating a diagnosis of COVID-19 in a more efficient and comprehensive\nmanner. As a breakthrough of artificial intelligence (AI), deep learning has\nbeen applied to perform COVID-19 infection region segmentation and disease\nclassification by analyzing CXR and CT data. However, prediction uncertainty of\ndeep learning models for these tasks, which is very important to\nsafety-critical applications like medical image processing, has not been\ncomprehensively investigated. In this work, we propose a novel ensemble deep\nlearning model through integrating bagging deep learning and model calibration\nto not only enhance segmentation performance, but also reduce prediction\nuncertainty. The proposed method has been validated on a large dataset that is\nassociated with CXR image segmentation. Experimental results demonstrate that\nthe proposed method can improve the segmentation performance, as well as\ndecrease prediction uncertainties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nwosu_L/0/1/0/all/0/1\">Lucy Nwosu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiangfang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seungchan Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterization of 3D Printers and X-Ray Computerized Tomography. (arXiv:2206.00041v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00041","description":"<p>The 3D printing process flow requires several inputs for the best printing\nquality. These settings may vary from sample to sample, printer to printer, and\ndepend upon users' previous experience. The involved operational parameters for\n3D Printing are varied to test the optimality. Thirty-eight samples are printed\nusing four commercially available 3D printers, namely: (a) Ultimaker 2\nExtended+, (b) Delta Wasp, (c) Raise E2, and (d) ProJet MJP. The sample\nprofiles contain uniform and non-uniform distribution of the assorted size of\ncubes and spheres with a known amount of porosity. These samples are scanned\nusing X-Ray Computed Tomography system. Functional Imaging analysis is\nperformed using AI-based segmentation codes to (a) characterize these 3D\nprinters and (b) find Three-dimensional surface roughness of three teeth and\none sandstone pebble (from riverbed) with naturally deposited layers is also\ncompared with printed sample values. Teeth has best quality. It is found that\nProJet MJP gives the best quality of printed samples with the least amount of\nsurface roughness and almost near to the actual porosity value. As expected,\n100% infill density value, best spatial resolution for printing or Layer\nheight, and minimum nozzle speed give the best quality of 3D printing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khod_S/0/1/0/all/0/1\">Sunita Khod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dvivedi_A/0/1/0/all/0/1\">Akshay Dvivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goswami_M/0/1/0/all/0/1\">Mayank Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs. (arXiv:2206.00048v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00048","description":"<p>Recent advances in the understanding of Generative Adversarial Networks\n(GANs) have led to remarkable progress in visual editing and synthesis tasks,\ncapitalizing on the rich semantics that are embedded in the latent spaces of\npre-trained GANs. However, existing methods are often tailored to specific GAN\narchitectures and are limited to either discovering global semantic directions\nthat do not facilitate localized control, or require some form of supervision\nthrough manually provided regions or segmentation masks. In this light, we\npresent an architecture-agnostic approach that jointly discovers factors\nrepresenting spatial parts and their appearances in an entirely unsupervised\nfashion. These factors are obtained by applying a semi-nonnegative tensor\nfactorization on the feature maps, which in turn enables context-aware local\nimage editing with pixel-level control. In addition, we show that the\ndiscovered appearance factors correspond to saliency maps that localize\nconcepts of interest, without using any labels. Experiments on a wide range of\nGAN architectures and datasets show that, in comparison to the state of the\nart, our method is far more efficient in terms of training time and, most\nimportantly, provides much more accurate localized control. Our code is\navailable at: https://github.com/james-oldfield/PandA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1\">James Oldfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing feature fusion strategies for Deep Learning-based kidney stone identification. (arXiv:2206.00069v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00069","description":"<p>This contribution presents a deep-learning method for extracting and fusing\nimage information acquired from different viewpoints with the aim to produce\nmore discriminant object features. Our approach was specifically designed to\nmimic the morpho-constitutional analysis used by urologists to visually\nclassify kidney stones by inspecting the sections and surfaces of their\nfragments. Deep feature fusion strategies improved the results of single view\nextraction backbone models by more than 10\\% in terms of precision of the\nkidney stones classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villalvazo_Avila_E/0/1/0/all/0/1\">Elias Villalvazo-Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Tiro_F/0/1/0/all/0/1\">Francisco Lopez-Tiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_Araiza_D/0/1/0/all/0/1\">Daniel Flores-Araiza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Beze_J/0/1/0/all/0/1\">Jonathan El-Beze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubert_J/0/1/0/all/0/1\">Jacques Hubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FHIST: A Benchmark for Few-shot Classification of Histological Images. (arXiv:2206.00092v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00092","description":"<p>Few-shot learning has recently attracted wide interest in image\nclassification, but almost all the current public benchmarks are focused on\nnatural images. The few-shot paradigm is highly relevant in medical-imaging\napplications due to the scarcity of labeled data, as annotations are expensive\nand require specialized expertise. However, in medical imaging, few-shot\nlearning research is sparse, limited to private data sets and is at its early\nstage. In particular, the few-shot setting is of high interest in histology due\nto the diversity and fine granularity of cancer related tissue classification\ntasks, and the variety of data-preparation techniques. This paper introduces a\nhighly diversified public benchmark, gathered from various public datasets, for\nfew-shot histology data classification. We build few-shot tasks and\nbase-training data with various tissue types, different levels of domain shifts\nstemming from various cancer sites, and different class-granularity levels,\nthereby reflecting realistic scenarios. We evaluate the performances of\nstate-of-the-art few-shot learning methods on our benchmark, and observe that\nsimple fine-tuning and regularization methods achieve better results than the\npopular meta-learning and episodic-training paradigm. Furthermore, we introduce\nthree scenarios based on the domain shifts between the source and target\nhistology data: near-domain, middle-domain and out-domain. Our experiments\ndisplay the potential of few-shot learning in histology classification, with\nstate-of-art few shot learning methods approaching the supervised-learning\nbaselines in the near-domain setting. In our out-domain setting, for 5-way\n5-shot, the best performing method reaches 60% accuracy. We believe that our\nwork could help in building realistic evaluations and fair comparisons of\nfew-shot learning methods and will further encourage research in the few-shot\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_F/0/1/0/all/0/1\">Fereshteh Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Sina Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1\">Ivaxi Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaei_M/0/1/0/all/0/1\">Mohammad Havaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VALHALLA: Visual Hallucination for Machine Translation. (arXiv:2206.00100v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00100","description":"<p>Designing better machine translation systems by considering auxiliary inputs\nsuch as images has attracted much attention in recent years. While existing\nmethods show promising performance over the conventional text-only translation\nsystems, they typically require paired text and image as input during\ninference, which limits their applicability to real-world scenarios. In this\npaper, we introduce a visual hallucination framework, called VALHALLA, which\nrequires only source sentences at inference time and instead uses hallucinated\nvisual representations for multimodal machine translation. In particular, given\na source sentence an autoregressive hallucination transformer is used to\npredict a discrete visual representation from the input text, and the combined\ntext and hallucinated representations are utilized to obtain the target\ntranslation. We train the hallucination transformer jointly with the\ntranslation transformer using standard backpropagation with cross-entropy\nlosses while being guided by an additional loss that encourages consistency\nbetween predictions using either ground-truth or hallucinated visual\nrepresentations. Extensive experiments on three standard translation datasets\nwith a diverse set of language pairs demonstrate the effectiveness of our\napproach over both text-only baselines and state-of-the-art methods. Project\npage: <a href=\"http://www.svcl.ucsd.edu/projects/valhalla.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning pipeline for image classification on mobile phones. (arXiv:2206.00105v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00105","description":"<p>This article proposes and documents a machine-learning framework and tutorial\nfor classifying images using mobile phones. Compared to computers, the\nperformance of deep learning model performance degrades when deployed on a\nmobile phone and requires a systematic approach to find a model that performs\noptimally on both computers and mobile phones. By following the proposed\npipeline, which consists of various computational tools, simple procedural\nrecipes, and technical considerations, one can bring the power of deep learning\nmedical image classification to mobile devices, potentially unlocking new\ndomains of applications. The pipeline is demonstrated on four different\npublicly available datasets: COVID X-rays, COVID CT scans, leaves, and\ncolorectal cancer. We used two application development frameworks: TensorFlow\nLite (real-time testing) and Flutter (digital image testing) to test the\nproposed pipeline. We found that transferring deep learning models to a mobile\nphone is limited by hardware and classification accuracy drops. To address this\nissue, we proposed this pipeline to find an optimized model for mobile phones.\nFinally, we discuss additional applications and computational concerns related\nto deploying deep-learning models on phones, including real-time analysis and\nimage preprocessing. We believe the associated documentation and code can help\nphysicians and medical experts develop medical image classification\napplications for distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Muneeb_M/0/1/0/all/0/1\">Muhammad Muneeb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1\">Samuel F. Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henschel_A/0/1/0/all/0/1\">Andreas Henschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glo-In-One: Holistic Glomerular Detection, Segmentation, and Lesion Characterization with Large-scale Web Image Mining. (arXiv:2206.00123v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00123","description":"<p>The quantitative detection, segmentation, and characterization of glomeruli\nfrom high-resolution whole slide imaging (WSI) play essential roles in the\ncomputer-assisted diagnosis and scientific research in digital renal pathology.\nHistorically, such comprehensive quantification requires extensive programming\nskills in order to be able to handle heterogeneous and customized computational\ntools. To bridge the gap of performing glomerular quantification for\nnon-technical users, we develop the Glo-In-One toolkit to achieve holistic\nglomerular detection, segmentation, and characterization via a single line of\ncommand. Additionally, we release a large-scale collection of 30,000 unlabeled\nglomerular images to further facilitate the algorithmic development of\nself-supervised deep learning. The inputs of the Glo-In-One toolkit are WSIs,\nwhile the outputs are (1) WSI-level multi-class circle glomerular detection\nresults (which can be directly manipulated with ImageScope), (2) glomerular\nimage patches with segmentation masks, and (3) different lesion types. To\nleverage the performance of the Glo-In-One toolkit, we introduce\nself-supervised deep learning to glomerular quantification via large-scale web\nimage mining. The GGS fine-grained classification model achieved a decent\nperformance compared with baseline supervised methods while only using 10% of\nthe annotated data. The glomerular detection achieved an average precision of\n0.627 with circle representations, while the glomerular segmentation achieved a\n0.955 patch-wise Dice Similarity Coefficient (DSC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhe Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection. (arXiv:2206.00148v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00148","description":"<p>Over the past few years there has been major progress in the field of\nsynthetic data generation using simulation based techniques. These methods use\nhigh-end graphics engines and physics-based ray-tracing rendering in order to\nrepresent the world in 3D and create highly realistic images. Datagen has\nspecialized in the generation of high-quality 3D humans, realistic 3D\nenvironments and generation of realistic human motion. This technology has been\ndeveloped into a data generation platform which we used for these experiments.\nThis work demonstrates the use of synthetic photo-realistic in-cabin data to\ntrain a Driver Monitoring System that uses a lightweight neural network to\ndetect whether the driver's hands are on the wheel. We demonstrate that when\nonly a small amount of real data is available, synthetic data can be a simple\nway to boost performance. Moreover, we adopt the data-centric approach and show\nhow performing error analysis and generating the missing edge-cases in our\nplatform boosts performance. This showcases the ability of human-centric\nsynthetic data to generalize well to the real world, and help train algorithms\nin computer vision settings where data from the target domain is scarce or hard\nto collect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yudkin_P/0/1/0/all/0/1\">Paul Yudkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1\">Eli Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zvitia_O/0/1/0/all/0/1\">Orly Zvitia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbaz_G/0/1/0/all/0/1\">Gil Elbaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAGER: Progressive Attribute-Guided Extendable Robust Image Generation. (arXiv:2206.00162v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00162","description":"<p>This work presents a generative modeling approach based on successive\nsubspace learning (SSL). Unlike most generative models in the literature, our\nmethod does not utilize neural networks to analyze the underlying source\ndistribution and synthesize images. The resulting method, called the\nprogressive attribute-guided extendable robust image generative (PAGER) model,\nhas advantages in mathematical transparency, progressive content generation,\nlower training time, robust performance with fewer training samples, and\nextendibility to conditional image generation. PAGER consists of three modules:\ncore generator, resolution enhancer, and quality booster. The core generator\nlearns the distribution of low-resolution images and performs unconditional\nimage generation. The resolution enhancer increases image resolution via\nconditional generation. Finally, the quality booster adds finer details to\ngenerated images. Extensive experiments on MNIST, Fashion-MNIST, and CelebA\ndatasets are conducted to demonstrate generative performance of PAGER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_Z/0/1/0/all/0/1\">Zohreh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering the Hidden Vocabulary of DALLE-2. (arXiv:2206.00169v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00169","description":"<p>We discover that DALLE-2 seems to have a hidden vocabulary that can be used\nto generate images with absurd prompts. For example, it seems that\n\\texttt{Apoploe vesrreaitais} means birds and \\texttt{Contarra ccetnxniams\nluryca tanniounons} (sometimes) means bugs or pests. We find that these prompts\nare often consistent in isolation but also sometimes in combinations. We\npresent our black-box method to discover words that seem random but have some\ncorrespondence to visual concepts. This creates important security and\ninterpretability challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation. (arXiv:2206.00171v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00171","description":"<p>3D hand pose estimation (HPE) is the process of locating the joints of the\nhand in 3D from any visual input. HPE has recently received an increased amount\nof attention due to its key role in a variety of human-computer interaction\napplications. Recent HPE methods have demonstrated the advantages of employing\nvideos or multi-view images, allowing for more robust HPE systems. Accordingly,\nin this study, we propose a new method to perform Sequential learning with\nTransformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins\nby extracting visual embeddings from individual hand images. We then use a\ntransformer encoder to learn the sequential context along time or viewing\nangles and generate accurate 2D hand joint locations. Then, a graph\nconvolutional neural network with a U-Net configuration is used to convert the\n2D hand joint locations to 3D poses. Our experiments show that SeTHPose\nperforms well on both hand sequence varieties, temporal and angular. Also,\nSeTHPose outperforms other methods in the field to achieve new state-of-the-art\nresults on two public available sequential datasets, STB and MuViHand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khaleghi_L/0/1/0/all/0/1\">Leyla Khaleghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_J/0/1/0/all/0/1\">Joshua Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection. (arXiv:2206.00181v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00181","description":"<p>Training models dedicated to semantic segmentation requires a large amount of\npixel-wise annotated data. Due to their costly nature, these annotations might\nnot be available for the task at hand. To alleviate this problem, unsupervised\ndomain adaptation approaches aim at aligning the feature distributions between\nthe labeled source and the unlabeled target data. While these strategies lead\nto noticeable improvements, their effectiveness remains limited. To guide the\ndomain adaptation task more efficiently, previous works attempted to include\nhuman interactions in this process under the form of sparse single-pixel\nannotations in the target data. In this work, we propose a new domain\nadaptation framework for semantic segmentation with annotated points via active\nselection. First, we conduct an unsupervised domain adaptation of the model;\nfrom this adaptation, we use an entropy-based uncertainty measurement for\ntarget points selection. Finally, to minimize the domain gap, we propose a\ndomain adaptation framework utilizing these target points annotated by human\nannotators. Experimental results on benchmark datasets show the effectiveness\nof our methods against existing unsupervised domain adaptation approaches. The\npropose pipeline is generic and can be included as an extra module to existing\ndomain adaptation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Soft-Masked Attention. (arXiv:2206.00182v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00182","description":"<p>Transformers have become prevalent in computer vision due to their\nperformance and flexibility in modelling complex operations. Of particular\nsignificance is the 'cross-attention' operation, which allows a vector\nrepresentation (e.g. of an object in an image) to be learned by attending to an\narbitrarily sized set of input features. Recently, \"Masked Attention\" was\nproposed in which a given object representation only attends to those image\npixel features for which the segmentation mask of that object is active. This\nspecialization of attention proved beneficial for various image and video\nsegmentation tasks. In this paper, we propose another specialization of\nattention which enables attending over `soft-masks' (those with continuous mask\nprobabilities instead of binary values), and is also differentiable through\nthese mask probabilities, thus allowing the mask used for attention to be\nlearned within the network without requiring direct loss supervision. This can\nbe useful for several applications. Specifically, we employ our \"Differentiable\nSoft-Masked Attention\" for the task of Weakly-Supervised Video Object\nSegmentation (VOS), where we develop a transformer-based network for VOS which\nonly requires a single annotated image frame for training, but can also benefit\nfrom cycle consistency training on a video with just one annotated frame.\nAlthough there is no loss for masks in unlabeled frames, the network is still\nable to segment objects in those frames due to our novel attention formulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1\">Ali Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1\">Jonathon Luiten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1\">Alexander Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAFA: Class-Aware Feature Alignment for Test-Time Adaptation. (arXiv:2206.00205v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00205","description":"<p>Despite recent advancements in deep learning, deep networks still suffer from\nperformance degradation when they face new and different data from their\ntraining distributions. Addressing such a problem, test-time adaptation (TTA)\naims to adapt a model to unlabeled test data on test time while making\npredictions simultaneously. TTA applies to pretrained networks without\nmodifying their training procedures, which enables to utilize the already\nwell-formed source distribution for adaptation. One possible approach is to\nalign the representation space of test samples to the source distribution\n(\\textit{i.e.,} feature alignment). However, performing feature alignments in\nTTA is especially challenging in that the access to labeled source data is\nrestricted during adaptation. That is, a model does not have a chance to learn\ntest data in a class-discriminative manner, which was feasible in other\nadaptation tasks (\\textit{e.g.,} unsupervised domain adaptation) via supervised\nloss on the source data. Based on such an observation, this paper proposes\n\\emph{a simple yet effective} feature alignment loss, termed as Class-Aware\nFeature Alignment (CAFA), which 1) encourages a model to learn target\nrepresentations in a class-discriminative manner and 2) effectively mitigates\nthe distribution shifts in test time, simultaneously. Our method does not\nrequire any hyper-parameters or additional losses, which are required in the\nprevious approaches. We conduct extensive experiments and show our proposed\nmethod consistently outperforms existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nanhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection. (arXiv:2206.00214v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00214","description":"<p>The estimation of uncertainty in robotic vision, such as 3D object detection,\nis an essential component in developing safe autonomous systems aware of their\nown performance. However, the deployment of current uncertainty estimation\nmethods in 3D object detection remains challenging due to timing and\ncomputational constraints. To tackle this issue, we propose LiDAR-MIMO, an\nadaptation of the multi-input multi-output (MIMO) uncertainty estimation method\nto the LiDAR-based 3D object detection task. Our method modifies the original\nMIMO by performing multi-input at the feature level to ensure the detection,\nuncertainty estimation, and runtime performance benefits are retained despite\nthe limited capacity of the underlying detector and the large computational\ncosts of point cloud processing. We compare LiDAR-MIMO with MC dropout and\nensembles as baselines and show comparable uncertainty estimation results with\nonly a small number of output heads. Further, LiDAR-MIMO can be configured to\nbe twice as fast as MC dropout and ensembles, while achieving higher mAP than\nMC dropout and approaching that of ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pitropov_M/0/1/0/all/0/1\">Matthew Pitropov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelzad_V/0/1/0/all/0/1\">Vahdat Abdelzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Detection Transformer based on Spatial-aware and Semantic-aware Token Alignment. (arXiv:2206.00222v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00222","description":"<p>Detection transformers like DETR have recently shown promising performance on\nmany object detection tasks, but the generalization ability of those methods is\nstill quite challenging for cross-domain adaptation scenarios. To address the\ncross-domain issue, a straightforward way is to perform token alignment with\nadversarial training in transformers. However, its performance is often\nunsatisfactory as the tokens in detection transformers are quite diverse and\nrepresent different spatial and semantic information. In this paper, we propose\na new method called Spatial-aware and Semantic-aware Token Alignment (SSTA) for\ncross-domain detection transformers. In particular, we take advantage of the\ncharacteristics of cross-attention as used in detection transformer and propose\nthe spatial-aware token alignment (SpaTA) and the semantic-aware token\nalignment (SemTA) strategies to guide the token alignment across domains. For\nspatial-aware token alignment, we can extract the information from the\ncross-attention map (CAM) to align the distribution of tokens according to\ntheir attention to object queries. For semantic-aware token alignment, we\ninject the category information into the cross-attention map and construct\ndomain embedding to guide the learning of a multi-class discriminator so as to\nmodel the category relationship and achieve category-level token alignment\nduring the entire adaptation process. We conduct extensive experiments on\nseveral widely-used benchmarks, and the results clearly show the effectiveness\nof our proposed method over existing state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lixin Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views. (arXiv:2206.00227v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00227","description":"<p>A data augmentation module is utilized in contrastive learning to transform\nthe given data example into two views, which is considered essential and\nirreplaceable. However, the predetermined composition of multiple data\naugmentations brings two drawbacks. First, the artificial choice of\naugmentation types brings specific representational invariances to the model,\nwhich have different degrees of positive and negative effects on different\ndownstream tasks. Treating each type of augmentation equally during training\nmakes the model learn non-optimal representations for various downstream tasks\nand limits the flexibility to choose augmentation types beforehand. Second, the\nstrong data augmentations used in classic contrastive learning methods may\nbring too much invariance in some cases, and fine-grained information that is\nessential to some downstream tasks may be lost. This paper proposes a general\nmethod to alleviate these two problems by considering where and what to\ncontrast in a general contrastive learning framework. We first propose to learn\ndifferent augmentation invariances at different depths of the model according\nto the importance of each data augmentation instead of learning\nrepresentational invariances evenly in the backbone. We then propose to expand\nthe contrast content with augmentation embeddings to reduce the misleading\neffects of strong data augmentations. Experiments based on several baseline\nmethods demonstrate that we learn better representations for various benchmarks\non classification, detection, and segmentation downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Comparison between Efficient Attentions. (arXiv:2206.00244v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00244","description":"<p>Transformers have been successfully used in various fields and are becoming\nthe standard tools in computer vision. However, self-attention, a core\ncomponent of transformers, has a quadratic complexity problem, which limits the\nuse of transformers in various vision tasks that require dense prediction. Many\nstudies aiming at solving this problem have been reported proposed. However, no\ncomparative study of these methods using the same scale has been reported due\nto different model configurations, training schemes, and new methods. In our\npaper, we validate these efficient attention models on the ImageNet1K\nclassification task by changing only the attention operation and examining\nwhich efficient attention is better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jiuk Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chaehyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_S/0/1/0/all/0/1\">Soyoun Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Heechul Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00252","description":"<p>Identifying the type of kidney stones can allow urologists to determine their\nformation cause, improving the early prescription of appropriate treatments to\ndiminish future relapses. However, currently, the associated ex-vivo diagnosis\n(known as morpho-constitutional analysis, MCA) is time-consuming, expensive,\nand requires a great deal of experience, as it requires a visual analysis\ncomponent that is highly operator dependant. Recently, machine learning methods\nhave been developed for in-vivo endoscopic stone recognition. Shallow methods\nhave been demonstrated to be reliable and interpretable but exhibit low\naccuracy, while deep learning-based methods yield high accuracy but are not\nexplainable. However, high stake decisions require understandable\ncomputer-aided diagnosis (CAD) to suggest a course of action based on\nreasonable evidence, rather than merely prescribe one. Herein, we investigate\nmeans for learning part-prototypes (PPs) that enable interpretable models. Our\nproposal suggests a classification for a kidney stone patch image and provides\nexplanations in a similar way as those used on the MCA method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flores_Araiza_D/0/1/0/all/0/1\">Daniel Flores-Araiza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Tiro_F/0/1/0/all/0/1\">Francisco Lopez-Tiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villalvazo_Avila_E/0/1/0/all/0/1\">Elias Villalvazo-Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Beze_J/0/1/0/all/0/1\">Jonathan El-Beze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubert_J/0/1/0/all/0/1\">Jacques Hubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1\">Cristian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaGO-LOAM: Robust Ground-Optimized LiDAR Odometry. (arXiv:2206.00266v1 [cs.RO])","link":"http://arxiv.org/abs/2206.00266","description":"<p>Numerous researchers have conducted studies to achieve fast and robust\nground-optimized LiDAR odometry methods for terrestrial mobile platforms. In\nparticular, ground-optimized LiDAR odometry usually employs ground segmentation\nas a preprocessing method. This is because most of the points in a 3D point\ncloud captured by a 3D LiDAR sensor on a terrestrial platform are from the\nground. However, the effect of the performance of ground segmentation on LiDAR\nodometry is still not closely examined. In this paper, a robust\nground-optimized LiDAR odometry framework is proposed to facilitate the study\nto check the effect of ground segmentation on LiDAR SLAM based on the\nstate-of-the-art (SOTA) method. By using our proposed odometry framework, it is\neasy and straightforward to test whether ground segmentation algorithms help\nextract well-described features and thus improve SLAM performance. In addition,\nby leveraging the SOTA ground segmentation method called Patchwork, which shows\nrobust ground segmentation even in complex and uneven urban environments with\nlittle performance perturbation, a novel ground-optimized LiDAR odometry is\nproposed, called PaGO-LOAM. The methods were tested using the KITTI odometry\ndataset. \\textit{PaGO-LOAM} shows robust and accurate performance compared with\nthe baseline method. Our code is available at\nhttps://github.com/url-kaist/AlterGround-LeGO-LOAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dong-Uk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision GNN: An Image is Worth Graph of Nodes. (arXiv:2206.00272v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00272","description":"<p>Network architecture plays a key role in the deep learning-based computer\nvision system. The widely-used convolutional neural network and transformer\ntreat the image as a grid or sequence structure, which is not flexible to\ncapture irregular and complex objects. In this paper, we propose to represent\nthe image as a graph structure and introduce a new Vision GNN (ViG)\narchitecture to extract graph-level feature for visual tasks. We first split\nthe image to a number of patches which are viewed as nodes, and construct a\ngraph by connecting the nearest neighbors. Based on the graph representation of\nimages, we build our ViG model to transform and exchange information among all\nthe nodes. ViG consists of two basic modules: Grapher module with graph\nconvolution for aggregating and updating graph information, and FFN module with\ntwo linear layers for node feature transformation. Both isotropic and pyramid\narchitectures of ViG are built with different model sizes. Extensive\nexperiments on image recognition and object detection tasks demonstrate the\nsuperiority of our ViG architecture. We hope this pioneering study of GNN on\ngeneral visual tasks will provide useful inspiration and experience for future\nresearch. The PyTroch code will be available at\nhttps://github.com/huawei-noah/CV-Backbones and the MindSpore code will be\navaiable at https://gitee.com/mindspore/models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Teaching: Weakly Semi-Supervised Object Detection with Point Annotations. (arXiv:2206.00274v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00274","description":"<p>Point annotations are considerably more time-efficient than bounding box\nannotations. However, how to use cheap point annotations to boost the\nperformance of semi-supervised object detection remains largely unsolved. In\nthis work, we present Point-Teaching, a weakly semi-supervised object detection\nframework to fully exploit the point annotations. Specifically, we propose a\nHungarian-based point matching method to generate pseudo labels for point\nannotated images. We further propose multiple instance learning (MIL)\napproaches at the level of images and points to supervise the object detector\nwith point annotations. Finally, we propose a simple-yet-effective data\naugmentation, termed point-guided copy-paste, to reduce the impact of the\nunmatched points. Experiments demonstrate the effectiveness of our method on a\nfew datasets and various data regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongtao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Bounding Box Annotation with Small Training Data Sets for Industrial Manufacturing. (arXiv:2206.00280v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00280","description":"<p>In the past few years, object detection has attracted a lot of attention in\nthe context of human-robot collaboration and Industry 5.0 due to enormous\nquality improvements in deep learning technologies. In many applications,\nobject detection models have to be able to quickly adapt to a changing\nenvironment, i.e., to learn new objects. A crucial but challenging prerequisite\nfor this is the automatic generation of new training data which currently still\nlimits the broad application of object detection methods in industrial\nmanufacturing. In this work, we discuss how to adapt state-of-the-art object\ndetection methods for the task of automatic bounding box annotation for the use\ncase where the background is homogeneous and the object's label is provided by\na human. We compare an adapted version of Faster R-CNN and the Scaled Yolov4-p5\narchitecture and show that both can be trained to distinguish unknown objects\nfrom a complex but homogeneous background using only a small amount of training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiss_M/0/1/0/all/0/1\">Manuela Gei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_R/0/1/0/all/0/1\">Raphael Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baresch_M/0/1/0/all/0/1\">Martin Baresch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_J/0/1/0/all/0/1\">Josef Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwick_M/0/1/0/all/0/1\">Michael Zwick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Needle In A Haystack, Fast: Benchmarking Image Perceptual Similarity Metrics At Scale. (arXiv:2206.00282v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00282","description":"<p>The advent of the internet, followed shortly by the social media made it\nubiquitous in consuming and sharing information between anyone with access to\nit. The evolution in the consumption of media driven by this change, led to the\nemergence of images as means to express oneself, convey information and\nconvince others efficiently. With computer vision algorithms progressing\nradically over the last decade, it is become easier and easier to study at\nscale the role of images in the flow of information online. While the research\nquestions and overall pipelines differ radically, almost all start with a\ncrucial first step - evaluation of global perceptual similarity between\ndifferent images. That initial step is crucial for overall pipeline performance\nand processes most images. A number of algorithms are available and currently\nused to perform it, but so far no comprehensive review was available to guide\nthe choice of researchers as to the choice of an algorithm best suited to their\nquestion, assumptions and computational resources. With this paper we aim to\nfill this gap, showing that classical computer vision methods are not\nnecessarily the best approach, whereas a pair of relatively little used methods\n- Dhash perceptual hash and SimCLR v2 ResNets achieve excellent performance,\nscale well and are computationally efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vallez_C/0/1/0/all/0/1\">Cyril Vallez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Purpose Cross-Attention Based Image Alignment Block for Edge Devices. (arXiv:2206.00291v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00291","description":"<p>Image alignment, also known as image registration, is a critical block used\nin many computer vision problems. One of the key factors in alignment is\nefficiency, as inefficient aligners can cause significant overhead to the\noverall problem. In the literature, there are some blocks that appear to do the\nalignment operation, although most do not focus on efficiency. Therefore, an\nimage alignment block which can both work in time and/or space and can work on\nedge devices would be beneficial for almost all networks dealing with multiple\nimages. Given its wide usage and importance, we propose an efficient,\ncross-attention-based, multi-purpose image alignment block (XABA) suitable to\nwork within edge devices. Using cross-attention, we exploit the relationships\nbetween features extracted from images. To make cross-attention feasible for\nreal-time image alignment problems and handle large motions, we provide a\npyramidal block based cross-attention scheme. This also captures local\nrelationships besides reducing memory requirements and number of operations.\nEfficient XABA models achieve real-time requirements of running above 20 FPS\nperformance on NVIDIA Jetson Xavier with 30W power consumption compared to\nother powerful computers. Used as a sub-block in a larger network, XABA also\nimproves multi-image super-resolution network performance in comparison to\nother alignment methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bilecen_B/0/1/0/all/0/1\">Bahri Batuhan Bilecen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisne_A/0/1/0/all/0/1\">Alparslan Fisne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayazoglu_M/0/1/0/all/0/1\">Mustafa Ayazoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Denoising of Diffusion-Weighted Magnetic Resonance Images Using a Convolutional Neural Network and Transfer Learning. (arXiv:2206.00305v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00305","description":"<p>In this paper, we propose a method for denoising diffusion-weighted images\n(DWI) of the brain using a convolutional neural network trained on realistic,\nsynthetic MR data. We compare our results to averaging of repeated scans, a\nwidespread method used in clinics to improve signal-to-noise ratio of MR\nimages. To obtain training data for transfer learning, we model, in a\ndata-driven fashion, the effects of echo-planar imaging (EPI): Nyquist ghosting\nand ramp sampling. We introduce these effects to the digital phantom of brain\nanatomy (BrainWeb). Instead of simulating pseudo-random noise with a defined\nprobability distribution, we perform noise scans with a brain-DWI-designed\nprotocol to obtain realistic noise maps. We combine them with the simulated,\nnoise-free EPI images. We also measure the Point Spread Function in a DW image\nof an AJR-approved geometrical phantom and inter-scan movement in a brain scan\nof a healthy volunteer. Their influence on image denoising and averaging of\nrepeated images is investigated at different signal-to-noise ratio levels.\nDenoising performance is evaluated quantitatively using the simulated EPI\nimages and qualitatively in real EPI DWI of the brain. We show that the\napplication of our method allows for a significant reduction in scan time by\nlowering the number of repeated scans. Visual comparisons made in the acquired\nbrain images indicate that the denoised single-repetition images are less noisy\nthan multi-repetition averaged images. We also analyse the convolutional neural\nnetwork denoiser and point out the challenges accompanying this denoising\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jurek_J/0/1/0/all/0/1\">Jakub Jurek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Materka_A/0/1/0/all/0/1\">Andrzej Materka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ludwisiak_K/0/1/0/all/0/1\">Kamil Ludwisiak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majos_A/0/1/0/all/0/1\">Agata Majos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorczewski_K/0/1/0/all/0/1\">Kamil Gorczewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cepuch_K/0/1/0/all/0/1\">Kamil Cepuch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zawadzka_A/0/1/0/all/0/1\">Agata Zawadzka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Efficient Online Continual Object Detection in Streaming Video. (arXiv:2206.00309v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00309","description":"<p>To thrive in evolving environments, humans are capable of continual\nacquisition and transfer of new knowledge, from a continuous video stream, with\nminimal supervisions, while retaining previously learnt experiences. In\ncontrast to human learning, most standard continual learning benchmarks focus\non learning from static iid images in fully supervised settings. Here, we\nexamine a more realistic and challenging\nproblem$\\unicode{x2014}$Label-Efficient Online Continual Object Detection\n(LEOCOD) in video streams. By addressing this problem, it would greatly benefit\nmany real-world applications with reduced annotation costs and retraining time.\nTo tackle this problem, we seek inspirations from complementary learning\nsystems (CLS) in human brains and propose a computational model, dubbed as\nEfficient-CLS. Functionally correlated with the hippocampus and the neocortex\nin CLS, Efficient-CLS posits a memory encoding mechanism involving\nbidirectional interaction between fast and slow learners via synaptic weight\ntransfers and pattern replays. We test Efficient-CLS and competitive baselines\nin two challenging real-world video stream datasets. Like humans, Efficient-CLS\nlearns to detect new object classes incrementally from a continuous temporal\nstream of non-repeating video with minimal forgetting. Remarkably, with only\n25% annotated video frames, our Efficient-CLS still leads among all comparative\nmodels, which are trained with 100% annotations on all video frames. The data\nand source code will be publicly available at\nhttps://github.com/showlab/Efficient-CLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jay Zhangjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining. (arXiv:2206.00311v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00311","description":"<p>In this paper, we present a model pretraining technique, named MaskOCR, for\ntext recognition. Our text recognition architecture is an encoder-decoder\ntransformer: the encoder extracts the patch-level representations, and the\ndecoder recognizes the text from the representations. Our approach pretrains\nboth the encoder and the decoder in a sequential manner. (i) We pretrain the\nencoder in a self-supervised manner over a large set of unlabeled real text\nimages. We adopt the masked image modeling approach, which shows the\neffectiveness for general images, expecting that the representations take on\nsemantics. (ii) We pretrain the decoder over a large set of synthesized text\nimages in a supervised manner and enhance the language modeling capability of\nthe decoder by randomly masking some text image patches occupied by characters\ninput to the encoder and accordingly the representations input to the decoder.\nExperiments show that the proposed MaskOCR approach achieves superior results\non the benchmark datasets, including Chinese and English text images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pengyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shanshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Meina Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangliu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection. (arXiv:2206.00338v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00338","description":"<p>Cell detection in microscopy images is important to study how cells move and\ninteract with their environment. Most recent deep learning-based methods for\ncell detection use convolutional neural networks (CNNs). However, inspired by\nthe success in other computer vision applications, vision transformers (ViTs)\nare also used for this purpose. We propose a novel hybrid CNN-ViT model for\ncell detection in microscopy images to exploit the advantages of both types of\ndeep learning models. We employ an efficient CNN, that was pre-trained on the\nImageNet dataset, to extract image features and utilize transfer learning to\nreduce the amount of required training data. Extracted image features are\nfurther processed by a combination of convolutional and transformer layers, so\nthat the convolutional layers can focus on local information and the\ntransformer layers on global information. Our centroid-based cell detection\nmethod represents cells as ellipses and is end-to-end trainable. Furthermore,\nwe show that our proposed model can outperform a fully convolutional baseline\nmodel on four different 2D microscopy datasets. Code is available at:\nhttps://github.com/roydenwa/cell-centroid-former\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_R/0/1/0/all/0/1\">Royden Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohr_K/0/1/0/all/0/1\">Karl Rohr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards view-invariant vehicle speed detection from driving simulator images. (arXiv:2206.00343v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00343","description":"<p>The use of cameras for vehicle speed measurement is much more cost effective\ncompared to other technologies such as inductive loops, radar or laser.\nHowever, accurate speed measurement remains a challenge due to the inherent\nlimitations of cameras to provide accurate range estimates. In addition,\nclassical vision-based methods are very sensitive to extrinsic calibration\nbetween the camera and the road. In this context, the use of data-driven\napproaches appears as an interesting alternative. However, data collection\nrequires a complex and costly setup to record videos under real traffic\nconditions from the camera synchronized with a high-precision speed sensor to\ngenerate the ground truth speed values. It has recently been demonstrated that\nthe use of driving simulators (e.g., CARLA) can serve as a robust alternative\nfor generating large synthetic datasets to enable the application of deep\nlearning techniques for vehicle speed estimation for a single camera. In this\npaper, we study the same problem using multiple cameras in different virtual\nlocations and with different extrinsic parameters. We address the question of\nwhether complex 3D-CNN architectures are capable of implicitly learning\nview-invariant speeds using a single model, or whether view-specific models are\nmore appropriate. The results are very promising as they show that a single\nmodel with data from multiple views reports even better accuracy than\ncamera-specific models, paving the way towards a view-invariant vehicle speed\nmeasurement system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Antonio Hern&#xe1;ndez Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llorca_D/0/1/0/all/0/1\">David Fernandez Llorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_I/0/1/0/all/0/1\">Iv&#xe1;n Garc&#xed;a Daza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning as a Means To Reduce the Need for Labeled Data in Medical Image Analysis. (arXiv:2206.00344v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00344","description":"<p>One of the largest problems in medical image processing is the lack of\nannotated data. Labeling medical images often requires highly trained experts\nand can be a time-consuming process. In this paper, we evaluate a method of\nreducing the need for labeled data in medical image object detection by using\nself-supervised neural network pretraining. We use a dataset of chest X-ray\nimages with bounding box labels for 13 different classes of anomalies. The\nnetworks are pretrained on a percentage of the dataset without labels and then\nfine-tuned on the rest of the dataset. We show that it is possible to achieve\nsimilar performance to a fully supervised model in terms of mean average\nprecision and accuracy with only 60\\% of the labeled data. We also show that it\nis possible to increase the maximum performance of a fully-supervised model by\nadding a self-supervised pretraining step, and this effect can be observed with\neven a small amount of unlabeled data for pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bencevic_M/0/1/0/all/0/1\">Marin Ben&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habijan_M/0/1/0/all/0/1\">Marija Habijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galic_I/0/1/0/all/0/1\">Irena Gali&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizurica_A/0/1/0/all/0/1\">Aleksandra Pizurica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning for Skin Lesion Segmentation. (arXiv:2206.00356v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00356","description":"<p>Skin cancer is a major public health problem that could benefit from\ncomputer-aided diagnosis to reduce the burden of this common disease. Skin\nlesion segmentation from images is an important step toward achieving this\ngoal. However, the presence of natural and artificial artifacts (e.g., hair and\nair bubbles), intrinsic factors (e.g., lesion shape and contrast), and\nvariations in image acquisition conditions make skin lesion segmentation a\nchallenging task. Recently, various researchers have explored the applicability\nof deep learning models to skin lesion segmentation. In this survey, we\ncross-examine 134 research papers that deal with deep learning based\nsegmentation of skin lesions. We analyze these works along several dimensions,\nincluding input data (datasets, preprocessing, and synthetic data generation),\nmodel design (architecture, modules, and losses), and evaluation aspects (data\nannotation requirements and segmentation performance). We discuss these\ndimensions both from the viewpoint of select seminal works, and from a\nsystematic viewpoint, examining how those choices have influenced current\ntrends, and how their limitations should be addressed. We summarize all\nexamined works in a comprehensive table to facilitate comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mirikharaji_Z/0/1/0/all/0/1\">Zahra Mirikharaji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barata_C/0/1/0/all/0/1\">Catarina Barata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abhishek_K/0/1/0/all/0/1\">Kumar Abhishek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bissoto_A/0/1/0/all/0/1\">Alceu Bissoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Celebi_M/0/1/0/all/0/1\">M. Emre Celebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks. (arXiv:2206.00359v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00359","description":"<p>Deep clustering has recently emerged as a promising technique for complex\nimage clustering. Despite the significant progress, previous deep clustering\nworks mostly tend to construct the final clustering by utilizing a single layer\nof representation, e.g., by performing $K$-means on the last fully-connected\nlayer or by associating some clustering loss to a specific layer. However, few\nof them have considered the possibilities and potential benefits of jointly\nleveraging multi-layer representations for enhancing the deep clustering\nperformance. In light of this, this paper presents a Deep Clustering via\nEnsembles (DeepCluE) approach, which bridges the gap between deep clustering\nand ensemble clustering by harnessing the power of multiple layers in deep\nneural networks. Particularly, we utilize a weight-sharing convolutional neural\nnetwork as the backbone, which is trained with both the instance-level\ncontrastive learning (via an instance projector) and the cluster-level\ncontrastive learning (via a cluster projector) in an unsupervised manner.\nThereafter, multiple layers of feature representations are extracted from the\ntrained network, upon which a set of diversified base clusterings can be\ngenerated via a highly efficient clusterer. Then, the reliability of the\nclusters in multiple base clusterings is automatically estimated by exploiting\nan entropy-based criterion, based on which the multiple base clusterings are\nfurther formulated into a weighted-cluster bipartite graph. By partitioning\nthis bipartite graph via transfer cut, the final image clustering result can\ntherefore be obtained. Experimental results on six image datasets confirm the\nadvantages of our DeepCluE approach over the state-of-the-art deep clustering\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Ding-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangji Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elucidating the Design Space of Diffusion-Based Generative Models. (arXiv:2206.00364v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00364","description":"<p>We argue that the theory and practice of diffusion-based generative models\nare currently unnecessarily convoluted and seek to remedy the situation by\npresenting a design space that clearly separates the concrete design choices.\nThis lets us identify several changes to both the sampling and training\nprocesses, as well as preconditioning of the score networks. Together, our\nimprovements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a\nclass-conditional setting and 1.97 in an unconditional setting, with much\nfaster sampling (35 network evaluations per image) than prior designs. To\nfurther demonstrate their modular nature, we show that our design changes\ndramatically improve both the efficiency and quality obtainable with\npre-trained score networks from previous work, including improving the FID of\nan existing ImageNet-64 model from 2.07 to near-SOTA 1.55.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strongly Augmented Contrastive Clustering. (arXiv:2206.00380v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00380","description":"<p>Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed strongly augmented contrastive clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the superior\nperformance of the proposed SACC approach over the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaozhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Ding-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalized Supervised Contrastive Learning Framework. (arXiv:2206.00384v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00384","description":"<p>Based on recent remarkable achievements of contrastive learning in\nself-supervised representation learning, supervised contrastive learning\n(SupCon) has successfully extended the batch contrastive approaches to the\nsupervised context and outperformed cross-entropy on various datasets on\nResNet. In this work, we present GenSCL: a generalized supervised contrastive\nlearning framework that seamlessly adapts modern image-based regularizations\n(such as Mixup-Cutmix) and knowledge distillation (KD) to SupCon by our\ngeneralized supervised contrastive loss. Generalized supervised contrastive\nloss is a further extension of supervised contrastive loss measuring\ncross-entropy between the similarity of labels and that of latent features.\nThen a model can learn to what extent contrastives should be pulled closer to\nan anchor in the latent space. By explicitly and fully leveraging label\ninformation, GenSCL breaks the boundary between conventional positives and\nnegatives, and any kind of pre-trained teacher classifier can be utilized.\nResNet-50 trained in GenSCL with Mixup-Cutmix and KD achieves state-of-the-art\naccuracies of 97.6% and 84.7% on CIFAR10 and CIFAR100 without external data,\nwhich significantly improves the results reported in the original SupCon (1.6%\nand 8.2%, respectively). Pytorch implementation is available at\nhttps://t.ly/yuUO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jooyoung Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Min Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder. (arXiv:2206.00386v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00386","description":"<p>Recently most successful image synthesis models are multi stage process to\ncombine the advantages of different methods, which always includes a VAE-like\nmodel for faithfully reconstructing embedding to image and a prior model to\ngenerate image embedding. At the same time, diffusion models have shown be\ncapacity to generate high-quality synthetic images. Our work proposes a VQ-VAE\narchitecture model with a diffusion decoder (DiVAE) to work as the\nreconstructing component in image synthesis. We explore how to input image\nembedding into diffusion model for excellent performance and find that simple\nmodification on diffusion's UNet can achieve it. Training on ImageNet, Our\nmodel achieves state-of-the-art results and generates more photorealistic\nimages specifically. In addition, we apply the DiVAE with an Auto-regressive\ngenerator on conditional synthesis tasks to perform more human-feeling and\ndetailed samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative study between vision transformers and CNNs in digital pathology. (arXiv:2206.00389v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00389","description":"<p>Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficient amounts of data. In\ncomparison to convolutional neural networks, vision transformers have a weaker\ninductive bias and therefore allow a more flexible feature detection. Due to\ntheir promising feature detection, this work explores vision transformers for\ntumor detection in digital pathology whole slide images in four tissue types,\nand for tissue type identification. We compared the patch-wise classification\nperformance of the vision transformer DeiT-Tiny to the state-of-the-art\nconvolutional neural network ResNet18. Due to the sparse availability of\nannotated whole slide images, we further compared both models pretrained on\nlarge amounts of unlabeled whole-slide images using state-of-the-art\nself-supervised approaches. The results show that the vision transformer\nperformed slightly better than the ResNet18 for three of four tissue types for\ntumor detection while the ResNet18 performed slightly better for the remaining\ntasks. The aggregated predictions of both models on slide level were\ncorrelated, indicating that the models captured similar imaging features. All\ntogether, the vision transformer models performed on par with the ResNet18\nwhile requiring more effort to train. In order to surpass the performance of\nconvolutional neural networks, vision transformers might require more\nchallenging tasks to benefit from their weak inductive bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deininger_L/0/1/0/all/0/1\">Luca Deininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stimpel_B/0/1/0/all/0/1\">Bernhard Stimpel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuce_A/0/1/0/all/0/1\">Anil Yuce</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_Sureshjani_S/0/1/0/all/0/1\">Samaneh Abbasi-Sureshjani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonenberger_S/0/1/0/all/0/1\">Simon Sch&#xf6;nenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ocampo_P/0/1/0/all/0/1\">Paolo Ocampo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korski_K/0/1/0/all/0/1\">Konstanty Korski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaire_F/0/1/0/all/0/1\">Fabien Gaire</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalisable Audio Representations for Audio-Visual Navigation. (arXiv:2206.00393v1 [cs.SD])","link":"http://arxiv.org/abs/2206.00393","description":"<p>In audio-visual navigation (AVN), an intelligent agent needs to navigate to a\nconstantly sound-making object in complex 3D environments based on its audio\nand visual perceptions. While existing methods attempt to improve the\nnavigation performance with preciously designed path planning or intricate task\nsettings, none has improved the model generalisation on unheard sounds with\ntask settings unchanged. We thus propose a contrastive learning-based method to\ntackle this challenge by regularising the audio encoder, where the\nsound-agnostic goal-driven latent representations can be learnt from various\naudio signals of different classes. In addition, we consider two data\naugmentation strategies to enrich the training sounds. We demonstrate that our\ndesigns can be easily equipped to existing AVN frameworks to obtain an\nimmediate performance gain (13.4%$\\uparrow$ in SPL on Replica and\n12.2%$\\uparrow$ in SPL on MP3D). Our project is available at\nhttps://AV-GeN.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shunqi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00415","description":"<p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nusing knowledge learned from seen attribute-object compositions in the training\nset. Previous works mainly project an image and a composition into a common\nembedding space to measure their compatibility score. However, both attributes\nand objects share the visual representations learned above, leading the model\nto exploit spurious correlations and bias towards seen pairs. Instead, we\nreconsider CZSL as an out-of-distribution generalization problem. If an object\nis treated as a domain, we can learn object-invariant features to recognize the\nattributes attached to any object reliably. Similarly, attribute-invariant\nfeatures can also be learned when recognizing the objects with attributes as\ndomains. Specifically, we propose an invariant feature learning framework to\nalign different domains at the representation and gradient levels to capture\nthe intrinsic characteristics associated with the tasks. Experiments on two\nCZSL benchmarks demonstrate that the proposed method significantly outperforms\nthe previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kongming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Gaussian Grasp Maps for Generative Grasping Models. (arXiv:2206.00432v1 [cs.RO])","link":"http://arxiv.org/abs/2206.00432","description":"<p>Generalising robotic grasping to previously unseen objects is a key task in\ngeneral robotic manipulation. The current method for training many antipodal\ngenerative grasping models rely on a binary ground truth grasp map generated\nfrom the centre thirds of correctly labelled grasp rectangles. However, these\nbinary maps do not accurately reflect the positions in which a robotic arm can\ncorrectly grasp a given object. We propose a continuous Gaussian representation\nof annotated grasps to generate ground truth training data which achieves a\nhigher success rate on a simulated robotic grasping benchmark. Three modern\ngenerative grasping networks are trained with either binary or Gaussian grasp\nmaps, along with recent advancements from the robotic grasping literature, such\nas discretisation of grasp angles into bins and an attentional loss function.\nDespite negligible difference according to the standard rectangle metric,\nGaussian maps better reproduce the training data and therefore improve success\nrates when tested on the same simulated robot arm by avoiding collisions with\nthe object: achieving 87.94\\% accuracy. Furthermore, the best performing model\nis shown to operate with a high success rate when transferred to a real robotic\narm, at high inference speeds, without the need for transfer learning. The\nsystem is then shown to be capable of performing grasps on an antagonistic\nphysical object dataset benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prew_W/0/1/0/all/0/1\">William Prew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordewich_M/0/1/0/all/0/1\">Magnus Bordewich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beierholm_U/0/1/0/all/0/1\">Ulrik Beierholm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CD$^2$: Fine-grained 3D Mesh Reconstruction with Twice Chamfer Distance. (arXiv:2206.00447v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00447","description":"<p>Monocular 3D reconstruction is to reconstruct the shape of object and its\nother detailed information from a single RGB image. In 3D reconstruction,\npolygon mesh is the most prevalent expression form obtained from deep learning\nmodels, with detailed surface information and low computational cost. However,\nsome state-of-the-art works fail to generate well-structured meshes, these\nmeshes have two severe problems which we call Vertices Clustering and Illegal\nTwist. By delving into the mesh deformation procedure, we pinpoint the\ninadequate usage of Chamfer Distance(CD) metric in deep learning model. In this\npaper, we initially demonstrate the problems resulting from CD with visual\nexamples and quantitative analyses. To solve these problems, we propose a\nfine-grained reconstruction method CD$^2$ with Chamfer distance adopted twice\nto perform a plausible and adaptive deformation. Extensive experiments on two\n3D datasets and the comparison of our newly proposed mesh quality metrics\ndemonstrate that our CD$^2$ outperforms others by generating better-structured\nmeshes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1\">Rongfei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1\">Mai Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A robust and lightweight deep attention multiple instance learning algorithm for predicting genetic alterations. (arXiv:2206.00455v1 [q-bio.QM])","link":"http://arxiv.org/abs/2206.00455","description":"<p>Deep-learning models based on whole-slide digital pathology images (WSIs)\nbecome increasingly popular for predicting molecular biomarkers. Instance-based\nmodels has been the mainstream strategy for predicting genetic alterations\nusing WSIs although bag-based models along with self-attention mechanism-based\nalgorithms have been proposed for other digital pathology applications. In this\npaper, we proposed a novel Attention-based Multiple Instance Mutation Learning\n(AMIML) model for predicting gene mutations. AMIML was comprised of successive\n1-D convolutional layers, a decoder, and a residual weight connection to\nfacilitate further integration of a lightweight attention mechanism to detect\nthe most predictive image patches. Using data for 24 clinically relevant genes\nfrom four cancer cohorts in The Cancer Genome Atlas (TCGA) studies (UCEC, BRCA,\nGBM and KIRC), we compared AMIML with one popular instance-based model and four\nrecently published bag-based models (e.g., CHOWDER, HE2RNA, etc.). AMIML\ndemonstrated excellent robustness, not only outperforming all the five baseline\nalgorithms in the vast majority of the tested genes (17 out of 24), but also\nproviding near-best-performance for the other seven genes. Conversely, the\nperformance of the baseline published algorithms varied across different\ncancers/genes. In addition, compared to the published models for genetic\nalterations, AMIML provided a significant improvement for predicting a wide\nrange of genes (e.g., KMT2C, TP53, and SETD2 for KIRC; ERBB2, BRCA1, and BRCA2\nfor BRCA; JAK1, POLE, and MTOR for UCEC) as well as produced outstanding\npredictive models for other clinically relevant gene mutations, which have not\nbeen reported in the current literature. Furthermore, with the flexible and\ninterpretable attention-based MIL pooling mechanism, AMIML could further\nzero-in and detect predictive image patches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_B/0/1/0/all/0/1\">Bangwei Guo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_M/0/1/0/all/0/1\">Miaomiao Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_X/0/1/0/all/0/1\">Xu Steven Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation. (arXiv:2206.00468v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00468","description":"<p>This paper presents a unified framework for depth-aware panoptic segmentation\n(DPS), which aims to reconstruct 3D scene with instance-level semantics from\none single image. Prior works address this problem by simply adding a dense\ndepth regression head to panoptic segmentation (PS) networks, resulting in two\nindependent task branches. This neglects the mutually-beneficial relations\nbetween these two tasks, thus failing to exploit handy instance-level semantic\ncues to boost depth accuracy while also producing sub-optimal depth maps. To\novercome these limitations, we propose a unified framework for the DPS task by\napplying a dynamic convolution technique to both the PS and depth prediction\ntasks. Specifically, instead of predicting depth for all pixels at a time, we\ngenerate instance-specific kernels to predict depth and segmentation masks for\neach instance. Moreover, leveraging the instance-wise depth estimation scheme,\nwe add additional instance-level depth cues to assist with supervising the\ndepth learning via a new depth loss. Extensive experiments on Cityscapes-DPS\nand SemKITTI-DPS show the effectiveness and promise of our method. We hope our\nunified solution to DPS can lead a new paradigm in this area. Code is available\nat https://github.com/NaiyuGao/PanopticDepth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Naiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jian Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yanhu Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Principal Component Learning: Modeling Similarity by Augmentation Overlap. (arXiv:2206.00471v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00471","description":"<p>Traditional self-supervised contrastive learning methods learn embeddings by\npulling views of the same sample together and pushing views of different\nsamples away. Since views of a sample are usually generated via data\naugmentations, the semantic relationship between samples is ignored. Based on\nthe observation that semantically similar samples are more likely to have\nsimilar augmentations, we propose to measure similarity via the distribution of\naugmentations, i.e., how much the augmentations of two samples overlap. To\nhandle the dimensional and computational complexity, we propose a novel\nContrastive Principal Component Learning (CPCL) method composed of a\ncontrastive-like loss and an on-the-fly projection loss to efficiently perform\nPCA on the augmentation feature, which encodes the augmentation distribution.\nBy CPCL, the learned low-dimensional embeddings theoretically preserve the\nsimilarity of augmentation distribution between samples. Empirical results show\nour method can achieve competitive results against various traditional\ncontrastive learning methods on different benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer. (arXiv:2206.00481v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00481","description":"<p>Vision Transformers (ViTs) enabled the use of transformer architecture on\nvision tasks showing impressive performances when trained on big datasets.\nHowever, on relatively small datasets, ViTs are less accurate given their lack\nof inductive bias. To this end, we propose a simple but still effective\nself-supervised learning (SSL) strategy to train ViTs, that without any\nexternal annotation, can significantly improve the results. Specifically, we\ndefine a set of SSL tasks based on relations of image patches that the model\nhas to solve before or jointly during the downstream training. Differently from\nViT, our RelViT model optimizes all the output tokens of the transformer\nencoder that are related to the image patches, thus exploiting more training\nsignal at each training step. We investigated our proposed methods on several\nimage benchmarks finding that RelViT improves the SSL state-of-the-art methods\nby a large margin, especially on small datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izzo_E/0/1/0/all/0/1\">Elena Izzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attack-Agnostic Adversarial Detection. (arXiv:2206.00489v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00489","description":"<p>The growing number of adversarial attacks in recent years gives attackers an\nadvantage over defenders, as defenders must train detectors after knowing the\ntypes of attacks, and many models need to be maintained to ensure good\nperformance in detecting any upcoming attacks. We propose a way to end the\ntug-of-war between attackers and defenders by treating adversarial attack\ndetection as an anomaly detection problem so that the detector is agnostic to\nthe attack. We quantify the statistical deviation caused by adversarial\nperturbations in two aspects. The Least Significant Component Feature (LSCF)\nquantifies the deviation of adversarial examples from the statistics of benign\nsamples and Hessian Feature (HF) reflects how adversarial examples distort the\nlandscape of the model's optima by measuring the local loss curvature.\nEmpirical results show that our method can achieve an overall ROC AUC of 94.9%,\n89.7%, and 94.6% on CIFAR10, CIFAR100, and SVHN, respectively, and has\ncomparable performance to adversarial detectors trained with adversarial\nexamples on most of the attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiaxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_M/0/1/0/all/0/1\">Mohamed Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billa_J/0/1/0/all/0/1\">Jay Billa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1\">Wael AbdAlmageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Room Wireframe Detection from a Single View. (arXiv:2206.00491v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00491","description":"<p>Reconstruction of indoor surfaces with limited texture information or with\nrepeated textures, a situation common in walls and ceilings, may be difficult\nwith a monocular Structure from Motion system. We propose a Semantic Room\nWireframe Detection task to predict a Semantic Wireframe from a single\nperspective image. Such predictions may be used with shape priors to estimate\nthe Room Layout and aid reconstruction. To train and test the proposed\nalgorithm we create a new set of annotations from the simulated Structured3D\ndataset. We show qualitatively that the SRW-Net handles complex room geometries\nbetter than previous Room Layout Estimation algorithms while quantitatively\nout-performing the baseline in non-semantic Wireframe Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gillsjo_D/0/1/0/all/0/1\">David Gillsj&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flood_G/0/1/0/all/0/1\">Gabrielle Flood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dstrom_K/0/1/0/all/0/1\">Kalle &#xc5;str&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximally Sensitive Error for Anomaly Detection and Feature Learning. (arXiv:2206.00506v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00506","description":"<p>Mean squared error (MSE) is one of the most widely used metrics to expression\ndifferences between multi-dimensional entities, including images. However, MSE\nis not locally sensitive as it does not take into account the spatial\narrangement of the (pixel) differences, which matters for structured data types\nlike images. Such spatial arrangements carry information about the source of\nthe differences; therefore, an error function that also incorporates the\nlocation of errors can lead to a more meaningful distance measure. We introduce\nProximally Sensitive Error (PSE), through which we suggest that a regional\nemphasis in the error measure can 'highlight' semantic differences between\nimages over syntactic/random deviations. We demonstrate that this emphasis can\nbe leveraged upon for the task of anomaly/occlusion detection. We further\nexplore its utility as a loss function to help a model focus on learning\nrepresentations of semantic objects instead of minimizing syntactic\nreconstruction noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gudi_A/0/1/0/all/0/1\">Amogh Gudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttner_F/0/1/0/all/0/1\">Fritjof B&#xfc;ttner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection. (arXiv:2206.00515v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00515","description":"<p>This study introduces \\textit{Landslide4Sense}, a reference benchmark for\nlandslide detection from remote sensing. The repository features 3,799 image\npatches fusing optical layers from Sentinel-2 sensors with the digital\nelevation model and slope layer derived from ALOS PALSAR. The added\ntopographical information facilitates an accurate detection of landslide\nborders, which recent researches have shown to be challenging using optical\ndata alone. The extensive data set supports deep learning (DL) studies in\nlandslide detection and the development and validation of methods for the\nsystematic update of landslide inventories. The benchmark data set has been\ncollected at four different times and geographical locations: Iburi (September\n2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009).\nEach image pixel is labelled as belonging to a landslide or not, incorporating\nvarious sources and thorough manual annotation. We then evaluate the landslide\ndetection performance of 11 state-of-the-art DL segmentation models: U-Net,\nResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A,\nFRRN-B, and SQNet. All models were trained from scratch on patches from one\nquarter of each study area and tested on independent patches from the other\nthree quarters. Our experiments demonstrate that ResU-Net outperformed the\nother models for the landslide detection task. We make the multi-source\nlandslide benchmark data (Landslide4Sense) and the tested DL models publicly\navailable at \\url{www.landslide4sense.org}, establishing an important resource\nfor remote sensing, computer vision, and machine learning communities in\nstudies of image classification in general and applications to landslide\ndetection in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbanzadeh_O/0/1/0/all/0/1\">Omid Ghorbanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamis_P/0/1/0/all/0/1\">Pedram Ghamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Michael Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreil_D/0/1/0/all/0/1\">David Kreil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline. (arXiv:2206.00527v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00527","description":"<p>Amodal perception terms the ability of humans to imagine the entire shapes of\noccluded objects. This gives humans an advantage to keep track of everything\nthat is going on, especially in crowded situations. Typical perception\nfunctions, however, lack amodal perception abilities and are therefore at a\ndisadvantage in situations with occlusions. Complex urban driving scenarios\noften experience many different types of occlusions and, therefore, amodal\nperception for automated vehicles is an important task to investigate. In this\npaper, we consider the task of amodal semantic segmentation and propose a\ngeneric way to generate datasets to train amodal semantic segmentation methods.\nWe use this approach to generate an amodal Cityscapes dataset. Moreover, we\npropose and evaluate a method as baseline on Amodal Cityscapes, showing its\napplicability for amodal semantic segmentation in automotive environment\nperception. We provide the means to re-generate this dataset on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Breitenstein_J/0/1/0/all/0/1\">Jasmin Breitenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. (arXiv:2206.00535v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00535","description":"<p>Deepfakes pose a serious threat to our digital society by fueling the spread\nof misinformation. It is essential to develop techniques that both detect them,\nand effectively alert the human user to their presence. Here, we introduce a\nnovel deepfake detection framework that meets both of these needs. Our approach\nlearns to generate attention maps of video artifacts, semi-supervised on human\nannotations. These maps make two contributions. First, they improve the\naccuracy and generalizability of a deepfake classifier, demonstrated across\nseveral deepfake detection datasets. Second, they allow us to generate an\nintuitive signal for the human user, in the form of \"Deepfake Caricatures\":\ntransformations of the original deepfake video where attended artifacts are\nexacerbated to improve human recognition. Our approach, based on a mixture of\nhuman and artificial supervision, aims to further the development of\ncountermeasures against fake visual content, and grants humans the ability to\nmake their own judgment when presented with dubious visual media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fosco_C/0/1/0/all/0/1\">Camilo Fosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josephs_E/0/1/0/all/0/1\">Emilie Josephs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Allen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of loss function in Deep Learning methods for accurate retinal vessel segmentation. (arXiv:2206.00536v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00536","description":"<p>The retinal vessel network studied through fundus images contributes to the\ndiagnosis of multiple diseases not only found in the eye. The segmentation of\nthis system may help the specialized task of analyzing these images by\nassisting in the quantification of morphological characteristics. Due to its\nrelevance, several Deep Learning-based architectures have been tested for\ntackling this problem automatically. However, the impact of loss function\nselection on the segmentation of the intricate retinal blood vessel system\nhasn't been systematically evaluated. In this work, we present the comparison\nof the loss functions Binary Cross Entropy, Dice, Tversky, and Combo loss using\nthe deep learning architectures (i.e. U-Net, Attention U-Net, and Nested UNet)\nwith the DRIVE dataset. Their performance is assessed using four metrics: the\nAUC, the mean squared error, the dice score, and the Hausdorff distance. The\nmodels were trained with the same number of parameters and epochs. Using dice\nscore and AUC, the best combination was SA-UNet with Combo loss, which had an\naverage of 0.9442 and 0.809 respectively. The best average of Hausdorff\ndistance and mean square error were obtained using the Nested U-Net with the\nDice loss function, which had an average of 6.32 and 0.0241 respectively. The\nresults showed that there is a significant difference in the selection of loss\nfunction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Herrera_D/0/1/0/all/0/1\">Daniela Herrera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Fully Convolutional Transformer for Medical Image Segmentation. (arXiv:2206.00566v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00566","description":"<p>We propose a novel transformer model, capable of segmenting medical images of\nvarying modalities. Challenges posed by the fine grained nature of medical\nimage analysis mean that the adaptation of the transformer for their analysis\nis still at nascent stages. The overwhelming success of the UNet lay in its\nability to appreciate the fine-grained nature of the segmentation task, an\nability which existing transformer based models do not currently posses. To\naddress this shortcoming, we propose The Fully Convolutional Transformer (FCT),\nwhich builds on the proven ability of Convolutional Neural Networks to learn\neffective image representations, and combines them with the ability of\nTransformers to effectively capture long-term dependencies in its inputs. The\nFCT is the first fully convolutional Transformer model in medical imaging\nliterature. It processes its input in two stages, where first, it learns to\nextract long range semantic dependencies from the input image, and then learns\nto capture hierarchical global attributes from the features. FCT is compact,\naccurate and robust. Our results show that it outperforms all existing\ntransformer architectures by large margins across multiple medical image\nsegmentation datasets of varying data modalities without the need for any\npre-training. FCT outperforms its immediate competitor on the ACDC dataset by\n1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC\n2017 dataset by 1.1% on the dice metric, with up to five times fewer\nparameters. Our code, environments and models will be available via GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tragakis_A/0/1/0/all/0/1\">Athanasios Tragakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaul_C/0/1/0/all/0/1\">Chaitanya Kaul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Husmeier_D/0/1/0/all/0/1\">Dirk Husmeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dog nose print matching with dual global descriptor based on Contrastive Learning. (arXiv:2206.00580v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00580","description":"<p>Recent studies in biometric-based identification tasks have shown that deep\nlearning methods can achieve better performance. These methods generally\nextract the global features as descriptor to represent the original image.\nNonetheless, it does not perform well for biometric identification under\nfine-grained tasks. The main reason is that the single image descriptor\ncontains insufficient information to represent image. In this paper, we present\na dual global descriptor model, which combines multiple global descriptors to\nexploit multi level image features. Moreover, we utilize a contrastive loss to\nenlarge the distance between image representations of confusing classes. The\nproposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet\nBiometric Challenge. The source code and trained models are publicly available\nat: https://github.com/flyingsheepbin/pet-biometrics\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qijun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher-Order Attention Networks. (arXiv:2206.00606v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00606","description":"<p>This paper introduces higher-order attention networks (HOANs), a novel class\nof attention-based neural networks defined on a generalized higher-order domain\ncalled a combinatorial complex (CC). Similar to hypergraphs, CCs admit\narbitrary set-like relations between a collection of abstract entities.\nSimultaneously, CCs permit the construction of hierarchical higher-order\nrelations analogous to those supported by cell complexes. Thus, CCs effectively\ngeneralize both hypergraphs and cell complexes and combine their desirable\ncharacteristics. By exploiting the rich combinatorial nature of CCs, HOANs\ndefine a new class of message-passing attention-based networks that unifies\nhigher-order neural networks. Our evaluation on tasks related to mesh shape\nanalysis and graph learning demonstrates that HOANs attain competitive, and in\nsome examples superior, predictive performance in comparison to\nstate-of-the-art neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papamarkou_T/0/1/0/all/0/1\">Theodore Papamarkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_Saenz_A/0/1/0/all/0/1\">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Choice of Data for Efficient Training and Validation of End-to-End Driving Models. (arXiv:2206.00608v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00608","description":"<p>The emergence of data-driven machine learning (ML) has facilitated\nsignificant progress in many complicated tasks such as highly-automated\ndriving. While much effort is put into improving the ML models and learning\nalgorithms in such applications, little focus is put into how the training data\nand/or validation setting should be designed. In this paper we investigate the\ninfluence of several data design choices regarding training and validation of\ndeep driving models trainable in an end-to-end fashion. Specifically, (i) we\ninvestigate how the amount of training data influences the final driving\nperformance, and which performance limitations are induced through currently\nused mechanisms to generate training data. (ii) Further, we show by correlation\nanalysis, which validation design enables the driving performance measured\nduring validation to generalize well to unknown test environments. (iii)\nFinally, we investigate the effect of random seeding and non-determinism,\ngiving insights which reported improvements can be deemed significant. Our\nevaluations using the popular CARLA simulator provide recommendations regarding\ndata generation and driving route selection for an efficient future development\nof end-to-end driving models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klingner_M/0/1/0/all/0/1\">Marvin Klingner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Konstantin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaie_M/0/1/0/all/0/1\">Mona Mirzaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breitenstein_J/0/1/0/all/0/1\">Jasmin Breitenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Termohlen_J/0/1/0/all/0/1\">Jan-Aike Term&#xf6;hlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage. (arXiv:2206.00614v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00614","description":"<p>This paper presents a spatiotemporal deep learning approach for mouse\nbehavioural classification in the home cage. Using a series of dual-stream\narchitectures with assorted modifications to increase performance, we introduce\na novel feature-sharing approach that jointly processes the streams at regular\nintervals throughout the network. Using a publicly available labelled dataset\nof singly-housed mice, we achieve a prediction accuracy of 86.47% using an\nensemble of Inception-based networks that utilize feature sharing. We also\ndemonstrate through ablation studies that for all models, the feature-sharing\narchitectures consistently perform better than conventional ones having\nseparate streams. The best performing models were further evaluated on other\nactivity datasets, both mouse and human, and achieved state-of-the-art results.\nFuture work will investigate the effectiveness of feature sharing in\nbehavioural classification in the unsupervised anomaly detection domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nwokedi_E/0/1/0/all/0/1\">Ezechukwu I. Nwokedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1\">Rasneer S. Bains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidaut_L/0/1/0/all/0/1\">Luc Bidaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xujiong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_S/0/1/0/all/0/1\">Sara Wells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James M. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00621","description":"<p>In this paper, we introduce Cross-View Language Modeling, a simple and\neffective language model pre-training framework that unifies cross-lingual\ncross-modal pre-training with shared architectures and objectives. Our approach\nis motivated by a key observation that cross-lingual and cross-modal\npre-training share the same goal of aligning two different views of the same\nobject into a common semantic space. To this end, the cross-view language\nmodeling framework considers both multi-modal data (i.e., image-caption pairs)\nand multi-lingual data (i.e., parallel sentence pairs) as two different views\nof the same object, and trains the model to align the two views by maximizing\nthe mutual information between them with conditional masked language modeling\nand contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal\nLanguage Model, with the cross-view language modeling framework. Empirical\nresults on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual\nimage-text retrieval datasets show that while conceptually simpler, CCLM\nsignificantly outperforms the prior state-of-the-art with an average absolute\nimprovement of over 10%. Notably, CCLM is the first multi-lingual multi-modal\nmodel that surpasses the translate-test performance of representative English\nvision-language models by zero-shot cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Ao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP4IDC: CLIP for Image Difference Captioning. (arXiv:2206.00629v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00629","description":"<p>Image Difference Captioning (IDC) aims at generating sentences to describe\nthe differences between two similar-looking images. The conventional approaches\nlearn captioning models on the offline-extracted visual features and the\nlearning can not be propagated back to the fixed feature extractors pre-trained\non image classification datasets. Accordingly, potential improvements can be\nmade by fine-tuning the visual features for: 1) narrowing the gap when\ngeneralizing the visual extractor trained on image classification to IDC, and\n2) relating the extracted visual features to the descriptions of the\ncorresponding changes. We thus propose CLIP4IDC to transfer a CLIP model for\nthe IDC task to attain these improvements. Different from directly fine-tuning\nCLIP to generate sentences, a task-specific domain adaptation is used to\nimprove the extracted features. Specifically, the target is to train CLIP on\nraw pixels to relate the image pairs to the described changes. Afterwards, a\nvanilla Transformer is trained for IDC on the features extracted by the vision\nencoder of CLIP. Experiments on three IDC benchmark datasets, CLEVR-Change,\nSpot-the-Diff and Image-Editing-Request, demonstrate the effectiveness of\nCLIP4IDC. Our code and models will be released at\nhttps://github.com/sushizixin/CLIP4IDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tzu-Jui Julius Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1\">Jorma Laaksonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Voxel-based Representation with Transformer for 3D Object Detection. (arXiv:2206.00630v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00630","description":"<p>In this work, we present a unified framework for multi-modality 3D object\ndetection, named UVTR. The proposed method aims to unify multi-modality\nrepresentations in the voxel space for accurate and robust single- or\ncross-modality 3D detection. To this end, the modality-specific space is first\ndesigned to represent different inputs in the voxel feature space. Different\nfrom previous work, our approach preserves the voxel space without height\ncompression to alleviate semantic ambiguity and enable spatial interactions.\nBenefit from the unified manner, cross-modality interaction is then proposed to\nmake full use of inherent properties from different sensors, including\nknowledge transfer and modality fusion. In this way, geometry-aware expressions\nin point clouds and context-rich features in images are well utilized for\nbetter performance and robustness. The transformer decoder is applied to\nefficiently sample features from the unified space with learnable positions,\nwhich facilitates object-level interactions. In general, UVTR presents an early\nattempt to represent different modalities in a unified framework. It surpasses\nprevious work in single- and multi-modality entries and achieves leading\nperformance in the nuScenes test set with 69.7%, 55.1%, and 71.1% NDS for\nLiDAR, camera, and multi-modality inputs, respectively. Code is made available\nat https://github.com/dvlab-research/UVTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Floorplan Reconstruction by Structure-Hallucinating Transformer Cascades. (arXiv:2206.00645v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00645","description":"<p>This paper presents an extreme floorplan reconstruction task, a new benchmark\nfor the task, and a neural architecture as a solution. Given a partial\nfloorplan reconstruction inferred or curated from panorama images, the task is\nto reconstruct a complete floorplan including invisible architectural\nstructures. The proposed neural network 1) encodes an input partial floorplan\ninto a set of latent vectors by convolutional neural networks and a\nTransformer; and 2) reconstructs an entire floorplan while hallucinating\ninvisible rooms and doors by cascading Transformer decoders. Qualitative and\nquantitative evaluations demonstrate effectiveness of our approach over the\nbenchmark of 701 houses, outperforming the state-of-the-art reconstruction\ntechniques. We will share our code, models, and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Sepidehsadat Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction. (arXiv:2206.00665v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00665","description":"<p>In recent years, neural implicit surface reconstruction methods have become\npopular for multi-view 3D reconstruction. In contrast to traditional multi-view\nstereo methods, these approaches tend to produce smoother and more complete\nreconstructions due to the inductive smoothness bias of neural networks.\nState-of-the-art neural implicit methods allow for high-quality reconstructions\nof simple scenes from many input views. Yet, their performance drops\nsignificantly for larger and more complex scenes and scenes captured from\nsparse viewpoints. This is caused primarily by the inherent ambiguity in the\nRGB reconstruction loss that does not provide enough constraints, in particular\nin less-observed and textureless areas. Motivated by recent advances in the\narea of monocular geometry prediction, we systematically explore the utility\nthese cues provide for improving neural implicit surface reconstruction. We\ndemonstrate that depth and normal cues, predicted by general-purpose monocular\nestimators, significantly improve reconstruction quality and optimization time.\nFurther, we analyse and investigate multiple design choices for representing\nneural implicit surfaces, ranging from monolithic MLP models over single-grid\nto multi-resolution grid representations. We observe that geometric monocular\npriors improve performance both for small-scale single-object as well as\nlarge-scale multi-object scenes, independent of the choice of representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Songyou Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemeyer_M/0/1/0/all/0/1\">Michael Niemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor Pruning for Object Detection. (arXiv:2104.00432v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00432","description":"<p>This paper proposes anchor pruning for object detection in one-stage\nanchor-based detectors. While pruning techniques are widely used to reduce the\ncomputational cost of convolutional neural networks, they tend to focus on\noptimizing the backbone networks where often most computations are. In this\nwork we demonstrate an additional pruning technique, specifically for object\ndetection: anchor pruning. With more efficient backbone networks and a growing\ntrend of deploying object detectors on embedded systems where post-processing\nsteps such as non-maximum suppression can be a bottleneck, the impact of the\nanchors used in the detection head is becoming increasingly more important. In\nthis work, we show that many anchors in the object detection head can be\nremoved without any loss in accuracy. With additional retraining, anchor\npruning can even lead to improved accuracy. Extensive experiments on SSD and MS\nCOCO show that the detection head can be made up to 44% more efficient while\nsimultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL\nVOC show the general effectiveness of our approach. We also introduce\n`overanchorized' models that can be used together with anchor pruning to\neliminate hyperparameters related to the initial shape of anchors. Code and\nmodels are available at https://github.com/Mxbonn/anchor_pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1\">Maxim Bonnaerens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutions for Spatial Interaction Modeling. (arXiv:2104.07182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07182","description":"<p>In many different fields interactions between objects play a critical role in\ndetermining their behavior. Graph neural networks (GNNs) have emerged as a\npowerful tool for modeling interactions, although often at the cost of adding\nconsiderable complexity and latency. In this paper, we consider the problem of\nspatial interaction modeling in the context of predicting the motion of actors\naround autonomous vehicles, and investigate alternative approaches to GNNs. We\nrevisit convolutions and show that they can demonstrate comparable performance\nto graph networks in modeling spatial interactions with lower latency, thus\nproviding an effective and efficient alternative in time-critical systems.\nMoreover, we propose a novel interaction loss to further improve the\ninteraction modeling of the considered methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_D/0/1/0/all/0/1\">David Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallespi_Gonzalez_C/0/1/0/all/0/1\">Carlos Vallespi-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellington_C/0/1/0/all/0/1\">Carl Wellington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1\">Nemanja Djuric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoveaTer: Foveated Transformer for Image Classification. (arXiv:2105.14173v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14173","description":"<p>Many animals and humans process the visual field with a varying spatial\nresolution (foveated vision) and use peripheral processing to make eye\nmovements and point the fovea to acquire high-resolution information about\nobjects of interest. This architecture results in computationally efficient\nrapid scene exploration. Recent progress in self-attention-based vision\nTransformers allow global interactions between feature locations and result in\nincreased robustness to adversarial attacks. However, the Transformer models do\nnot explicitly model the foveated properties of the visual system nor the\ninteraction between eye movements and the classification task. We propose\nfoveated Transformer (FoveaTer) model, which uses pooling regions and eye\nmovements to perform object classification tasks. Our proposed model pools the\nimage features using squared pooling regions, an approximation to the\nbiologically-inspired foveated architecture. It decides on subsequent fixation\nlocations based on the attention assigned by the Transformer to various\nlocations from past and present fixations. It dynamically allocates more\nfixation/computational resources to more challenging images before making the\nfinal object category decision. We compare FoveaTer against a Full-resolution\nbaseline model, which does not contain any pooling. On the ImageNet dataset,\nthe Foveated model with Dynamic-stop achieves an accuracy of $1.9\\%$ below the\nfull-resolution model with a throughput gain of $51\\%$. Using a Foveated model\nwith Dynamic-stop and the Full-resolution model, the ensemble outperforms the\nbaseline Full-resolution by $0.2\\%$ with a throughput gain of $7.7\\%$. We also\ndemonstrate our model's robustness against adversarial attacks. Finally, we\ncompare the Foveated model to human performance in a scene categorization task\nand show similar dependence of accuracy with number of exploratory fixations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jonnalagadda_A/0/1/0/all/0/1\">Aditya Jonnalagadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B. S. Manjunath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel P. Eckstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization. (arXiv:2109.02934v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02934","description":"<p>Learning robust models that generalize well under changes in the data\ndistribution is critical for real-world applications. To this end, there has\nbeen a growing surge of interest to learn simultaneously from multiple training\ndomains - while enforcing different types of invariance across those domains.\nYet, all existing approaches fail to show systematic benefits under controlled\nevaluation protocols. In this paper, we introduce a new regularization - named\nFishr - that enforces domain invariance in the space of the gradients of the\nloss: specifically, the domain-level variances of gradients are matched across\ntraining domains. Our approach is based on the close relations between the\ngradient covariance, the Fisher Information and the Hessian of the loss: in\nparticular, we show that Fishr eventually aligns the domain-level loss\nlandscapes locally around the final weights. Extensive experiments demonstrate\nthe effectiveness of Fishr for out-of-distribution generalization. Notably,\nFishr improves the state of the art on the DomainBed benchmark and performs\nconsistently better than Empirical Risk Minimization. Our code is available at\nhttps://github.com/alexrame/fishr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval. (arXiv:2110.15609v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15609","description":"<p>The task of text-video retrieval aims to understand the correspondence\nbetween language and vision, has gained increasing attention in recent years.\nPrevious studies either adopt off-the-shelf 2D/3D-CNN and then use average/max\npooling to directly capture spatial features with aggregated temporal\ninformation as global video embeddings, or introduce graph-based models and\nexpert knowledge to learn local spatial-temporal relations. However, the\nexisting methods have two limitations: 1) The global video representations\nlearn video temporal information in a simple average/max pooling manner and do\nnot fully explore the temporal information between every two frames. 2) The\ngraph-based local video representations are handcrafted, it depends heavily on\nexpert knowledge and empirical feedback, which may not be able to effectively\nmine the higher-level fine-grained visual relations. These limitations result\nin their inability to distinguish videos with the same visual components but\nwith different relations. To solve this problem, we propose a novel cross-modal\nretrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies\ntransformer architecture to effectively bridge text-video modalities in a\ncomplementary manner via combining local spatial-temporal relation and global\ntemporal information. Specifically, local video representations are encoded\nusing multiple transformer blocks and additional residual blocks to learn\nspatio-temporal relation features, calling the module a Spatio-Temporal\nResidual transformer (SRT). Meanwhile, Global video representations are encoded\nusing a multi-layer transformer block to learn global temporal features.\nFinally, we align the spatio-temporal relation and global temporal features\nwith the text feature on two embedding spaces for cross-modal text-video\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_N/0/1/0/all/0/1\">Ning Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yawen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intensity Mapping Functions For HDR Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07283","description":"<p>It is challenging to stitch multiple images with different exposures due to\npossible color distortion and loss of details in the brightest and darkest\nregions of input images. In this paper, a novel intensity mapping algorithm is\nfirst proposed by introducing a new concept of weighted histogram averaging\n(WHA). The proposed WHA algorithm leverages the correspondence between the\nhistogram bins of two images which are built up by using the non-decreasing\nproperty of the intensity mapping functions (IMFs). The WHA algorithm is then\nadopted to synthesize a set of differently exposed panorama images. The\nintermediate panorama images are finally fused via a state-of-the-art\nmulti-scale exposure fusion (MEF) algorithm to produce the final panorama\nimage. Extensive experiments indicate that the proposed WHA algorithm\nsignificantly surpasses the related state-of-the-art intensity mapping methods.\nThe proposed high dynamic range (HDR) stitching algorithm also preserves\ndetails in the brightest and darkest regions of the input images well. The\nrelated materials will be publicly accessible at\nhttps://github.com/yilun-xu/WHA for reproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yilun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Changyun Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08276","description":"<p>Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection and make fine-grained alignments\nbetween the extracted features and texts. It is challenging for these methods\nto learn relations among multiple objects. To this end, we propose a new method\ncalled X-VLM to perform `multi-grained vision language pre-training.' The key\nto learning multi-grained alignments is to locate visual concepts in the image\ngiven the associated texts, and in the meantime align the texts with the visual\nconcepts, where the alignments are in multi-granularity. Experimental results\nshow that X-VLM effectively leverages the learned multi-grained alignments to\nmany downstream vision language tasks and consistently outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BA-Net: Bridge Attention for Deep Convolutional Neural Networks. (arXiv:2112.04150v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04150","description":"<p>In recent years, channel attention mechanism has been widely investigated due\nto its great potential in improving the performance of deep convolutional\nneural networks (CNNs) in many vision tasks. However, in most of the existing\nmethods, only the output of the adjacent convolution layer is fed into the\nattention layer for calculating the channel weights. Information from other\nconvolution layers has been ignored. With these observations, a simple\nstrategy, named Bridge Attention Net (BA-Net), is proposed in this paper for\nbetter performance with channel attention mechanisms. The core idea of this\ndesign is to bridge the outputs of the previous convolution layers through skip\nconnections for channel weights generation. Based on our experiment and theory\nanalysis, we find that features from previous layers also contribute to the\nweights significantly. The Comprehensive evaluation demonstrates that the\nproposed approach achieves state-of-the-art(SOTA) performance compared with the\nexisting methods in accuracy and speed. which shows that Bridge Attention\nprovides a new perspective on the design of neural network architectures with\ngreat potential in improving performance. The code is available at\nhttps://github.com/zhaoy376/Bridge-Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ronghui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forensic Analysis of Synthetically Generated Western Blot Images. (arXiv:2112.08739v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08739","description":"<p>The widespread diffusion of synthetically generated content is a serious\nthreat that needs urgent countermeasures. As a matter of fact, the generation\nof synthetic content is not restricted to multimedia data like videos,\nphotographs or audio sequences, but covers a significantly vast area that can\ninclude biological images as well, such as western blot and microscopic images.\nIn this paper, we focus on the detection of synthetically generated western\nblot images. These images are largely explored in the biomedical literature and\nit has been already shown they can be easily counterfeited with few hopes to\nspot manipulations by visual inspection or by using standard forensics\ndetectors. To overcome the absence of publicly available data for this task, we\ncreate a new dataset comprising more than 14K original western blot images and\n24K synthetic western blot images, generated using four different\nstate-of-the-art generation methods. We investigate different strategies to\ndetect synthetic western blots, exploring binary classification methods as well\nas one-class detectors. In both scenarios, we never exploit synthetic western\nblot images at training stage. The achieved results show that synthetically\ngenerated western blot images can be spot with good accuracy, even though the\nexploited detectors are not optimized over synthetic versions of these\nscientific images. We also test the robustness of the developed detectors\nagainst post-processing operations commonly performed on scientific images,\nshowing that we can be robust to JPEG compression and that some generative\nmodels are easily recognizable, despite the application of editing might alter\nthe artifacts they leave.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1\">Davide Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannas_E/0/1/0/all/0/1\">Edoardo D. Cannas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardenuto_J/0/1/0/all/0/1\">Joao P. Cardenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_D/0/1/0/all/0/1\">Daniel Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1\">Luisa Verdoliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10325","description":"<p>Due to the constraints of the imaging device and high cost in operation time,\ncomputer tomography (CT) scans are usually acquired with low intra-slice\nresolution. Improving the intra-slice resolution is beneficial to the disease\ndiagnosis for both human experts and computer-aided systems. To this end, this\npaper builds a novel medical slice synthesis to increase the between-slice\nresolution. Considering that the ground-truth intermediate medical slices are\nalways absent in clinical practice, we introduce the incremental cross-view\nmutual distillation strategy to accomplish this task in the self-supervised\nlearning manner. Specifically, we model this problem from three different\nviews: slice-wise interpolation from axial view and pixel-wise interpolation\nfrom coronal and sagittal views. Under this circumstance, the models learned\nfrom different views can distill valuable knowledge to guide the learning\nprocesses of each other. We can repeat this process to make the models\nsynthesize intermediate slice data with increasing inter-slice resolution. To\ndemonstrate the effectiveness of the proposed approach, we conduct\ncomprehensive experiments on a large-scale CT dataset. Quantitative and\nqualitative comparison results show that our method outperforms\nstate-of-the-art algorithms by clear margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14683","description":"<p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis\nframeworks treat them discretely in time. In this work, we think of videos of\nwhat they should be $-$ time-continuous signals, and extend the paradigm of\nneural representations to build a continuous-time video generator. For this, we\nfirst design continuous motion representations through the lens of positional\nembeddings. Then, we explore the question of training on very sparse videos and\ndemonstrate that a good generator can be learned by using as few as 2 frames\nper clip. After that, we rethink the traditional image + video discriminators\npair and design a holistic discriminator that aggregates temporal information\nby simply concatenating frames' features. This decreases the training cost and\nprovides richer learning signal to the generator, making it possible to train\ndirectly on 1024$^2$ videos for the first time. We build our model on top of\nStyleGAN2 and it is just ${\\approx}5\\%$ more expensive to train at the same\nresolution while achieving almost the same image quality. Moreover, our latent\nspace features similar properties, enabling spatial manipulations that our\nmethod can propagate in time. We can generate arbitrarily long videos at\narbitrary high frame rate, while prior work struggles to generate even 64\nframes at a fixed rate. Our model is tested on four modern 256$^2$ and one\n1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it\nperforms on average ${\\approx}30\\%$ better than the closest runner-up. Project\nwebsite: https://universome.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.15011","description":"<p>In clinics, a radiology report is crucial for guiding a patient's treatment.\nHowever, writing radiology reports is a heavy burden for radiologists. To this\nend, we present an automatic, multi-modal approach for report generation from a\nchest x-ray. Our approach, motivated by the observation that the descriptions\nin radiology reports are highly correlated with specific information of the\nx-ray images, features two distinct modules: (i) Learned knowledge base: To\nabsorb the knowledge embedded in the radiology reports, we build a knowledge\nbase that can automatically distil and restore medical knowledge from textual\nembedding without manual labour; (ii) Multi-modal alignment: to promote the\nsemantic alignment among reports, disease labels, and images, we explicitly\nutilize textual embedding to guide the learning of the visual feature space. We\nevaluate the performance of the proposed model using metrics from both natural\nlanguage generation and clinic efficacy on the public IU-Xray and MIMIC-CXR\ndatasets. Our ablation study shows that each module contributes to improving\nthe quality of generated reports. Furthermore, with the assistance of both\nmodules, our approach outperforms state-of-the-art methods over almost all the\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S.Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07459","description":"<p>Labeling a large set of data is expensive. Active learning aims to tackle\nthis problem by asking to annotate only the most informative data from the\nunlabeled set. We propose a novel active learning approach that utilizes\nself-supervised pretext tasks and a unique data sampler to select data that are\nboth difficult and representative. We discover that the loss of a simple\nself-supervised pretext task, such as rotation prediction, is closely\ncorrelated to the downstream task loss. Before the active learning iterations,\nthe pretext task learner is trained on the unlabeled set, and the unlabeled\ndata are sorted and split into batches by their pretext task losses. In each\nactive learning iteration, the main task model is used to sample the most\nuncertain data in a batch to be annotated. We evaluate our method on various\nimage classification and segmentation benchmarks and achieve compelling\nperformances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show\nthat our method performs well on imbalanced datasets, and can be an effective\nsolution to the cold-start problem where active learning performance is\naffected by the randomly sampled initial labeled set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dual Contouring. (arXiv:2202.01999v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01999","description":"<p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh\nreconstruction based on dual contouring (DC). Like traditional DC, it produces\nexactly one vertex per grid cell and one quad for each grid edge intersection,\na natural and efficient structure for reproducing sharp features. However,\nrather than computing vertex locations and edge crossings with hand-crafted\nfunctions that depend directly on difficult-to-obtain surface gradients, NDC\nuses a neural network to predict them. As a result, NDC can be trained to\nproduce meshes from signed or unsigned distance fields, binary voxel grids, or\npoint clouds (with or without normals); and it can produce open surfaces in\ncases where the input represents a sheet or partial surface. During experiments\nwith five prominent datasets, we find that NDC, when trained on one of the\ndatasets, generalizes well to the others. Furthermore, NDC provides better\nsurface reconstruction accuracy, feature preservation, output complexity,\ntriangle quality, and inference time in comparison to previous learned (e.g.,\nneural marching cubes, convolutional occupancy networks) and traditional (e.g.,\nPoisson) methods. Code and data are available at\nhttps://github.com/czq142857/NDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. (arXiv:2202.03052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03052","description":"<p>In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &amp;\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.12104","description":"<p>Deformable medical image registration plays an important role in clinical\ndiagnosis and treatment. Recently, the deep learning (DL) based image\nregistration methods have been widely investigated and showed excellent\nperformance in computational speed. However, these methods cannot provide\nenough registration accuracy because of insufficient ability in representing\nboth the global and local features of the moving and fixed images. To address\nthis issue, this paper has proposed the transformer based image registration\nmethod. This method uses the distinctive transformer to extract the global and\nlocal image features for generating the deformation fields, based on which the\nregistered image is produced in an unsupervised way. Our method can improve the\nregistration accuracy effectively by means of self-attention mechanism and\nbi-level information flow. Experimental results on such brain MR image datasets\nas LPBA40 and OASIS-1 demonstrate that compared with several traditional and DL\nbased registration methods, our method provides higher registration accuracy in\nterms of dice values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_W/0/1/0/all/0/1\">Wen Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xuming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Completion using Geometry-Aware Embedding. (arXiv:2203.10912v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10912","description":"<p>Exploiting internal spatial geometric constraints of sparse LiDARs is\nbeneficial to depth completion, however, has been not explored well. This paper\nproposes an efficient method to learn geometry-aware embedding, which encodes\nthe local and global geometric structure information from 3D points, e.g.,\nscene layout, object's sizes and shapes, to guide dense depth estimation.\nSpecifically, we utilize the dynamic graph representation to model generalized\ngeometric relationship from irregular point clouds in a flexible and efficient\nmanner. Further, we joint this embedding and corresponded RGB appearance\ninformation to infer missing depths of the scene with well structure-preserved\ndetails. The key to our method is to integrate implicit 3D geometric\nrepresentation into a 2D learning architecture, which leads to a better\ntrade-off between the performance and efficiency. Extensive experiments\ndemonstrate that the proposed method outperforms previous works and could\nreconstruct fine depths with crisp boundaries in regions that are over-smoothed\nby them. The ablation study gives more insights into our method that could\nachieve significant gains with a simple design, while having better\ngeneralization capability and stability. The code is available at\nhttps://github.com/Wenchao-Du/GAENet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenchao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12023","description":"<p>Many promising applications of supervised machine learning face hurdles in\nthe acquisition of labeled data in sufficient quantity and quality, creating an\nexpensive bottleneck. To overcome such limitations, techniques that do not\ndepend on ground truth labels have been studied, including weak supervision and\ngenerative modeling. While these techniques would seem to be usable in concert,\nimproving one another, how to build an interface between them is not\nwell-understood. In this work, we propose a model fusing programmatic weak\nsupervision and generative adversarial networks and provide theoretical\njustification motivating this fusion. The proposed approach captures discrete\nlatent variables in the data alongside the weak supervision derived label\nestimate. Alignment of the two allows for better modeling of sample-dependent\naccuracies of the weak supervision sources, improving the estimate of\nunobserved labels. It is the first approach to enable data augmentation through\nweakly supervised synthetic images and pseudolabels. Additionally, its learned\nlatent variables can be inspected qualitatively. The model outperforms baseline\nweak supervision label models on a number of multiclass image classification\ndatasets, improves the quality of generated images, and further improves\nend-model performance through data augmentation with synthetic samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14260","description":"<p>Understanding realistic visual scene images together with language\ndescriptions is a fundamental task towards generic visual understanding.\nPrevious works have shown compelling comprehensive results by building\nhierarchical structures for visual scenes (e.g., scene graphs) and natural\nlanguages (e.g., dependency trees), individually. However, how to construct a\njoint vision-language (VL) structure has barely been investigated. More\nchallenging but worthwhile, we introduce a new task that targets on inducing\nsuch a joint VL structure in an unsupervised manner. Our goal is to bridge the\nvisual scene graphs and linguistic dependency trees seamlessly. Due to the lack\nof VL structural data, we start by building a new dataset VLParse. Rather than\nusing labor-intensive labeling from scratch, we propose an automatic alignment\nprocedure to produce coarse structures followed by human refinement to produce\nhigh-quality ones. Moreover, we benchmark our dataset by proposing a\ncontrastive learning (CL)-based framework VLGAE, short for Vision-Language\nGraph Autoencoder. Our model obtains superior performance on two derived tasks,\ni.e., language grammar induction and VL phrase grounding. Ablations show the\neffectiveness of both visual cues and dependency relationships on fine-grained\nVL structure construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Min-Max Similarity: A Contrastive Semi-Supervised Deep Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15177","description":"<p>Segmentation of images is a popular topic in medical AI. This is mainly due\nto the difficulty to obtain a significant number of pixel-level annotated data\nto train a neural network. To address this issue, we proposed a semi-supervised\nsegmentation network based on contrastive learning. In contrast to the previous\nstate-of-the-art, we introduce Min-Max Similarity (MMS), a contrastive learning\nform of dual-view training by employing classifiers and projectors to build\nall-negative, and positive and negative feature pairs respectively to formulate\nthe learning problem as solving min-max similarity problem. The all-negative\npairs are used to supervise the networks learning from different views and make\nsure to capture general features, and the consistency of unlabeled predictions\nis measured by pixel-wise contrastive loss between positive and negative pairs.\nTo quantitative and qualitative evaluate our proposed method, we test it on two\npublic endoscopy surgical tool segmentation datasets and one cochlear implant\nsurgery dataset which we manually annotate the cochlear implant in surgical\nvideos. The segmentation performance (dice coefficients) indicates that our\nproposed method outperforms state-of-the-art semi-supervised and fully\nsupervised segmentation algorithms consistently. And our semi-supervised\nsegmentation algorithm can successfully recognize unknown surgical tools and\nprovide good predictions. Also, our MMS could achieve about 40 frames per\nsecond (fps) and suitable to deal with the real-time video segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tawfik_K/0/1/0/all/0/1\">Kareem Tawfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">Jack Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02078","description":"<p>This paper studies semi-supervised learning of semantic segmentation, which\nassumes that only a small portion of training images are labeled and the others\nremain unlabeled. The unlabeled images are usually assigned pseudo labels to be\nused in training, which however often causes the risk of performance\ndegradation due to the confirmation bias towards errors on the pseudo labels.\nWe present a novel method that resolves this chronic issue of pseudo labeling.\nAt the heart of our method lies error localization network (ELN), an auxiliary\nmodule that takes an image and its segmentation prediction as input and\nidentifies pixels whose pseudo labels are likely to be wrong. ELN enables\nsemi-supervised learning to be robust against inaccurate pseudo labels by\ndisregarding label noises during training and can be naturally integrated with\nself-training and contrastive learning. Moreover, we introduce a new learning\nstrategy for ELN that simulates plausible and diverse segmentation errors\nduring training of ELN to enhance its generalization. Our method is evaluated\non PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in\nevery evaluation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Donghyeon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05381","description":"<p>Self-supervised learning (SSL) has attracted much interest in remote sensing\nand earth observation due to its ability to learn task-agnostic representations\nwithout human annotation. While most of the existing SSL works in remote\nsensing utilize ConvNet backbones and focus on a single modality, we explore\nthe potential of vision transformers (ViTs) for joint SAR-optical\nrepresentation learning. Based on DINO, a state-of-the-art SSL algorithm that\ndistills knowledge from two augmented views of an input image, we combine SAR\nand optical imagery by concatenating all channels to a unified input.\nSubsequently, we randomly mask out channels of one modality as a data\naugmentation strategy. While training, the model gets fed optical-only,\nSAR-only, and SAR-optical image pairs learning both inner- and intra-modality\nrepresentations. Experimental results employing the BigEarthNet-MM dataset\ndemonstrate the benefits of both, the ViT backbones and the proposed multimodal\nSSL algorithm DINO-MM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (arXiv:2204.08612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08612","description":"<p>Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic\nmedia where the likeness of one person is replaced with another. There are\ngrowing concerns that deepfakes can be maliciously used to create misleading\nand harmful digital contents. As deepfakes become more common, there is a dire\nneed for deepfake detection technology to help spot deepfake media. Present\ndeepfake detection models are able to achieve outstanding accuracy (&gt;90%).\nHowever, most of them are limited to within-dataset scenario, where the same\ndataset is used for training and testing. Most models do not generalise well\nenough in cross-dataset scenario, where models are tested on unseen datasets\nfrom another source. Furthermore, state-of-the-art deepfake detection models\nrely on neural network-based classification models that are known to be\nvulnerable to adversarial attacks. Motivated by the need for a robust deepfake\ndetection model, this study adapts metamorphic testing (MT) principles to help\nidentify potential factors that could influence the robustness of the examined\nmodel, while overcoming the test oracle problem in this domain. Metamorphic\ntesting is specifically chosen as the testing technique as it fits our demand\nto address learning-based system testing with probabilistic outcomes from\nlargely black-box components, based on potentially large input domains. We\nperformed our evaluations on MesoInception-4 and TwoStreamNet models, which are\nthe state-of-the-art deepfake detection models. This study identified makeup\napplication as an adversarial attack that could fool deepfake detectors. Our\nexperimental results demonstrate that both the MesoInception-4 and TwoStreamNet\nmodels degrade in their performance by up to 30\\% when the input data is\nperturbed with makeup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_N/0/1/0/all/0/1\">Nyee Thoang Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuan_M/0/1/0/all/0/1\">Meng Yi Kuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Muxin Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Structure For Building A Robust Model. (arXiv:2204.11596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11596","description":"<p>As deep learning applications, especially programs of computer vision, are\nincreasingly deployed in our lives, we have to think more urgently about the\nsecurity of these applications.One effective way to improve the security of\ndeep learning models is to perform adversarial training, which allows the model\nto be compatible with samples that are deliberately created for use in\nattacking the model.Based on this, we propose a simple architecture to build a\nmodel with a certain degree of robustness, which improves the robustness of the\ntrained network by adding an adversarial sample detection network for\ncooperative training. At the same time, we design a new data sampling strategy\nthat incorporates multiple existing attacks, allowing the model to adapt to\nmany different adversarial attacks with a single training.We conducted some\nexperiments to test the effectiveness of this design based on Cifar10 dataset,\nand the results indicate that it has some degree of positive effect on the\nrobustness of the model.Our code could be found at\nhttps://github.com/dowdyboy/simple_structure_for_robust_model .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruolin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNNs are Myopic. (arXiv:2205.10760v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10760","description":"<p>We claim that Convolutional Neural Networks (CNNs) learn to classify images\nusing only small seemingly unrecognizable tiles. We show experimentally that\nCNNs trained only using such tiles can match or even surpass the performance of\nCNNs trained on full images. Conversely, CNNs trained on full images show\nsimilar predictions on small tiles. We also propose the first a priori\ntheoretical model for convolutional data sets that seems to explain this\nbehavior. This gives additional support to the long standing suspicion that\nCNNs do not need to understand the global structure of images to achieve\nstate-of-the-art accuracies. Surprisingly it also suggests that over-fitting is\nnot needed either.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madala_V/0/1/0/all/0/1\">Vamshi C. Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.13147","description":"<p>Learned representations are a central component in modern ML systems, serving\na multitude of downstream tasks. When training such representations, it is\noften the case that computational and statistical constraints for each\ndownstream task are unknown. In this context rigid, fixed capacity\nrepresentations can be either over or under-accommodating to the task at hand.\nThis leads us to ask: can we design a flexible representation that can adapt to\nmultiple downstream tasks with varying computational resources? Our main\ncontribution is Matryoshka Representation Learning (MRL) which encodes\ninformation at different granularities and allows a single embedding to adapt\nto the computational constraints of downstream tasks. MRL minimally modifies\nexisting representation learning pipelines and imposes no additional cost\nduring inference and deployment. MRL learns coarse-to-fine representations that\nare at least as accurate and rich as independently trained low-dimensional\nrepresentations. The flexibility within the learned Matryoshka Representations\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\nlong-tail few-shot classification, all while being as robust as the original\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\nare open-sourced at https://github.com/RAIVNLab/MRL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gantavya Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rege_A/0/1/0/all/0/1\">Aniket Rege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1\">Matthew Wallingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Aditya Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_Snyder_W/0/1/0/all/0/1\">William Howard-Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13943","description":"<p>Masked image modeling (MIM), an emerging self-supervised pre-training method,\nhas shown impressive success across numerous downstream vision tasks with\nVision transformers (ViT). Its underlying idea is simple: a portion of the\ninput image is randomly masked out and then reconstructed via the pre-text\ntask. However, why MIM works well is not well explained, and previous studies\ninsist that MIM primarily works for the Transformer family but is incompatible\nwith CNNs. In this paper, we first study interactions among patches to\nunderstand what knowledge is learned and how it is acquired via the MIM task.\nWe observe that MIM essentially teaches the model to learn better middle-level\ninteractions among patches and extract more generalized features. Based on this\nfact, we propose an Architecture-Agnostic Masked Image Modeling framework\n(A$^2$MIM), which is compatible with not only Transformers but also CNNs in a\nunified way. Extensive experiments on popular benchmarks show that our A$^2$MIM\nlearns better representations and endows the backbone model with the stronger\ncapability to transfer to various downstream tasks for both Transformers and\nCNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan.Z.Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14100","description":"<p>In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Masked Autoencoders Learn Transferable Representations. (arXiv:2205.14204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14204","description":"<p>Building scalable models to learn from diverse, multimodal data remains an\nopen challenge. For vision-language data, the dominant approaches are based on\ncontrastive learning objectives that train a separate encoder for each\nmodality. While effective, contrastive learning approaches introduce sampling\nbias depending on the data augmentations used, which can degrade performance on\ndownstream tasks. Moreover, these methods are limited to paired image-text\ndata, and cannot leverage widely-available unpaired data. In this paper, we\ninvestigate whether a large multimodal model trained purely via masked token\nprediction, without using modality-specific encoders or contrastive learning,\ncan learn transferable representations for downstream tasks. We propose a\nsimple and scalable network architecture, the Multimodal Masked Autoencoder\n(M3AE), which learns a unified encoder for both vision and language data via\nmasked token prediction. We provide an empirical study of M3AE trained on a\nlarge-scale image-text dataset, and find that M3AE is able to learn\ngeneralizable representations that transfer well to downstream tasks.\nSurprisingly, we find that M3AE benefits from a higher text mask ratio\n(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the\njoint training of two data modalities. We also provide qualitative analysis\nshowing that the learned representation incorporates meaningful information\nfrom both image and language. Lastly, we demonstrate the scalability of M3AE\nwith larger model size and training time, and its flexibility to train on both\npaired image-text data as well as unpaired data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lisa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14375","description":"<p>Gains in the ability to generalize on image analysis tasks for neural\nnetworks have come at the cost of increased number of parameters and layers,\ndataset sizes, training and test computations, and GPU RAM. We introduce a new\narchitecture -- WaveMix-Lite -- that can generalize on par with contemporary\ntransformers and convolutional neural networks (CNNs) while needing fewer\nresources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix\nspatial information from pixels. WaveMix-Lite seems to be a versatile and\nscalable architectural framework that can be used for multiple vision tasks,\nsuch as image classification and semantic segmentation, without requiring\nsignificant architectural changes, unlike transformers and CNNs. It is able to\nmeet or exceed several accuracy benchmarks while training on a single GPU. For\ninstance, it achieves state-of-the-art accuracy on five EMNIST datasets,\noutperforms CNNs and transformers in ImageNet-1K (64$\\times$64 images), and\nachieves an mIoU of 75.32 % on Cityscapes validation set, while using less than\none-fifth the number parameters and half the GPU RAM of comparable CNNs or\ntransformers. Our experiments show that while the convolutional elements of\nneural architectures exploit the shift-invariance property of images, new types\nof layers (e.g., wavelet transform) can exploit additional properties of\nimages, such as scale-invariance and finite spatial extents of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_K/0/1/0/all/0/1\">Kavitha Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14625","description":"<p>Cervical glandular cell (GC) detection is a key step in computer-aided\ndiagnosis for cervical adenocarcinomas screening. It is challenging to\naccurately recognize GCs in cervical smears in which squamous cells are the\nmajor. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads\ndecreasing reliability of machine learning system for GC detection. Although,\nthe State-Of-The-Art (SOTA) deep learning model can outperform pathologists in\npreselected regions of interest, the mass False Positive (FP) prediction with\nhigh probability is still unsolved when facing such gigapixel whole slide\nimage. This paper proposed a novel PolarNet based on the morphological prior\nknowledge of GC trying to solve the FP problem via a self-attention mechanism\nin eight-neighbor. It estimates the polar orientation of nucleus of GC. As a\nplugin module, PolarNet can guide the deep feature and predicted confidence of\ngeneral object detection models. In experiments, we discovered that general\nmodels based on four different frameworks can reject FP in small image set and\nincrease the mean of average precision (mAP) by $\\text{0.007}\\sim\\text{0.015}$\nin average, where the highest exceeds the recent cervical cell detection model\n0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\\% on\naccuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of\ncomputational time. Code is available in\nhttps://github.com/Chrisa142857/PolarNet-GCdet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite. (arXiv:2205.15360v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2205.15360","description":"<p>Asthma is a common, usually long-term respiratory disease with negative\nimpact on society and the economy worldwide. Treatment involves using medical\ndevices (inhalers) that distribute medication to the airways, and its\nefficiency depends on the precision of the inhalation technique. Health\nmonitoring systems equipped with sensors and embedded with sound signal\ndetection enable the recognition of drug actuation and could be powerful tools\nfor reliable audio content analysis. This paper revisits audio pattern\nrecognition and machine learning techniques for asthma medication adherence\nassessment and presents the Respiratory and Drug Actuation (RDA)\nSuite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for\nbenchmarking and further research. The RDA Suite includes a set of tools for\naudio processing, feature extraction and classification and is provided along\nwith a dataset consisting of respiratory and drug actuation sounds. The\nclassification models in RDA are implemented based on conventional and advanced\nmachine learning and deep network architectures. This study provides a\ncomparative evaluation of the implemented approaches, examines potential\nimprovements and discusses challenges and future tendencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fakotakis_N/0/1/0/all/0/1\">Nikos D. Fakotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvanitis_G/0/1/0/all/0/1\">Gerasimos Arvanitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zacharaki_E/0/1/0/all/0/1\">Evangelia I. Zacharaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakas_K/0/1/0/all/0/1\">Konstantinos Moustakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gator: Customizable Channel Pruning of Neural Networks with Gating. (arXiv:2205.15404v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15404","description":"<p>The rise of neural network (NN) applications has prompted an increased\ninterest in compression, with a particular focus on channel pruning, which does\nnot require any additional hardware. Most pruning methods employ either\nsingle-layer operations or global schemes to determine which channels to remove\nfollowed by fine-tuning of the network. In this paper we present Gator, a\nchannel-pruning method which temporarily adds learned gating mechanisms for\npruning of individual channels, and which is trained with an additional\nauxiliary loss, aimed at reducing the computational cost due to memory,\n(theoretical) speedup (in terms of FLOPs), and practical, hardware-specific\nspeedup. Gator introduces a new formulation of dependencies between NN layers\nwhich, in contrast to most previous methods, enables pruning of non-sequential\nparts, such as layers on ResNet's highway, and even removing entire ResNet\nblocks. Gator's pruning for ResNet-50 trained on ImageNet produces\nstate-of-the-art (SOTA) results, such as 50% FLOPs reduction with only\n0.4%-drop in top-5 accuracy. Also, Gator outperforms previous pruning models,\nin terms of GPU latency by running 1.4 times faster. Furthermore, Gator\nachieves improved top-5 accuracy results, compared to MobileNetV2 and\nSqueezeNet, for similar runtimes. The source code of this work is available at:\nhttps://github.com/EliPassov/gator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passov_E/0/1/0/all/0/1\">Eli Passov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Eli David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netanyahu_N/0/1/0/all/0/1\">Nathan S. Netanyahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector. (arXiv:2205.15469v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15469","description":"<p>In this paper, we present a novel end-to-end group collaborative learning\nnetwork, termed GCoNet+, which can effectively and efficiently (250 fps)\nidentify co-salient objects in natural scenes. The proposed GCoNet+ achieves\nthe new state-of-the-art performance for co-salient object detection (CoSOD)\nthrough mining consensus representations based on the following two essential\ncriteria: 1) intra-group compactness to better formulate the consistency among\nco-salient objects by capturing their inherent shared attributes using our\nnovel group affinity module (GAM); 2) inter-group separability to effectively\nsuppress the influence of noisy objects on the output by introducing our new\ngroup collaborating module (GCM) conditioning on the inconsistent consensus. To\nfurther improve the accuracy, we design a series of simple yet effective\ncomponents as follows: i) a recurrent auxiliary classification module (RACM)\npromoting the model learning at the semantic level; ii) a confidence\nenhancement module (CEM) helping the model to improve the quality of the final\npredictions; and iii) a group-based symmetric triplet (GST) loss guiding the\nmodel to learn more discriminative features. Extensive experiments on three\nchallenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that\nour GCoNet+ outperforms the existing 12 cutting-edge models. Code has been\nreleased at https://github.com/ZhengPeng7/GCoNet_plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15688","description":"<p>In the field of post-disaster assessment, for timely and accurate rescue and\nlocalization after a disaster, people need to know the location of damaged\nbuildings. In deep learning, some scholars have proposed methods to make\nautomatic and highly accurate building damage assessments by remote sensing\nimages, which are proved to be more efficient than assessment by domain\nexperts. However, due to the lack of a large amount of labeled data, these\nkinds of tasks can suffer from being able to do an accurate assessment, as the\nefficiency of deep learning models relies highly on labeled data. Although\nexisting semi-supervised and unsupervised studies have made breakthroughs in\nthis area, none of them has completely solved this problem. Therefore, we\npropose adopting a self-supervised comparative learning approach to address the\ntask without the requirement of labeled data. We constructed a novel asymmetric\ntwin network architecture and tested its performance on the xBD dataset.\nExperiment results of our model show the improvement compared to baseline and\ncommonly used methods. We also demonstrated the potential of self-supervised\nmethods for building damage recognition awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zaishuo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jinze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adriano_B/0/1/0/all/0/1\">Bruno Adriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Iterative Recovery from Nonlinear Observations using Generative Models. (arXiv:2205.15749v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15749","description":"<p>In this paper, we aim to estimate the direction of an underlying signal from\nits nonlinear observations following the semi-parametric single index model\n(SIM). Unlike conventional compressed sensing where the signal is assumed to be\nsparse, we assume that the signal lies in the range of an $L$-Lipschitz\ncontinuous generative model with bounded $k$-dimensional inputs. This is mainly\nmotivated by the tremendous success of deep generative models in various real\napplications. Our reconstruction method is non-iterative (though approximating\nthe projection step may use an iterative procedure) and highly efficient, and\nit is shown to attain the near-optimal statistical rate of order $\\sqrt{(k \\log\nL)/m}$, where $m$ is the number of measurements. We consider two specific\ninstances of the SIM, namely noisy $1$-bit and cubic measurement models, and\nperform experiments on image datasets to demonstrate the efficacy of our\nmethod. In particular, for the noisy $1$-bit measurement model, we show that\nour non-iterative method significantly outperforms a state-of-the-art iterative\nmethod in terms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymFormer: End-to-end symbolic regression using transformer-based architecture. (arXiv:2205.15764v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15764","description":"<p>Many real-world problems can be naturally described by mathematical formulas.\nThe task of finding formulas from a set of observed inputs and outputs is\ncalled symbolic regression. Recently, neural networks have been applied to\nsymbolic regression, among which the transformer-based ones seem to be the most\npromising. After training the transformer on a large number of formulas (in the\norder of days), the actual inference, i.e., finding a formula for new, unseen\ndata, is very fast (in the order of seconds). This is considerably faster than\nstate-of-the-art evolutionary methods. The main drawback of transformers is\nthat they generate formulas without numerical constants, which have to be\noptimized separately, so yielding suboptimal results. We propose a\ntransformer-based approach called SymFormer, which predicts the formula by\noutputting the individual symbols and the corresponding constants\nsimultaneously. This leads to better performance in terms of fitting the\navailable data. In addition, the constants provided by SymFormer serve as a\ngood starting point for subsequent tuning via gradient descent to further\nimprove the performance. We show on a set of benchmarks that SymFormer\noutperforms two state-of-the-art methods while having faster inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vastl_M/0/1/0/all/0/1\">Martin Vastl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubalik_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Kubal&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derner_E/0/1/0/all/0/1\">Erik Derner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babu&#x161;ka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video. (arXiv:2205.15838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15838","description":"<p>Given a monocular video, segmenting and decoupling dynamic objects while\nrecovering the static environment is a widely studied problem in machine\nintelligence. Existing solutions usually approach this problem in the image\ndomain, limiting their performance and understanding of the environment. We\nintroduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a\nself-supervised approach that takes a monocular video and learns a 3D scene\nrepresentation which decouples moving objects, including their shadows, from\nthe static background. Our method represents the moving objects and the static\nbackground by two separate neural radiance fields with only one allowing for\ntemporal changes. A naive implementation of this approach leads to the dynamic\ncomponent taking over the static one as the representation of the former is\ninherently more general and prone to overfitting. To this end, we propose a\nnovel loss to promote correct separation of phenomena. We further propose a\nshadow field network to detect and decouple dynamically moving shadows. We\nintroduce a new dataset containing various dynamic objects and shadows and\ndemonstrate that our method can achieve better performance than\nstate-of-the-art approaches in decoupling dynamic and static 3D objects,\nocclusion and shadow removal, and image segmentation for moving objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangcheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1\">Cengiz Oztireli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Competitive Method for Dog Nose-print Re-identification. (arXiv:2205.15934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15934","description":"<p>Vision-based pattern identification (such as face, fingerprint, iris etc.)\nhas been successfully applied in human biometrics for a long history. However,\ndog nose-print authentication is a challenging problem since the lack of a\nlarge amount of labeled data. For that, this paper presents our proposed\nmethods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet\nbiometric challenge. First, considering the problem that each class only with\nfew samples in the training set, we propose an automatic offline data\naugmentation strategy. Then, for the difference in sample styles between the\ntraining and test datasets, we employ joint cross-entropy, triplet and\npair-wise circle losses function for network optimization. Finally, with\nmultiple models ensembled adopted, our methods achieve 86.67\\% AUC on the test\nset. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiaode Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jinhui Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}