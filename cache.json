{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"LinkBERT: Pretraining Language Models with Document Links. (arXiv:2203.15827v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15827","description":"<p>Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture. (arXiv:2203.15833v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15833","description":"<p>Person name capture from human speech is a difficult task in human-machine\nconversations. In this paper, we propose a novel approach to capture the person\nnames from the caller utterances in response to the prompt \"say and spell your\nfirst/last name\". Inspired from work on spell correction, disfluency removal\nand text normalization, we propose a lightweight Seq-2-Seq system which\ngenerates a name spell from a varying user input. Our proposed method\noutperforms the strong baseline which is based on LM-driven rule-based\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pressel_D/0/1/0/all/0/1\">Daniel Pressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Co-Training for Learning Discrete Speech Representations. (arXiv:2203.15840v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15840","description":"<p>While several self-supervised approaches for learning discrete speech\nrepresentation have been proposed, it is unclear how these seemingly similar\napproaches relate to each other. In this paper, we consider a generative model\nwith discrete latent variables that learns a discrete representation for\nspeech. The objective of learning the generative model is formulated as\ninformation-theoretic co-training. Besides the wide generality, the objective\ncan be optimized with several approaches, subsuming HuBERT-like training and\nvector quantization for learning discrete representation. Empirically, we find\nthat the proposed approach learns discrete representation that is highly\ncorrelated with phonetic units, more correlated than HuBERT-like training and\nvector quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_S/0/1/0/all/0/1\">Sung-Lin Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics. (arXiv:2203.15858v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15858","description":"<p>Current practices in metric evaluation focus on one single dataset, e.g.,\nNewstest dataset in each year's WMT Metrics Shared Task. However, in this\npaper, we qualitatively and quantitatively show that the performances of\nmetrics are sensitive to data. The ranking of metrics varies when the\nevaluation is conducted on different datasets. Then this paper further\ninvestigates two potential hypotheses, i.e., insignificant data points and the\ndeviation of Independent and Identically Distributed (i.i.d) assumption, which\nmay take responsibility for the issue of data variance. In conclusion, our\nfindings suggest that when evaluating automatic translation metrics,\nresearchers should take data variance into account and be cautious to claim the\nresult on a single dataset, because it may leads to inconsistent results with\nmost of other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing the Relationship Between Encoded Linguistic Information and Task Performance. (arXiv:2203.15860v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15860","description":"<p>Probing is popular to analyze whether linguistic information can be captured\nby a well-trained deep neural model, but it is hard to answer how the change of\nthe encoded linguistic information will affect task performance. To this end,\nwe study the dynamic relationship between the encoded linguistic information\nand task performance from the viewpoint of Pareto Optimality. Its key idea is\nto obtain a set of models which are Pareto-optimal in terms of both objectives.\nFrom this viewpoint, we propose a method to optimize the Pareto-optimal models\nby formalizing it as a multi-objective optimization problem. We conduct\nexperiments on two popular NLP tasks, i.e., machine translation and language\nmodeling, and investigate the relationship between several kinds of linguistic\ninformation and task performances. Experimental results demonstrate that the\nproposed method is better than a baseline method. Our empirical findings\nsuggest that some syntactic information is helpful for NLP tasks whereas\nencoding more syntactic information does not necessarily lead to better\nperformance, because the model architecture is also an important factor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. (arXiv:2203.15863v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15863","description":"<p>Large-scale auto-regressive language models pretrained on massive text have\ndemonstrated their impressive ability to perform new natural language tasks\nwith only a few text examples, without the need for fine-tuning. Recent studies\nfurther show that such a few-shot learning ability can be extended to the\ntext-image setting by training an encoder to encode the images into embeddings\nfunctioning like the text embeddings of the language model. Interested in\nexploring the possibility of transferring the few-shot learning ability to the\naudio-text setting, we propose a novel speech understanding framework,\nWavPrompt, where we finetune a wav2vec model to generate a sequence of audio\nembeddings understood by the language model. We show that WavPrompt is a\nfew-shot learner that can perform speech understanding tasks better than a\nnaive text baseline. We conduct detailed ablation studies on different\ncomponents and hyperparameters to empirically identify the best model\nconfiguration. In addition, we conduct a non-speech understanding experiment to\nshow WavPrompt can extract more information than just the transcriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_H/0/1/0/all/0/1\">Heting Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_J/0/1/0/all/0/1\">Junrui Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Retrieval from Contextual Descriptions. (arXiv:2203.15867v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15867","description":"<p>The ability to integrate context, including perceptual and temporal cues,\nplays a pivotal role in grounding the meaning of a linguistic utterance. In\norder to measure to what extent current vision-and-language models master this\nability, we devise a new multimodal challenge, Image Retrieval from Contextual\nDescriptions (ImageCoDe). In particular, models are tasked with retrieving the\ncorrect image from a set of 10 minimally contrastive candidates based on a\ncontextual description. As such, each description contains only the details\nthat help distinguish between images. Because of this, descriptions tend to be\ncomplex in terms of syntax and discourse and require drawing pragmatic\ninferences. Images are sourced from both static pictures and video frames. We\nbenchmark several state-of-the-art models, including both cross-encoders such\nas ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that\nthese models dramatically lag behind human performance: the best variant\nachieves an accuracy of 20.9 on video frames and 59.4 on static pictures,\ncompared with 90.8 in humans. Furthermore, we experiment with new model\nvariants that are better equipped to incorporate visual and temporal context\ninto their representations, which achieve modest gains. Our hope is that\nImageCoDE will foster progress in grounded language understanding by\nencouraging models to focus on fine-grained visual differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1\">Benno Krojer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_Y/0/1/0/all/0/1\">Yash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization. (arXiv:2203.15917v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15917","description":"<p>Text normalization (TN) systems in production are largely rule-based using\nweighted finite-state transducers (WFST). However, WFST-based systems struggle\nwith ambiguous input when the normalized form is context-dependent. On the\nother hand, neural text normalization systems can take context into account but\nthey suffer from unrecoverable errors and require labeled normalization\ndatasets, which are hard to collect. We propose a new hybrid approach that\ncombines the benefits of rule-based and neural systems. First, a\nnon-deterministic WFST outputs all normalization candidates, and then a neural\nlanguage model picks the best one -- similar to shallow fusion for automatic\nspeech recognition. While the WFST prevents unrecoverable errors, the language\nmodel resolves contextual ambiguity. The approach is easy to extend and we show\nit is effective. It achieves comparable or better results than existing\nstate-of-the-art TN models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1\">Evelina Bakhturina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15937","description":"<p>Current leading mispronunciation detection and diagnosis (MDD) systems\nachieve promising performance via end-to-end phoneme recognition. One challenge\nof such end-to-end solutions is the scarcity of human-annotated phonemes on\nnatural L2 speech. In this work, we leverage unlabeled L2 speech via a\npseudo-labeling (PL) procedure and extend the fine-tuning approach based on\npre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec\n2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples\nplus the created pseudo-labeled L2 speech samples. Our pseudo labels are\ndynamic and are produced by an ensemble of the online model on-the-fly, which\nensures that our model is robust to pseudo label noise. We show that\nfine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and\n2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning\nbaseline. The proposed PL method is also shown to outperform conventional\noffline PL methods. Compared to the state-of-the-art MDD systems, our MDD\nsolution achieves a more accurate and consistent phonetic error diagnosis. In\naddition, we conduct an open test on a separate UTD-4Accents dataset, where our\nsystem recognition outputs show a strong correlation with human perception,\nbased on accentedness and intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirschi_K/0/1/0/all/0/1\">Kevin Hirschi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Looney_S/0/1/0/all/0/1\">Stephen D. Looney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_O/0/1/0/all/0/1\">Okim Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_J/0/1/0/all/0/1\">John H. L. Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-driven Fact-aware Abstractive Summarization of Biomedical Literature. (arXiv:2203.15959v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15959","description":"<p>As part of the large number of scientific articles being published every\nyear, the publication rate of biomedical literature has been increasing.\nConsequently, there has been considerable effort to harness and summarize the\nmassive amount of biomedical research articles. While transformer-based\nencoder-decoder models in a vanilla source document-to-summary setting have\nbeen extensively studied for abstractive summarization in different domains,\ntheir major limitations continue to be entity hallucination (a phenomenon where\ngenerated summaries constitute entities not related to or present in source\narticle(s)) and factual inconsistency. This problem is exacerbated in a\nbiomedical setting where named entities and their semantics (which can be\ncaptured through a knowledge base) constitute the essence of an article. The\nuse of named entities and facts mined from background knowledge bases\npertaining to the named entities to guide abstractive summarization has not\nbeen studied in biomedical article summarization literature. In this paper, we\npropose an entity-driven fact-aware framework for training end-to-end\ntransformer-based encoder-decoder models for abstractive summarization of\nbiomedical articles. We call the proposed approach, whose building block is a\ntransformer-based model, EFAS, Entity-driven Fact-aware Abstractive\nSummarization. We conduct experiments using five state-of-the-art\ntransformer-based models (two of which are specifically designed for long\ndocument summarization) and demonstrate that injecting knowledge into the\ntraining/inference phase of these models enables the models to achieve\nsignificantly better performance than the standard source document-to-summary\nsetting in terms of entity-level factual accuracy, N-gram novelty, and semantic\nequivalence while performing comparably on ROUGE metrics. The proposed approach\nis evaluated on ICD-11-Summ-1000, and PubMed-50k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alambo_A/0/1/0/all/0/1\">Amanuel Alambo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tanvi Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thirunarayan_K/0/1/0/all/0/1\">Krishnaprasad Thirunarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raymer_M/0/1/0/all/0/1\">Michael Raymer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Domain Adaptation for ASR with Full Self-Supervision. (arXiv:2203.15966v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15966","description":"<p>Cross-device federated learning (FL) protects user privacy by collaboratively\ntraining a model on user devices, therefore eliminating the need for\ncollecting, storing, and manually labeling user data. Previous works have\nconsidered cross-device FL for automatic speech recognition (ASR), however,\nthere are a few important challenges that have not been fully addressed. These\ninclude the lack of ground-truth ASR transcriptions, and the scarcity of\ncompute resource and network bandwidth on edge devices. In this paper, we\naddress these two challenges. First, we propose a federated learning system to\nsupport on-device ASR adaptation with full self-supervision, which uses\nself-labeling together with data augmentation and filtering techniques. The\nproposed system can improve a strong Emformer-Transducer based ASR model\npretrained on out-of-domain data, using in-domain audios without any\nground-truth transcriptions. Second, to reduce the training cost, we propose a\nself-restricted RNN Transducer (SR-RNN-T) loss, a new variant of\nalignment-restricted RNN-T that uses Viterbi forced-alignment from\nself-supervision. To further reduce the compute and network cost, we\nsystematically explore adapting only a subset of weights in the\nEmformer-Transducer. Our best training recipe achieves a 12.9% relative WER\nreduction over the strong out-of-domain baseline, which equals 70% of the\nreduction achievable with full human supervision and centralized training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Junteng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seide_F/0/1/0/all/0/1\">Frank Seide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Speaker Diarization with Dynamic Scale Weighting. (arXiv:2203.15974v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15974","description":"<p>Speaker diarization systems are challenged by a trade-off between the\ntemporal resolution and the fidelity of the speaker representation. By\nobtaining a superior temporal resolution with an enhanced accuracy, a\nmulti-scale approach is a way to cope with such a trade-off. In this paper, we\npropose a more advanced multi-scale diarization system based on a multi-scale\ndiarization decoder. There are two main contributions in this study that\nsignificantly improve the diarization performance. First, we use multi-scale\nclustering as an initialization to estimate the number of speakers and obtain\nthe average speaker representation vector for each speaker and each scale.\nNext, we propose the use of 1-D convolutional neural networks that dynamically\ndetermine the importance of each scale at each time step. To handle a variable\nnumber of speakers and overlapping speech, the proposed system can estimate the\nnumber of existing speakers. Our proposed system achieves a state-of-art\nperformance on the CALLHOME and AMI MixHeadset datasets, with 3.92% and 1.05%\ndiarization error rates, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Tae Jin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koluguri_N/0/1/0/all/0/1\">Nithin Rao Koluguri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balam_J/0/1/0/all/0/1\">Jagadeesh Balam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models. (arXiv:2203.15996v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15996","description":"<p>Pre-trained language models have been prevailed in natural language\nprocessing and become the backbones of many NLP tasks, but the demands for\ncomputational resources have limited their applications. In this paper, we\nintroduce TextPruner, an open-source model pruning toolkit designed for\npre-trained language models, targeting fast and easy model compression.\nTextPruner offers structured post-training pruning methods, including\nvocabulary pruning and transformer pruning, and can be applied to various\nmodels and tasks. We also propose a self-supervised pruning method that can be\napplied without the labeled data. Our experiments with several NLP tasks\ndemonstrate the ability of TextPruner to reduce the model size without\nre-training the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension. (arXiv:2203.16027v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16027","description":"<p>Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and\nprovides performance lift by adapting unlabelled data to downstream task.\nUnfortunately, existing adaptations mainly involve deterministic rules that\ncannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze\nanswer extraction method used in TAPT that is extendable for adaptation on any\ncloze-style machine reading comprehension (MRC) downstream tasks. We experiment\non multiple-choice cloze-style MRC tasks, and show that Clozer performs\nsignificantly better compared to the oracle and state-of-the-art in escalating\nTAPT effectiveness in lifting model performance, and prove that Clozer is able\nto recognize the gold answers independently of any heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Su Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Classification with Structured Information for Disfluency Detection in Spoken Utterances. (arXiv:2203.16028v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16028","description":"<p>Existing approaches in disfluency detection focus on solving a token-level\nclassification task for identifying and removing disfluencies in text.\nMoreover, most works focus on leveraging only contextual information captured\nby the linear sequences in text, thus ignoring the structured information in\ntext which is efficiently captured by dependency trees. In this paper, building\non the span classification paradigm of entity recognition, we propose a novel\narchitecture for detecting disfluencies in transcripts from spoken utterances,\nincorporating both contextual information through transformers and\nlong-distance structured information captured by dependency trees, through\ngraph convolutional networks (GCNs). Experimental results show that our\nproposed model achieves state-of-the-art results on the widely used English\nSwitchboard for disfluency detection and outperforms prior-art by a significant\nmargin. We make all our codes publicly available on GitHub\n(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sonal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks. (arXiv:2203.16040v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16040","description":"<p>Because the performance of speech separation is excellent for speech in which\ntwo speakers completely overlap, research attention has been shifted to dealing\nwith more realistic scenarios. However, domain mismatch between training/test\nsituations due to factors, such as speaker, content, channel, and environment,\nremains a severe problem for speech separation. Speaker and environment\nmismatches have been studied in the existing literature. Nevertheless, there\nare few studies on speech content and channel mismatches. Moreover, the impacts\nof language and channel in these studies are mostly tangled. In this study, we\ncreate several datasets for various experiments. The results show that the\nimpacts of different languages are small enough to be ignored compared to the\nimpacts of different channels. In our experiments, training on data recorded by\nAndroid phones leads to the best generalizability. Moreover, we provide a new\nsolution for channel mismatch by evaluating projection, where the channel\nsimilarity can be measured and used to effectively select additional training\ndata to improve the performance of in-the-wild test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan-Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Graph Convolutional Networks for Text Classification. (arXiv:2203.16060v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16060","description":"<p>Graph Convolutional Networks (GCN) have been effective at tasks that have\nrich relational structure and can preserve global structure information of a\ndataset in graph embeddings. Recently, many researchers focused on examining\nwhether GCNs could handle different Natural Language Processing tasks,\nespecially text classification. While applying GCNs to text classification is\nwell-studied, its graph construction techniques, such as node/edge selection\nand their feature representation, and the optimal GCN learning mechanism in\ntext classification is rather neglected. In this paper, we conduct a\ncomprehensive analysis of the role of node and edge embeddings in a graph and\nits GCN learning techniques in text classification. Our analysis is the first\nof its kind and provides useful insights into the importance of each graph\nnode/edge construction mechanism when applied at the GCN training/testing in\ndifferent text classification benchmarks, as well as under its semi-supervised\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zihan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16127","description":"<p>In this paper, we survey Text Summarization (TS) datasets in Indian\nLan-guages (ILs), which are also low-resource languages (LRLs). We seek to\nanswer one primary question: is the pool of Indian Language Text Summarization\n(ILTS) dataset growing or is there a serious resource poverty? To an-swer the\nprimary question, we pose two sub-questions that we seek about ILTS datasets:\nfirst, what characteristics: format and domain do ILTS da-tasets have? Second,\nhow different are those characteristics of ILTS datasets from high-resource\nlanguages (HRLs) particularly English. The survey of ILTS and English datasets\nreveals two similarities and one contrast. The two similarities are: first, the\ndomain of dataset commonly is news (Hermann et al., 2015). The second\nsimilarity is the format of the dataset which is both extractive and\nabstractive. The contrast is in how the research in dataset development has\nprogressed. ILs face a slow speed of development and public release of datasets\nas compared with English. We conclude that the relatively lower number of ILTS\ndatasets is because of two reasons: first, absence of a dedicated forum for\ndeveloping TS tools. And second, lack of shareable standard datasets in the\npublic domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Shagun Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_G/0/1/0/all/0/1\">Girish Nath Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling. (arXiv:2203.16169v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16169","description":"<p>This work presents a new resource for borrowing identification and analyzes\nthe performance and errors of several models on this task. We introduce a new\nannotated corpus of Spanish newswire rich in unassimilated lexical borrowings\n-- words from one language that are introduced into another without\northographic adaptation -- and use it to evaluate how several sequence labeling\nmodels (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus\ncontains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and\ntopic-varied than previous corpora available for this task. Our results show\nthat a BiLSTM-CRF model fed with subword embeddings along with either\nTransformer-based embeddings pretrained on codeswitched data or a combination\nof contextualized word embeddings outperforms results obtained by a\nmultilingual BERT-based model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Mellado_E/0/1/0/all/0/1\">Elena &#xc1;lvarez-Mellado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval. (arXiv:2203.16187v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16187","description":"<p>Contrastive learning (CL) has become a ubiquitous approach for several\nnatural language processing (NLP) downstream tasks, especially for question\nanswering (QA). However, the major challenge, how to efficiently train the\nknowledge retrieval model in an unsupervised manner, is still unresolved.\nRecently the commonly used methods are composed of CL and masked language model\n(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also\nneglects extraction of the internal info from the query. To optimize the CL\nhardly obtain internal information from the original query, we introduce a\njoint training method by combining CL and Auto-MLM for self-supervised\nmulti-lingual knowledge retrieval. First, we acquire the fixed dimensional\nsentence vector. Then, mask some words among the original sentences with random\nstrategy. Finally, we generate a new token representation for predicting the\nmasked tokens. Experimental results show that our proposed approach\nconsistently outperforms all the previous SOTA methods on both AliExpress $\\&amp;$\nLAZADA service corpus and openly available corpora in 8 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maimaiti_M/0/1/0/all/0/1\">Mieradilijiang Maimaiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic generation of semantic corpora for improving intent estimation of taxonomy-driven search engines. (arXiv:2203.16230v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16230","description":"<p>With the increasing demand of intelligent systems capable of operating in\ndifferent user contexts (e.g. users on the move) the correct interpretation of\nthe user-need by such systems has become crucial to give a consistent answer to\nthe user query. The most effective techniques which are used to address such\ntask are in the fields of natural language processing and semantic expansion of\nterms. Such systems are aimed at estimating the actual meaning of input\nqueries, addressing the concepts of the words which are expressed within the\nuser questions. The aim of this paper is to demonstrate which semantic relation\nimpacts the most in semantic expansion-based retrieval systems and to identify\nthe best tradeoff between accuracy and noise introduction when combining such\nrelations. The evaluations are made building a simple natural language\nprocessing system capable of querying any taxonomy-driven domain, making use of\nthe combination of different semantic expansions as knowledge resources. The\nproposed evaluation employs a wide and varied taxonomy as a use-case,\nexploiting its labels as basis for the expansions. To build the knowledge\nresources several corpora have been produced and integrated as gazetteers into\nthe NLP infrastructure with the purpose of estimating the pseudo-queries\ncorresponding to the taxonomy labels, considered as the possible intents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Massai_L/0/1/0/all/0/1\">Lorenzo Massai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Translation with Dependency-Aware Decoder. (arXiv:2203.16266v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16266","description":"<p>Non-autoregressive translation (NAT) models suffer from inferior translation\nquality due to removal of dependency on previous target tokens from inputs to\nthe decoder. In this paper, we propose a novel and general approach to enhance\nthe target dependency within the NAT decoder from two perspectives: decoder\ninput and decoder self-attention. First, we transform the initial decoder input\nfrom the source language space to the target language space through a novel\nattentive transformation process. The transformation reassembles the decoder\ninput based on target token embeddings and conditions the final output on the\ntarget-side information. Second, before NAT training, we introduce an effective\nforward-backward pre-training phase, implemented with different triangle\nattention masks. This pre-training phase enables the model to gradually learn\nbidirectional dependencies for the final NAT decoding process. Experimental\nresults demonstrate that the proposed approaches consistently improve highly\ncompetitive NAT models on four WMT translation directions by up to 1.88 BLEU\nscore, while overall maintaining inference latency comparable to other fully\nNAT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jiaao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Pipeline for Zero-Shot Data-to-Text Generation. (arXiv:2203.16279v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16279","description":"<p>In data-to-text (D2T) generation, training on in-domain data leads to\noverfitting to the data representation and repeating training data noise. We\nexamine how to avoid finetuning pretrained language models (PLMs) on D2T\ngeneration datasets while still taking advantage of surface realization\ncapabilities of PLMs. Inspired by pipeline approaches, we propose to generate\ntext by transforming single-item descriptions with a sequence of modules\ntrained on general-domain text-based operations: ordering, aggregation, and\nparagraph compression. We train PLMs for performing these operations on a\nsynthetic corpus WikiFluent which we build from English Wikipedia. Our\nexperiments on two major triple-to-text datasets -- WebNLG and E2E -- show that\nour approach enables D2T generation from RDF triples in zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1\">Zden&#x11b;k Kasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting. (arXiv:2203.16361v1 [cs.SD])","link":"http://arxiv.org/abs/2203.16361","description":"<p>Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. This problem will be more challenging if KWS\nmodels are further required for edge devices due to their limited memory. To\nalleviate such an issue, we propose a novel diversity-aware incremental\nlearning method named Rainbow Keywords (RK). Specifically, the proposed RK\napproach introduces a diversity-aware sampler to select a diverse set from\nhistorical and incoming keywords by calculating classification uncertainty. As\na result, the RK approach can incrementally learn new tasks without forgetting\nprior knowledge. Besides, the RK approach also proposes data augmentation and\nknowledge distillation loss function for efficient memory management on the\nedge device. Experimental results show that the proposed RK approach achieves\n4.2% absolute improvement in terms of average accuracy over the best baseline\non Google Speech Command dataset with less required memory. The scripts are\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis. (arXiv:2203.16369v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16369","description":"<p>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a\nspecific aspect in the given sentence. While pre-trained language models such\nas BERT have achieved great success, incorporating dynamic semantic changes\ninto ABSA remains challenging. To this end, in this paper, we propose to\naddress this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method\ndesigned to learn dynamic aspect-oriented semantics for ABSA. Specifically, we\nfirst take the Stack-BERT layers as a primary encoder to grasp the overall\nsemantic of the sentence and then fine-tune it by incorporating a lightweight\nDynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention\nto a small region of the sentences at each step and re-weigh the vitally\nimportant words for better aspect-aware sentiment understanding. Finally,\nexperimental results on three benchmark datasets demonstrate the effectiveness\nand the rationality of our proposed model and provide good interpretable\ninsights for future semantic modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16434","description":"<p>We consider the problem of localizing a spatio-temporal tube in a video\ncorresponding to a given text query. This is a challenging task that requires\nthe joint and efficient modeling of temporal, spatial and multi-modal\ninteractions. To address this task, we propose TubeDETR, a transformer-based\narchitecture inspired by the recent success of such models for text-conditioned\nobject detection. Our model notably includes: (i) an efficient video and text\nencoder that models spatial multi-modal interactions over sparsely sampled\nframes and (ii) a space-time decoder that jointly performs spatio-temporal\nlocalization. We demonstrate the advantage of our proposed components through\nan extensive ablation study. We also evaluate our full approach on the\nspatio-temporal video grounding task and demonstrate improvements over the\nstate of the art on the challenging VidSTG and HC-STVG benchmarks. Code and\ntrained models are publicly available at\nhttps://antoyang.github.io/tubedetr.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models. (arXiv:2203.16474v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16474","description":"<p>Eye tracking data during reading is a useful source of information to\nunderstand the cognitive processes that take place during language\ncomprehension processes. Different languages account for different brain\ntriggers , however there seems to be some uniform indicators. In this paper, we\ndescribe our submission to the CMCL 2022 shared task on predicting human\nreading patterns for multi-lingual dataset. Our model uses text representations\nfrom transformers and some hand engineered features with a regression layer on\ntop to predict statistical measures of mean and standard deviation for 2 main\neye-tracking features. We train an end to end model to extract meaningful\ninformation from different languages and test our model on two seperate\ndatasets. We compare different transformer models and show ablation studies\naffecting model performance. Our final submission ranked 4th place for\nSubTask-1 and 1st place for SubTask-2 for the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16487","description":"<p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel\napproach to accelerating autoregressive translation with no quality loss,\nthrough the collaboration of autoregressive and non-autoregressive translation\n(NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes\na number of tokens in parallel as a draft through NAT and then verifies them in\nthe autoregressive manner, where only the tokens that pass the verification are\nkept as decoded tokens. GAD can achieve the same performance as autoregressive\ntranslation but perform much more efficiently because both NAT drafting and\nautoregressive verification are fast due to parallel computing. We conduct\nexperiments in the WMT14 English-German translation task and confirm that the\nvanilla GAD yields exactly the same results as greedy decoding with about 3x\nspeedup, and that its variant (GAD++) with an advanced verification strategy\nnot only outperforms the greedy translation and even achieves the comparable\ntranslation quality with the beam search result, but also further improves the\ndecoding speed, resulting in an around 5x speedup over autoregressive\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Spoken Dialogue Language Modeling. (arXiv:2203.16502v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16502","description":"<p>We introduce dGSLM, the first \"textless\" model able to generate audio samples\nof naturalistic spoken dialogues. It uses recent work on unsupervised spoken\nunit discovery coupled with a dual-tower transformer architecture with\ncross-attention trained on 2000 hours of two-channel raw conversational audio\n(Fisher dataset) without any text or labels. It is able to generate speech,\nlaughter and other paralinguistic signals in the two channels simultaneously\nand reproduces naturalistic turn taking. Generation samples can be found at:\nhttps://speechbot.github.io/dgslm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Benoit Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vakyansh: ASR Toolkit for Low Resource Indic languages. (arXiv:2203.16512v1 [cs.CL])","link":"http://arxiv.org/abs/2203.16512","description":"<p>We present Vakyansh, an end to end toolkit for Speech Recognition in Indic\nlanguages. India is home to almost 121 languages and around 125 crore speakers.\nYet most of the languages are low resource in terms of data and pretrained\nmodels. Through Vakyansh, we introduce automatic data pipelines for data\ncreation, model training, model evaluation and deployment. We create 14,000\nhours of speech data in 23 Indic languages and train wav2vec 2.0 based\npretrained models. These pretrained models are then finetuned to create state\nof the art speech recognition models for 18 Indic languages which are followed\nby language models and punctuation restoration models. We open source all these\nresources with a mission that this will inspire the speech community to develop\nspeech first applications using our ASR models in Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotional Conversation Generation with Heterogeneous Graph Neural Network. (arXiv:2012.04882v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04882","description":"<p>The successful emotional conversation system depends on sufficient perception\nand appropriate expression of emotions. In a real-life conversation, humans\nfirstly instinctively perceive emotions from multi-source information,\nincluding the emotion flow hidden in dialogue history, facial expressions,\naudio, and personalities of speakers. Then, they convey suitable emotions\naccording to their personalities, but these multiple types of information are\ninsufficiently exploited in emotional conversation fields. To address this\nissue, in this paper, we propose a heterogeneous graph-based model for\nemotional conversation generation. Firstly, we design a Heterogeneous\nGraph-Based Encoder to represent the conversation content (i.e., the dialogue\nhistory, its emotion flow, facial expressions, audio, and speakers'\npersonalities) with a heterogeneous graph neural network, and then predict\nsuitable emotions for feedback. Secondly, we employ an\nEmotion-Personality-Aware Decoder to generate a response relevant to the\nconversation context as well as with appropriate emotions, through taking the\nencoded graph representations, the predicted emotions by the encoder and the\npersonality of the current speaker as inputs. Experiments on both automatic and\nhuman evaluation show that our method can effectively perceive emotions from\nmulti-source knowledge and generate a satisfactory response. Furthermore, based\non the up-to-date text generator BART, our model still can achieve consistent\nimprovement, which significantly outperforms some existing state-of-the-art\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10407","description":"<p>The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. (arXiv:2105.04387v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04387","description":"<p>Dialogue systems are a popular natural language processing (NLP) task as it\nis promising in real-life applications. It is also a complicated task since\nmany NLP tasks deserving study are involved. As a result, a multitude of novel\nworks on this task are carried out, and most of them are deep learning based\ndue to the outstanding performance. In this survey, we mainly focus on the deep\nlearning based dialogue systems. We comprehensively review state-of-the-art\nresearch outcomes in dialogue systems and analyze them from two angles: model\ntype and system type. Specifically, from the angle of model type, we discuss\nthe principles, characteristics, and applications of different models that are\nwidely used in dialogue systems. This will help researchers acquaint these\nmodels and see how they are applied in state-of-the-art frameworks, which is\nrather helpful when designing a new dialogue system. From the angle of system\ntype, we discuss task-oriented and open-domain dialogue systems as two streams\nof research, providing insight into the hot topics related. Furthermore, we\ncomprehensively review the evaluation methods and datasets for dialogue systems\nto pave the way for future research. Finally, some possible research trends are\nidentified based on the recent research outcomes. To the best of our knowledge,\nthis survey is the most comprehensive and up-to-date one at present for deep\nlearning based dialogue systems, extensively covering the popular techniques.\nWe speculate that this work is a good starting point for academics who are new\nto the dialogue systems or those who want to quickly grasp up-to-date\ntechniques in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1\">Vlad Pandelea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03322","description":"<p>Traditional event extraction methods require predefined event types and their\ncorresponding annotations to learn event extractors. These prerequisites are\noften hard to be satisfied in real-world applications. This work presents a\ncorpus-based open-domain event type induction method that automatically\ndiscovers a set of event types from a given corpus. As events of the same type\ncould be expressed in multiple ways, we propose to represent each event type as\na cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)\nselects salient predicates and object heads, (2) disambiguates predicate senses\nusing only a verb sense dictionary, and (3) obtains event types by jointly\nembedding and clustering &lt;predicate sense, object head&gt; pairs in a latent\nspherical space. Our experiments, on three datasets from different domains,\nshow our method can discover salient and high-quality event types, according to\nboth automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTC Variations Through New WFST Topologies. (arXiv:2110.03098v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03098","description":"<p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n&lt;epsilon&gt; back-off transitions; (2) the \"minimal-CTC\", that only adds &lt;blank&gt;\nself-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants,\nwhich disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times\nsmaller WFST decoding graphs and reduces memory consumption by two times when\ntraining CTC models with the LF-MMI objective without hurting the recognition\naccuracy. Minimal-CTC reduces graph size and memory consumption by two and four\ntimes for the cost of a small accuracy drop. Using selfless-CTC can improve the\naccuracy for wide context window models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04353","description":"<p>When a software bug is reported, developers engage in a discussion to\ncollaboratively resolve it. While the solution is likely formulated within the\ndiscussion, it is often buried in a large amount of text, making it difficult\nto comprehend and delaying its implementation. To expedite bug resolution, we\npropose generating a concise natural language description of the solution by\nsynthesizing relevant content within the discussion, which encompasses both\nnatural language and source code. We build a corpus for this task using a novel\ntechnique for obtaining noisy supervision from repository changes linked to bug\nreports, with which we establish benchmarks. We also design two systems for\ngenerating a description during an ongoing discussion by classifying when\nsufficient context for performing the task emerges in real-time. With automated\nand human evaluation, we find this task to form an ideal testbed for complex\nreasoning in long, bimodal dialogue context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panthaplackel_S/0/1/0/all/0/1\">Sheena Panthaplackel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_M/0/1/0/all/0/1\">Milos Gligoric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07596","description":"<p>Deep NLP models have been shown to learn spurious correlations, leaving them\nbrittle to input perturbations. Recent work has shown that counterfactual or\ncontrastive data -- i.e. minimally perturbed inputs -- can reveal these\nweaknesses, and that data augmentation using counterfactuals can help\nameliorate them. Proposed techniques for generating counterfactuals rely on\nhuman annotations, perturbations based on simple heuristics, and meaning\nrepresentation frameworks. We focus on the task of creating counterfactuals for\nquestion answering, which presents unique challenges related to world\nknowledge, semantic diversity, and answerability. To address these challenges,\nwe develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual\nevaluation and training data with minimal human supervision. Using an\nopen-domain QA framework and question generation model trained on original task\ndata, we create counterfactuals that are fluent, semantically diverse, and\nautomatically labeled. Data augmentation with RGF counterfactuals improves\nperformance on out-of-domain and challenging evaluation sets over and above\nexisting methods, in both the reading comprehension and open-domain QA\nsettings. Moreover, we find that RGF data leads to significant improvements in\na model's robustness to local perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models. (arXiv:2110.08151v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08151","description":"<p>Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations. Our source code and pretrained models are available at\nhttps://github.com/studio-ousia/luke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00600","description":"<p>We train neural networks to optimize a Minimum Description Length score,\ni.e., to balance between the complexity of the network and its accuracy at a\ntask. We show that networks optimizing this objective function master tasks\ninvolving memory challenges and go beyond context-free languages. These\nlearners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,\n$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with\n100% accuracy. The networks are small, and their inner workings are\ntransparent. We thus provide formal proofs that their perfect accuracy holds\nnot only on a given test set, but for any input sequence. To our knowledge, no\nother connectionist model has been shown to capture the underlying grammars for\nthese languages in full generality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1\">Michal Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04482","description":"<p>State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a \"foundation\", that targets all\nmodalities at once -- a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07534","description":"<p>Document summarization, as a fundamental task in natural language generation,\naims to generate a short and coherent summary for a given document.\nControllable summarization, especially of the length, is an important issue for\nsome practical applications, especially how to trade-off the length constraint\nand information integrity. In this paper, we propose an \\textbf{A}daptive\n\\textbf{L}ength \\textbf{C}ontrolling \\textbf{O}ptimization (\\textbf{ALCO})\nmethod to leverage two-stage abstractive summarization model via reinforcement\nlearning. ALCO incorporates length constraint into the stage of sentence\nextraction to penalize the overlength extracted sentences. Meanwhile, a\nsaliency estimation mechanism is designed to preserve the salient information\nin the generated sentences. A series of experiments have been conducted on a\nwildly-used benchmark dataset \\textit{CNN/Daily Mail}. The results have shown\nthat ALCO performs better than the popular baselines in terms of length\ncontrollability and content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcing Semantic-Symmetry for Document Summarization. (arXiv:2112.07583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07583","description":"<p>Document summarization condenses a long document into a short version with\nsalient information and accurate semantic descriptions. The main issue is how\nto make the output summary semantically consistent with the input document. To\nreach this goal, recently, researchers have focused on supervised end-to-end\nhybrid approaches, which contain an extractor module and abstractor module.\nAmong them, the extractor identifies the salient sentences from the input\ndocument, and the abstractor generates a summary from the salient sentences.\nThis model successfully keeps the consistency between the generated summary and\nthe reference summary via various strategies (e.g., reinforcement learning).\nThere are two semantic gaps when training the hybrid model (one is between\ndocument and extracted sentences, and the other is between extracted sentences\nand summary). However, they are not explicitly considered in the existing\nmethods, which usually results in a semantic bias of summary. To mitigate the\nabove issue, in this paper, a new \\textbf{r}einforcing\ns\\textbf{e}mantic-\\textbf{sy}mmetry learning \\textbf{m}odel is proposed for\ndocument summarization (\\textbf{ReSyM}). ReSyM introduces a\nsemantic-consistency reward in the extractor to bridge the first gap. A\nsemantic dual-reward is designed to bridge the second gap in the abstractor.\nThe whole document summarization process is implemented via reinforcement\nlearning with a hybrid reward mechanism (combining the above two rewards).\nMoreover, a comprehensive sentence representation learning method is presented\nto sufficiently capture the information from the original document. A series of\nexperiments have been conducted on two wildly used benchmark datasets CNN/Daily\nMail and BigPatent. The results have shown the superiority of ReSyM by\ncomparing it with the state-of-the-art baselines in terms of various evaluation\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09572","description":"<p>Document summarization provides an instrument for faster understanding the\ncollection of text documents and has several real-life applications. With the\ngrowth of online text data, numerous summarization models have been proposed\nrecently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model\nis the most widely used in the summarization field due to its high performance.\nThis is because semantic information and structure information in the text is\nadequately considered when encoding. However, the existing extractive\nsummarization models pay little attention to and use the central topic\ninformation to assist the generation of summaries, which leads to models not\nensuring the generated summary under the primary topic. A lengthy document can\nspan several topics, and a single summary cannot do justice to all the topics.\nTherefore, the key to generating a high-quality summary is determining the\ncentral topic and building a summary based on it, especially for a long\ndocument. We propose a topic-aware encoding for document summarization to deal\nwith this issue. This model effectively combines syntactic-level and\ntopic-level information to build a comprehensive sentence representation.\nSpecifically, a neural topic model is added in the neural-based sentence-level\nrepresentation learning to adequately consider the central topic information\nfor capturing the critical content in the original document. The experimental\nresults on three public datasets show that our model outperforms the\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2201.01647","description":"<p>Biomedical research is growing at such an exponential pace that scientists,\nresearchers, and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a way that claims and hypotheses can be easily\nfound, accessed, and validated. Knowledge graphs can provide such a framework\nfor semantic knowledge representation from literature. However, in order to\nbuild a knowledge graph, it is necessary to extract knowledge as relationships\nbetween biomedical entities and normalize both entities and relationship types.\nIn this paper, we present and compare a few rule-based and machine\nlearning-based (Naive Bayes, Random Forests as examples of traditional machine\nlearning methods and DistilBERT and T5-based models as examples of modern deep\nlearning transformers) methods for scalable relationship extraction from\nbiomedical literature, and for the integration into the knowledge graphs. We\nexamine how resilient are these various methods to unbalanced and fairly small\ndatasets, showing that transformer-based models handle well both small\ndatasets, due to pre-training on large C4 dataset, as well as unbalanced data.\nThe best performing model was the DistilBERT-based model fine-tuned on balanced\ndata, with a reported F1-score of 0.89.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milosevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemann_W/0/1/0/all/0/1\">Wolfgang Thielemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effectiveness of Pinyin-Character Dual-Decoding for End-to-End Mandarin Chinese ASR. (arXiv:2201.10792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10792","description":"<p>End-to-end automatic speech recognition (ASR) has achieved promising results.\nHowever, most existing end-to-end ASR methods neglect the use of specific\nlanguage characteristics. For Mandarin Chinese ASR tasks, there exist mutual\npromotion relationship between Pinyin and Character where Chinese characters\ncan be romanized by Pinyin. Based on the above intuition, we first investigate\ntypes of end-to-end encoder-decoder based models in the single-input\ndual-output (SIDO) multi-task framework, after which a novel asynchronous\ndecoding with fuzzy Pinyin sampling method is proposed according to the\none-to-one correspondence characteristics between Pinyin and Character.\nFurthermore, we proposed a two-stage training strategy to make training more\nstable and converge faster. The results on the test sets of AISHELL-1 dataset\nshow that the proposed enhanced dual-decoder model without a language model is\nimproved by a big margin compared to strong baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Dianwen Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liping Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1\">Wei Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jizhong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media. (arXiv:2203.04356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04356","description":"<p>Social media is a breeding ground for threat narratives and related\nconspiracy theories. In these, an outside group threatens the integrity of an\ninside group, leading to the emergence of sharply defined group identities:\nInsiders -- agents with whom the authors identify and Outsiders -- agents who\nthreaten the insiders. Inferring the members of these groups constitutes a\nchallenging new NLP task: (i) Information is distributed over many\npoorly-constructed posts; (ii) Threats and threat agents are highly contextual,\nwith the same post potentially having multiple agents assigned to membership in\neither group; (iii) An agent's identity is often implicit and transitive; and\n(iv) Phrases used to imply Outsider status often do not follow common negative\nsentiment patterns. To address these challenges, we define a novel\nInsider-Outsider classification task. Because we are not aware of any\nappropriate existing datasets or attendant models, we introduce a labeled\ndataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages\npretrained language modeling to classify Insiders and Outsiders. NP2IO is shown\nto be robust, generalizing to noun phrases not seen during training, and\nexceeding the performance of non-trivial baseline models by $20\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holur_P/0/1/0/all/0/1\">Pavan Holur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahsavari_S/0/1/0/all/0/1\">Shadi Shahsavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangherlini_T/0/1/0/all/0/1\">Timothy Tangherlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_V/0/1/0/all/0/1\">Vwani Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06386","description":"<p>The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11476","description":"<p>Human speakers encode information into raw speech which is then decoded by\nthe listeners. This complex relationship between encoding (production) and\ndecoding (perception) is often modeled separately. Here, we test how encoding\nand decoding of lexical semantic information can emerge automatically from raw\nspeech in unsupervised generative deep convolutional networks that combine the\nproduction and perception principles of speech. We introduce, to our knowledge,\nthe most challenging objective in unsupervised lexical learning: a network that\nmust learn unique representations for lexical items with no direct access to\ntraining data. We train several models (ciwGAN and fiwGAN <a href=\"/abs/2006.02951\">arXiv:2006.02951</a>) and\ntest how the networks classify acoustic lexical items in unobserved test data.\nStrong evidence in favor of lexical learning and a causal relationship between\nlatent codes and meaningful sublexical units emerge. The architecture that\ncombines the production and perception principles is thus able to learn to\ndecode unique information from raw acoustic data without accessing real\ntraining data directly. We propose a technique to explore lexical (holistic)\nand sublexical (featural) learned representations in the classifier network.\nThe results bear implications for unsupervised speech technology, as well as\nfor unsupervised semantic modeling as language models increasingly bypass text\nand operate from raw acoustics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12865","description":"<p>The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation\nof NLP systems has revealed high failure rates for basic capabilities for\nmultiple state-of-the-art and commercial models. However, the CheckList\ncreation process is manual which creates a bottleneck towards creation of\nmultilingual CheckLists catering 100s of languages. In this work, we explore\nmultiple approaches to generate and evaluate the quality of Multilingual\nCheckList. We device an algorithm -- Automated Multilingual Checklist\nGeneration (AMCG) for automatically transferring a CheckList from a source to a\ntarget language that relies on a reasonable machine translation system. We then\ncompare the CheckList generated by AMCG with CheckLists generated with\ndifferent levels of human intervention. Through in-depth crosslingual\nexperiments between English and Hindi, and broad multilingual experiments\nspanning 11 languages, we show that the automatic approach can provide accurate\nestimates of failure rates of a model across capabilities, as would a\nhuman-verified CheckList, and better than CheckLists generated by humans from\nscratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthikeyan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pankaj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion. (arXiv:2203.13224v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13224","description":"<p>Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine-&gt;Knowledge-&gt;Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conversational Paradigm for Program Synthesis. (arXiv:2203.13474v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13474","description":"<p>Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification. We propose a conversational program synthesis\napproach via large language models, which addresses the challenges of searching\nover a vast program space and user intent specification faced in prior\napproaches. Our new approach casts the process of writing a specification and\nprogram as a multi-turn conversation between a user and a system. It treats\nprogram synthesis as a sequence prediction problem, in which the specification\nis expressed in natural language and the desired program is conditionally\nsampled. We train a family of large language models, called CodeGen, on natural\nlanguage and programming language data. With weak supervision in the data and\nthe scaling up of data size and model size, conversational capacities emerge\nfrom the simple autoregressive language modeling. To study the model behavior\non conversational program synthesis, we develop a multi-turn programming\nbenchmark (MTPB), where solving each problem requires multi-step synthesis via\nmulti-turn conversation between the user and the model. Our findings show the\nemergence of conversational capabilities and the effectiveness of the proposed\nconversational program synthesis paradigm. In addition, our model CodeGen (with\nup to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the\nHumanEval benchmark. We make the training library JaxFormer including\ncheckpoints available as open source contribution:\nhttps://github.com/salesforce/CodeGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1\">Erik Nijkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Roadmap for Big Model. (arXiv:2203.14101v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14101","description":"<p>With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yangxiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiaao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zuobai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaoyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Weicheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, et al. (34 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15319","description":"<p>Neural Machine Translation (NMT) has reached a level of maturity to be\nrecognized as the premier method for the translation between different\nlanguages and aroused interest in different research areas, including software\nengineering. A key step to validate the robustness of the NMT models consists\nin evaluating the performance of the models on adversarial inputs, i.e., inputs\nobtained from the original ones by adding small amounts of perturbation.\nHowever, when dealing with the specific task of the code generation (i.e., the\ngeneration of code starting from a description in natural language), it has not\nyet been defined an approach to validate the robustness of the NMT models. In\nthis work, we address the problem by identifying a set of perturbations and\nmetrics tailored for the robustness assessment of such models. We present a\npreliminary experimental evaluation, showing what type of perturbations affect\nthe model the most and deriving useful insights for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1\">Pietro Liguori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1\">Cristina Improta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivo_S/0/1/0/all/0/1\">Simona De Vivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1\">Roberto Natella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1\">Bojan Cukic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1\">Domenico Cotroneo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heuristic-based Inter-training to Improve Few-shot Multi-perspective Dialog Summarization. (arXiv:2203.15590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15590","description":"<p>Many organizations require their customer-care agents to manually summarize\ntheir conversations with customers. These summaries are vital for decision\nmaking purposes of the organizations. The perspective of the summary that is\nrequired to be created depends on the application of the summaries. With this\nwork, we study the multi-perspective summarization of customer-care\nconversations between support agents and customers. We observe that there are\ndifferent heuristics that are associated with summaries of different\nperspectives, and explore these heuristics to create weak-labeled data for\nintermediate training of the models before fine-tuning with scarce human\nannotated summaries. Most importantly, we show that our approach supports\nmodels to generate multi-perspective summaries with a very small amount of\nannotated data. For example, our approach achieves 94\\% of the performance\n(Rouge-2) of a model trained with the original data, by training only with 7\\%\nof the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachin Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2105.00171","description":"<p>The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis. (arXiv:2203.15829v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15829","description":"<p>Emotion is an experience associated with a particular pattern of\nphysiological activity along with different physiological, behavioral and\ncognitive changes. One behavioral change is facial expression, which has been\nstudied extensively over the past few decades. Facial behavior varies with a\nperson's emotion according to differences in terms of culture, personality,\nage, context, and environment. In recent years, physiological activities have\nbeen used to study emotional responses. A typical signal is the\nelectroencephalogram (EEG), which measures brain activity. Most of existing\nEEG-based emotion analysis has overlooked the role of facial expression\nchanges. There exits little research on the relationship between facial\nbehavior and brain signals due to the lack of dataset measuring both EEG and\nfacial action signals simultaneously. To address this problem, we propose to\ndevelop a new database by collecting facial expressions, action units, and EEGs\nsimultaneously. We recorded the EEGs and face videos of both posed facial\nactions and spontaneous expressions from 29 participants with different ages,\ngenders, ethnic backgrounds. Differing from existing approaches, we designed a\nprotocol to capture the EEG signals by evoking participants' individual action\nunits explicitly. We also investigated the relation between the EEG signals and\nfacial action units. As a baseline, the database has been evaluated through the\nexperiments on both posed and spontaneous emotion recognition with images\nalone, EEG alone, and EEG fused with images, respectively. The database will be\nreleased to the research community to advance the state of the art for\nautomatic emotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_W/0/1/0/all/0/1\">Wenna Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weiying Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lijun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACR Loss: Adaptive Coordinate-based Regression Loss for Face Alignment. (arXiv:2203.15835v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15835","description":"<p>Although deep neural networks have achieved reasonable accuracy in solving\nface alignment, it is still a challenging task, specifically when we deal with\nfacial images, under occlusion, or extreme head poses. Heatmap-based Regression\n(HBR) and Coordinate-based Regression (CBR) are among the two mainly used\nmethods for face alignment. CBR methods require less computer memory, though\ntheir performance is less than HBR methods. In this paper, we propose an\nAdaptive Coordinate-based Regression (ACR) loss to improve the accuracy of CBR\nfor face alignment. Inspired by the Active Shape Model (ASM), we generate\nSmooth-Face objects, a set of facial landmark points with less variations\ncompared to the ground truth landmark points. We then introduce a method to\nestimate the level of difficulty in predicting each landmark point for the\nnetwork by comparing the distribution of the ground truth landmark points and\nthe corresponding Smooth-Face objects. Our proposed ACR Loss can adaptively\nmodify its curvature and the influence of the loss based on the difficulty\nlevel of predicting each landmark point in a face. Accordingly, the ACR Loss\nguides the network toward challenging points than easier points, which improves\nthe accuracy of the face alignment task. Our extensive evaluation shows the\ncapabilities of the proposed ACR Loss in predicting facial landmark points in\nvarious facial images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fard_A/0/1/0/all/0/1\">Ali Pourramezan Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1\">Mohammah H. Mahoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPTR: Efficient Transformers for Video Prediction. (arXiv:2203.15836v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15836","description":"<p>In this paper, we propose a new Transformer block for video future frames\nprediction based on an efficient local spatial-temporal separation attention\nmechanism. Based on this new Transformer block, a fully autoregressive video\nfuture frames prediction Transformer is proposed. In addition, a\nnon-autoregressive video prediction Transformer is also proposed to increase\nthe inference speed and reduce the accumulated inference errors of its\nautoregressive counterpart. In order to avoid the prediction of very similar\nfuture frames, a contrastive feature loss is applied to maximize the mutual\ninformation between predicted and ground-truth future frame features. This work\nis the first that makes a formal comparison of the two types of attention-based\nvideo future frames prediction models over different scenarios. The proposed\nmodels reach a performance competitive with more complex state-of-the-art\nmodels. The source code is available at \\emph{https://github.com/XiYe20/VPTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NNLander-VeriF: A Neural Network Formal Verification Framework for Vision-Based Autonomous Aircraft Landing. (arXiv:2203.15841v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15841","description":"<p>In this paper, we consider the problem of formally verifying a Neural Network\n(NN) based autonomous landing system. In such a system, a NN controller\nprocesses images from a camera to guide the aircraft while approaching the\nrunway. A central challenge for the safety and liveness verification of\nvision-based closed-loop systems is the lack of mathematical models that\ncaptures the relation between the system states (e.g., position of the\naircraft) and the images processed by the vision-based NN controller. Another\nchallenge is the limited abilities of state-of-the-art NN model checkers. Such\nmodel checkers can reason only about simple input-output robustness properties\nof neural networks. This limitation creates a gap between the NN model checker\nabilities and the need to verify a closed-loop system while considering the\naircraft dynamics, the perception components, and the NN controller. To this\nend, this paper presents NNLander-VeriF, a framework to verify vision-based NN\ncontrollers used for autonomous landing. NNLander-VeriF addresses the\nchallenges above by exploiting geometric models of perspective cameras to\nobtain a mathematical model that captures the relation between the aircraft\nstates and the inputs to the NN controller. By converting this model into a NN\n(with manually assigned weights) and composing it with the NN controller, one\ncan capture the relation between aircraft states and control actions using one\naugmented NN. Such an augmented NN model leads to a natural encoding of the\nclosed-loop verification into several NN robustness queries, which\nstate-of-the-art NN model checkers can handle. Finally, we evaluate our\nframework to formally verify the properties of a trained NN and we show its\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_U/0/1/0/all/0/1\">Ulices Santa Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoukry_Y/0/1/0/all/0/1\">Yasser Shoukry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Inertial Localization. (arXiv:2203.15851v1 [cs.RO])","link":"http://arxiv.org/abs/2203.15851","description":"<p>This paper proposes the inertial localization problem, the task of estimating\nthe absolute location from a sequence of inertial sensor measurements. This is\nan exciting and unexplored area of indoor localization research, where we\npresent a rich dataset with 53 hours of inertial sensor data and the associated\nground truth locations. We developed a solution, dubbed neural inertial\nlocalization (NILoc) which 1) uses a neural inertial navigation technique to\nturn inertial sensor history to a sequence of velocity vectors; then 2) employs\na transformer-based neural architecture to find the device location from the\nsequence of velocities. We only use an IMU sensor, which is energy efficient\nand privacy preserving compared to WiFi, cameras, and other data sources. Our\napproach is significantly faster and achieves competitive results even compared\nwith state-of-the-art methods that require a floorplan and run 20 to 30 times\nslower. We share our code, model and data at https://sachini.github.io/niloc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herath_S/0/1/0/all/0/1\">Sachini Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruso_D/0/1/0/all/0/1\">David Caruso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OdontoAI: A human-in-the-loop labeled data set and an online platform to boost research on dental panoramic radiographs. (arXiv:2203.15856v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15856","description":"<p>Deep learning has remarkably advanced in the last few years, supported by\nlarge labeled data sets. These data sets are precious yet scarce because of the\ntime-consuming labeling procedures, discouraging researchers from producing\nthem. This scarcity is especially true in dentistry, where deep learning\napplications are still in an embryonic stage. Motivated by this background, we\naddress in this study the construction of a public data set of dental panoramic\nradiographs. Our objects of interest are the teeth, which are segmented and\nnumbered, as they are the primary targets for dentists when screening a\npanoramic radiograph. We benefited from the human-in-the-loop (HITL) concept to\nexpedite the labeling procedure, using predictions from deep neural networks as\nprovisional labels, later verified by human annotators. All the gathering and\nlabeling procedures of this novel data set is thoroughly analyzed. The results\nwere consistent and behaved as expected: At each HITL iteration, the model\npredictions improved. Our results demonstrated a 51% labeling time reduction\nusing HITL, saving us more than 390 continuous working hours. In a novel online\nplatform, called OdontoAI, created to work as task central for this novel data\nset, we released 4,000 images, from which 2,000 have their labels publicly\navailable for model fitting. The labels of the other 2,000 images are private\nand used for model evaluation considering instance and semantic segmentation\nand numbering. To the best of our knowledge, this is the largest-scale publicly\navailable data set for panoramic radiographs, and the OdontoAI is the first\nplatform of its kind in dentistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1\">Bernardo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_L/0/1/0/all/0/1\">La&#xed;s Pinheiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobrinho_B/0/1/0/all/0/1\">Brenda Sobrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_F/0/1/0/all/0/1\">Fernanda Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobrinho_B/0/1/0/all/0/1\">Bruna Sobrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_K/0/1/0/all/0/1\">Kalyf Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pithon_M/0/1/0/all/0/1\">Matheus Pithon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cury_P/0/1/0/all/0/1\">Patr&#xed;cia Cury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luciano Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models. (arXiv:2203.15859v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15859","description":"<p>Neural image caption generation (NICG) models have received massive attention\nfrom the research community due to their excellent performance in visual\nunderstanding. Existing work focuses on improving NICG model accuracy while\nefficiency is less explored. However, many real-world applications require\nreal-time feedback, which highly relies on the efficiency of NICG models.\nRecent research observed that the efficiency of NICG models could vary for\ndifferent inputs. This observation brings in a new attack surface of NICG\nmodels, i.e., An adversary might be able to slightly change inputs to cause the\nNICG models to consume more computational resources. To further understand such\nefficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to\nevaluate the efficiency robustness of NICG models. Our experimental results\nshow that NICGSlowDown can generate images with human-unnoticeable\nperturbations that will increase the NICG model latency up to 483.86%. We hope\nthis research could raise the community's concern about the efficiency\nrobustness of NICG models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zihe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Mirazul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15865","description":"<p>Supervised approaches to 3D pose estimation from single images are remarkably\neffective when labeled data is abundant. Therefore, much of the recent\nattention has shifted towards semi and (or) weakly supervised learning.\nGenerating an effective form of supervision with little annotations still poses\nmajor challenges in crowded scenes. However, since it is easy to observe a\nscene from multiple cameras, we propose to impose multi-view geometrical\nconstraints by means of a differentiable triangulation and to use it as form of\nself-supervision during training when no labels are available. We therefore\ntrain a 2D pose estimator in such a way that its predictions correspond to the\nre-projection of the triangulated 3D one and train an auxiliary network on them\nto produce the final 3D poses. We complement the triangulation with a weighting\nmechanism that nullify the impact of noisy predictions caused by self-occlusion\nor occlusion from other subjects. Our experimental results on Human3.6M and\nMPI-INF-3DHP substantiate the significance of our weighting strategy where we\nobtain state-of-the-art results in the semi and weakly supervised learning\nsetup. We also contribute a new multi-player sports dataset that features\nocclusion, and show the effectiveness of our algorithm over baseline\ntriangulation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Soumava Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_L/0/1/0/all/0/1\">Leonardo Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Retrieval from Contextual Descriptions. (arXiv:2203.15867v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15867","description":"<p>The ability to integrate context, including perceptual and temporal cues,\nplays a pivotal role in grounding the meaning of a linguistic utterance. In\norder to measure to what extent current vision-and-language models master this\nability, we devise a new multimodal challenge, Image Retrieval from Contextual\nDescriptions (ImageCoDe). In particular, models are tasked with retrieving the\ncorrect image from a set of 10 minimally contrastive candidates based on a\ncontextual description. As such, each description contains only the details\nthat help distinguish between images. Because of this, descriptions tend to be\ncomplex in terms of syntax and discourse and require drawing pragmatic\ninferences. Images are sourced from both static pictures and video frames. We\nbenchmark several state-of-the-art models, including both cross-encoders such\nas ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that\nthese models dramatically lag behind human performance: the best variant\nachieves an accuracy of 20.9 on video frames and 59.4 on static pictures,\ncompared with 90.8 in humans. Furthermore, we experiment with new model\nvariants that are better equipped to incorporate visual and temporal context\ninto their representations, which achieve modest gains. Our hope is that\nImageCoDE will foster progress in grounded language understanding by\nencouraging models to focus on fine-grained visual differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1\">Benno Krojer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_Y/0/1/0/all/0/1\">Yash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep learning model for burn depth classification using ultrasound imaging. (arXiv:2203.15879v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15879","description":"<p>Identification of burn depth with sufficient accuracy is a challenging\nproblem. This paper presents a deep convolutional neural network to classify\nburn depth based on altered tissue morphology of burned skin manifested as\ntexture patterns in the ultrasound images. The network first learns a\nlow-dimensional manifold of the unburned skin images using an encoder-decoder\narchitecture that reconstructs it from ultrasound images of burned skin. The\nencoder is then re-trained to classify burn depths. The encoder-decoder network\nis trained using a dataset comprised of B-mode ultrasound images of unburned\nand burned ex vivo porcine skin samples. The classifier is developed using\nB-mode images of burned in situ skin samples obtained from freshly euthanized\npostmortem pigs. The performance metrics obtained from 20-fold cross-validation\nshow that the model can identify deep-partial thickness burns, which is the\nmost difficult to diagnose clinically, with 99% accuracy, 98% sensitivity, and\n100% specificity. The diagnostic accuracy of the classifier is further\nillustrated by the high area under the curve values of 0.99 and 0.95,\nrespectively, for the receiver operating characteristic and precision-recall\ncurves. A post hoc explanation indicates that the classifier activates the\ndiscriminative textural features in the B-mode images for burn classification.\nThe proposed model has the potential for clinical utility in assisting the\nclinical assessment of burn depths using a widely available clinical imaging\ndevice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sangrock Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahul/0/1/0/all/0/1\">Rahul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lukan_J/0/1/0/all/0/1\">James Lukan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyko_T/0/1/0/all/0/1\">Tatiana Boyko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelenova_K/0/1/0/all/0/1\">Kateryna Zelenova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makled_B/0/1/0/all/0/1\">Basiel Makled</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parsey_C/0/1/0/all/0/1\">Conner Parsey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norfleet_J/0/1/0/all/0/1\">Jack Norfleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+De_S/0/1/0/all/0/1\">Suvranu De</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proactive Image Manipulation Detection. (arXiv:2203.15880v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15880","description":"<p>Image manipulation detection algorithms are often trained to discriminate\nbetween images manipulated with particular Generative Models (GMs) and\ngenuine/real images, yet generalize poorly to images manipulated with GMs\nunseen in the training. Conventional detection algorithms receive an input\nimage passively. By contrast, we propose a proactive scheme to image\nmanipulation detection. Our key enabling technique is to estimate a set of\ntemplates which when added onto the real image would lead to more accurate\nmanipulation detection. That is, a template protected real image, and its\nmanipulated version, is better discriminated compared to the original real\nimage vs. its manipulated one. These templates are estimated using certain\nconstraints based on the desired properties of templates. For image\nmanipulation detection, our proposed approach outperforms the prior work by an\naverage precision of 16% for CycleGAN and 32% for GauGAN. Our approach is\ngeneralizable to a variety of GMs showing an improvement over prior work by an\naverage precision of 10% averaged across 12 GMs. Our code is available at\nhttps://www.github.com/vishal3477/proactive_IMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1\">Vishal Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Detect Mobile Objects from LiDAR Scans Without Labels. (arXiv:2203.15882v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15882","description":"<p>Current 3D object detectors for autonomous driving are almost entirely\ntrained on human-annotated data. Although of high quality, the generation of\nsuch data is laborious and costly, restricting them to a few specific locations\nand object types. This paper proposes an alternative approach entirely based on\nunlabeled data, which can be collected cheaply and in abundance almost\neverywhere on earth. Our approach leverages several simple common sense\nheuristics to create an initial set of approximate seed labels. For example,\nrelevant traffic participants are generally not persistent across multiple\ntraversals of the same route, do not fly, and are never under ground. We\ndemonstrate that these seed labels are highly effective to bootstrap a\nsurprisingly accurate detector through repeated self-training without a single\nhuman annotated label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yurong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Katie Z Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1\">Cheng Perng Phoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Mark Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Equilibrium Assisted Block Sparse Coding of Inter-dependent Signals: Application to Hyperspectral Imaging. (arXiv:2203.15901v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15901","description":"<p>In this study, the problem of computing a sparse representation for datasets\nof inter-dependent signals, given a fixed dictionary, is considered. A dataset\nof inter-dependent signals is defined as a matrix whose columns demonstrate\nstrong dependencies. A computational efficient sparse coding optimization\nproblem is derived by employing regularization terms that are adapted to the\nproperties of the signals of interest. Exploiting the merits of the learnable\nregularization techniques, a neural network is employed to act as structure\nprior and reveal the underlying signal interdependencies. To solve the\noptimization problem Deep unrolling and Deep equilibrium based algorithms are\ndeveloped, forming highly interpretable and concise deep-learning-based\narchitectures, that process the input dataset in a block-by-block fashion.\nExtensive simulation results, in the context of hyperspectral image denoising,\nare provided, that demonstrate that the proposed algorithms outperform\nsignificantly other sparse coding approaches and exhibit superior performance\nagainst recent state-of-the-art deep-learning-based denoising models. In a\nwider perspective, our work provides a unique bridge between a classic\napproach, that is the sparse representation theory, and modern representation\ntools that are based on deep learning modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkillas_A/0/1/0/all/0/1\">Alexandros Gkillas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ampeliotis_D/0/1/0/all/0/1\">Dimitris Ampeliotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberidis_K/0/1/0/all/0/1\">Kostas Berberidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images. (arXiv:2203.15926v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15926","description":"<p>Learning 3D generative models from a dataset of monocular images enables\nself-supervised 3D reasoning and controllable synthesis. State-of-the-art 3D\ngenerative models are GANs which use neural 3D volumetric representations for\nsynthesis. Images are synthesized by rendering the volumes from a given camera.\nThese models can disentangle the 3D scene from the camera viewpoint in any\ngenerated image. However, most models do not disentangle other factors of image\nformation, such as geometry and appearance. In this paper, we design a 3D GAN\nwhich can learn a disentangled model of objects, just from monocular\nobservations. Our model can disentangle the geometry and appearance variations\nin the scene, i.e., we can independently sample from the geometry and\nappearance spaces of the generative model. This is achieved using a novel\nnon-rigid deformable scene formulation. A 3D volume which represents an object\ninstance is computed as a non-rigidly deformed canonical 3D volume. Our method\nlearns the canonical volume, as well as its deformations, jointly during\ntraining. This formulation also helps us improve the disentanglement between\nthe 3D scene and the camera viewpoints using a novel pose regularization loss\ndefined on the 3D deformation field. In addition, we further model the inverse\ndeformations, enabling the computation of dense correspondences between images\ngenerated by our model. Finally, we design an approach to embed real images\ninto the latent space of our disentangled generative model, enabling editing of\nreal images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_M/0/1/0/all/0/1\">Mallikarjun B R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1\">Maneesh Agrawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Leaf Segmentation under Complex Lighting Conditions. (arXiv:2203.15943v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15943","description":"<p>As an essential prerequisite task in image-based plant phenotyping, leaf\nsegmentation has garnered increasing attention in recent years. While\nself-supervised learning is emerging as an effective alternative to various\ncomputer vision tasks, its adaptation for image-based plant phenotyping remains\nrather unexplored. In this work, we present a self-supervised leaf segmentation\nframework consisting of a self-supervised semantic segmentation model, a\ncolor-based leaf segmentation algorithm, and a self-supervised color correction\nmodel. The self-supervised semantic segmentation model groups the semantically\nsimilar pixels by iteratively referring to the self-contained information,\nallowing the pixels of the same semantic object to be jointly considered by the\ncolor-based leaf segmentation algorithm for identifying the leaf regions.\nAdditionally, we propose to use a self-supervised color correction model for\nimages taken under complex illumination conditions. Experimental results on\ndatasets of different plant species demonstrate the potential of the proposed\nself-supervised framework in achieving effective and generalizable leaf\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xufeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Scott Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1\">Abbas Kouzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Richard Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Ligang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongjian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernon_M/0/1/0/all/0/1\">Michael Vernon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doeven_E/0/1/0/all/0/1\">Egan Doeven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_L/0/1/0/all/0/1\">Lawrence Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mcclellan_T/0/1/0/all/0/1\">Todd Mcclellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guskic_A/0/1/0/all/0/1\">Adam Guskic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15946","description":"<p>We present a method that learns neural scene representations from only\nshadows present in the scene. While traditional shape-from-shadow (SfS)\nalgorithms reconstruct geometry from shadows, they assume a fixed scanning\nsetup and fail to generalize to complex scenes. Neural rendering algorithms, on\nthe other hand, rely on photometric consistency between RGB images but largely\nignore physical cues such as shadows, which have been shown to provide valuable\ninformation about the scene. We observe that shadows are a powerful cue that\ncan constrain neural scene representations to learn SfS, and even outperform\nNeRF to reconstruct otherwise hidden geometry. We propose a graphics-inspired\ndifferentiable approach to render accurate shadows with volumetric rendering,\npredicting a shadow map that can be compared to the ground truth shadow. Even\nwith just binary shadow maps, we show that neural rendering can localize the\nobject and estimate coarse geometry. Our approach reveals that sparse cues in\nimages can be used to estimate geometry using differentiable volumetric\nrendering. Moreover, our framework is highly generalizable and can work\nalongside existing 3D reconstruction techniques that otherwise only use\nphotometric consistency. Our code is made available in our supplementary\nmaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_K/0/1/0/all/0/1\">Kushagra Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinghoffer_T/0/1/0/all/0/1\">Tzofi Klinghoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution Face Swapping via Latent Semantics Disentanglement. (arXiv:2203.15958v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15958","description":"<p>We present a novel high-resolution face swapping method using the inherent\nprior knowledge of a pre-trained GAN model. Although previous research can\nleverage generative priors to produce high-resolution results, their quality\ncan suffer from the entangled semantics of the latent space. We explicitly\ndisentangle the latent semantics by utilizing the progressive nature of the\ngenerator, deriving structure attributes from the shallow layers and appearance\nattributes from the deeper ones. Identity and pose information within the\nstructure attributes are further separated by introducing a landmark-driven\nstructure transfer latent direction. The disentangled latent code produces rich\ngenerative features that incorporate feature blending to produce a plausible\nswapping result. We further extend our method to video face swapping by\nenforcing two spatio-temporal constraints on the latent space and the image\nspace. Extensive experiments demonstrate that the proposed method outperforms\nstate-of-the-art image/video face swapping methods in terms of hallucination\nquality and consistency. Code can be found at:\nhttps://github.com/cnnlstm/FSLSD_HiRes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yanqing Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Active Speaker Faces for Diarization in TV shows. (arXiv:2203.15961v1 [cs.MM])","link":"http://arxiv.org/abs/2203.15961","description":"<p>Speaker diarization is one of the critical components of computational media\nintelligence as it enables a character-level analysis of story portrayals and\nmedia content understanding. Automated audio-based speaker diarization of\nentertainment media poses challenges due to the diverse acoustic conditions\npresent in media content, be it background music, overlapping speakers, or\nsound effects. At the same time, speaking faces in the visual modality provide\ncomplementary information and not prone to the errors seen in the audio\nmodality. In this paper, we address the problem of speaker diarization in TV\nshows using the active speaker faces. We perform face clustering on the active\nspeaker faces and show superior speaker diarization performance compared to the\nstate-of-the-art audio-based diarization methods. We additionally report a\nsystematic analysis of the impact of active speaker face detection quality on\nthe diarization performance. We also observe that a moderately well-performing\nactive speaker system could outperform the audio-based diarization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation. (arXiv:2203.15965v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15965","description":"<p>In this paper, we propose a new deep learning-based method for estimating\nroom layout given a pair of 360 panoramas. Our system, called Position-aware\nStereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.\nPSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel\nCross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is\nused to implicitly infer correspondences between views, and can handle noisy\nposes. The pose-aware CP2 layer is designed to render features from the\nadjacent view to the anchor (reference) view, in order to perform view fusion\nand estimate the visible layout. Our experiments and analysis validate our\nmethod, which significantly outperforms the state-of-the-art layout estimators,\nespecially for large and complex room spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchcroft_W/0/1/0/all/0/1\">Will Hutchcroft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyadzhiev_I/0/1/0/all/0/1\">Ivaylo Boyadzhiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sing Bing Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeply Interleaved Two-Stream Encoder for Referring Video Segmentation. (arXiv:2203.15969v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15969","description":"<p>Referring video segmentation aims to segment the corresponding video object\ndescribed by the language expression. To address this task, we first design a\ntwo-stream encoder to extract CNN-based visual features and transformer-based\nlinguistic features hierarchically, and a vision-language mutual guidance\n(VLMG) module is inserted into the encoder multiple times to promote the\nhierarchical and progressive fusion of multi-modal features. Compared with the\nexisting multi-modal fusion methods, this two-stream encoder takes into account\nthe multi-granularity linguistic context, and realizes the deep interleaving\nbetween modalities with the help of VLGM. In order to promote the temporal\nalignment between frames, we further propose a language-guided multi-scale\ndynamic filtering (LMDF) module to strengthen the temporal coherence, which\nuses the language-guided spatial-temporal features to generate a set of\nposition-specific dynamic filters to more flexibly and effectively update the\nfeature of current frame. Extensive experiments on four datasets verify the\neffectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Deep Homography Estimation. (arXiv:2203.15982v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15982","description":"<p>We propose Iterative Homography Network, namely IHN, a new deep homography\nestimation architecture. Different from previous works that achieve iterative\nrefinement by network cascading or untrainable IC-LK iterator, the iterator of\nIHN has tied weights and is completely trainable. IHN achieves state-of-the-art\naccuracy on several datasets including challenging scenes. We propose 2\nversions of IHN: (1) IHN for static scenes, (2) IHN-mov for dynamic scenes with\nmoving objects. Both versions can be arranged in 1-scale for efficiency or\n2-scale for accuracy. We show that the basic 1-scale IHN already outperforms\nmost of the existing methods. On a variety of datasets, the 2-scale IHN\noutperforms all competitors by a large gap. We introduce IHN-mov by producing\nan inlier mask to further improve the estimation accuracy of moving-objects\nscenes. We experimentally show that the iterative framework of IHN can achieve\n95% error reduction while considerably saving network parameters. When\nprocessing sequential image pairs, IHN can achieve 32.7 fps, which is about 8x\nthe speed of IC-LK iterator. Source code is available at\nhttps://github.com/imdumpl78/IHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Si-Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zehua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics. (arXiv:2203.15983v1 [cs.RO])","link":"http://arxiv.org/abs/2203.15983","description":"<p>One of the key challenges in high speed off road navigation on ground\nvehicles is that the kinodynamics of the vehicle terrain interaction can differ\ndramatically depending on the terrain. Previous approaches to addressing this\nchallenge have considered learning an inverse kinodynamics (IKD) model,\nconditioned on inertial information of the vehicle to sense the kinodynamic\ninteractions. In this paper, we hypothesize that to enable accurate high-speed\noff-road navigation using a learned IKD model, in addition to inertial\ninformation from the past, one must also anticipate the kinodynamic\ninteractions of the vehicle with the terrain in the future. To this end, we\nintroduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based\nIKD model that is conditioned on visual information from a terrain patch ahead\nof the robot in addition to past inertial information, enabling it to\nanticipate kinodynamic interactions in the future. We validate the\neffectiveness of VI-IKD in accurate high-speed off-road navigation\nexperimentally on a scale 1/5 UT-AlphaTruck off-road autonomous vehicle in both\nindoor and outdoor environments and show that compared to other\nstate-of-the-art approaches, VI-IKD enables more accurate and robust off-road\nnavigation on a variety of different terrains at speeds of up to 3.5 m/s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikand_K/0/1/0/all/0/1\">Kavan Singh Sikand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atreya_P/0/1/0/all/0/1\">Pranav Atreya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_S/0/1/0/all/0/1\">Sadegh Rabiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Object Classification via Self-Supervised Pose Alignment. (arXiv:2203.15987v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15987","description":"<p>Semantic patterns of fine-grained objects are determined by subtle appearance\ndifference of local parts, which thus inspires a number of part-based methods.\nHowever, due to uncontrollable object poses in images, distinctive details\ncarried by local regions can be spatially distributed or even self-occluded,\nleading to a large variation on object representation. For discounting pose\nvariations, this paper proposes to learn a novel graph based object\nrepresentation to reveal a global configuration of local parts for\nself-supervised pose alignment across classes, which is employed as an\nauxiliary feature regularization on a deep representation learning\nnetwork.Moreover, a coarse-to-fine supervision together with the proposed\npose-insensitive constraint on shallow-to-deep sub-networks encourages\ndiscriminative features in a curriculum learning manner. We evaluate our method\non three popular fine-grained object classification benchmarks, consistently\nachieving the state-of-the-art performance. Source codes are available at\nhttps://github.com/yangxh11/P2P-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Sound of Bounding-Boxes. (arXiv:2203.15991v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15991","description":"<p>In the task of audio-visual sound source separation, which leverages visual\ninformation for sound source separation, identifying objects in an image is a\ncrucial step prior to separating the sound source. However, existing methods\nthat assign sound on detected bounding boxes suffer from a problem that their\napproach heavily relies on pre-trained object detectors. Specifically, when\nusing these existing methods, it is required to predetermine all the possible\ncategories of objects that can produce sound and use an object detector\napplicable to all such categories. To tackle this problem, we propose a fully\nunsupervised method that learns to detect objects in an image and separate\nsound source simultaneously. As our method does not rely on any pre-trained\ndetector, our method is applicable to arbitrary categories without any\nadditional annotation. Furthermore, although being fully unsupervised, we found\nthat our method performs comparably in separation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oya_T/0/1/0/all/0/1\">Takashi Oya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shohei Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleFool: Fooling Video Classification Systems via Style Transfer. (arXiv:2203.16000v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16000","description":"<p>Video classification systems are vulnerable to adversarial attacks, which can\ncreate severe security problems in video verification. Current black-box\nattacks need a large number of queries to succeed, resulting in high\ncomputational overhead in the process of attack. On the other hand, attacks\nwith restricted perturbations are ineffective against defenses such as\ndenoising or adversarial training. In this paper, we focus on unrestricted\nperturbations and propose StyleFool, a black-box video adversarial attack via\nstyle transfer to fool the video classification system. StyleFool first\nutilizes color theme proximity to select the best style image, which helps\navoid unnatural details in the stylized videos. Meanwhile, the target class\nconfidence is additionally considered in targeted attack to influence the\noutput distribution of the classifier by moving the stylized video closer to or\neven across the decision boundary. A gradient-free method is then employed to\nfurther optimize the adversarial perturbation. We carry out extensive\nexperiments to evaluate StyleFool on two standard datasets, UCF-101 and\nHMDB-51. The experimental results suggest that StyleFool outperforms the\nstate-of-the-art adversarial attacks in terms of both number of queries and\nrobustness against existing defenses. We identify that 50% of the stylized\nvideos in untargeted attack do not need any query since they can already fool\nthe video classification model. Furthermore, we evaluate the\nindistinguishability through a user study to show that the adversarial samples\nof StyleFool look imperceptible to human eyes, despite unrestricted\nperturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Derui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Sheng Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Sampler: Almost-Universal yet Task-Oriented Sampling for Point Clouds. (arXiv:2203.16001v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16001","description":"<p>Sampling is a key operation in point-cloud task and acts to increase\ncomputational efficiency and tractability by discarding redundant points.\nUniversal sampling algorithms (e.g., Farthest Point Sampling) work without\nmodification across different tasks, models, and datasets, but by their very\nnature are agnostic about the downstream task/model. As such, they have no\nimplicit knowledge about which points would be best to keep and which to\nreject. Recent work has shown how task-specific point cloud sampling (e.g.,\nSampleNet) can be used to outperform traditional sampling approaches by\nlearning which points are more informative. However, these learnable samplers\nface two inherent issues: i) overfitting to a model rather than a task, and\n\\ii) requiring training of the sampling network from scratch, in addition to\nthe task network, somewhat countering the original objective of down-sampling\nto increase efficiency. In this work, we propose an almost-universal sampler,\nin our quest for a sampler that can learn to preserve the most useful points\nfor a particular task, yet be inexpensive to adapt to different tasks, models,\nor datasets. We first demonstrate how training over multiple models for the\nsame task (e.g., shape reconstruction) significantly outperforms the vanilla\nSampleNet in terms of accuracy by not overfitting the sample network to a\nparticular task network. Second, we show how we can train an almost-universal\nmeta-sampler across multiple tasks. This meta-sampler can then be rapidly\nfine-tuned when applied to different datasets, networks, or even different\ntasks, thus amortizing the initial cost of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Ta-Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITTR: Unpaired Image-to-Image Translation with Transformers. (arXiv:2203.16015v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16015","description":"<p>Unpaired image-to-image translation is to translate an image from a source\ndomain to a target domain without paired training data. By utilizing CNN in\nextracting local semantics, various techniques have been developed to improve\nthe translation performance. However, CNN-based generators lack the ability to\ncapture long-range dependency to well exploit global semantics. Recently,\nVision Transformers have been widely investigated for recognition tasks. Though\nappealing, it is inappropriate to simply transfer a recognition-based vision\ntransformer to image-to-image translation due to the generation difficulty and\nthe computation limitation. In this paper, we propose an effective and\nefficient architecture for unpaired Image-to-Image Translation with\nTransformers (ITTR). It has two main designs: 1) hybrid perception block (HPB)\nfor token mixing from different receptive fields to utilize global semantics;\n2) dual pruned self-attention (DPSA) to sharply reduce the computational\ncomplexity. Our ITTR outperforms the state-of-the-arts for unpaired\nimage-to-image translation on six benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wanfeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReplaceBlock: An improved regularization method based on background information. (arXiv:2203.16029v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16029","description":"<p>Attention mechanism, being frequently used to train networks for better\nfeature representations, can effectively disentangle the target object from\nirrelevant objects in the background. Given an arbitrary image, we find that\nthe background's irrelevant objects are most likely to occlude/block the target\nobject. We propose, based on this finding, a ReplaceBlock to simulate the\nsituations when the target object is partially occluded by the objects that are\ndeemed as background. Specifically, ReplaceBlock erases the target object in\nthe image, and then generates a feature map with only irrelevant objects and\nbackground by the model. Finally, some regions in the background feature map\nare used to replace some regions of the target object in the original image\nfeature map. In this way, ReplaceBlock can effectively simulate the feature map\nof the occluded image. The experimental results show that ReplaceBlock works\nbetter than DropBlock in regularizing convolutional networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16031","description":"<p>Mathematical modeling and aesthetic rule extraction of works of art are\ncomplex activities. This is because art is a multidimensional, subjective\ndiscipline. Perception and interpretation of art are, to many extents, relative\nand open-ended rather than measurable. Following the explainable Artificial\nIntelligence paradigm, this paper investigated in a human-understandable\nfashion the limits to which a single-task, single-modality benchmark computer\nvision model performs in classifying contemporary 2D visual arts. It is\nimportant to point out that this work does not introduce an interpreting method\nto open the black box of Deep Neural Networks, instead it uses existing\nevaluating metrics derived from the confusion matrix to try to uncover the\nmechanism with which Deep Neural Networks understand art. To achieve so,\nVGG-11, pre-trained on ImageNet and discriminatively fine-tuned, was used on\nhandcrafted small-data datasets designed from real-world photography gallery\nshows. We demonstrated that the artwork's Exhibited Properties or formal\nfactors such as shape and color, rather than Non-Exhibited Properties or\ncontent factors such as history and intention, have much higher potential to be\nthe determinant when art pieces have very similar Exhibited Properties. We also\nshowed that a single-task and single-modality model's understanding of art is\ninadequate as it largely ignores Non-Exhibited Properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zahedi_M/0/1/0/all/0/1\">Mahan Agha Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholamrezaei_N/0/1/0/all/0/1\">Niloofar Gholamrezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1\">Alex Doboli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitored Distillation for Positive Congruent Depth Completion. (arXiv:2203.16034v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16034","description":"<p>We propose a method to infer a dense depth map from a single image, its\ncalibration, and the associated sparse point cloud. In order to leverage\nexisting models that produce putative depth maps (teacher models), we propose\nan adaptive knowledge distillation approach that yields a positive congruent\ntraining process, where a student model avoids learning the error modes of the\nteachers. We consider the scenario of a blind ensemble where we do not have\naccess to ground truth for model selection nor training. The crux of our\nmethod, termed Monitored Distillation, lies in a validation criterion that\nallows us to learn from teachers by choosing predictions that best minimize the\nphotometric reprojection error for a given image. The result of which is a\ndistilled depth map and a confidence map, or \"monitor\", for how well a\nprediction from a particular teacher fits the observed image. The monitor\nadaptively weights the distilled depth where, if all of the teachers exhibit\nhigh residuals, the standard unsupervised image reconstruction loss takes over\nas the supervisory signal. On indoor scenes (VOID), we outperform blind\nensembling baselines by 13.3% and unsupervised methods by 20.3%; we boast a 79%\nmodel size reduction while maintaining comparable performance to the best\nsupervised method. For outdoors (KITTI), we tie for 5th overall on the\nbenchmark despite not using ground truth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Parth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Allison Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1\">Byung-Woo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels. (arXiv:2203.16038v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16038","description":"<p>Establishing dense correspondences across semantically similar images remains\na challenging task due to the significant intra-class variations and background\nclutters. Traditionally, a supervised learning was used for training the\nmodels, which required tremendous manually-labeled data, while some methods\nsuggested a self-supervised or weakly-supervised learning to mitigate the\nreliance on the labeled data, but with limited performance. In this paper, we\npresent a simple, but effective solution for semantic correspondence that\nlearns the networks in a semi-supervised manner by supplementing few\nground-truth correspondences via utilization of a large amount of confident\ncorrespondences as pseudo-labels, called SemiMatch. Specifically, our framework\ngenerates the pseudo-labels using the model's prediction itself between source\nand weakly-augmented target, and uses pseudo-labels to learn the model again\nbetween source and strongly-augmented target, which improves the robustness of\nthe model. We also present a novel confidence measure for pseudo-labels and\ndata augmentation tailored for semantic correspondence. In experiments,\nSemiMatch achieves state-of-the-art performance on various benchmarks,\nespecially on PF-Willow by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_K/0/1/0/all/0/1\">Kwangrok Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junyoung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyuseong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daehwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hansang Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Co-Training Transductive Framework for Zero Shot Learning. (arXiv:2203.16041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16041","description":"<p>In zero-shot learning (ZSL) community, it is generally recognized that\ntransductive learning performs better than inductive one as the unseen-class\nsamples are also used in its training stage. How to generate pseudo labels for\nunseen-class samples and how to use such usually noisy pseudo labels are two\ncritical issues in transductive learning. In this work, we introduce an\niterative co-training framework which contains two different base ZSL models\nand an exchanging module. At each iteration, the two different ZSL models are\nco-trained to separately predict pseudo labels for the unseen-class samples,\nand the exchanging module exchanges the predicted pseudo labels, then the\nexchanged pseudo-labeled samples are added into the training sets for the next\niteration. By such, our framework can gradually boost the ZSL performance by\nfully exploiting the potential complementarity of the two models'\nclassification capabilities. In addition, our co-training framework is also\napplied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector\nis proposed to pick out the most likely unseen-class samples before class-level\nclassification to alleviate the bias problem in GZSL. Extensive experiments on\nthree benchmarks show that our proposed methods could significantly outperform\nabout $31$ state-of-the-art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Threshold Matters in WSSS: Manipulating the Activation for the Robust and Accurate Segmentation Model Against Thresholds. (arXiv:2203.16045v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16045","description":"<p>Weakly-supervised semantic segmentation (WSSS) has recently gained much\nattention for its promise to train segmentation models only with image-level\nlabels. Existing WSSS methods commonly argue that the sparse coverage of CAM\nincurs the performance bottleneck of WSSS. This paper provides analytical and\nempirical evidence that the actual bottleneck may not be sparse coverage but a\nglobal thresholding scheme applied after CAM. Then, we show that this issue can\nbe mitigated by satisfying two conditions; 1) reducing the imbalance in the\nforeground activation and 2) increasing the gap between the foreground and the\nbackground activation. Based on these findings, we propose a novel activation\nmanipulation network with a per-pixel classification loss and a label\nconditioning module. Per-pixel classification naturally induces two-level\nactivation in activation maps, which can penalize the most discriminative\nparts, promote the less discriminative parts, and deactivate the background\nregions. Label conditioning imposes that the output label of pseudo-masks\nshould be any of true image-level labels; it penalizes the wrong activation\nassigned to non-target classes. Based on extensive analysis and evaluations, we\ndemonstrate that each component helps produce accurate pseudo-masks, achieving\nthe robustness against the choice of the global threshold. Finally, our model\nachieves state-of-the-art records on both PASCAL VOC 2012 and MS COCO 2014\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongseob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction. (arXiv:2203.16051v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16051","description":"<p>This paper presents a high-quality human motion prediction method that\naccurately predicts future human poses given observed ones. Our method is based\non the observation that a good initial guess of the future poses is very\nhelpful in improving the forecasting accuracy. This motivates us to propose a\nnovel two-stage prediction framework, including an init-prediction network that\njust computes the good guess and then a formal-prediction network that predicts\nthe target future poses based on the guess. More importantly, we extend this\nidea further and design a multi-stage prediction framework where each stage\npredicts initial guess for the next stage, which brings more performance gain.\nTo fulfill the prediction task at each stage, we propose a network comprising\nSpatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph\nConvolutional Networks (T-DGCN). Alternatively executing the two networks helps\nextract spatiotemporal features over the global receptive field of the whole\npose sequence. All the above design choices cooperating together make our\nmethod outperform previous approaches by large margins: 6%-7% on Human3.6M,\n5%-10% on CMU-MoCap, and 13%-16% on 3DPW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tiezheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Facial Skin Feature Detection for Everyone. (arXiv:2203.16056v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16056","description":"<p>Automatic assessment and understanding of facial skin condition have several\napplications, including the early detection of underlying health problems,\nlifestyle and dietary treatment, skin-care product recommendation, etc. Selfies\nin the wild serve as an excellent data resource to democratize skin quality\nassessment, but suffer from several data collection challenges.The key to\nguaranteeing an accurate assessment is accurate detection of different skin\nfeatures. We present an automatic facial skin feature detection method that\nworks across a variety of skin tones and age groups for selfies in the wild. To\nbe specific, we annotate the locations of acne, pigmentation, and wrinkle for\nselfie images with different skin tone colors, severity levels, and lighting\nconditions. The annotation is conducted in a two-phase scheme with the help of\na dermatologist to train volunteers for annotation. We employ Unet++ as the\nnetwork architecture for feature detection. This work shows that the two-phase\nannotation scheme can robustly detect the accurate locations of acne,\npigmentation, and wrinkle for selfie images with different ethnicities, skin\ntone colors, severity levels, age groups, and lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwar_A/0/1/0/all/0/1\">Ankur Purwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Guang Liang Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_D/0/1/0/all/0/1\">Debasish Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Min Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Rizhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werner_J/0/1/0/all/0/1\">Jennifer Werner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sng_D/0/1/0/all/0/1\">Dennis Sng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steensel_M/0/1/0/all/0/1\">Maurice van Steensel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised 360$^{\\circ}$ Room Layout Estimation. (arXiv:2203.16057v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16057","description":"<p>We present the first self-supervised method to train panoramic room layout\nestimation models without any labeled data. Unlike per-pixel dense depth that\nprovides abundant correspondence constraints, layout representation is sparse\nand topological, hindering the use of self-supervised reprojection consistency\non images. To address this issue, we propose Differentiable Layout View\nRendering, which can warp a source image to the target camera pose given the\nestimated layout from the target image. As each rendered pixel is\ndifferentiable with respect to the estimated layout, we can now train the\nlayout estimation model by minimizing reprojection loss. Besides, we introduce\nregularization losses to encourage Manhattan alignment, ceiling-floor\nalignment, cycle consistency, and layout stretch consistency, which further\nimprove our predictions. Finally, we present the first self-supervised results\non ZilloIndoor and MatterportLayout datasets. Our approach also shows promising\nsolutions in data-scarce scenarios and active learning, which would have an\nimmediate value in the real estate virtual tour software. Code is available at\nhttps://github.com/joshua049/Stereo-360-Layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ting_H/0/1/0/all/0/1\">Hao-Wen Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval. (arXiv:2203.16062v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16062","description":"<p>Evaluation measures have a crucial impact on the direction of research.\nTherefore, it is of utmost importance to develop appropriate and reliable\nevaluation measures for new applications where conventional measures are not\nwell suited. Video Moment Retrieval (VMR) is one such application, and the\ncurrent practice is to use R@$K,\\theta$ for evaluating VMR systems. However,\nthis measure has two disadvantages. First, it is rank-insensitive: It ignores\nthe rank positions of successfully localised moments in the top-$K$ ranked list\nby treating the list as a set. Second, it binarizes the Intersection over Union\n(IoU) of each retrieved video moment using the threshold $\\theta$ and thereby\nignoring fine-grained localisation quality of ranked moments.\n</p>\n<p>We propose an alternative measure for evaluating VMR, called Average Max IoU\n(AxIoU), which is free from the above two problems. We show that AxIoU\nsatisfies two important axioms for VMR evaluation, namely, \\textbf{Invariance\nagainst Redundant Moments} and \\textbf{Monotonicity with respect to the Best\nMoment}, and also that R@$K,\\theta$ satisfies the first axiom only. We also\nempirically examine how AxIoU agrees with R@$K,\\theta$, as well as its\nstability with respect to change in the test data and human-annotated temporal\nboundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Togashi_R/0/1/0/all/0/1\">Riku Togashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1\">Mayu Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention. (arXiv:2203.16063v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16063","description":"<p>Video deblurring models exploit information in the neighboring frames to\nremove blur caused by the motion of the camera and the objects. Recurrent\nNeural Networks~(RNNs) are often adopted to model the temporal dependency\nbetween frames via hidden states. When motion blur is strong, however, hidden\nstates are hard to deliver proper information due to the displacement between\ndifferent frames. While there have been attempts to update the hidden states,\nit is difficult to handle misaligned features beyond the receptive field of\nsimple modules. Thus, we propose 2 modules to supplement the RNN architecture\nfor video deblurring. First, we design Ping-Pong RNN~(PPRNN) that acts on\nupdating the hidden states by referring to the features from the current and\nthe previous time steps alternately. PPRNN gathers relevant information from\nthe both features in an iterative and balanced manner by utilizing its\nrecurrent architecture. Second, we use a Selective Non-Local Attention~(SNLA)\nmodule to additionally refine the hidden state by aligning it with the\npositional information from the input frame feature. The attention score is\nscaled by the relevance to the input feature to focus on the necessary\ninformation. By paying attention to hidden states with both modules, which have\nstrong synergy, our PAHS framework improves the representation powers of RNN\nstructures and achieves state-of-the-art deblurring performance on standard\nbenchmarks and real-world videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">JoonKyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Program Representations for Food Images and Cooking Recipes. (arXiv:2203.16071v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16071","description":"<p>In this paper, we are interested in modeling a how-to instructional\nprocedure, such as a cooking recipe, with a meaningful and rich high-level\nrepresentation. Specifically, we propose to represent cooking recipes and food\nimages as cooking programs. Programs provide a structured representation of the\ntask, capturing cooking semantics and sequential relationships of actions in\nthe form of a graph. This allows them to be easily manipulated by users and\nexecuted by agents. To this end, we build a model that is trained to learn a\njoint embedding between recipes and food images via self-supervision and\njointly generate a program from this embedding as a sequence. To validate our\nidea, we crowdsource programs for cooking recipes and show that: (a) projecting\nthe image-recipe embeddings into programs leads to better cross-modal retrieval\nresults; (b) generating programs from images leads to better recognition\nresults compared to predicting raw cooking instructions; and (c) we can\ngenerate food images by manipulating programs via optimizing the latent code of\na GAN. Code, data, and models are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1\">Dim P. Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mora_E/0/1/0/all/0/1\">Enrique Mora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Anchor-free Universal Lesion Detection in CT-scans. (arXiv:2203.16074v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16074","description":"<p>Existing universal lesion detection (ULD) methods utilize compute-intensive\nanchor-based architectures which rely on predefined anchor boxes, resulting in\nunsatisfactory detection performance, especially in small and mid-sized\nlesions. Further, these default fixed anchor-sizes and ratios do not generalize\nwell to different datasets. Therefore, we propose a robust one-stage\nanchor-free lesion detection network that can perform well across varying\nlesions sizes by exploiting the fact that the box predictions can be sorted for\nrelevance based on their center rather than their overlap with the object.\nFurthermore, we demonstrate that the ULD can be improved by explicitly\nproviding it the domain-specific information in the form of multi-intensity\nimages generated using multiple HU windows, followed by self-attention based\nfeature-fusion and backbone initialization using weights learned via\nself-supervision over CT-scans. We obtain comparable results to the\nstate-of-the-art methods, achieving an overall sensitivity of 86.05% on the\nDeepLesion dataset, which comprises of approximately 32K CT-scans with lesions\nannotated across various body organs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheoran_M/0/1/0/all/0/1\">Manu Sheoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_M/0/1/0/all/0/1\">Meghal Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Monika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction. (arXiv:2203.16084v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16084","description":"<p>Although many video prediction methods have obtained good performance in\nlow-resolution (64$\\sim$128) videos, predictive models for high-resolution\n(512$\\sim$4K) videos have not been fully explored yet, which are more\nmeaningful due to the increasing demand for high-quality videos. Compared with\nlow-resolution videos, high-resolution videos contain richer appearance\n(spatial) information and more complex motion (temporal) information. In this\npaper, we propose a Spatiotemporal Residual Predictive Model (STRPM) for\nhigh-resolution video prediction. On the one hand, we propose a Spatiotemporal\nEncoding-Decoding Scheme to preserve more spatiotemporal information for\nhigh-resolution videos. In this way, the appearance details for each frame can\nbe greatly preserved. On the other hand, we design a Residual Predictive Memory\n(RPM) which focuses on modeling the spatiotemporal residual features (STRF)\nbetween previous and future frames instead of the whole frame, which can\ngreatly help capture the complex motion information in high-resolution videos.\nIn addition, the proposed RPM can supervise the spatial encoder and temporal\nencoder to extract different features in the spatial domain and the temporal\ndomain, respectively. Moreover, the proposed model is trained using generative\nadversarial networks (GANs) with a learned perceptual loss (LP-loss) to improve\nthe perceptual quality of the predictions. Experimental results show that STRPM\ncan generate more satisfactory results compared with various existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1\">Zheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-DETR: Omni-Supervised Object Detection with Transformers. (arXiv:2203.16089v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16089","description":"<p>We consider the problem of omni-supervised object detection, which can use\nunlabeled, fully labeled and weakly labeled annotations, such as image tags,\ncounts, points, etc., for object detection. This is enabled by a unified\narchitecture, Omni-DETR, based on the recent progress on student-teacher\nframework and end-to-end transformer based object detection. Under this unified\narchitecture, different types of weak labels can be leveraged to generate\naccurate pseudo labels, by a bipartite matching based filtering mechanism, for\nthe model to learn. In the experiments, Omni-DETR has achieved state-of-the-art\nresults on multiple datasets and settings. And we have found that weak\nannotations can help to improve detection performance and a mixture of them can\nachieve a better trade-off between annotation cost and accuracy than the\nstandard complete annotation. These findings could encourage larger object\ndetection datasets with mixture annotations. The code is available at\nhttps://github.com/amazon-research/omni-detr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_G/0/1/0/all/0/1\">Gurumurthy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Tracking via Ensemble of Local Trackers. (arXiv:2203.16092v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16092","description":"<p>The crux of long-term tracking lies in the difficulty of tracking the target\nwith discontinuous moving caused by out-of-view or occlusion. Existing\nlong-term tracking methods follow two typical strategies. The first strategy\nemploys a local tracker to perform smooth tracking and uses another re-detector\nto detect the target when the target is lost. While it can exploit the temporal\ncontext like historical appearances and locations of the target, a potential\nlimitation of such strategy is that the local tracker tends to misidentify a\nnearby distractor as the target instead of activating the re-detector when the\nreal target is out of view. The other long-term tracking strategy tracks the\ntarget in the entire image globally instead of local tracking based on the\nprevious tracking results. Unfortunately, such global tracking strategy cannot\nleverage the temporal context effectively. In this work, we combine the\nadvantages of both strategies: tracking the target in a global view while\nexploiting the temporal context. Specifically, we perform global tracking via\nensemble of local trackers spreading the full image. The smooth moving of the\ntarget can be handled steadily by one local tracker. When the local tracker\naccidentally loses the target due to suddenly discontinuous moving, another\nlocal tracker close to the target is then activated and can readily take over\nthe tracking to locate the target. While the activated local tracker performs\ntracking locally by leveraging the temporal context, the ensemble of local\ntrackers renders our model the global view for tracking. Extensive experiments\non six datasets demonstrate that our method performs favorably against\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zikun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianqiu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kaige Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contribution of the Temperature of the Objects to the Problem of Thermal Imaging Focusing. (arXiv:2203.16106v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16106","description":"<p>When focusing an image, depth of field, aperture and distance from the camera\nto the object, must be taking into account, both, in visible and in infrared\nspectrum. Our experiments reveal that in addition, the focusing problem in\nthermal spectrum is also hardly dependent of the temperature of the object\nitself (and/or the scene).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Dur&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preliminary experiments on thermal emissivity adjustment for face images. (arXiv:2203.16107v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16107","description":"<p>In this paper we summarize several applications based on thermal imaging. We\nemphasize the importance of emissivity adjustment for a proper temperature\nmeasurement. A new set of face images acquired at different emissivity values\nwith steps of 0.01 is also presented and will be distributed for free for\nresearch purposes. Among the utilities, we can mention: a) the possibility to\napply corrections once an image is acquired with a wrong emissivity value and\nit is not possible to acquire a new one; b) privacy protection in thermal\nimages, which can be obtained with a low emissivity factor, which is still\nsuitable for several applications, but hides the identity of a user; c) image\nprocessing for improving temperature detection in scenes containing objects of\ndifferent emissivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragones_X/0/1/0/all/0/1\">Xavier Font Aragones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network. (arXiv:2203.16117v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16117","description":"<p>Spiking Neural Networks (SNNs) have piqued researchers' interest because of\ntheir capacity to process temporal information and low power consumption.\nHowever, current state-of-the-art methods limited their biological plausibility\nand performance because their neurons are generally built on the simple\nLeaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic\ncomplexity, modern neuron models have seldom been implemented in SNN practice.\nIn this study, we adopt the Phase Plane Analysis (PPA) technique, a technique\noften utilized in neurodynamics field, to integrate a recent neuron model,\nnamely, the Izhikevich neuron. Based on the findings in the advancement of\nneuroscience, the Izhikevich neuron model can be biologically plausible while\nmaintaining comparable computational cost with LIF neurons. By utilizing the\nadopted PPA, we have accomplished putting neurons built with the modified\nIzhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic\n(SIT) neuron. For performance, we evaluate the suggested technique for image\nclassification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid\nNeural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and\nneuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The\nexperimental results indicate that the suggested method achieves comparable\naccuracy while exhibiting more biologically realistic behaviors on nearly all\ntest datasets, demonstrating the efficiency of this novel strategy in bridging\nthe gap between neurodynamics and SNN practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (arXiv:2203.16130v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16130","description":"<p>Autonomous driving technology has drawn a lot of attention due to its fast\ndevelopment and extremely high commercial values. The recent technological leap\nof autonomous driving can be primarily attributed to the progress in the\nenvironment perception. Good environment perception provides accurate\nhigh-level environment information which is essential for autonomous vehicles\nto make safe and precise driving decisions and strategies. Moreover, such\nprogress in accurate environment perception would not be possible without deep\nlearning models and advanced onboard sensors, such as optical sensors (LiDARs\nand cameras), radars, GPS. However, the advanced sensors and deep learning\nmodels are prone to recently invented attack methods. For example, LiDARs and\ncameras can be compromised by optical attacks, and deep learning models can be\nattacked by adversarial examples. The attacks on advanced sensors and deep\nlearning models can largely impact the accuracy of the environment perception,\nposing great threats to the safety and security of autonomous vehicles. In this\nthesis, we study the detection methods against the attacks on onboard sensors\nand the linkage between attacked deep learning models and driving safety for\nautonomous vehicles. To detect the attacks, redundant data sources can be\nexploited, since information distortions caused by attacks in victim sensor\ndata result in inconsistency with the information from other redundant sources.\nTo study the linkage between attacked deep learning models and driving\nsafety...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tampered VAE for Improved Satellite Image Time Series Classification. (arXiv:2203.16149v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16149","description":"<p>The unprecedented availability of spatial and temporal high-resolution\nsatellite image time series (SITS) for crop type mapping is believed to\nnecessitate deep learning architectures to accommodate challenges arising from\nboth dimensions. Recent state-of-the-art deep learning models have shown\npromising results by stacking spatial and temporal encoders. However, we\npresent a Pyramid Time-Series Transformer (PTST) that operates solely on the\ntemporal dimension, i.e., neglecting the spatial dimension, can produce\nsuperior results with a drastic reduction in GPU memory consumption and easy\nextensibility. Furthermore, we augment it to perform semi-supervised learning\nby proposing a classification-friendly VAE framework that introduces clustering\nmechanisms into latent space and can promote linear separability therein.\nConsequently, a few principal axes of the latent space can explain the majority\nof variance in raw data. Meanwhile, the VAE framework with proposed tweaks can\nmaintain competitive classification performance as its purely discriminative\ncounterpart when only $40\\%$ of labelled data is used. We hope the proposed\nframework can serve as a baseline for crop classification with SITS for its\nmodularity and simplicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1\">Yaxin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholl_P/0/1/0/all/0/1\">Peter Nicholl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendation of Compatible Outfits Conditioned on Style. (arXiv:2203.16161v1 [cs.IR])","link":"http://arxiv.org/abs/2203.16161","description":"<p>Recommendation in the fashion domain has seen a recent surge in research in\nvarious areas, for example, shop-the-look, context-aware outfit creation,\npersonalizing outfit creation, etc. The majority of state of the art approaches\nin the domain of outfit recommendation pursue to improve compatibility among\nitems so as to produce high quality outfits. Some recent works have realized\nthat style is an important factor in fashion and have incorporated it in\ncompatibility learning and outfit generation. These methods often depend on the\navailability of fine-grained product categories or the presence of rich item\nattributes (e.g., long-skirt, mini-skirt, etc.). In this work, we aim to\ngenerate outfits conditional on styles or themes as one would dress in real\nlife, operating under the practical assumption that each item is mapped to a\nhigh level category as driven by the taxonomy of an online portal, like\noutdoor, formal etc and an image. We use a novel style encoder network that\nrenders outfit styles in a smooth latent space. We present an extensive\nanalysis of different aspects of our method and demonstrate its superiority\nover existing state of the art baselines through rigorous experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debopriyo Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_L/0/1/0/all/0/1\">Lucky Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Harsh Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelliah_M/0/1/0/all/0/1\">Muthusamy Chelliah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rabbit, toad, and the Moon: Can machine categorize them into one class?. (arXiv:2203.16163v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16163","description":"<p>Recent machine learning algorithms such as neural networks can classify\nobjects and actions in video frames with high accuracy. Here, I discuss a\nclassification of objects based on basal dynamic patterns referencing one\ntradition, the link between rabbit, toad, and the Moon, which can be seen in\nseveral cultures. In order for them to be classified into one class, a basic\npattern of behavior (cyclic appearance and disappearance) works as a feature\npoint. A static character such as the shape and time scale of the behavior are\nnot essential for this classification. In cognitive semantics, image schemas\nare introduced to describe basal patterns of events. If learning of these image\nschemas is attained, a machine may be able to categorize rabbit, toad, and the\nMoon as the same class. For learning, video frames that show boundary boxes or\nsegmentation may be helpful. Although this discussion is preliminary and many\ntasks remain to be solved, the classification based on basal behaviors can be\nan important topic for cognitive processes and computer science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoji_D/0/1/0/all/0/1\">Daigo Shoji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLOAT: Factorized Learning of Object Attributes for Improved Multi-object Multi-part Scene Parsing. (arXiv:2203.16168v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16168","description":"<p>Multi-object multi-part scene parsing is a challenging task which requires\ndetecting multiple object classes in a scene and segmenting the semantic parts\nwithin each object. In this paper, we propose FLOAT, a factorized label space\nframework for scalable multi-object multi-part parsing. Our framework involves\nindependent dense prediction of object category and part attributes which\nincreases scalability and reduces task complexity compared to the monolithic\nlabel space counterpart. In addition, we propose an inference-time 'zoom'\nrefinement technique which significantly improves segmentation quality,\nespecially for smaller objects/parts. Compared to state of the art, FLOAT\nobtains an absolute improvement of 2.0% for mean IOU (mIOU) and 4.8% for\nsegmentation quality IOU (sqIOU) on the Pascal-Part-58 dataset. For the larger\nPascal-Part-108 dataset, the improvements are 2.1% for mIOU and 3.9% for sqIOU.\nWe incorporate previously excluded part attributes and other minor parts of the\nPascal-Part dataset to create the most comprehensive and challenging version\nwhich we dub Pascal-Part-201. FLOAT obtains improvements of 8.6% for mIOU and\n7.5% for sqIOU on the new dataset, demonstrating its parsing effectiveness\nacross a challenging diversity of objects and parts. The code and datasets are\navailable at floatseg.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishubh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravikiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Distillation from the Last Mini-Batch for Consistency Regularization. (arXiv:2203.16172v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16172","description":"<p>Knowledge distillation (KD) shows a bright promise as a powerful\nregularization strategy to boost generalization ability by leveraging learned\nsample-level soft targets. Yet, employing a complex pre-trained teacher network\nor an ensemble of peer students in existing KD is both time-consuming and\ncomputationally costly. Various self KD methods have been proposed to achieve\nhigher distillation efficiency. However, they either require extra network\narchitecture modification or are difficult to parallelize. To cope with these\nchallenges, we propose an efficient and reliable self-distillation framework,\nnamed Self-Distillation from Last Mini-Batch (DLB). Specifically, we rearrange\nthe sequential sampling by constraining half of each mini-batch coinciding with\nthe previous iteration. Meanwhile, the rest half will coincide with the\nupcoming iteration. Afterwards, the former half mini-batch distills on-the-fly\nsoft targets generated in the previous iteration. Our proposed mechanism guides\nthe training stability and consistency, resulting in robustness to label noise.\nMoreover, our method is easy to implement, without taking up extra run-time\nmemory or requiring model structure modification. Experimental results on three\nclassification benchmarks illustrate that our approach can consistently\noutperform state-of-the-art self-distillation approaches with different network\narchitectures. Additionally, our method shows strong compatibility with\naugmentation strategies by gaining additional performance improvement. The code\nis available at https://github.com/Meta-knowledge-Lab/DLB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowFormer: A Transformer Architecture for Optical Flow. (arXiv:2203.16194v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16194","description":"<p>We introduce Optical Flow TransFormer (FlowFormer), a transformer-based\nneural network architecture for learning optical flow. FlowFormer tokenizes the\n4D cost volume built from an image pair, encodes the cost tokens into a cost\nmemory with alternate-group transformer (AGT) layers in a novel latent space,\nand decodes the cost memory via a recurrent transformer decoder with dynamic\npositional cost queries. On the Sintel benchmark clean pass, FlowFormer\nachieves 1.178 average end-ponit-error (AEPE), a 15.1% error reduction from the\nbest published result (1.388). Besides, FlowFormer also achieves strong\ngeneralization performance. Without being trained on Sintel, FlowFormer\nachieves 1.00 AEPE on the Sintel training set clean pass, outperforming the\nbest published result (1.29) by 22.4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Road to Online Adaptation for Semantic Image Segmentation. (arXiv:2203.16195v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16195","description":"<p>We propose a new problem formulation and a corresponding evaluation framework\nto advance research on unsupervised domain adaptation for semantic image\nsegmentation. The overall goal is fostering the development of adaptive\nlearning systems that will continuously learn, without supervision, in\never-changing environments. Typical protocols that study adaptation algorithms\nfor segmentation models are limited to few domains, adaptation happens offline,\nand human intervention is generally required, at least to annotate data for\nhyper-parameter tuning. We argue that such constraints are incompatible with\nalgorithms that can continuously adapt to different real-world situations. To\naddress this, we propose a protocol where models need to learn online, from\nsequences of temporally correlated images, requiring continuous, frame-by-frame\nadaptation. We accompany this new protocol with a variety of baselines to\ntackle the proposed formulation, as well as an extensive analysis of their\nbehaviors, which can serve as a starting point for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Volpi_R/0/1/0/all/0/1\">Riccardo Volpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_P/0/1/0/all/0/1\">Pau de Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation. (arXiv:2203.16202v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16202","description":"<p>We propose an approach to estimate arm and hand dynamics from monocular video\nby utilizing the relationship between arm and hand. Although monocular full\nhuman motion capture technologies have made great progress in recent years,\nrecovering accurate and plausible arm twists and hand gestures from in-the-wild\nvideos still remains a challenge. To solve this problem, our solution is\nproposed based on the fact that arm poses and hand gestures are highly\ncorrelated in most real situations. To fully exploit arm-hand correlation as\nwell as inter-frame information, we carefully design a Spatial-Temporal\nParallel Arm-Hand Motion Transformer (PAHMT) to predict the arm and hand\ndynamics simultaneously. We also introduce new losses to encourage the\nestimations to be smooth and accurate. Besides, we collect a motion capture\ndataset including 200K frames of hand gestures and use this data to train our\nmodel. By integrating a 2D hand pose estimation model and a 3D human pose\nestimation model, the proposed method can produce plausible arm and hand\ndynamics from monocular video. Extensive evaluations demonstrate that the\nproposed method has advantages over previous state-of-the-art approaches and\nshows robustness under various challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Contrastive Learning for Facial Attribute Classification. (arXiv:2203.16209v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16209","description":"<p>Learning visual representation of high quality is essential for image\nclassification. Recently, a series of contrastive representation learning\nmethods have achieved preeminent success. Particularly, SupCon outperformed the\ndominant methods based on cross-entropy loss in representation learning.\nHowever, we notice that there could be potential ethical risks in supervised\ncontrastive learning. In this paper, we for the first time analyze unfairness\ncaused by supervised contrastive learning and propose a new Fair Supervised\nContrastive Loss (FSCL) for fair visual representation learning. Inheriting the\nphilosophy of supervised contrastive learning, it encourages representation of\nthe same class to be closer to each other than that of different classes, while\nensuring fairness by penalizing the inclusion of sensitive attribute\ninformation in representation. In addition, we introduce a group-wise\nnormalization to diminish the disparities of intra-group compactness and\ninter-class separability between demographic groups that arouse unfair\nclassification. Through extensive experiments on CelebA and UTK Face, we\nvalidate that the proposed method significantly outperforms SupCon and existing\nstate-of-the-art methods in terms of the trade-off between top-1 accuracy and\nfairness. Moreover, our method is robust to the intensity of data bias and\neffectively works in incomplete supervised settings. Our code is available at\nhttps://github.com/sungho-CoolG/FSCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jewook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sunhee Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dohyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning of Global Objective for Network Flow in Multi-Object Tracking. (arXiv:2203.16210v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16210","description":"<p>This paper concerns the problem of multi-object tracking based on the\nmin-cost flow (MCF) formulation, which is conventionally studied as an instance\nof linear program. Given its computationally tractable inference, the success\nof MCF tracking largely relies on the learned cost function of underlying\nlinear program. Most previous studies focus on learning the cost function by\nonly taking into account two frames during training, therefore the learned cost\nfunction is sub-optimal for MCF where a multi-frame data association must be\nconsidered during inference. In order to address this problem, in this paper we\npropose a novel differentiable framework that ties training and inference\ntogether during learning by solving a bi-level optimization problem, where the\nlower-level solves a linear program and the upper-level contains a loss\nfunction that incorporates global tracking result. By back-propagating the loss\nthrough differentiable layers via gradient descent, the globally parameterized\ncost function is explicitly learned and regularized. With this approach, we are\nable to learn a better objective for global MCF tracking. As a result, we\nachieve competitive performances compared to the current state-of-the-art\nmethods on the popular multi-object tracking benchmarks such as MOT16, MOT17\nand MOT20.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16219","description":"<p>Due to the difficulty of collecting exhaustive multi-label annotations,\nmulti-label training data often contains partial labels. We consider an extreme\nof this problem, called single positive multi-label learning (SPML), where each\nmulti-label training image has only one positive label. Traditionally, all\nunannotated labels are assumed as negative labels in SPML, which would\nintroduce false negative labels and make model training be dominated by assumed\nnegative labels. In this work, we choose to treat all unannotated labels from a\ndifferent perspective, \\textit{i.e.} acknowledging they are unknown. Hence, we\npropose entropy-maximization (EM) loss to maximize the entropy of predicted\nprobabilities for all unannotated labels. Considering the positive-negative\nlabel imbalance of unannotated labels, we propose asymmetric pseudo-labeling\n(APL) with asymmetric-tolerance strategies and a self-paced procedure to\nprovide more precise supervision. Experiments show that our method\nsignificantly improves performance and achieves state-of-the-art results on all\nfour benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Donghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection. (arXiv:2203.16220v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16220","description":"<p>This study addresses the issue of fusing infrared and visible images that\nappear differently for object detection. Aiming at generating an image of high\nvisual quality, previous approaches discover commons underlying the two\nmodalities and fuse upon the common space either by iterative optimization or\ndeep networks. These approaches neglect that modality differences implying the\ncomplementary information are extremely important for both fusion and\nsubsequent detection task. This paper proposes a bilevel optimization\nformulation for the joint problem of fusion and detection, and then unrolls to\na target-aware Dual Adversarial Learning (TarDAL) network for fusion and a\ncommonly used detection network. The fusion network with one generator and dual\ndiscriminators seeks commons while learning from differences, which preserves\nstructural information of targets from the infrared and textural details from\nthe visible. Furthermore, we build a synchronized imaging system with\ncalibrated infrared and optical sensors, and collect currently the most\ncomprehensive benchmark covering a wide range of scenarios. Extensive\nexperiments on several public datasets and our benchmark demonstrate that our\nmethod outputs not only visually appealing fusion but also higher detection mAP\nthan the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhanbo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guanyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongxuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End to End Lip Synchronization with a Temporal AutoEncoder. (arXiv:2203.16224v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16224","description":"<p>We study the problem of syncing the lip movement in a video with the audio\nstream. Our solution finds an optimal alignment using a dual-domain recurrent\nneural network that is trained on synthetic data we generate by dropping and\nduplicating video frames. Once the alignment is found, we modify the video in\norder to sync the two sources. Our method is shown to greatly outperform the\nliterature methods on a variety of existing and new benchmarks. As an\napplication, we demonstrate our ability to robustly align text-to-speech\ngenerated audio with an existing video stream. Our code and samples are\navailable at\nhttps://github.com/itsyoavshalev/End-to-End-Lip-Synchronization-with-a-Temporal-AutoEncoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biclustering Algorithms Based on Metaheuristics: A Review. (arXiv:2203.16241v1 [cs.LG])","link":"http://arxiv.org/abs/2203.16241","description":"<p>Biclustering is an unsupervised machine learning technique that\nsimultaneously clusters rows and columns in a data matrix. Biclustering has\nemerged as an important approach and plays an essential role in various\napplications such as bioinformatics, text mining, and pattern recognition.\nHowever, finding significant biclusters is an NP-hard problem that can be\nformulated as an optimization problem. Therefore, different metaheuristics have\nbeen applied to biclustering problems because of their exploratory capability\nof solving complex optimization problems in reasonable computation time.\nAlthough various surveys on biclustering have been proposed, there is a lack of\na comprehensive survey on the biclustering problem using metaheuristics. This\nchapter will present a survey of metaheuristics approaches to address the\nbiclustering problem. The review focuses on the underlying optimization methods\nand their main search components: representation, objective function, and\nvariation operators. A specific discussion on single versus multi-objective\napproaches is presented. Finally, some emerging research directions are\npresented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jose_Garcia_A/0/1/0/all/0/1\">Adan Jose-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacques_J/0/1/0/all/0/1\">Julie Jacques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobanski_V/0/1/0/all/0/1\">Vincent Sobanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhaenens_C/0/1/0/all/0/1\">Clarisse Dhaenens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycDA: Unsupervised Cycle Domain Adaptation from Image to Video. (arXiv:2203.16244v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16244","description":"<p>Although action recognition has achieved impressive results over recent\nyears, both collection and annotation of video training data are still\ntime-consuming and cost intensive. Therefore, image-to-video adaptation has\nbeen proposed to exploit labeling-free web image source for adapting on\nunlabeled target videos. This poses two major challenges: (1) spatial domain\nshift between web images and video frames; (2) modality gap between image and\nvideo data. To address these challenges, we propose Cycle Domain Adaptation\n(CycDA), a cycle-based approach for unsupervised image-to-video domain\nadaptation by leveraging the joint spatial information in images and videos on\nthe one hand and, on the other hand, training an independent spatio-temporal\nmodel to bridge the modality gap. We alternate between the spatial and\nspatio-temporal learning with knowledge transfer between the two in each cycle.\nWe evaluate our approach on benchmark datasets for image-to-video as well as\nfor mixed-source domain adaptation achieving state-of-the-art results and\ndemonstrating the benefits of our cyclic adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1\">Anna Kukleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kunyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstaFormer: Instance-Aware Image-to-Image Translation with Transformer. (arXiv:2203.16248v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16248","description":"<p>We present a novel Transformer-based network architecture for instance-aware\nimage-to-image translation, dubbed InstaFormer, to effectively integrate\nglobal- and instance-level information. By considering extracted content\nfeatures from an image as tokens, our networks discover global consensus of\ncontent features by considering context information through a self-attention\nmodule in Transformers. By augmenting such tokens with an instance-level\nfeature extracted from the content feature with respect to bounding box\ninformation, our framework is capable of learning an interaction between object\ninstances and the global image, thus boosting the instance-awareness. We\nreplace layer normalization (LayerNorm) in standard Transformers with adaptive\ninstance normalization (AdaIN) to enable a multi-modal translation with style\ncodes. In addition, to improve the instance-awareness and translation quality\nat object regions, we present an instance-level content contrastive loss\ndefined between input and translated image. We conduct experiments to\ndemonstrate the effectiveness of our InstaFormer over the latest methods and\nprovide extensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soohyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jongbeom Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jihye Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongnyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16250","description":"<p>In this report, we present PP-YOLOE, an industrial state-of-the-art object\ndetector with high performance and friendly deployment. We optimize on the\nbasis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful\nbackbone and neck equipped with CSPRepResStage, ET-head and dynamic label\nassignment algorithm TAL. We provide s/m/l/x models for different practice\nscenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1\nFPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed\nup) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art\nindustrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference\nspeed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct\nextensive experiments to verify the effectiveness of our designs. Source code\nand pre-trained models are available at\nhttps://github.com/PaddlePaddle/PaddleDetection .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shangliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1\">Wenyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qinyao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kaipeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1\">Qingqing Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shengyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. (arXiv:2203.16258v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16258","description":"<p>Segmenting or detecting objects in sparse Lidar point clouds are two\nimportant tasks in autonomous driving to allow a vehicle to act safely in its\n3D environment. The best performing methods in 3D semantic segmentation or\nobject detection rely on a large amount of annotated data. Yet annotating 3D\nLidar data for these tasks is tedious and costly. In this context, we propose a\nself-supervised pre-training method for 3D perception models that is tailored\nto autonomous driving data. Specifically, we leverage the availability of\nsynchronized and calibrated image and Lidar sensors in autonomous driving\nsetups for distilling self-supervised pre-trained image representations into 3D\nmodels. Hence, our method does not require any point cloud nor image\nannotations. The key ingredient of our method is the use of superpixels which\nare used to pool 3D point features and 2D pixel features in visually similar\nregions. We then train a 3D network on the self-supervised task of matching\nthese pooled point features with the corresponding pooled image pixel features.\nThe advantages of contrasting regions obtained by superpixels are that: (1)\ngrouping together pixels and points of visually coherent regions leads to a\nmore meaningful contrastive task that produces features well adapted to 3D\nsemantic segmentation and 3D object detection; (2) all the different regions\nhave the same weight in the contrastive loss regardless of the number of 3D\npoints sampled in these regions; (3) it mitigates the noise produced by\nincorrect matching of points and pixels due to occlusions between the different\nsensors. Extensive experiments on autonomous driving datasets demonstrate the\nability of our image-to-Lidar distillation strategy to produce 3D\nrepresentations that transfer well on semantic segmentation and object\ndetection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sautier_C/0/1/0/all/0/1\">Corentin Sautier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1\">Spyros Gidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16265","description":"<p>In this paper, we propose a simple yet universal network termed SeqTR for\nvisual grounding tasks, e.g., phrase localization, referring expression\ncomprehension (REC) and segmentation (RES). The canonical paradigms for visual\ngrounding often require substantial expertise in designing network\narchitectures and loss functions, making them hard to generalize across tasks.\nTo simplify and unify the modeling, we cast visual grounding as a point\nprediction problem conditioned on image and text inputs, where either the\nbounding box or binary mask is represented as a sequence of discrete coordinate\ntokens. Under this paradigm, visual grounding tasks are unified in our SeqTR\nnetwork without task-specific branches or heads, e.g., the convolutional mask\ndecoder for RES, which greatly reduces the complexity of multi-task modeling.\nIn addition, SeqTR also shares the same optimization objective for all tasks\nwith a simple cross-entropy loss, further reducing the complexity of deploying\nhand-crafted loss functions. Experiments on five benchmark datasets demonstrate\nthat the proposed SeqTR outperforms (or is on par with) the existing\nstate-of-the-arts, proving that a simple yet universal approach for visual\ngrounding is indeed feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chaoyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Multi-scale Fusion of 2D and 3D Features for Multi-object Tracking. (arXiv:2203.16268v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16268","description":"<p>Multiple object tracking (MOT) is a significant task in achieving autonomous\ndriving. Traditional works attempt to complete this task, either based on point\nclouds (PC) collected by LiDAR, or based on images captured from cameras.\nHowever, relying on one single sensor is not robust enough, because it might\nfail during the tracking process. On the other hand, feature fusion from\nmultiple modalities contributes to the improvement of accuracy. As a result,\nnew techniques based on different sensors integrating features from multiple\nmodalities are being developed. Texture information from RGB cameras and 3D\nstructure information from Lidar have respective advantages under different\ncircumstances. However, it's not easy to achieve effective feature fusion\nbecause of completely distinct information modalities. Previous fusion methods\nusually fuse the top-level features after the backbones extract the features\nfrom different modalities. In this paper, we first introduce PointNet++ to\nobtain multi-scale deep representations of point cloud to make it adaptive to\nour proposed Interactive Feature Fusion between multi-scale features of images\nand point clouds. Specifically, through multi-scale interactive query and\nfusion between pixel-level and point-level features, our method, can obtain\nmore distinguishing features to improve the performance of multiple object\ntracking. Besides, we explore the effectiveness of pre-training on each single\nmodality and fine-tuning on the fusion-based model. The experimental results\ndemonstrate that our method can achieve good performance on the KITTI benchmark\nand outperform other approaches without using multi-scale feature fusion.\nMoreover, the ablation studies indicates the effectiveness of multi-scale\nfeature fusion and pre-training on single modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chensheng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Vertebral Fracture Diagnosis. (arXiv:2203.16273v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16273","description":"<p>Do black-box neural network models learn clinically relevant features for\nfracture diagnosis? The answer not only establishes reliability quenches\nscientific curiosity but also leads to explainable and verbose findings that\ncan assist the radiologists in the final and increase trust. This work\nidentifies the concepts networks use for vertebral fracture diagnosis in CT\nimages. This is achieved by associating concepts to neurons highly correlated\nwith a specific diagnosis in the dataset. The concepts are either associated\nwith neurons by radiologists pre-hoc or are visualized during a specific\nprediction and left for the user's interpretation. We evaluate which concepts\nlead to correct diagnosis and which concepts lead to false positives. The\nproposed frameworks and analysis pave the way for reliable and explainable\nvertebral fracture diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Engstler_P/0/1/0/all/0/1\">Paul Engstler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schinz_D/0/1/0/all/0/1\">David Schinz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mach_K/0/1/0/all/0/1\">Kristina Mach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gersing_A/0/1/0/all/0/1\">Alexandra S. Gersing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foreman_S/0/1/0/all/0/1\">Sarah C. Foreman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goller_S/0/1/0/all/0/1\">Sophia S. Goller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissinger_J/0/1/0/all/0/1\">Juergen Weissinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rischewski_J/0/1/0/all/0/1\">Jon Rischewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietrich_A/0/1/0/all/0/1\">Anna-Sophia Dietrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan S. Kirschke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDSDF: Hybrid Directional and Signed Distance Functions for Fast Inverse Rendering. (arXiv:2203.16284v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16284","description":"<p>Implicit neural representations of 3D shapes form strong priors that are\nuseful for various applications, such as single and multiple view 3D\nreconstruction. A downside of existing neural representations is that they\nrequire multiple network evaluations for rendering, which leads to high\ncomputational costs. This limitation forms a bottleneck particularly in the\ncontext of inverse problems, such as image-based 3D reconstruction. To address\nthis issue, in this paper (i) we propose a novel hybrid 3D object\nrepresentation based on a signed distance function (SDF) that we augment with a\ndirectional distance function (DDF), so that we can predict distances to the\nobject surface from any point on a sphere enclosing the object. Moreover, (ii)\nusing the proposed hybrid representation we address the multi-view consistency\nproblem common in existing DDF representations. We evaluate our novel hybrid\nrepresentation on the task of single-view depth reconstruction and show that\nour method is several times faster compared to competing methods, while at the\nsame time achieving better reconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yenamandra_T/0/1/0/all/0/1\">Tarun Yenamandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region of Interest focused MRI to Synthetic CT Translation using Regression and Classification Multi-task Network. (arXiv:2203.16288v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16288","description":"<p>In this work, we present a method for synthetic CT (sCT) generation from\nzero-echo-time (ZTE) MRI aimed at structural and quantitative accuracies of the\nimage, with a particular focus on the accurate bone density value prediction.\nWe propose a loss function that favors a spatially sparse region in the image.\nWe harness the ability of a multi-task network to produce correlated outputs as\na framework to enable localisation of region of interest (RoI) via\nclassification, emphasize regression of values within RoI and still retain the\noverall accuracy via global regression. The network is optimized by a composite\nloss function that combines a dedicated loss from each task. We demonstrate how\nthe multi-task network with RoI focused loss offers an advantage over other\nconfigurations of the network to achieve higher accuracy of performance. This\nis relevant to sCT where failure to accurately estimate high Hounsfield Unit\nvalues of bone could lead to impaired accuracy in clinical applications. We\ncompare the dose calculation maps from the proposed sCT and the real CT in a\nradiation therapy treatment planning setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kaushik_S/0/1/0/all/0/1\">Sandeep Kaushik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bylund_M/0/1/0/all/0/1\">Mikael Bylund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cozzini_C/0/1/0/all/0/1\">Cristina Cozzini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shanbhag_D/0/1/0/all/0/1\">Dattesh Shanbhag</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petit_S/0/1/0/all/0/1\">Steven F Petit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wyatt_J/0/1/0/all/0/1\">Jonathan J Wyatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menzel_M/0/1/0/all/0/1\">Marion I Menzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pirkl_C/0/1/0/all/0/1\">Carolin Pirkl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_B/0/1/0/all/0/1\">Bhairav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chauhan_V/0/1/0/all/0/1\">Vikas Chauhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandrasekharan_K/0/1/0/all/0/1\">Kesavadas Chandrasekharan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nyholm_T/0/1/0/all/0/1\">Tufve Nyholm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiesinger_F/0/1/0/all/0/1\">Florian Wiesinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift. (arXiv:2203.16291v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16291","description":"<p>We introduce AmsterTime: a challenging dataset to benchmark visual place\nrecognition (VPR) in presence of a severe domain shift. AmsterTime offers a\ncollection of 2,500 well-curated images matching the same scene from a street\nview matched to historical archival image data from Amsterdam city. The image\npairs capture the same place with different cameras, viewpoints, and\nappearances. Unlike existing benchmark datasets, AmsterTime is directly\ncrowdsourced in a GIS navigation platform (Mapillary). We evaluate various\nbaselines, including non-learning, supervised and self-supervised methods,\npre-trained on different relevant datasets, for both verification and retrieval\ntasks. Our result credits the best accuracy to the ResNet-101 model pre-trained\non the Landmarks dataset for both verification and retrieval tasks by 84% and\n24%, respectively. Additionally, a subset of Amsterdam landmarks is collected\nfor feature evaluation in a classification task. Classification labels are\nfurther used to extract the visual explanations using Grad-CAM for inspection\nof the learned similar visuals in a deep metric learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yildiz_B/0/1/0/all/0/1\">Burak Yildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_S/0/1/0/all/0/1\">Seyran Khademi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebes_R/0/1/0/all/0/1\">Ronald Maria Siebes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16297","description":"<p>Object detection and forecasting are fundamental components of embodied\nperception. These two problems, however, are largely studied in isolation by\nthe community. In this paper, we propose an end-to-end approach for detection\nand motion forecasting based on raw sensor measurement as opposed to ground\ntruth tracks. Instead of predicting the current frame locations and forecasting\nforward in time, we directly predict future object locations and backcast to\ndetermine where each trajectory began. Our approach not only improves overall\naccuracy compared to other modular or end-to-end baselines, it also prompts us\nto rethink the role of explicit tracking for embodied perception. Additionally,\nby linking future and current locations in a many-to-one manner, our approach\nis able to reason about multiple futures, a capability that was previously\nconsidered difficult for end-to-end approaches. We conduct extensive\nexperiments on the popular nuScenes dataset and demonstrate the empirical\neffectiveness of our approach. In addition, we investigate the appropriateness\nof reusing standard forecasting metrics for an end-to-end setup, and find a\nnumber of limitations which allow us to build simple baselines to game these\nmetrics. We address this issue with a novel set of joint forecasting and\ndetection metrics that extend the commonly used AP metrics from the detection\ncommunity to measuring forecasting accuracy. Our code is available on\n\\href{https://github.com/neeharperi/FutureDet}{GitHub}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1\">Neehar Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1\">Jonathon Luiten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljo&#x161;a O&#x161;ep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEGG-Net: Background Agnostic Pixel-Wise Efficient Grasp Generation Under Closed-Loop Conditions. (arXiv:2203.16301v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16301","description":"<p>Performing closed-loop grasping at close proximity to an object requires a\nlarge field of view. However, such images will inevitably bring large amounts\nof unnecessary background information, especially when the camera is far away\nfrom the target object at the initial stage, resulting in performance\ndegradation of the grasping network. To address this problem, we design a novel\nPEGG-Net, a real-time, pixel-wise, robotic grasp generation network. The\nproposed lightweight network is inherently able to learn to remove background\nnoise that can reduce grasping accuracy. Our proposed PEGG-Net achieves\nimproved state-of-the-art performance on both Cornell dataset (98.9%) and\nJacquard dataset (93.8%). In the real-world tests, PEGG-Net can support\nclosed-loop grasping at up to 50Hz using an image size of 480x480 in dynamic\nenvironments. The trained model also generalizes to previously unseen objects\nwith complex geometrical shapes, household objects and workshop tools and\nachieved an overall grasp success rate of 91.2% in our real-world grasping\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16317","description":"<p>In this paper, we delve into two key techniques in Semi-Supervised Object\nDetection (SSOD), namely pseudo labeling and consistency training. We observe\nthat these two techniques currently neglect some important properties of object\ndetection, hindering efficient learning on unlabeled data. Specifically, for\npseudo labeling, existing works only focus on the classification score yet fail\nto guarantee the localization precision of pseudo boxes; For consistency\ntraining, the widely adopted random-resize training only considers the\nlabel-level consistency but misses the feature-level one, which also plays an\nimportant role in ensuring the scale invariance. To address the problems\nincurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that\nincludes Prediction-guided Label Assignment (PLA) and Positive-proposal\nConsistency Voting (PCV). PLA relies on model predictions to assign labels and\nmakes it robust to even coarse pseudo boxes; while PCV leverages the regression\nconsistency of positive proposals to reflect the localization quality of pseudo\nboxes. Furthermore, in consistency training, we propose Multi-view\nScale-invariant Learning (MSL) that includes mechanisms of both label- and\nfeature-level consistency, where feature consistency is achieved by aligning\nshifted feature pyramids between two images with identical content but varied\nscales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency\ntraining (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points\nunder 1%, 5%, and 10% labelling ratios, respectively. It also significantly\nimproves the learning efficiency for SSOD, e.g., PseCo halves the training time\nof the SOTA approach but achieves even better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Robot Active Mapping via Neural Bipartite Graph Matching. (arXiv:2203.16319v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16319","description":"<p>We study the problem of multi-robot active mapping, which aims for complete\nscene map construction in minimum time steps. The key to this problem lies in\nthe goal position estimation to enable more efficient robot movements. Previous\napproaches either choose the frontier as the goal position via a myopic\nsolution that hinders the time efficiency, or maximize the long-term value via\nreinforcement learning to directly regress the goal position, but does not\nguarantee the complete map construction. In this paper, we propose a novel\nalgorithm, namely NeuralCoMapping, which takes advantage of both approaches. We\nreduce the problem to bipartite graph matching, which establishes the node\ncorrespondences between two graphs, denoting robots and frontiers. We introduce\na multiplex graph neural network (mGNN) that learns the neural distance to fill\nthe affinity matrix for more effective graph matching. We optimize the mGNN\nwith a differentiable linear assignment layer by maximizing the long-term\nvalues that favor time efficiency and map completeness via reinforcement\nlearning. We compare our algorithm with several state-of-the-art multi-robot\nactive mapping approaches and adapted reinforcement-learning baselines.\nExperimental results demonstrate the superior performance and exceptional\ngeneralization ability of our algorithm on various indoor scenes and unseen\nnumber of robots, when only trained with 9 indoor scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16325","description":"<p>Existing deep learning based methods effectively prompt the performance of\naerial scene classification. However, due to the large amount of parameters and\ncomputational cost, it is rather difficult to apply these methods to multiple\nreal-time remote sensing applications such as on-board data preception on\ndrones and satellites. In this paper, we address this task by developing a\nlight-weight ConvNet named multi-stage duplex fusion network (MSDF-Net). The\nkey idea is to use parameters as little as possible while obtaining as strong\nas possible scene representation capability. To this end, a residual-dense\nduplex fusion strategy is developed to enhance the feature propagation while\nre-using parameters as much as possible, and is realized by our duplex fusion\nblock (DFblock). Specifically, our MSDF-Net consists of multi-stage structures\nwith DFblock. Moreover, duplex semantic aggregation (DSA) module is developed\nto mine the remote sensing scene information from extracted convolutional\nfeatures, which also contains two parallel branches for semantic description.\nExtensive experiments are conducted on three widely-used aerial scene\nclassification benchmarks, and reflect that our MSDF-Net can achieve a\ncompetitive performance against the recent state-of-art while reducing up to\n80% parameter numbers. Particularly, an accuracy of 92.96% is achieved on AID\nwith only 0.49M parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingjun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Beichen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee. (arXiv:2203.16328v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16328","description":"<p>The objective of this study is to address the problem of\nbackground/foreground separation with missing pixels by combining the video\nacquisition, video recovery, background/foreground separation into a single\nframework. To achieve this, a smooth robust tensor completion (SRTC) model is\nproposed to recover the data and decompose it into the static background and\nsmooth foreground, respectively. Specifically, the static background is modeled\nby the low-rank tucker decomposition and the smooth foreground (moving objects)\nis modeled by the spatiotemporal continuity, which is enforced by the total\nvariation regularization. An efficient algorithm based on tensor proximal\nalternating minimization (tenPAM) is implemented to solve the proposed model\nwith global convergence guarantee under very mild conditions. Extensive\nexperiments on real data demonstrate that the proposed method significantly\noutperforms the state-of-the-art approaches for background/foreground\nseparation with missing pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weijun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenyu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-efficient Fine-tuning for Vision Transformers. (arXiv:2203.16329v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16329","description":"<p>In computer vision, it has achieved great success in adapting large-scale\npretrained vision models (e.g., Vision Transformer) to downstream tasks via\nfine-tuning. Common approaches for fine-tuning either update all model\nparameters or leverage linear probes. In this paper, we aim to study\nparameter-efficient fine-tuning strategies for Vision Transformers on vision\ntasks. We formulate efficient fine-tuning as a subspace training problem and\nperform a comprehensive benchmarking over different efficient fine-tuning\nmethods. We conduct an empirical study on each efficient fine-tuning method\nfocusing on its performance alongside parameter cost. Furthermore, we also\npropose a parameter-efficient fine-tuning framework, which first selects\nsubmodules by measuring local intrinsic dimensions and then projects them into\nsubspace for further decomposition via a novel Kronecker Adaptation method. We\nanalyze and compare our method with a diverse set of baseline fine-tuning\nmethods (including state-of-the-art methods for pretrained language models).\nOur method performs the best in terms of the tradeoff between accuracy and\nparameter efficiency across three commonly used image classification datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On handwriting pressure normalization for interoperability of different acquisition stylus. (arXiv:2203.16337v1 [eess.SP])","link":"http://arxiv.org/abs/2203.16337","description":"<p>In this paper, we present a pressure characterization and normalization\nprocedure for online handwritten acquisition. Normalization process has been\ntested in biometric recognition experiments (identification and verification)\nusing online signature database MCYT, which consists of the signatures from 330\nusers. The goal is to analyze the real mismatch scenarios where users are\nenrolled with one stylus and then, later on, they produce some testing samples\nusing a different stylus model with different pressure response. Experimental\nresults show: 1) a saturation behavior in pressure signal 2) different dynamic\nranges in the different stylus studied 3) improved biometric recognition\naccuracy by means of pressure signal normalization as well as a performance\ndegradation in mismatched conditions 4) interoperability between different\nstylus can be obtained by means of pressure normalization. Normalization\nproduces an improvement in signature identification rates higher than 7%\n(absolute value) when compared with mismatched scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brotons_Rufes_O/0/1/0/all/0/1\">Olga Brotons-Rufes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paul_Recarens_C/0/1/0/all/0/1\">Carles Paul-Recarens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plamondon_R/0/1/0/all/0/1\">R&#xe9;jean Plamondon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stack operation of tensor networks. (arXiv:2203.16338v1 [cs.LG])","link":"http://arxiv.org/abs/2203.16338","description":"<p>The tensor network, as a facterization of tensors, aims at performing the\noperations that are common for normal tensors, such as addition, contraction\nand stacking. However, due to its non-unique network structure, only the tensor\nnetwork contraction is so far well defined. In this paper, we propose a\nmathematically rigorous definition for the tensor network stack approach, that\ncompress a large amount of tensor networks into a single one without changing\ntheir structures and configurations. We illustrate the main ideas with the\nmatrix product states based machine learning as an example. Our results are\ncompared with the for loop and the efficient coding method on both CPU and GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_L/0/1/0/all/0/1\">L. K. Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Erping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Learned Lossless JPEG Recompression with Multi-Level Cross-Channel Entropy Model in the DCT Domain. (arXiv:2203.16357v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16357","description":"<p>JPEG is a popular image compression method widely used by individuals, data\ncenter, cloud storage and network filesystems. However, most recent progress on\nimage compression mainly focuses on uncompressed images while ignoring\ntrillions of already-existing JPEG images. To compress these JPEG images\nadequately and restore them back to JPEG format losslessly when needed, we\npropose a deep learning based JPEG recompression method that operates on DCT\ndomain and propose a Multi-Level Cross-Channel Entropy Model to compress the\nmost informative Y component. Experiments show that our method achieves\nstate-of-the-art performance compared with traditional JPEG recompression\nmethods including Lepton, JPEG XL and CMIX. To the best of our knowledge, this\nis the first learned compression method that losslessly transcodes JPEG images\nto more storage-saving bitstreams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lina Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xinjie Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dailan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CardioID: Mitigating the Effects of Irregular Cardiac Signals for Biometric Identification. (arXiv:2203.16381v1 [cs.CR])","link":"http://arxiv.org/abs/2203.16381","description":"<p>Cardiac patterns are being used to obtain hard-to-forge biometric signatures\nand have led to high accuracy in state-of-the-art (SoA) identification\napplications. However, this performance is obtained under controlled scenarios\nwhere cardiac signals maintain a relatively uniform pattern, facilitating the\nidentification process. In this work, we analyze cardiac signals collected in\nmore realistic (uncontrolled) scenarios and show that their high signal\nvariability (i.e., irregularity) makes it harder to obtain stable and distinct\nuser features. Furthermore, SoA usually fails to identify specific groups of\nusers, rendering existing identification methods futile in uncontrolled\nscenarios. To solve these problems, we propose a framework with three novel\nproperties. First, we design an adaptive method that achieves stable and\ndistinct features by tailoring the filtering spectrum to each user. Second, we\nshow that users can have multiple cardiac morphologies, offering us a much\nbigger pool of cardiac signals and users compared to SoA. Third, we overcome\nother distortion effects present in authentication applications with a\nmulti-cluster approach and the Mahalanobis distance. Our evaluation shows that\nthe average balanced accuracy (BAC) of SoA drops from above 90% in controlled\nscenarios to 75% in uncontrolled ones, while our method maintains an average\nBAC above 90% in uncontrolled scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_M/0/1/0/all/0/1\">Marco Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On learning adaptive acquisition policies for undersampled multi-coil MRI reconstruction. (arXiv:2203.16392v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16392","description":"<p>Most current approaches to undersampled multi-coil MRI reconstruction focus\non learning the reconstruction model for a fixed, equidistant acquisition\ntrajectory. In this paper, we study the problem of joint learning of the\nreconstruction model together with acquisition policies. To this end, we extend\nthe End-to-End Variational Network with learnable acquisition policies that can\nadapt to different data points. We validate our model on a coil-compressed\nversion of the large scale undersampled multi-coil fastMRI dataset using two\nundersampling factors: $4\\times$ and $8\\times$. Our experiments show on-par\nperformance with the learnable non-adaptive and handcrafted equidistant\nstrategies at $4\\times$, and an observed improvement of more than $2\\%$ in SSIM\nat $8\\times$ acceleration, suggesting that potentially-adaptive $k$-space\nacquisition trajectories can improve reconstructed image quality for larger\nacceleration factors. However, and perhaps surprisingly, our best performing\npolicies learn to be explicitly non-adaptive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakker_T/0/1/0/all/0/1\">Tim Bakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muckley_M/0/1/0/all/0/1\">Matthew Muckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Motion Style Transfer for Interactive Character Control. (arXiv:2203.16393v1 [cs.GR])","link":"http://arxiv.org/abs/2203.16393","description":"<p>Motion style transfer is highly desired for motion generation systems for\ngaming. Compared to its offline counterpart, the research on online motion\nstyle transfer under interactive control is limited. In this work, we propose\nan end-to-end neural network that can generate motions with different styles\nand transfer motion styles in real-time under user control. Our approach\neliminates the use of handcrafted phase features, and could be easily trained\nand directly deployed in game systems. In the experiment part, we evaluate our\napproach from three aspects that are essential for industrial game design:\naccuracy, flexibility, and variety, and our model performs a satisfying result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yingtian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingguang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of polar lows in Sentinel-1 SAR images with deep learning. (arXiv:2203.16401v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16401","description":"<p>In this paper, we explore the possibility of detecting polar lows in C-band\nSAR images by means of deep learning. Specifically, we introduce a novel\ndataset consisting of Sentinel-1 images labeled as positive; representing a\nmaritime mesocyclone, or negative; representing a normal sea state. The dataset\nis constructed using the ERA5 dataset as baseline and it consists of 2004\nannotated images. To our knowledge, this is the first dataset of its kind to be\npublicly released. The dataset is used to train a deep learning model to\nclassify the labeled images. Evaluated on an independent test set, the model\nyields an F-1 score of 0.95, indicating that polar lows can be consistently\ndetected from SAR images. Interpretability techniques applied to the deep\nlearning model reveal that atmospheric fronts and cyclonic eyes are key\nfeatures in the classification. Moreover, experimental results show that the\nmodel is accurate even if: (i) such features are significantly cropped due to\nthe limited swath width of the SAR, (ii) the features are partly covered by sea\nice and (iii) land is covering significant parts of the images. By evaluating\nthe model performance on multiple input image resolutions (pixel sizes of 500m,\n1km and 2km), it is found that higher resolution yield the best performance.\nThis emphasises the potential of using high resolution sensors like SAR for\ndetecting polar lows, as compared to conventionally used sensors such as\nscatterometers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grahn_J/0/1/0/all/0/1\">Jakob Grahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Filippo Maria Bianchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis. (arXiv:2203.16414v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16414","description":"<p>The extension of convolutional neural networks (CNNs) to non-Euclidean\ngeometries has led to multiple frameworks for studying manifolds. Many of those\nmethods have shown design limitations resulting in poor modelling of long-range\nassociations, as the generalisation of convolutions to irregular surfaces is\nnon-trivial. Motivated by the success of attention-modelling in computer\nvision, we translate convolution-free vision transformer approaches to surface\ndata, to introduce a domain-agnostic architecture to study any surface data\nprojected onto a spherical manifold. Here, surface patching is achieved by\nrepresenting spherical data as a sequence of triangular patches, extracted from\na subdivided icosphere. A transformer model encodes the sequence of patches via\nsuccessive multi-head self-attention layers while preserving the sequence\nresolution. We validate the performance of the proposed Surface Vision\nTransformer (SiT) on the task of phenotype regression from cortical surface\nmetrics derived from the Developing Human Connectome Project (dHCP).\nExperiments show that the SiT generally outperforms surface CNNs, while\nperforming comparably on registered and unregistered data. Analysis of\ntransformer attention maps offers strong potential to characterise subtle\ncognitive developmental patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coalson_T/0/1/0/all/0/1\">Timothy S. Coalson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glasser_M/0/1/0/all/0/1\">Matthew F. Glasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1\">A. David Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16415","description":"<p>Dice similarity coefficient (DSC) and Hausdorff distance (HD) are widely used\nfor evaluating medical image segmentation. They have also been criticised, when\nreported alone, for their unclear or even misleading clinical interpretation.\nDSCs may also differ substantially from HDs, due to boundary smoothness or\nmultiple regions of interest (ROIs) within a subject. More importantly, either\nmetric can also have a nonlinear, non-monotonic relationship with outcomes\nbased on Type 1 and 2 errors, designed for specific clinical decisions that use\nthe resulting segmentation. Whilst cases causing disagreement between these\nmetrics are not difficult to postulate. This work first proposes a new\nasymmetric detection metric, adapting those used in object detection, for\nplanning prostate cancer procedures. The lesion-level metrics is then compared\nwith the voxel-level DSC and HD, whereas a 3D UNet is used for segmenting\nlesions from multiparametric MR (mpMR) images. Based on experimental results we\nreport pairwise agreement and correlation 1) between DSC and HD, and 2) between\nvoxel-level DSC and recall-controlled precision at lesion-level, with Cohen's\n[0.49, 0.61] and Pearson's [0.66, 0.76] (p-values}&lt;0.001) at varying cut-offs.\nHowever, the differences in false-positives and false-negatives, between the\nactual errors and the perceived counterparts if DSC is used, can be as high as\n152 and 154, respectively, out of the 357 test set lesions. We therefore\ncarefully conclude that, despite of the significant correlations, voxel-level\nmetrics such as DSC can misrepresent lesion-level detection accuracy for\nevaluating localisation of multifocal prostate cancer and should be interpreted\nwith caution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1\">Wen Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Syer_T/0/1/0/all/0/1\">Tom Syer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1\">Zhe Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punwani_S/0/1/0/all/0/1\">Shonit Punwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emberton_M/0/1/0/all/0/1\">Mark Emberton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_B/0/1/0/all/0/1\">Bernard Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPD: Single-view 3D Openable Part Detection. (arXiv:2203.16421v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16421","description":"<p>We address the task of predicting what parts of an object can open and how\nthey move when they do so. The input is a single image of an object, and as\noutput we detect what parts of the object can open, and the motion parameters\ndescribing the articulation of each openable part. To tackle this task, we\ncreate two datasets of 3D objects: OPDSynth based on existing synthetic\nobjects, and OPDReal based on RGBD reconstructions of real objects. We then\ndesign OPDRCNN, a neural architecture that detects openable parts and predicts\ntheir motion parameters. Our experiments show that this is a challenging task\nespecially when considering generalization across object categories, and the\nlimited amount of information in a single image. Our architecture outperforms\nbaselines and prior work especially for RGB image inputs. Short video summary\nat https://www.youtube.com/watch?v=P85iCaD0rfc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanxiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongsen Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced MSE for Imbalanced Visual Regression. (arXiv:2203.16427v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16427","description":"<p>Data imbalance exists ubiquitously in real-world visual regressions, e.g.,\nage estimation and pose estimation, hurting the model's generalizability and\nfairness. Thus, imbalanced regression gains increasing research attention\nrecently. Compared to imbalanced classification, imbalanced regression focuses\non continuous labels, which can be boundless and high-dimensional and hence\nmore challenging. In this work, we identify that the widely used Mean Square\nError (MSE) loss function can be ineffective in imbalanced regression. We\nrevisit MSE from a statistical view and propose a novel loss function, Balanced\nMSE, to accommodate the imbalanced training label distribution. We further\ndesign multiple implementations of Balanced MSE to tackle different real-world\nscenarios, particularly including the one that requires no prior knowledge\nabout the training label distribution. Moreover, to the best of our knowledge,\nBalanced MSE is the first general solution to high-dimensional imbalanced\nregression. Extensive experiments on both synthetic and three real-world\nbenchmarks demonstrate the effectiveness of Balanced MSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cunjun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16434","description":"<p>We consider the problem of localizing a spatio-temporal tube in a video\ncorresponding to a given text query. This is a challenging task that requires\nthe joint and efficient modeling of temporal, spatial and multi-modal\ninteractions. To address this task, we propose TubeDETR, a transformer-based\narchitecture inspired by the recent success of such models for text-conditioned\nobject detection. Our model notably includes: (i) an efficient video and text\nencoder that models spatial multi-modal interactions over sparsely sampled\nframes and (ii) a space-time decoder that jointly performs spatio-temporal\nlocalization. We demonstrate the advantage of our proposed components through\nan extensive ablation study. We also evaluate our full approach on the\nspatio-temporal video grounding task and demonstrate improvements over the\nstate of the art on the challenging VidSTG and HC-STVG benchmarks. Code and\ntrained models are publicly available at\nhttps://antoyang.github.io/tubedetr.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v1 [cs.LG])","link":"http://arxiv.org/abs/2203.16475","description":"<p>Deep neural networks (DNNs) have been widely used for decision making,\nprompting a surge of interest in interpreting how these complex models work.\nRecent literature on DNN interpretation has revolved around already-trained\nmodels; however, much less research focuses on interpreting how the models\nevolve as they are trained. Interpreting model evolution is crucial to monitor\nnetwork training and can aid proactive decisions about necessary interventions.\nIn this work, we present ConceptEvo, a general interpretation framework for\nDNNs that reveals the inception and evolution of detected concepts during\ntraining. Through a large-scale human evaluation with 260 participants and\nquantitative experiments, we show that ConceptEvo discovers evolution across\ndifferent models that are meaningful to humans, helpful for early-training\nintervention decisions, and crucial to the prediction for a given class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1\">Austin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duggal_R/0/1/0/all/0/1\">Rahul Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nilaksh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds. (arXiv:2203.16482v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16482","description":"<p>Object reconstruction from 3D point clouds has achieved impressive progress\nin the computer vision and computer graphics research field. However,\nreconstruction from time-varying point clouds (a.k.a. 4D point clouds) is\ngenerally overlooked. In this paper, we propose a new network architecture,\nnamely RFNet-4D, that jointly reconstructs objects and their motion flows from\n4D point clouds. The key insight is that simultaneously performing both tasks\nvia learning spatial and temporal features from a sequence of point clouds can\nleverage individual tasks and lead to improved overall performance. The\nproposed network can be trained using both supervised and unsupervised\nlearning. To prove this ability, we design a temporal vector field learning\nmodule using an unsupervised learning approach for flow estimation, leveraged\nby supervised learning of spatial structures for object reconstruction.\nExtensive experiments and analyses on benchmark dataset validated the\neffectiveness and efficiency of our method. As shown in experimental results,\nour method achieves state-of-the-art performance on both flow estimation and\nobject reconstruction while performing much faster than existing methods in\nboth training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Anh Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang-Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foveation-based Deep Video Compression without Motion Search. (arXiv:2203.16490v1 [eess.IV])","link":"http://arxiv.org/abs/2203.16490","description":"<p>The requirements of much larger file sizes, different storage formats, and\nimmersive viewing conditions of VR pose significant challenges to the goals of\nacquiring, transmitting, compressing, and displaying high-quality VR content.\nAt the same time, the great potential of deep learning to advance progress on\nthe video compression problem has driven a significant research effort. Because\nof the high bandwidth requirements of VR, there has also been significant\ninterest in the use of space-variant, foveated compression protocols. We have\nintegrated these techniques to create an end-to-end deep learning video\ncompression framework. A feature of our new compression model is that it\ndispenses with the need for expensive search-based motion prediction\ncomputations. This is accomplished by exploiting statistical regularities\ninherent in video motion expressed by displaced frame differences. Foveation\nprotocols are desirable since only a small portion of a video viewed in VR may\nbe visible as a user gazes in any given direction. Moreover, even within a\ncurrent field of view (FOV), the resolution of retinal neurons rapidly\ndecreases with distance (eccentricity) from the projected point of gaze. In our\nlearning based approach, we implement foveation by introducing a Foveation\nGenerator Unit (FGU) that generates foveation masks which direct the allocation\nof bits, significantly increasing compression efficiency while making it\npossible to retain an impression of little to no additional visual loss given\nan appropriate viewing geometry. Our experiment results reveal that our new\ncompression model, which we call the Foveated MOtionless VIdeo Codec (Foveated\nMOVI-Codec), is able to efficiently compress videos without computing motion,\nwhile outperforming foveated version of both H.264 and H.265 on the widely used\nUVG dataset and on the HEVC Standard Class B Test Sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Meixu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Webb_R/0/1/0/all/0/1\">Richard Webb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16505","description":"<p>Previous partial permutation synchronization (PPS) algorithms, which are\ncommonly used for multi-object matching, often involve computation-intensive\nand memory-demanding matrix operations. These operations become intractable for\nlarge scale structure-from-motion datasets. For pure permutation\nsynchronization, the recent Cycle-Edge Message Passing (CEMP) framework\nsuggests a memory-efficient and fast solution. Here we overcome the restriction\nof CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for\nestimating the corruption levels of the observed partial permutations. It\nallows us to subsequently implement a nonconvex weighted projected power method\nwithout the need of spectral initialization. The resulting new PPS algorithm,\nMatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse\nmatrix operations, and thus enjoys lower time and space complexities in\ncomparison to previous PPS algorithms. We prove that under adversarial\ncorruption, though without additive noise and with certain assumptions,\nCEMP-Partial is able to exactly classify corrupted and clean partial\npermutations. We demonstrate the state-of-the-art accuracy, speed and memory\nefficiency of our method on both synthetic and real datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16506","description":"<p>Coronavirus 2019 has brought severe challenges to social stability and public\nhealth worldwide. One effective way of curbing the epidemic is to require\npeople to wear masks in public places and monitor mask-wearing states by\nutilizing suitable automatic detectors. However, existing deep learning based\nmodels struggle to simultaneously achieve the requirements of both high\nprecision and real-time performance. To solve this problem, we propose an\nimproved lightweight face mask detector based on YOLOv5, which can achieve an\nexcellent balance of precision and speed. Firstly, a novel backbone\nShuffleCANet that combines ShuffleNetV2 network with Coordinate Attention\nmechanism is proposed as the backbone. Then we use BiFPN as the feature fusion\nneck. Furthermore, we replace the loss function of localization with -CIoU to\nobtain higher-quality anchors. Some valuable strategies such as data\naugmentation, adaptive image scaling, and anchor cluster operation are also\nutilized. Experimental results show the performance and effectiveness of the\nproposed model. On the basis of the original YOLOv5 model, our work increases\nthe inference speed by 28.3% while still improving the precision by 0.58% on\nthe AIZOO face mask dataset. It achieves a mean average precision of 95.2%,\nwhich is 4.4% higher than the baseline and is also more accurate compared with\nother existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16507","description":"<p>Traditional object detectors employ the dense paradigm of scanning over\nlocations and scales in an image. The recent query-based object detectors break\nthis convention by decoding image features with a set of learnable queries.\nHowever, this paradigm still suffers from slow convergence, limited\nperformance, and design complexity of extra networks between backbone and\ndecoder. In this paper, we find that the key to these issues is the\nadaptability of decoders for casting queries to varying objects. Accordingly,\nwe propose a fast-converging query-based detector, named AdaMixer, by improving\nthe adaptability of query-based decoding processes in two aspects. First, each\nquery adaptively samples features over space and scales based on estimated\noffsets, which allows AdaMixer to efficiently attend to the coherent regions of\nobjects. Then, we dynamically decode these sampled features with an adaptive\nMLP-Mixer under the guidance of each query. Thanks to these two critical\ndesigns, AdaMixer enjoys architectural simplicity without requiring dense\nattentional encoders or explicit pyramid networks. On the challenging MS COCO\nbenchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs,\nreaches up to 45.0 AP on the validation set along with 27.9 APs in detecting\nsmall objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN\nand Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple,\naccurate, and fast converging architecture for query-based object detectors.\nThe code is made available at https://github.com/MCG-NJU/AdaMixer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ziteng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptDet: Expand Your Detector Vocabulary with Uncurated Images. (arXiv:2203.16513v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16513","description":"<p>The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector that\ncategorises each box proposal by a classifier generated from the text encoder\nof a pre-trained visual-language model; (ii) To pair the visual latent space\n(from RPN box proposal) with that of the pre-trained text encoder, we propose\nthe idea of regional prompt learning to optimise a couple of learnable prompt\nvectors, converting the textual embedding space to fit those visually\nobject-centric images; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online\nresource, iteratively updating the prompts, and later self-training the\nproposed detector with pseudo labels generated on a large corpus of noisy,\nuncurated web images. The self-trained detector, termed as PromptDet,\nsignificantly improves the detection performance on categories for which manual\nannotations are unavailable or hard to obtain, e.g. rare categories. Finally,\n(iv) to validate the necessity of our proposed components, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset, showing superior\nperformance over existing approaches with fewer additional training images and\nzero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Light-Weight Near-Field Photometric Stereo. (arXiv:2203.16515v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16515","description":"<p>We introduce the first end-to-end learning-based solution to near-field\nPhotometric Stereo (PS), where the light sources are close to the object of\ninterest. This setup is especially useful for reconstructing large immobile\nobjects. Our method is fast, producing a mesh from 52 512$\\times$384 resolution\nimages in about 1 second on a commodity GPU, thus potentially unlocking several\nAR/VR applications. Existing approaches rely on optimization coupled with a\nfar-field PS network operating on pixels or small patches. Using optimization\nmakes these approaches slow and memory intensive (requiring 17GB GPU and 27GB\nof CPU memory) while using only pixels or patches makes them highly susceptible\nto noise and calibration errors. To address these issues, we develop a\nrecursive multi-resolution scheme to estimate surface normal and depth maps of\nthe whole image at each step. The predicted depth map at each scale is then\nused to estimate `per-pixel lighting' for the next scale. This design makes our\napproach almost 45$\\times$ faster and 2$^{\\circ}$ more accurate (11.3$^{\\circ}$\nvs. 13.3$^{\\circ}$ Mean Angular Error) than the state-of-the-art near-field PS\nreconstruction technique, which uses iterative optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lichy_D/0/1/0/all/0/1\">Daniel Lichy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Soumyadip Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David W. Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Classes at a Later Time? No Problem. (arXiv:2203.16517v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16517","description":"<p>Recent progress towards learning from limited supervision has encouraged\nefforts towards designing models that can recognize novel classes at test time\n(generalized zero-shot learning or GZSL). GZSL approaches assume knowledge of\nall classes, with or without labeled data, beforehand. However, practical\nscenarios demand models that are adaptable and can handle dynamic addition of\nnew seen and unseen classes on the fly (that is continual generalized zero-shot\nlearning or CGZSL). One solution is to sequentially retrain and reuse\nconventional GZSL methods, however, such an approach suffers from catastrophic\nforgetting leading to suboptimal generalization performance. A few recent\nefforts towards tackling CGZSL have been limited by difference in settings,\npracticality, data splits and protocols followed-inhibiting fair comparison and\na clear direction forward. Motivated from these observations, in this work, we\nfirstly consolidate the different CGZSL setting variants and propose a new\nOnline-CGZSL setting which is more practical and flexible. Secondly, we\nintroduce a unified feature-generative framework for CGZSL that leverages\nbi-directional incremental alignment to dynamically adapt to addition of new\nclasses, with or without labeled data, that arrive over time in any of these\nCGZSL settings. Our comprehensive experiments and analysis on five benchmark\ndatasets and comparison with baselines show that our approach consistently\noutperforms existing methods, especially on the more practical Online setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuchibhotla_H/0/1/0/all/0/1\">Hari Chandana Kuchibhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malagi_S/0/1/0/all/0/1\">Sumitra S Malagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1\">Shivam Chandhok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Transformers for Grounded Situation Recognition. (arXiv:2203.16518v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16518","description":"<p>Grounded situation recognition is the task of predicting the main activity,\nentities playing certain roles within the activity, and bounding-box groundings\nof the entities in the given image. To effectively deal with this challenging\ntask, we introduce a novel approach where the two processes for activity\nclassification and entity estimation are interactive and complementary. To\nimplement this idea, we propose Collaborative Glance-Gaze TransFormer\n(CoFormer) that consists of two modules: Glance transformer for activity\nclassification and Gaze transformer for entity estimation. Glance transformer\npredicts the main activity with the help of Gaze transformer that analyzes\nentities and their relations, while Gaze transformer estimates the grounded\nentities by focusing only on the entities relevant to the activity predicted by\nGlance transformer. Our CoFormer achieves the state of the art in all\nevaluation metrics on the SWiG dataset. Training code and model weights are\navailable at https://github.com/jhcho99/CoFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhyeong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1\">Youngseok Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs. (arXiv:2203.16521v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16521","description":"<p>Recent advances show that Generative Adversarial Networks (GANs) can\nsynthesize images with smooth variations along semantically meaningful latent\ndirections, such as pose, expression, layout, etc. While this indicates that\nGANs implicitly learn pixel-level correspondences across images, few studies\nexplored how to extract them explicitly. In this work, we introduce Coordinate\nGAN (CoordGAN), a structure-texture disentangled GAN that learns a dense\ncorrespondence map for each generated image. We represent the correspondence\nmaps of different images as warped coordinate frames transformed from a\ncanonical coordinate frame, i.e., the correspondence map, which describes the\nstructure (e.g., the shape of a face), is controlled via a transformation.\nHence, finding correspondences boils down to locating the same coordinate in\ndifferent correspondence maps. In CoordGAN, we sample a transformation to\nrepresent the structure of a synthesized instance, while an independent texture\nbranch is responsible for rendering appearance details orthogonal to the\nstructure. Our approach can also extract dense correspondence maps for real\nimages by adding an encoder on top of the generator. We quantitatively\ndemonstrate the quality of the learned dense correspondences through\nsegmentation mask transfer on multiple datasets. We also show that the proposed\ngenerator achieves better structure and texture disentanglement compared to\nexisting approaches. Project page: https://jitengmu.github.io/CoordGAN/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiteng Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Plain Vision Transformer Backbones for Object Detection. (arXiv:2203.16527v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16527","description":"<p>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone\nnetwork for object detection. This design enables the original ViT architecture\nto be fine-tuned for object detection without needing to redesign a\nhierarchical backbone for pre-training. With minimal adaptations for\nfine-tuning, our plain-backbone detector can achieve competitive results.\nSurprisingly, we observe: (i) it is sufficient to build a simple feature\npyramid from a single-scale feature map (without the common FPN design) and\n(ii) it is sufficient to use window attention (without shifting) aided with\nvery few cross-window propagation blocks. With plain ViT backbones pre-trained\nas Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the\nprevious leading methods that were all based on hierarchical backbones,\nreaching up to 61.3 box AP on the COCO dataset using only ImageNet-1K\npre-training. We hope our study will draw attention to research on\nplain-backbone detectors. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hanzi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors. (arXiv:2203.16528v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16528","description":"<p>In this research, we propose a tiny image segmentation model, L^3U-net, that\nworks on low-resource edge devices in real-time. We introduce a data folding\ntechnique that reduces inference latency by leveraging the parallel\nconvolutional layer processing capability of the CNN accelerators. We also\ndeploy the proposed model to such a device, MAX78000, and the results show that\nL^3U-net achieves more than 90% accuracy over two different segmentation\ndatasets with 10 fps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okman_O/0/1/0/all/0/1\">Osman Erman Okman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulkar_M/0/1/0/all/0/1\">Mehmet Gorkem Ulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyanik_G/0/1/0/all/0/1\">Gulnur Selda Uyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism. (arXiv:2203.16529v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16529","description":"<p>While neural representations for static 3D shapes are widely studied,\nrepresentations for deformable surfaces are limited to be template-dependent or\nlack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a\nunified representation of both shape and nonrigid motion. Our key insight is\nthe factorization of the deformation between frames by continuous bijective\ncanonical maps (homeomorphisms) and their inverses that go through a learned\ncanonical shape. Our novel deformation representation and its implementation\nare simple, efficient, and guarantee cycle consistency, topology preservation,\nand, if needed, volume conservation. Our modelling of the learned canonical\nshapes provides a flexible and stable space for shape prior learning. We\ndemonstrate state-of-the-art performance in modelling a wide range of\ndeformable geometries: human bodies, animal bodies, and articulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiahui Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Instance-Specific Adaptation for Cross-Domain Segmentation. (arXiv:2203.16530v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16530","description":"<p>We propose a test-time adaptation method for cross-domain image segmentation.\nOur method is simple: Given a new unseen instance at test time, we adapt a\npre-trained model by conducting instance-specific BatchNorm (statistics)\ncalibration. Our approach has two core components. First, we replace the\nmanually designed BatchNorm calibration rule with a learnable module. Second,\nwe leverage strong data augmentation to simulate random domain shifts for\nlearning the calibration rule. In contrast to existing domain adaptation\nmethods, our method does not require accessing the target domain data at\ntraining time or conducting computationally expensive test-time model\ntraining/optimization. Equipping our method with models trained by standard\nrecipes achieves significant improvement, comparing favorably with several\nstate-of-the-art domain generalization and one-shot unsupervised domain\nadaptation approaches. Combining our method with the domain generalization\nmethods further improves performance, reaching a new state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuliang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding 3D Object Articulation in Internet Videos. (arXiv:2203.16531v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16531","description":"<p>We propose to investigate detecting and characterizing the 3D planar\narticulation of objects from ordinary videos. While seemingly easy for humans,\nthis problem poses many challenges for computers. We propose to approach this\nproblem by combining a top-down detection system that finds planes that can be\narticulated along with an optimization approach that solves for a 3D plane that\ncan explain a sequence of observed articulations. We show that this system can\nbe trained on a combination of videos and 3D scan datasets. When tested on a\ndataset of challenging Internet videos and the Charades dataset, our approach\nobtains strong performance. Project site:\nhttps://jasonqsy.github.io/Articulation3D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengyi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linyi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockwell_C/0/1/0/all/0/1\">Chris Rockwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v1 [cs.CV])","link":"http://arxiv.org/abs/2203.16533","description":"<p>This paper aims to address the problem of pre-training for person\nre-identification (Re-ID) with noisy labels. To setup the pre-training task, we\napply a simple online multi-object tracking system on raw videos of an existing\nunlabeled Re-ID dataset \"LUPerson\" nd build the Noisy Labeled variant called\n\"LUPerson-NL\". Since theses ID labels automatically derived from tracklets\ninevitably contain noises, we develop a large-scale Pre-training framework\nutilizing Noisy Labels (PNL), which consists of three learning modules:\nsupervised Re-ID learning, prototype-based contrastive learning, and\nlabel-guided contrastive learning. In principle, joint learning of these three\nmodules not only clusters similar examples to one prototype, but also rectifies\nnoisy labels based on the prototype assignment. We demonstrate that learning\ndirectly from raw videos is a promising alternative for pre-training, which\nutilizes spatial and temporal correlations as weak supervision. This simple\npre-training task provides a scalable way to learn SOTA Re-ID representations\nfrom scratch on \"LUPerson-NL\" without bells and whistles. For example, by\napplying on the same supervised Re-ID method MGN, our pre-trained model\nimproves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,\n2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or\nfew-shot setting, the performance gain is even more significant, suggesting a\nbetter transferability of the learned representation. Code is available at\nhttps://github.com/DengpanFu/LUPerson-NL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dengpan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Least Squares Normalized Cross Correlation. (arXiv:1810.04320v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1810.04320","description":"<p>Direct methods are widely used for alignment of models to images, due to\ntheir accuracy, since they minimize errors in the domain of measurement noise.\nThey have leveraged least squares minimizations, for simple, efficient,\nvariational optimization, since the seminal 1981 work of Lucas &amp; Kanade, and\nnormalized cross correlation (NCC), for robustness to intensity variations,\nsince at least 1972. Despite the complementary benefits of these two well known\nmethods, they have not been effectively combined to address local variations in\nintensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and\nimage transformation approaches have thus been proposed instead, each with\ntheir own limitations. This work shows that a least squares optimization of NCC\nwithout approximation is not only possible, but straightforward and efficient.\nA robust, locally normalized formulation is introduced to mitigate local\nintensity variations and partial occlusions. Finally, sparse features with\noriented patches are proposed for further efficiency. The resulting framework\nis simple to implement, computationally efficient and robust to local intensity\nvariations. It is evaluated on the image alignment problem, showing\nimprovements in both convergence rate and computation time over existing\nlighting invariant methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Sensor Periocular Biometrics in a Global Pandemic: Comparative Benchmark and Novel Multialgorithmic Approach. (arXiv:1902.08123v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1902.08123","description":"<p>The massive availability of cameras results in a wide variability of imaging\nconditions, producing large intra-class variations and a significant\nperformance drop if heterogeneous images are compared for person recognition.\nHowever, as biometrics is deployed, it is common to replace damaged or obsolete\nhardware, or to exchange information between heterogeneous applications.\nVariations in spectral bands can also occur. For example, surveillance face\nimages (typically acquired in the visible spectrum, VIS) may need to be\ncompared against a legacy iris database (typically acquired in near-infrared,\nNIR). Here, we propose a multialgorithmic approach to cope with periocular\nimages from different sensors. With face masks in the front line against\nCOVID-19, periocular recognition is regaining popularity since it is the only\nface region that remains visible. We integrate different comparators with a\nfusion scheme based on linear logistic regression, in which scores are\nrepresented by log-likelihood ratios. This allows easy interpretation of scores\nand the use of Bayes thresholds for optimal decision-making since scores from\ndifferent comparators are in the same probabilistic range. We evaluate our\napproach in the context of the Cross-Eyed Competition, whose aim was to compare\nrecognition approaches when NIR and VIS periocular images are matched. Our\napproach achieves EER=0.2% and FRR of just 0.47% at FAR=0.01%, representing the\nbest overall approach of the competition. Experiments are also reported with a\ndatabase of VIS images from different smartphones. We also discuss the impact\nof template size and computation times, with the most computationally heavy\ncomparator playing an important role in the results. Lastly, the proposed\nmethod is shown to outperform other popular fusion approaches, such as the\naverage of scores, SVMs or Random Forest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran B. Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavendra_R/0/1/0/all/0/1\">R. Raghavendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Cristoph Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance. (arXiv:2007.00548v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.00548","description":"<p>Intra-operative anticipation of instrument usage is a necessary component for\ncontext-aware assistance in surgery, e.g. for instrument preparation or\nsemi-automation of robotic tasks. However, the sparsity of instrument\noccurrences in long videos poses a challenge. Current approaches are limited as\nthey assume knowledge on the timing of future actions or require dense temporal\nsegmentations during training and inference. We propose a novel learning task\nfor anticipation of instrument usage in laparoscopic videos that overcomes\nthese limitations. During training, only sparse instrument annotations are\nrequired and inference is done solely on image data. We train a probabilistic\nmodel to address the uncertainty associated with future events. Our approach\noutperforms several baselines and is competitive to a variant using richer\nannotations. We demonstrate the model's ability to quantify task-relevant\nuncertainties. To the best of our knowledge, we are the first to propose a\nmethod for anticipating instruments in surgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1\">Dominik Rivoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1\">Sebastian Bodenstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_I/0/1/0/all/0/1\">Isabel Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bechtolsheim_F/0/1/0/all/0/1\">Felix von Bechtolsheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Distler_M/0/1/0/all/0/1\">Marius Distler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1\">J&#xfc;rgen Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Knowledge Distillation via Full Kernel Matrix Transfer. (arXiv:2009.14416v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.14416","description":"<p>Knowledge distillation is an effective way for model compression in deep\nlearning. Given a large model (i.e., teacher model), it aims to improve the\nperformance of a compact model (i.e., student model) by transferring the\ninformation from the teacher. Various information for distillation has been\nstudied. Recently, a number of works propose to transfer the pairwise\nsimilarity between examples to distill relative information. However, most of\nefforts are devoted to developing different similarity measurements, while only\na small matrix consisting of examples within a mini-batch is transferred at\neach iteration that can be inefficient for optimizing the pairwise similarity\nover the whole data set. In this work, we aim to transfer the full similarity\nmatrix effectively. The main challenge is from the size of the full matrix that\nis quadratic to the number of examples. To address the challenge, we decompose\nthe original full matrix with Nystr{\\\"{o}}m method. By selecting appropriate\nlandmark points, our theoretical analysis indicates that the loss for transfer\ncan be further simplified. Concretely, we find that the difference between the\noriginal full kernel matrices between teacher and student can be well bounded\nby that of the corresponding partial matrices, which only consists of\nsimilarities between original examples and landmark points. Compared with the\nfull matrix, the size of the partial matrix is linear in the number of\nexamples, which improves the efficiency of optimization significantly. The\nempirical study on benchmark data sets demonstrates the effectiveness of the\nproposed algorithm. Code is available at \\url{https://github.com/idstcv/KDA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Juhua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fooling the primate brain with minimal, targeted image manipulation. (arXiv:2011.05623v3 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2011.05623","description":"<p>Artificial neural networks (ANNs) are considered the current best models of\nbiological vision. ANNs are the best predictors of neural activity in the\nventral stream; moreover, recent work has demonstrated that ANN models fitted\nto neuronal activity can guide the synthesis of images that drive pre-specified\nresponse patterns in small neuronal populations. Despite the success in\npredicting and steering firing activity, these results have not been connected\nwith perceptual or behavioral changes. Here we propose an array of methods for\ncreating minimal, targeted image perturbations that lead to changes in both\nneuronal activity and perception as reflected in behavior. We generated\n'deceptive images' of human faces, monkey faces, and noise patterns so that\nthey are perceived as a different, pre-specified target category, and measured\nboth monkey neuronal responses and human behavior to these images. We found\nseveral effective methods for changing primate visual categorization that\nrequired much smaller image change compared to untargeted noise. Our work\nshares the same goal with adversarial attack, namely the manipulation of images\nwith minimal, targeted noise that leads ANN models to misclassify the images.\nOur results represent a valuable step in quantifying and characterizing the\ndifferences in perturbation robustness of biological and artificial vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiao_W/0/1/0/all/0/1\">Will Xiao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dellaferrera_G/0/1/0/all/0/1\">Giorgia Dellaferrera</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tay_F/0/1/0/all/0/1\">Francis E.H. Tay</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Livingstone_M/0/1/0/all/0/1\">Margaret S. Livingstone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCLR: Temporal Contrastive Learning for Video Representation. (arXiv:2101.07974v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07974","description":"<p>Contrastive learning has nearly closed the gap between supervised and\nself-supervised learning of image representations, and has also been explored\nfor videos. However, prior work on contrastive learning for video data has not\nexplored the effect of explicitly encouraging the features to be distinct\nacross the temporal dimension. We develop a new temporal contrastive learning\nframework consisting of two novel losses to improve upon existing contrastive\nself-supervised video representation learning methods. The local-local temporal\ncontrastive loss adds the task of discriminating between non-overlapping clips\nfrom the same video, whereas the global-local temporal contrastive aims to\ndiscriminate between timesteps of the feature map of an input clip in order to\nincrease the temporal diversity of the learned features. Our proposed temporal\ncontrastive learning framework achieves significant improvement over the\nstate-of-the-art results in various downstream video understanding tasks such\nas action recognition, limited-label action classification, and\nnearest-neighbor video retrieval on multiple video datasets and backbones. We\nalso demonstrate significant improvement in fine-grained action classification\nfor visually similar classes. With the commonly used 3D ResNet-18 architecture\nwith UCF101 pretraining, we achieve 82.4\\% (+5.1\\% increase over the previous\nbest) top-1 accuracy on UCF101 and 52.9\\% (+5.4\\% increase) on HMDB51 action\nclassification, and 56.2\\% (+11.7\\% increase) Top-1 Recall on UCF101 nearest\nneighbor video retrieval. Code released at github.com/DAVEISHAN/TCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1\">Ishan Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10407","description":"<p>The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning ABCs: Approximate Bijective Correspondence for isolating factors of variation with weak supervision. (arXiv:2103.03240v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03240","description":"<p>Representational learning forms the backbone of most deep learning\napplications, and the value of a learned representation is intimately tied to\nits information content regarding different factors of variation. Finding good\nrepresentations depends on the nature of supervision and the learning\nalgorithm. We propose a novel algorithm that utilizes a weak form of\nsupervision where the data is partitioned into sets according to certain\ninactive (common) factors of variation which are invariant across elements of\neach set. Our key insight is that by seeking correspondence between elements of\ndifferent sets, we learn strong representations that exclude the inactive\nfactors of variation and isolate the active factors that vary within all sets.\nAs a consequence of focusing on the active factors, our method can leverage a\nmix of set-supervised and wholly unsupervised data, which can even belong to a\ndifferent domain. We tackle the challenging problem of synthetic-to-real object\npose transfer, without pose annotations on anything, by isolating pose\ninformation which generalizes to the category level and across the\nsynthetic/real domain gap. The method can also boost performance in supervised\nsettings, by strengthening intermediate representations, as well as operate in\npractically attainable scenarios with set-supervised natural images, where\nquantity is limited and nuisance factors of variation are more plentiful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kieran A. Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The hidden label-marginal biases of segmentation losses. (arXiv:2104.08717v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08717","description":"<p>Most segmentation losses are arguably variants of the Cross-Entropy (CE) or\nDice losses. In the abundant segmentation literature, there is no clear\nconsensus as to which of these losses is a better choice, with varying\nperformances for each across different benchmarks and applications. In this\nwork, we develop a theoretical analysis that links these two types of losses,\nexposing their advantages and weaknesses. First, we provide a\nconstrained-optimization perspective showing that CE and Dice share a much\ndeeper connection than previously thought: They both decompose into\nlabel-marginal penalties and closely related ground-truth matching penalties.\nThen, we provide bound relationships and an information-theoretic analysis,\nwhich uncover hidden label-marginal biases: Dice has an intrinsic bias towards\nspecific extremely imbalanced solutions, whereas CE implicitly encourages the\nground-truth region proportions. Our theoretical results explain the wide\nexperimental evidence in the medical-imaging literature, whereby Dice losses\nbring improvements for imbalanced segmentation. It also explains why CE\ndominates natural-image problems with diverse class proportions, in which case\nDice might have difficulty adapting to different label-marginal distributions.\nBased on our theoretical analysis, we propose a principled and simple solution,\nwhich enables to control explicitly the label-marginal bias. Our loss\nintegrates CE with explicit ${\\cal L}_1$ regularization, which encourages label\nmarginals to match target class proportions, thereby mitigating class imbalance\nbut without losing generality. Comprehensive experiments and ablation studies\nover different losses and applications validate our theoretical analysis, as\nwell as the effectiveness of our explicit label-marginal regularizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbi_R/0/1/0/all/0/1\">Riadh Kobbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01883","description":"<p>We propose RepMLP, a multi-layer-perceptron-style neural network building\nblock for image recognition, which is composed of a series of fully-connected\n(FC) layers. Compared to convolutional layers, FC layers are more efficient,\nbetter at modeling the long-range dependencies and positional patterns, but\nworse at capturing the local structures, hence usually less favored for image\nrecognition. We propose a structural re-parameterization technique that adds\nlocal prior into an FC to make it powerful for image recognition. Specifically,\nwe construct convolutional layers inside a RepMLP during training and merge\nthem into the FC for inference. On CIFAR, a simple pure-MLP model shows\nperformance very close to CNN. By inserting RepMLP in traditional CNN, we\nimprove ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and\n2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight\nthat combining the global representational capacity and positional perception\nof FC with the local prior of convolution can improve the performance of neural\nnetwork with faster speed on both the tasks with translation invariance (e.g.,\nsemantic segmentation) and those with aligned images and positional patterns\n(e.g., face recognition). The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chunlong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08825","description":"<p>Human motion prediction aims to forecast future poses given a sequence of\npast 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper, we explore this problem when dealing with humans performing\ncollaborative tasks, we seek to predict the future motion of two interacted\npersons given two sequences of their past skeletons. We propose a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons, and learns to predict cross dependencies between the two pose\nsequences. Since no dataset to train such interactive situations is available,\nwe collected ExPI (Extreme Pose Interaction), a new lab-based person\ninteraction dataset of professional dancers performing Lindy-hop dancing\nactions, which contains 115 sequences with 30K frames annotated with 3D body\nposes and shapes. We thoroughly evaluate our cross interaction network on ExPI\nand show that both in short- and long-term predictions, it consistently\noutperforms state-of-the-art methods for single-person motion prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairCal: Fairness Calibration for Face Verification. (arXiv:2106.03761v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03761","description":"<p>Despite being widely used, face recognition models suffer from bias: the\nprobability of a false positive (incorrect face match) strongly depends on\nsensitive attributes such as the ethnicity of the face. As a result, these\nmodels can disproportionately and negatively impact minority groups,\nparticularly when used by law enforcement. The majority of bias reduction\nmethods have several drawbacks: they use an end-to-end retraining approach, may\nnot be feasible due to privacy issues, and often reduce accuracy. An\nalternative approach is post-processing methods that build fairer decision\nclassifiers using the features of pre-trained models, thus avoiding the cost of\nretraining. However, they still have drawbacks: they reduce accuracy (AGENDA,\nPASS, FTC), or require retuning for different false positive rates (FSN). In\nthis work, we introduce the Fairness Calibration (FairCal) method, a\npost-training approach that simultaneously: (i) increases model accuracy\n(improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\n(iv) does not require knowledge of the sensitive attribute, and (v) does not\nrequire retraining, training an additional model, or retuning. We apply it to\nthe task of Face Verification, and obtain state-of-the-art results with all the\nabove advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salvador_T/0/1/0/all/0/1\">Tiago Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cairns_S/0/1/0/all/0/1\">Stephanie Cairns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_N/0/1/0/all/0/1\">Noah Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06112","description":"<p>Though unsupervised domain adaptation (UDA) has achieved very impressive\nprogress recently, it remains a great challenge due to missing target\nannotations and the rich discrepancy between source and target distributions.\nWe propose Spectral UDA (SUDA), an effective and efficient UDA technique that\nworks in the spectral space and can generalize across different visual\nrecognition tasks. SUDA addresses the UDA challenges from two perspectives.\nFirst, it introduces a spectrum transformer (ST) that mitigates inter-domain\ndiscrepancies by enhancing domain-invariant spectra while suppressing\ndomain-variant spectra of source and target samples simultaneously. Second, it\nintroduces multi-view spectral learning that learns useful unsupervised\nrepresentations by maximizing mutual information among multiple ST-generated\nspectral views of each target sample. Extensive experiments show that SUDA\nachieves superior accuracy consistently across different visual tasks in object\ndetection, semantic segmentation and image classification. Additionally, SUDA\nalso works with the transformer-based network and achieves state-of-the-art\nperformance on object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zichen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06946","description":"<p>Randomized Smoothing (RS) is a promising method for obtaining robustness\ncertificates by evaluating a base model under noise. In this work, we: (i)\ntheoretically motivate why ensembles are a particularly suitable choice as base\nmodels for RS, and (ii) empirically confirm this choice, obtaining\nstate-of-the-art results in multiple settings. The key insight of our work is\nthat the reduced variance of ensembles over the perturbations introduced in RS\nleads to significantly more consistent classifications for a given input. This,\nin turn, leads to substantially increased certifiable radii for samples close\nto the decision boundary. Additionally, we introduce key optimizations which\nenable an up to 55-fold decrease in sample complexity of RS for predetermined\nradii, thus drastically reducing its computational overhead. Experimentally, we\nshow that ensembles of only 3 to 10 classifiers consistently improve on their\nstrongest constituting model with respect to their average certified radius\n(ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new\nstate-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and\nmodels required to reproduce our results at\nhttps://github.com/eth-sri/smoothing-ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1\">Mikl&#xf3;s Z. Horv&#xe1;th</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mark Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Few-shot Learning with Task-specific Adapters. (arXiv:2107.00358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00358","description":"<p>In this paper, we look at the problem of cross-domain few-shot classification\nthat aims to learn a classifier from previously unseen classes and domains with\nfew labeled samples. Recent approaches broadly solve this problem by\nparameterizing their few-shot classifiers with task-agnostic and task-specific\nweights where the former is typically learned on a large training set and the\nlatter is dynamically predicted through an auxiliary network conditioned on a\nsmall support set. In this work, we focus on the estimation of the latter, and\npropose to learn task-specific weights from scratch directly on a small support\nset, in contrast to dynamically estimating them. In particular, through\nsystematic analysis, we show that task-specific weights through parametric\nadapters in matrix form with residual connections to multiple intermediate\nlayers of a backbone network significantly improves the performance of the\nstate-of-the-art models in the Meta-Dataset benchmark with minor additional\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Level Semantics Conditioned 3D Point Cloud Segmentation. (arXiv:2107.00430v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00430","description":"<p>In this work, a language-level Semantics Conditioned framework for 3D Point\ncloud segmentation, called SeCondPoint, is proposed, where language-level\nsemantics are introduced to condition the modeling of point feature\ndistribution as well as the pseudo-feature generation, and a\nfeature-geometry-based mixup approach is further proposed to facilitate the\ndistribution learning. To our knowledge, this is the first attempt in\nliterature to introduce language-level semantics to the 3D point cloud\nsegmentation task. Since a large number of point features could be generated\nfrom the learned distribution thanks to the semantics conditioned modeling, any\nexisting segmentation network could be embedded into the proposed framework to\nboost its performance. In addition, the proposed framework has the inherent\nadvantage of dealing with novel classes, which seems an impossible feat for the\ncurrent segmentation networks. Extensive experimental results on two public\ndatasets demonstrate that three typical segmentation networks could achieve\nsignificant improvements over their original performances after enhancement by\nthe proposed framework in the conventional 3D segmentation task. Two benchmarks\nare also introduced for a newly introduced zero-shot 3D segmentation task, and\nthe results also validate the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.08760","description":"<p>Deep networks often make confident, yet, incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models (DGMs) are a candidate metric\nfor outlier detection with unlabeled data. Yet, previous studies have shown\nthat DGM likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest of DGMs. We propose novel\nanalytical and algorithmic approaches to ameliorate key biases with VAE\nlikelihoods. Our bias corrections are sample-specific, computationally\ninexpensive, and readily computed for various decoder visible distributions.\nNext, we show that a well-known image pre-processing technique -- contrast\nstretching -- extends the effectiveness of bias correction to further improve\noutlier detection. Our approach achieves state-of-the-art accuracies with nine\ngrayscale and natural image datasets, and demonstrates significant advantages\n-- both with speed and performance -- over four recent, competing approaches.\nIn summary, lightweight remedies suffice to achieve robust outlier detection\nwith VAEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+U_B/0/1/0/all/0/1\">Barath Mohan U</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Hard-case Mining across Pyramid Levels for Object Detection. (arXiv:2109.07217v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07217","description":"<p>In object detection, multi-level prediction (e.g., FPN) and reweighting\nskills (e.g., focal loss) have drastically improved one-stage detector\nperformance. However, the synergy between these two techniques is not fully\nexplored in a unified framework. We find that, during training, the one-stage\ndetector's optimization is not only restricted to the static hard-case mining\nloss (gradient drift) but also suffered from the diverse positive samples'\nproportions split by different pyramid levels (level discrepancy). Under this\nconcern, we propose Hierarchical Progressive Focus (HPF) consisting of two key\ndesigns: 1) progressive focus, a more flexible hard-case mining setting\ncalculated adaptive to the convergence progress, 2) hierarchical sampling,\nautomatically generating a set of progressive focus for level-specific target\noptimization. Based on focal loss with ATSS-R50, our approach achieves 40.5 AP,\nsurpassing the state-of-the-art QFL (Quality Focal Loss, 39.9 AP) and VFL\n(Varifocal Loss, 40.1 AP). Our best model achieves 55.1 AP on COCO test-dev,\nobtaining excellent results with only a typical training setting. Moreover, as\na plug-and-play scheme, HPF can cooperate well with recent advances, providing\na stable performance improvement on nine mainstream detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Binghong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Images into Maps. (arXiv:2110.00966v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00966","description":"<p>We approach instantaneous mapping, converting images to a top-down view of\nthe world, as a translation problem. We show how a novel form of transformer\nnetwork can be used to map from images and video directly to an overhead map or\nbird's-eye-view (BEV) of the world, in a single end-to-end network. We assume a\n1-1 correspondence between a vertical scanline in the image, and rays passing\nthrough the camera location in an overhead map. This lets us formulate map\ngeneration from an image as a set of sequence-to-sequence translations. Posing\nthe problem as translation allows the network to use the context of the image\nwhen interpreting the role of each pixel. This constrained formulation, based\nupon a strong physical grounding of the problem, leads to a restricted\ntransformer network that is convolutional in the horizontal direction only. The\nstructure allows us to make efficient use of data when training, and obtains\nstate-of-the-art results for instantaneous mapping of three large-scale\ndatasets, including a 15% and 30% relative gain against existing best\nperforming methods on the nuScenes and Argoverse datasets, respectively. We\nmake our code available on\nhttps://github.com/avishkarsaha/translating-images-into-maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Avishkar Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maldonado_O/0/1/0/all/0/1\">Oscar Mendez Maldonado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EditVAE: Unsupervised Part-Aware Controllable 3D Point Cloud Shape Generation. (arXiv:2110.06679v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06679","description":"<p>This paper tackles the problem of parts-aware point cloud generation. Unlike\nexisting works which require the point cloud to be segmented into parts a\npriori, our parts-aware editing and generation are performed in an unsupervised\nmanner. We achieve this with a simple modification of the Variational\nAuto-Encoder which yields a joint model of the point cloud itself along with a\nschematic representation of it as a combination of shape primitives. In\nparticular, we introduce a latent representation of the point cloud which can\nbe decomposed into a disentangled representation for each part of the shape.\nThese parts are in turn disentangled into both a shape primitive and a point\ncloud representation, along with a standardising transformation to a canonical\ncoordinate system. The dependencies between our standardising transformations\npreserve the spatial dependencies between the parts in a manner that allows\nmeaningful parts-aware point cloud generation and shape editing. In addition to\nthe flexibility afforded by our disentangled representation, the inductive bias\nintroduced by our joint modeling approach yields state-of-the-art experimental\nresults on the ShapeNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projective Manifold Gradient Layer for Deep Rotation Regression. (arXiv:2110.11657v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11657","description":"<p>Regressing rotations on SO(3) manifold using deep neural networks is an\nimportant yet unsolved problem. The gap between the Euclidean network output\nspace and the non-Euclidean SO(3) manifold imposes a severe challenge for\nneural network learning in both forward and backward passes. While several\nworks have proposed different regression-friendly rotation representations,\nvery few works have been devoted to improving the gradient backpropagating in\nthe backward pass. In this paper, we propose a manifold-aware gradient that\ndirectly backpropagates into deep network weights. Leveraging Riemannian\noptimization to construct a novel projective gradient, our proposed regularized\nprojective manifold gradient (RPMG) method helps networks achieve new\nstate-of-the-art performance in a variety of rotation estimation tasks. Our\nproposed gradient layer can also be applied to other smooth manifolds such as\nthe unit sphere. Our project page is at https://jychen18.github.io/RPMG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingda Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in Neural Rendering. (arXiv:2111.05849v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2111.05849","description":"<p>Synthesizing photo-realistic images and videos is at the heart of computer\ngraphics and has been the focus of decades of research. Traditionally,\nsynthetic images of a scene are generated using rendering algorithms such as\nrasterization or ray tracing, which take specifically defined representations\nof geometry and material properties as input. Collectively, these inputs define\nthe actual scene and what is rendered, and are referred to as the scene\nrepresentation (where a scene consists of one or more objects). Example scene\nrepresentations are triangle meshes with accompanied textures (e.g., created by\nan artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g.,\nfrom a CT scan), or implicit surface functions (e.g., truncated signed distance\nfields). The reconstruction of such a scene representation from observations\nusing differentiable rendering losses is known as inverse graphics or inverse\nrendering. Neural rendering is closely related, and combines ideas from\nclassical computer graphics and machine learning to create algorithms for\nsynthesizing images from real-world observations. Neural rendering is a leap\nforward towards the goal of synthesizing photo-realistic image and video\ncontent. In recent years, we have seen immense progress in this field through\nhundreds of publications that show different ways to inject learnable\ncomponents into the rendering pipeline. This state-of-the-art report on\nadvances in neural rendering focuses on methods that combine classical\nrendering principles with learned 3D scene representations, often now referred\nto as neural scene representations. A key advantage of these methods is that\nthey are 3D-consistent by design, enabling applications such as novel viewpoint\nsynthesis of a captured scene. In addition to methods that handle static\nscenes, we cover neural scene representations for modeling non-rigidly\ndeforming objects...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tretschk_E/0/1/0/all/0/1\">Edgar Tretschk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_S/0/1/0/all/0/1\">Stephen Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling equivariance for arbitrary Lie groups. (arXiv:2111.08251v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08251","description":"<p>Although provably robust to translational perturbations, convolutional neural\nnetworks (CNNs) are known to suffer from extreme performance degradation when\npresented at test time with more general geometric transformations of inputs.\nRecently, this limitation has motivated a shift in focus from CNNs to Capsule\nNetworks (CapsNets). However, CapsNets suffer from admitting relatively few\ntheoretical guarantees of invariance. We introduce a rigourous mathematical\nframework to permit invariance to any Lie group of warps, exclusively using\nconvolutions (over Lie groups), without the need for capsules. Previous work on\ngroup convolutions has been hampered by strong assumptions about the group,\nwhich precludes the application of such techniques to common warps in computer\nvision such as affine and homographic. Our framework enables the implementation\nof group convolutions over any finite-dimensional Lie group. We empirically\nvalidate our approach on the benchmark affine-invariant classification task,\nwhere we achieve 30% improvement in accuracy against conventional CNNs while\noutperforming most CapsNets. As further illustration of the generality of our\nframework, we train a homography-convolutional model which achieves superior\nrobustness on a homography-perturbed dataset, where CapsNet results degrade.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_L/0/1/0/all/0/1\">Lachlan Ewen MacDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HARA: A Hierarchical Approach for Robust Rotation Averaging. (arXiv:2111.08831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08831","description":"<p>We propose a novel hierarchical approach for multiple rotation averaging,\ndubbed HARA. Our method incrementally initializes the rotation graph based on a\nhierarchy of triplet support. The key idea is to build a spanning tree by\nprioritizing the edges with many strong triplet supports and gradually adding\nthose with weaker and fewer supports. This reduces the risk of adding outliers\nin the spanning tree. As a result, we obtain a robust initial solution that\nenables us to filter outliers prior to nonlinear optimization. With minimal\nmodification, our approach can also integrate the knowledge of the number of\nvalid 2D-2D correspondences. We perform extensive evaluations on both synthetic\nand real datasets, demonstrating state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong Hun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing. (arXiv:2111.12608v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12608","description":"<p>Multi-task indoor scene understanding is widely considered as an intriguing\nformulation, as the affinity of different tasks may lead to improved\nperformance. In this paper, we tackle the new problem of joint semantic,\naffordance and attribute parsing. However, successfully resolving it requires a\nmodel to capture long-range dependency, learn from weakly aligned data and\nproperly balance sub-tasks during training. To this end, we propose an\nattention-based architecture named Cerberus and a tailored training framework.\nOur method effectively addresses the aforementioned challenges and achieves\nstate-of-the-art performance on all three tasks. Moreover, an in-depth analysis\nshows concept affinity consistent with human cognition, which inspires us to\nexplore the possibility of weakly supervised learning. Surprisingly, Cerberus\nachieves strong results using only 0.1%-1% annotation. Visualizations further\nconfirm that this success is credited to common attention maps across tasks.\nCode and models can be accessed at https://github.com/OPEN-AIR-SUN/Cerberus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amortized Prompt: Guide CLIP to Domain Transfer Learning. (arXiv:2111.12853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12853","description":"<p>Domain generalization (DG) is a problematic Domain Transfer Learning problem\naiming to learn a generalizable model to unseen domains. Recent massive\npre-trained models such as CLIP and GPT-3, i.e. foundation models (FMs), are\nrobust to many distribution shifts and therefore should lead to substantial\nimprovements in DG. In this work, we study generic ways to adopt CLIP for DG\nproblems in image classification. We evaluate Test-Time Adaptation (TTA) and\nfull DG learning settings on several standard benchmarks. We propose AP\n(Amortized Prompt) as a novel prompt strategy for domain inference in the form\nof prompt generation. Moreover, we show that combining domain prompt inference\nwith CLIP enables the model to outperform strong DG baselines and other prompt\nstrategies. Since AP generate prompts to automatically adapt to the target\ndomain, it can be seen as a TTA method. Therefore, we also conduct a fair\ncomparison with SOTA TTA methods. The results demonstrate AP can outperform all\nbaselines with a significant margin. Then, we further analyze the properties of\nAP with insightful ablation experiments. We hope the simplicity and success of\nour approach emphasize the importance of and lead to broader adoption and\nanalysis of foundation models in the field of TTA and DG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multiple Dense Prediction Tasks from Partially Annotated Data. (arXiv:2111.14893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14893","description":"<p>Despite the recent advances in multi-task learning of dense prediction\nproblems, most methods rely on expensive labelled datasets. In this paper, we\npresent a label efficient approach and look at jointly learning of multiple\ndense prediction tasks on partially annotated data (i.e. not all the task\nlabels are available for each image), which we call multi-task\npartially-supervised learning. We propose a multi-task training procedure that\nsuccessfully leverages task relations to supervise its multi-task learning when\ndata is partially annotated. In particular, we learn to map each task pair to a\njoint pairwise task-space which enables sharing information between them in a\ncomputationally efficient way through another network conditioned on task\npairs, and avoids learning trivial cross-task relations by retaining high-level\ninformation about the input image. We rigorously demonstrate that our proposed\nmethod effectively exploits the images with unlabelled tasks and outperforms\nexisting semi-supervised learning approaches and related methods on three\nstandard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISNAS-DIP: Image-Specific Neural Architecture Search for Deep Image Prior. (arXiv:2111.15362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15362","description":"<p>Recent works show that convolutional neural network (CNN) architectures have\na spectral bias towards lower frequencies, which has been leveraged for various\nimage restoration tasks in the Deep Image Prior (DIP) framework. The benefit of\nthe inductive bias the network imposes in the DIP framework depends on the\narchitecture. Therefore, researchers have studied how to automate the search to\ndetermine the best-performing model. However, common neural architecture search\n(NAS) techniques are resource and time-intensive. Moreover, best-performing\nmodels are determined for a whole dataset of images instead of for each image\nindependently, which would be prohibitively expensive. In this work, we first\nshow that optimal neural architectures in the DIP framework are\nimage-dependent. Leveraging this insight, we then propose an image-specific NAS\nstrategy for the DIP framework that requires substantially less training than\ntypical NAS approaches, effectively enabling image-specific NAS. We justify the\nproposed strategy's effectiveness by (1) demonstrating its performance on a NAS\nDataset for DIP that includes 522 models from a particular search space (2)\nconducting extensive experiments on image denoising, inpainting, and\nsuper-resolution tasks. Our experiments show that image-specific metrics can\nreduce the search space to a small cohort of models, of which the best model\noutperforms current NAS approaches for image restoration. Codes and datasets\nare available at https://github.com/ozgurkara99/ISNAS-DIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arican_M/0/1/0/all/0/1\">Metin Ersin Arican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kara_O/0/1/0/all/0/1\">Ozgur Kara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1\">Gustav Bredell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15430","description":"<p>In spite of the dominant performances of deep neural networks, recent works\nhave shown that they are poorly calibrated, resulting in over-confident\npredictions. Miscalibration can be exacerbated by overfitting due to the\nminimization of the cross-entropy during training, as it promotes the predicted\nsoftmax probabilities to match the one-hot label assignments. This yields a\npre-softmax activation of the correct class that is significantly larger than\nthe remaining activations. Recent evidence from the literature suggests that\nloss functions that embed implicit or explicit maximization of the entropy of\npredictions yield state-of-the-art calibration performances. We provide a\nunifying constrained-optimization perspective of current state-of-the-art\ncalibration losses. Specifically, these losses could be viewed as\napproximations of a linear penalty (or a Lagrangian) imposing equality\nconstraints on logit distances. This points to an important limitation of such\nunderlying equality constraints, whose ensuing gradients constantly push\ntowards a non-informative solution, which might prevent from reaching the best\ncompromise between the discriminative performance and calibration of the model\nduring gradient-based optimization. Following our observations, we propose a\nsimple and flexible generalization based on inequality constraints, which\nimposes a controllable margin on logit distances. Comprehensive experiments on\na variety of image classification, semantic segmentation and NLP benchmarks\ndemonstrate that our method sets novel state-of-the-art results on these tasks\nin terms of network calibration, without affecting the discriminative\nperformance. The code is available at https://github.com/by-liu/MbLS .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation. (arXiv:2111.15483v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15483","description":"<p>Video frame interpolation (VFI) is currently a very active research topic,\nwith applications spanning computer vision, post production and video encoding.\nVFI can be extremely challenging, particularly in sequences containing large\nmotions, occlusions or dynamic textures, where existing approaches fail to\noffer perceptually robust interpolation performance. In this context, we\npresent a novel deep learning based VFI method, ST-MFNet, based on a\nSpatio-Temporal Multi-Flow architecture. ST-MFNet employs a new multi-scale\nmulti-flow predictor to estimate many-to-one intermediate flows, which are\ncombined with conventional one-to-one optical flows to capture both large and\ncomplex motions. In order to enhance interpolation performance for various\ntextures, a 3D CNN is also employed to model the content dynamics over an\nextended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN\nframework, which was originally developed for texture synthesis, with the aim\nof further improving perceptual interpolation quality. Our approach has been\ncomprehensively evaluated -- compared with fourteen state-of-the-art VFI\nalgorithms -- clearly demonstrating that ST-MFNet consistently outperforms\nthese benchmarks on varied and representative test datasets, with significant\ngains up to 1.09dB in PSNR for cases including large motions and dynamic\ntextures. Project page: https://danielism97.github.io/ST-MFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Danier_D/0/1/0/all/0/1\">Duolikun Danier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Emotion Director: Speech-preserving semantic control of facial expressions in \"in-the-wild\" videos. (arXiv:2112.00585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00585","description":"<p>In this paper, we introduce a novel deep learning method for photo-realistic\nmanipulation of the emotional state of actors in \"in-the-wild\" videos. The\nproposed method is based on a parametric 3D face representation of the actor in\nthe input scene that offers a reliable disentanglement of the facial identity\nfrom the head pose and facial expressions. It then uses a novel deep domain\ntranslation framework that alters the facial expressions in a consistent and\nplausible manner, taking into account their dynamics. Finally, the altered\nfacial expressions are used to photo-realistically manipulate the facial region\nin the input scene based on an especially-designed neural face renderer. To the\nbest of our knowledge, our method is the first to be capable of controlling the\nactor's facial expressions by even using as a sole input the semantic labels of\nthe manipulated emotions, while at the same time preserving the speech-related\nlip movements. We conduct extensive qualitative and quantitative evaluations\nand comparisons, which demonstrate the effectiveness of our approach and the\nespecially promising results that we obtain. Our method opens a plethora of new\npossibilities for useful applications of neural rendering technologies, ranging\nfrom movie post-production and video games to photo-realistic affective\navatars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papantoniou_F/0/1/0/all/0/1\">Foivos Paraperas Papantoniou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roussos_A/0/1/0/all/0/1\">Anastasios Roussos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras. (arXiv:2112.01524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01524","description":"<p>We present an approach for 3D global human mesh recovery from monocular\nvideos recorded with dynamic cameras. Our approach is robust to severe and\nlong-term occlusions and tracks human bodies even when they go outside the\ncamera's field of view. To achieve this, we first propose a deep generative\nmotion infiller, which autoregressively infills the body motions of occluded\nhumans based on visible motions. Additionally, in contrast to prior work, our\napproach reconstructs human meshes in consistent global coordinates even with\ndynamic cameras. Since the joint reconstruction of human motions and camera\nposes is underconstrained, we propose a global trajectory predictor that\ngenerates global human trajectories based on local body movements. Using the\npredicted trajectories as anchors, we present a global optimization framework\nthat refines the predicted trajectories and optimizes the camera poses to match\nthe video evidence such as 2D keypoints. Experiments on challenging indoor and\nin-the-wild datasets with dynamic cameras demonstrate that the proposed\napproach outperforms prior methods significantly in terms of motion infilling\nand global mesh recovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection. (arXiv:2112.01526v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01526","description":"<p>In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified\narchitecture for image and video classification, as well as object detection.\nWe present an improved version of MViT that incorporates decomposed relative\npositional embeddings and residual pooling connections. We instantiate this\narchitecture in five sizes and evaluate it for ImageNet classification, COCO\ndetection and Kinetics video recognition where it outperforms prior work. We\nfurther compare MViTv2s' pooling attention to window attention mechanisms where\nit outperforms the latter in accuracy/compute. Without bells-and-whistles,\nMViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on\nImageNet classification, 58.7 boxAP on COCO object detection as well as 86.1%\non Kinetics-400 video classification. Code and models are available at\nhttps://github.com/facebookresearch/mvit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao-Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Bo Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forward Compatible Training for Large-Scale Embedding Retrieval Systems. (arXiv:2112.02805v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02805","description":"<p>In visual retrieval systems, updating the embedding model requires\nrecomputing features for every piece of data. This expensive process is\nreferred to as backfilling. Recently, the idea of backward compatible training\n(BCT) was proposed. To avoid the cost of backfilling, BCT modifies training of\nthe new model to make its representations compatible with those of the old\nmodel. However, BCT can significantly hinder the performance of the new model.\nIn this work, we propose a new learning paradigm for representation learning:\nforward compatible training (FCT). In FCT, when the old model is trained, we\nalso prepare for a future unknown version of the model. We propose learning\nside-information, an auxiliary feature for each sample which facilitates future\nupdates of the model. To develop a powerful and flexible framework for model\ncompatibility, we combine side-information with a forward transformation from\nold to new embeddings. Training of the new model is not modified, hence, its\naccuracy is not degraded. We demonstrate significant retrieval accuracy\nimprovement compared to BCT for various datasets: ImageNet-1k (+18.1%),\nPlaces-365 (+5.4%), and VGG-Face2 (+8.3%). FCT obtains model compatibility when\nthe new and old models are trained across different datasets, losses, and\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1\">Pavan Kumar Anasosalu Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouransari_H/0/1/0/all/0/1\">Hadi Pouransari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02857","description":"<p>In a point cloud sequence, 3D object tracking aims to predict the location\nand orientation of an object in the current search point cloud given a template\npoint cloud. Motivated by the success of transformers, we propose Point\nTracking TRansformer (PTTR), which efficiently predicts high-quality 3D\ntracking results in a coarse-to-fine manner with the help of transformer\noperations. PTTR consists of three novel designs. 1) Instead of random\nsampling, we design Relation-Aware Sampling to preserve relevant points to\ngiven templates during subsampling. 2) Furthermore, we propose a Point Relation\nTransformer (PRT) consisting of a self-attention and a cross-attention module.\nThe global self-attention operation captures long-range dependencies to enhance\nencoded point features for the search area and the template, respectively.\nSubsequently, we generate the coarse tracking results by matching the two sets\nof point features via cross-attention. 3) Based on the coarse tracking results,\nwe employ a novel Prediction Refinement Module to obtain the final refined\nprediction. In addition, we create a large-scale point cloud single object\ntracking benchmark based on the Waymo Open Dataset. Extensive experiments show\nthat PTTR achieves superior point cloud tracking in both accuracy and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yueru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation. (arXiv:2112.04177v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04177","description":"<p>For online video instance segmentation (VIS), fully utilizing the information\nfrom previous frames in an efficient manner is essential for real-time\napplications. Most previous methods follow a two-stage approach requiring\nadditional computations such as RPN and RoIAlign, and do not fully exploit the\navailable information in the video for all subtasks in VIS. In this paper, we\npropose a novel single-stage framework for online VIS built based on the grid\nstructured feature representation. The grid-based features allow us to employ\nfully convolutional networks for real-time processing, and also to easily reuse\nand share features within different components. We also introduce cooperatively\noperating modules that aggregate information from available frames, in order to\nenrich the features for all subtasks in VIS. Our design fully takes advantage\nof previous information in a grid form for all tasks in VIS in an efficient\nway, and we achieved the new state-of-the-art accuracy (38.6 AP and 36.9 AP)\nand speed (40.0 FPS) on YouTube-VIS 2019 and 2021 datasets among online VIS\nmethods. The code is available at https://github.com/SuHoHan95/VISOLO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Su Ho Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sukjun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yeonchool Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min-Jung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04312","description":"<p>In this work we develop a generalizable and efficient Neural Radiance Field\n(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under\nsettings with sparse camera views. Though existing NeRF-based methods can\nsynthesize rather realistic details for human body, they tend to produce poor\nresults when the input has self-occlusion, especially for unseen humans under\nsparse views. Moreover, these methods often require a large number of sampling\npoints for rendering, which leads to low efficiency and limits their real-world\napplicability. To address these challenges, we propose a Geometry-guided\nProgressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we\ndevise a geometry-guided multi-view feature integration approach that utilizes\nthe estimated geometry prior to integrate the incomplete information from input\nviews and construct a complete geometry volume for the target human body.\nMeanwhile, for achieving higher rendering efficiency, we introduce a\nprogressive rendering pipeline through geometry guidance, which leverages the\ngeometric feature volume and the predicted density values to progressively\nreduce the number of sampling points and speed up the rendering process.\nExperiments on the ZJU-MoCap and THUman datasets show that our method\noutperforms the state-of-the-arts significantly across multiple generalization\nsettings, while the time cost is reduced &gt; 70% via applying our efficient\nprogressive rendering pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04482","description":"<p>State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a \"foundation\", that targets all\nmodalities at once -- a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIST: Unpaired Neural Implicit Shape Translation Network. (arXiv:2112.05381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05381","description":"<p>We introduce UNIST, the first deep neural implicit model for general-purpose,\nunpaired shape-to-shape translation, in both 2D and 3D domains. Our model is\nbuilt on autoencoding implicit fields, rather than point clouds which\nrepresents the state of the art. Furthermore, our translation network is\ntrained to perform the task over a latent grid representation which combines\nthe merits of both latent-space processing and position awareness, to not only\nenable drastic shape transforms but also well preserve spatial features and\nfine local details for natural shape translations. With the same network\narchitecture and only dictated by the input domain pairs, our model can learn\nboth style-preserving content alteration and content-preserving style transfer.\nWe demonstrate the generality and quality of the translation results, and\ncompare them to well-known baselines. Code is available at\nhttps://qiminchen.github.io/unist/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merz_J/0/1/0/all/0/1\">Johannes Merz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PartGlot: Learning Shape Part Segmentation from Language Reference Games. (arXiv:2112.06390v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06390","description":"<p>We introduce PartGlot, a neural framework and associated architectures for\nlearning semantic part segmentation of 3D shape geometry, based solely on part\nreferential language. We exploit the fact that linguistic descriptions of a\nshape can provide priors on the shape's parts -- as natural language has\nevolved to reflect human perception of the compositional structure of objects,\nessential to their recognition and use. For training, we use the paired\ngeometry / language data collected in the ShapeGlot work for their reference\ngame, where a speaker creates an utterance to differentiate a target shape from\ntwo distractors and the listener has to find the target based on this\nutterance. Our network is designed to solve this target discrimination problem,\ncarefully incorporating a Transformer-based attention module so that the output\nattention can precisely highlight the semantic part or parts described in the\nlanguage. Furthermore, the network operates without any direct supervision on\nthe 3D geometry itself. Surprisingly, we further demonstrate that the learned\npart information is generalizable to shape classes unseen during training. Our\napproach opens the possibility of learning 3D shape parts from language alone,\nwithout the need for large-scale part geometry annotations, thus facilitating\nannotation acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Juil Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_I/0/1/0/all/0/1\">Ian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07471","description":"<p>Traditional 3D morphable face models (3DMMs) provide fine-grained control\nover expression but cannot easily capture geometric and appearance details.\nNeural volumetric representations approach photorealism but are hard to animate\nand do not generalize well to unseen expressions. To tackle this problem, we\npropose IMavatar (Implicit Morphable avatar), a novel method for learning\nimplicit head avatars from monocular videos. Inspired by the fine-grained\ncontrol mechanisms afforded by conventional 3DMMs, we represent the expression-\nand pose- related deformations via learned blendshapes and skinning fields.\nThese attributes are pose-independent and can be used to morph the canonical\ngeometry and texture fields given novel expression and pose parameters. We\nemploy ray marching and iterative root-finding to locate the canonical surface\nintersection for each pixel. A key contribution is our novel analytical\ngradient formulation that enables end-to-end training of IMavatars from videos.\nWe show quantitatively and qualitatively that our method improves geometry and\ncovers a more complete expression space compared to state-of-the-art methods.\nCode, video, and data can be found at\nhttps://ait.ethz.ch/projects/2022/IMavatar/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yufeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1\">Victoria Fern&#xe1;ndez Abrevaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-ViT: Adaptive Tokens for Efficient Vision Transformer. (arXiv:2112.07658v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07658","description":"<p>We introduce A-ViT, a method that adaptively adjusts the inference cost of\nvision transformer (ViT) for images of different complexity. A-ViT achieves\nthis by automatically reducing the number of tokens in vision transformers that\nare processed in the network as inference proceeds. We reformulate Adaptive\nComputation Time (ACT) for this task, extending halting to discard redundant\nspatial tokens. The appealing architectural properties of vision transformers\nenables our adaptive token reduction mechanism to speed up inference without\nmodifying the network architecture or inference hardware. We demonstrate that\nA-ViT requires no extra parameters or sub-network for halting, as we base the\nlearning of adaptive halting on the original network parameters. We further\nintroduce distributional prior regularization that stabilizes training compared\nto prior ACT approaches. On the image classification task (ImageNet1K), we show\nthat our proposed A-ViT yields high efficacy in filtering informative spatial\nfeatures and cutting down on the overall compute. The proposed method improves\nthe throughput of DeiT-Tiny by 62% and DeiT-Small by 38% with only 0.3%\naccuracy drop, outperforming prior art by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallya_A/0/1/0/all/0/1\">Arun Mallya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09081","description":"<p>We present a visual localization system that learns to estimate camera poses\nin the real world with the help of synthetic data. Despite significant progress\nin recent years, most learning-based approaches to visual localization target\nat a single domain and require a dense database of geo-tagged images to\nfunction well. To mitigate the data scarcity issue and improve the scalability\nof the neural localization models, we introduce TOPO-DataGen, a versatile\nsynthetic data generation tool that traverses smoothly between the real and\nvirtual world, hinged on the geographic camera viewpoint. New large-scale\nsim-to-real benchmark datasets are proposed to showcase and evaluate the\nutility of the said synthetic data. Our experiments reveal that synthetic data\ngenerically enhances the neural network performance on real data. Furthermore,\nwe introduce CrossLoc, a cross-modal visual representation learning approach to\npose estimation that makes full use of the scene coordinate ground truth via\nself-supervision. Without any extra data, CrossLoc significantly outperforms\nthe state-of-the-art methods and achieves substantially higher real-data sample\nefficiency. Our code and datasets are all available at\nhttps://crossloc.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reding_S/0/1/0/all/0/1\">Simon Reding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doytchinov_I/0/1/0/all/0/1\">Iordan Doytchinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09459","description":"<p>Current weakly supervised semantic segmentation (WSSS) frameworks usually\ncontain the separated mask-refinement model and the main semantic region mining\nmodel. These approaches would contain redundant feature extraction backbones\nand biased learning objectives, making them computational complex yet\nsub-optimal to addressing the WSSS task. To solve this problem, this paper\nestablishes a compact learning framework that embeds the classification and\nmask-refinement components into a unified deep model. With the shared feature\nextraction backbone, our model is able to facilitate knowledge sharing between\nthe two components while preserving a low computational complexity. To\nencourage high-quality knowledge interaction, we propose a novel alternative\nself-dual teaching (ASDT) mechanism. Unlike the conventional distillation\nstrategy, the knowledge of the two teacher branches in our model is\nalternatively distilled to the student branch by a Pulse Width Modulation\n(PWM), which generates PW wave-like selection signal to guide the knowledge\ndistillation process. In this way, the student branch can help prevent the\nmodel from falling into local minimum solutions caused by the imperfect\nknowledge provided of either teacher branch. Comprehensive experiments on the\nPASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of the\nproposed alternative self-dual teaching mechanism as well as the new\nstate-of-the-art performance of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Segmentation Using Text and Image Prompts. (arXiv:2112.10003v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10003","description":"<p>Image segmentation is usually addressed by training a model for a fixed set\nof object classes. Incorporating additional classes or more complex queries\nlater is expensive as it requires re-training the model on a dataset that\nencompasses these expressions. Here we propose a system that can generate image\nsegmentations based on arbitrary prompts at test time. A prompt can be either a\ntext or an image. This approach enables us to create a unified model (trained\nonce) for three common segmentation tasks, which come with distinct challenges:\nreferring expression segmentation, zero-shot segmentation and one-shot\nsegmentation. We build upon the CLIP model as a backbone which we extend with a\ntransformer-based decoder that enables dense prediction. After training on an\nextended version of the PhraseCut dataset, our system generates a binary\nsegmentation map for an image based on a free-text prompt or on an additional\nimage expressing the query. We analyze different variants of the latter\nimage-based prompts in detail. This novel hybrid input allows for dynamic\nadaptation not only to the three segmentation tasks mentioned above, but to any\nbinary segmentation task where a text or image query can be formulated.\nFinally, we find our system to adapt well to generalized queries involving\naffordances or properties. Code is available at\nhttps://eckerlab.org/code/clipseg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1\">Timo L&#xfc;ddecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1\">Alexander S. Ecker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology Preserving Local Road Network Estimation from Single Onboard Camera Image. (arXiv:2112.10155v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10155","description":"<p>Knowledge of the road network topology is crucial for autonomous planning and\nnavigation. Yet, recovering such topology from a single image has only been\nexplored in part. Furthermore, it needs to refer to the ground plane, where\nalso the driving actions are taken. This paper aims at extracting the local\nroad network topology, directly in the bird's-eye-view (BEV), all in a complex\nurban setting. The only input consists of a single onboard, forward looking\ncamera image. We represent the road topology using a set of directed lane\ncurves and their interactions, which are captured using their intersection\npoints. To better capture topology, we introduce the concept of \\emph{minimal\ncycles} and their covers. A minimal cycle is the smallest cycle formed by the\ndirected curve segments (between two intersections). The cover is a set of\ncurves whose segments are involved in forming a minimal cycle. We first show\nthat the covers suffice to uniquely represent the road topology. The covers are\nthen used to supervise deep neural networks, along with the lane curve\nsupervision. These learn to predict the road topology from a single input\nimage. The results on the NuScenes and Argoverse benchmarks are significantly\nbetter than those obtained with baselines. Code:\nhttps://github.com/ybarancan/TopologicalLaneGraph\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11081","description":"<p>Compared to convolutional layers, fully-connected (FC) layers are better at\nmodeling the long-range dependencies but worse at capturing the local patterns,\nhence usually less favored for image recognition. In this paper, we propose a\nmethodology, Locality Injection, to incorporate local priors into an FC layer\nvia merging the trained parameters of a parallel conv kernel into the FC\nkernel. Locality Injection can be viewed as a novel Structural\nRe-parameterization method since it equivalently converts the structures via\ntransforming the parameters. Based on that, we propose a multi-layer-perceptron\n(MLP) block named RepMLP Block, which uses three FC layers to extract features,\nand a novel architecture named RepMLPNet. The hierarchical design distinguishes\nRepMLPNet from the other concurrently proposed vision MLPs. As it produces\nfeature maps of different levels, it qualifies as a backbone model for\ndownstream tasks like semantic segmentation. Our results reveal that 1)\nLocality Injection is a general methodology for MLP models; 2) RepMLPNet has\nfavorable accuracy-efficiency trade-off compared to the other MLPs; 3)\nRepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic\nsegmentation. The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11427","description":"<p>We introduce a high resolution, 3D-consistent image and shape generation\ntechnique which we call StyleSDF. Our method is trained on single-view RGB data\nonly, and stands on the shoulders of StyleGAN2 for image generation, while\nsolving two main challenges in 3D-aware GANs: 1) high-resolution,\nview-consistent generation of the RGB images, and 2) detailed 3D shape. We\nachieve this by merging a SDF-based 3D representation with a style-based 2D\ngenerator. Our 3D implicit network renders low-resolution feature maps, from\nwhich the style-based network generates view-consistent, 1024x1024 images.\nNotably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to\nconsistent volume rendering. Our method shows higher quality results compared\nto state of the art in terms of visual and geometric quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Or_El_R/0/1/0/all/0/1\">Roy Or-El</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mengyi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeong Joon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1\">Ira Kemelmacher-Shlizerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch-based Facial Synthesis: A New Challenge. (arXiv:2112.15439v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15439","description":"<p>This paper aims to conduct a comprehensive study on the facial sketch\nsynthesis (FSS) problem. However, due to the high costs in obtaining hand-drawn\nsketch datasets, there lacks a complete benchmark for assessing the development\nof FSS algorithms over the last decade. As such, we first introduce a\nhigh-quality dataset for FSS, named FS2K, which consists of 2,104 image-sketch\npairs spanning three types of sketch styles, image backgrounds, lighting\nconditions, skin colors, and facial attributes. FS2K differs from previous FSS\ndatasets in difficulty, diversity, and scalability and should thus facilitate\nthe progress of FSS research. Second, we present the largest-scale FSS study by\nreviewing 139 classical methods, including 24 handcrafted feature-based facial\nsketch synthesis approaches, 37 general neural-style transfer methods, 43 deep\nimage-to-image translation methods, and 35 image-to-sketch approaches. Besides,\nwe elaborate comprehensive experiments on the existing 19 cutting-edge models.\nThird, we present a simple baseline for FSS, named FSGAN. With only two\nstraightforward components, i.e., facial-aware masking and style-vector\nexpansion, FSGAN surpasses the performance of all previous state-of-the-art\nmodels on the proposed FS2K dataset by a large margin. Finally, we conclude\nwith lessons learned over the past years and point out several unsolved\nchallenges. Our open-source code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01831","description":"<p>Implicit neural networks have been successfully used for surface\nreconstruction from point clouds. However, many of them face scalability issues\nas they encode the isosurface function of a whole object or scene into a single\nlatent vector. To overcome this limitation, a few approaches infer latent\nvectors on a coarse regular 3D grid or on 3D patches, and interpolate them to\nanswer occupancy queries. In doing so, they loose the direct connection with\nthe input points sampled on the surface of objects, and they attach information\nuniformly in space rather than where it matters the most, i.e., near the\nsurface. Besides, relying on fixed patch sizes may require discretization\ntuning. To address these issues, we propose to use point cloud convolutions and\ncompute latent vectors at each input point. We then perform a learning-based\ninterpolation on nearest neighbors using inferred weights. Experiments on both\nobject and scene datasets show that our approach significantly outperforms\nother methods on most classical metrics, producing finer details and better\nreconstructing thinner volumes. The code is available at\nhttps://github.com/valeoai/POCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04584","description":"<p>Automatic segmentation of lung lesions associated with COVID-19 in CT images\nrequires large amount of annotated volumes. Annotations mandate expert\nknowledge and are time-intensive to obtain through fully manual segmentation\nmethods. Additionally, lung lesions have large inter-patient variations, with\nsome pathologies having similar visual appearance as healthy lung tissues. This\nposes a challenge when applying existing semi-automatic interactive\nsegmentation techniques for data labelling. To address these challenges, we\npropose an efficient convolutional neural networks (CNNs) that can be learned\nonline while the annotator provides scribble-based interaction. To accelerate\nlearning from only the samples labelled through user-interactions, a\npatch-based approach is used for training the network. Moreover, we use\nweighted cross-entropy loss to address the class imbalance that may result from\nuser-interactions. During online inference, the learned network is applied to\nthe whole input volume using a fully convolutional approach. We compare our\nproposed method with state-of-the-art using synthetic scribbles and show that\nit outperforms existing methods on the task of annotating lung lesions\nassociated with COVID-19, achieving 16% higher Dice score while reducing\nexecution time by 3$\\times$ and requiring 9000 lesser scribbles-based labelled\nvoxels. Due to the online learning aspect, our approach adapts quickly to user\ninput, resulting in high quality segmentation labels. Source code for ECONet is\navailable at: https://github.com/masadcv/ECONet-MONAILabel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2201.07610","description":"<p>Observability is a fundamental structural property of any dynamic system and\ndescribes the possibility of reconstructing the state that characterizes the\nsystem from observing its inputs and outputs. Despite the huge effort made to\nstudy this property and to introduce analytical criteria able to check whether\na dynamic system satisfies this property or not, there is no general analytical\ncriterion to automatically check the state observability when the dynamics are\nalso driven by unknown inputs. Here, we introduce the general analytical\nsolution of this fundamental problem, often called the unknown input\nobservability problem. This paper provides the general analytical solution of\nthis problem, namely, it provides the systematic procedure, based on automatic\ncomputation (differentiation and matrix rank determination), that allows us to\nautomatically check the state observability even in the presence of unknown\ninputs. A first solution of this problem was presented in the second part of\nthe book: \"Observability: A New Theory Based on the Group of Invariance\" [45].\nThe solution presented by this paper completes the previous solution in [45].\nIn particular, the new solution exhaustively accounts for the systems that do\nnot belong to the category of the systems that are canonic with respect to\ntheir unknown inputs. The new solution is also provided in the form of a new\nalgorithm. A further novelty with respect to the algorithm provided in [45]\nconsists of a new convergence criterion that holds in all the cases (the\nconvergence criterion of the algorithm provided in [45] can fail in some\ncases). Finally, we also provide the answer to the problem of unknown input\nreconstruction which is intimately related to the problem of state\nobservability. We illustrate the implementation of the new algorithm by\nstudying a nonlinear system in the framework of visual-inertial sensor fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Martinelli_A/0/1/0/all/0/1\">Agostino Martinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance of Textlines in Historical Document Classification. (arXiv:2201.09575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09575","description":"<p>This paper describes a system prepared at Brno University of Technology for\nICDAR 2021 Competition on Historical Document Classification, experiments\nleading to its design, and the main findings. The solved tasks include script\nand font classification, document origin localization, and dating. We combined\npatch-level and line-level approaches, where the line-level system utilizes an\nexisting, publicly available page layout analysis engine. In both systems,\nneural networks provide local predictions which are combined into page-level\ndecisions, and the results of both systems are fused using linear or log-linear\ninterpolation. We propose loss functions suitable for weakly supervised\nclassification problem where multiple possible labels are provided, and we\npropose loss functions suitable for interval regression in the dating task. The\nline-level system significantly improves results in script and font\nclassification and in the dating task. The full system achieved 98.48 %, 88.84\n%, and 79.69 % accuracy in the font, script, and location classification tasks\nrespectively. In the dating task, our system achieved a mean absolute error of\n21.91 years. Our system achieved the best results in all tasks and became the\noverall winner of the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiss_M/0/1/0/all/0/1\">Martin Ki&#x161;&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohut_J/0/1/0/all/0/1\">Jan Koh&#xfa;t</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benes_K/0/1/0/all/0/1\">Karel Bene&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hradis_M/0/1/0/all/0/1\">Michal Hradi&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10943","description":"<p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that\nreports asynchronous, continuously per-pixel brightness changes called `events'\nwith high temporal resolution and high dynamic range. So far, the event-based\nimage reconstruction methods are based on artificial neural networks (ANN) or\nhand-crafted spatiotemporal smoothing techniques. In this paper, we first\nimplement the image reconstruction work via fully spiking neural network (SNN)\narchitecture. As the bio-inspired neural networks, SNNs operating with\nasynchronous binary spikes distributed over time, can potentially lead to\ngreater computational efficiency on event-driven hardware. We propose a novel\nEvent-based Video reconstruction framework based on a fully Spiking Neural\nNetwork (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and\nMembrane Potential (MP) neuron. We find that the spiking neurons have the\npotential to store useful temporal information (memory) to complete such\ntime-dependent tasks. Furthermore, to better utilize the temporal information,\nwe propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane\npotential of spiking neuron. The proposed neuron is referred as Adaptive\nMembrane Potential (AMP) neuron, which adaptively updates the membrane\npotential according to the input spikes. The experimental results demonstrate\nthat our models achieve comparable performance to ANN-based models on IJRR,\nMVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are\n19.36$\\times$ and 7.75$\\times$ more computationally efficient than their ANN\narchitectures, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12329","description":"<p>We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as\nqueries in Transformer decoders and dynamically updates them layer-by-layer.\nUsing box coordinates not only helps using explicit positional priors to\nimprove the query-to-feature similarity and eliminate the slow training\nconvergence issue in DETR, but also allows us to modulate the positional\nattention map using the box width and height information. Such a design makes\nit clear that queries in DETR can be implemented as performing soft ROI pooling\nlayer-by-layer in a cascade manner. As a result, it leads to the best\nperformance on MS-COCO benchmark among the DETR-like detection models under the\nsame setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50\nepochs. We also conducted extensive experiments to confirm our analysis and\nverify the effectiveness of our methods. Code is available at\n\\url{https://github.com/SlongLiu/DAB-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03278","description":"<p>Recent self-supervised contrastive learning methods greatly benefit from the\nSiamese structure that aims at minimizing distances between positive pairs. For\nhigh performance Siamese representation learning, one of the keys is to design\ngood contrastive pairs. Most previous works simply apply random sampling to\nmake different crops of the same image, which overlooks the semantic\ninformation that may degrade the quality of views. In this work, we propose\nContrastiveCrop, which could effectively generate better crops for Siamese\nrepresentation learning. Firstly, a semantic-aware object localization strategy\nis proposed within the training process in a fully unsupervised manner. This\nguides us to generate contrastive views which could avoid most false positives\n(i.e., object vs. background). Moreover, we empirically find that views with\nsimilar appearances are trivial for the Siamese model training. Thus, a\ncenter-suppressed sampling is further designed to enlarge the variance of\ncrops. Remarkably, our method takes a careful consideration of positive pairs\nfor contrastive learning with negligible extra training overhead. As a\nplug-and-play and framework-agnostic module, ContrastiveCrop consistently\nimproves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on\nCIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also\nachieved on downstream detection and segmentation tasks when pre-trained on\nImageNet-1K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01391","description":"<p>Traditional MVS methods have good accuracy but struggle with completeness,\nwhile recently developed learning-based multi-view stereo (MVS) techniques have\nimproved completeness except accuracy being compromised. We propose depth\ndiscontinuity learning for MVS methods, which further improves accuracy while\nretaining the completeness of the reconstruction. Our idea is to jointly\nestimate the depth and boundary maps where the boundary maps are explicitly\nused for further refinement of the depth maps. We validate our idea and\ndemonstrate that our strategies can be easily integrated into the existing\nlearning-based MVS pipeline where the reconstruction depends on high-quality\ndepth map estimation. Extensive experiments on various datasets show that our\nmethod improves reconstruction quality compared to baseline. Experiments also\ndemonstrate that the presented model and strategies have good generalization\ncapabilities. The source code will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimli_N/0/1/0/all/0/1\">Nail Ibrahimli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1\">Julian Kooij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Facial Paralysis Estimation with Facial Action Units. (arXiv:2203.01800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01800","description":"<p>Facial palsy is unilateral facial nerve weakness or paralysis of rapid onset\nwith unknown causes. Automatically estimating facial palsy severeness can be\nhelpful for the diagnosis and treatment of people suffering from it across the\nworld. In this work, we develop and experiment with a novel model for\nestimating facial palsy severity. For this, an effective Facial Action Units\n(AU) detection technique is incorporated into our model, where AUs refer to a\nunique set of facial muscle movements used to describe almost every\nanatomically possible facial expression. In this paper, we propose a novel\nAdaptive Local-Global Relational Network (ALGRNet) for facial AU detection and\nuse it to classify facial paralysis severity. ALGRNet mainly consists of three\nmain novel structures: (i) an adaptive region learning module that learns the\nadaptive muscle regions based on the detected landmarks; (ii) a skip-BiLSTM\nthat models the latent relationships among local AUs; and (iii) a feature\nfusion&amp;refining module that investigates the complementary between the local\nand global face. Quantitative results on two AU benchmarks, i.e., BP4D and\nDISFA, demonstrate our ALGRNet can achieve promising AU detection accuracy. We\nfurther demonstrate the effectiveness of its application to facial paralysis\nestimation by migrating ALGRNet to a facial paralysis dataset collected and\nannotated by medical professionals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arunachalam Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoNIC Solution. (arXiv:2203.03415v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03415","description":"<p>Nuclei segmentation and classification have been a challenge in digital\npathology due to the specific domain characteristics. First, annotating a\nlarge-scale dataset is quite consuming. It requires specific domain knowledge\nand large efforts. Second, some nuclei are clustered together and hard to\nsegment from each other. Third, the classes are often extremely unbalanced. As\nin Lizard, the number of epithelial nuclei is around 67 times larger than the\nnumber of eosinophil nuclei. Fourth, the nuclei often exhibit high inter-class\nsimilarity and intra-class variability. Connective nuclei may look very\ndifferent from each other while some of them share a similar shape with the\nepithelial ones. Last but not least, pathological patches may have very\ndifferent color distributions among different datasets. Thus, a large-scale\ngenerally annotated dataset and a specially-designed algorithm are needed to\nsolve this problem. The CoNIC challenge aims to promote the automatic\nsegmentation and classification task and requires researchers to develop\nalgorithms that perform segmentation, classification, and counting of 6\ndifferent types of nuclei with the large-scale annotated dataset: Lizard. Due\nto the 60-minute time limit, the algorithm has to be simple and quick. In this\npaper, we briefly describe the final method we used in the CoNIC challenge. Our\nalgorithm is based on Hover-Net and we added several modifications to it to\nimprove its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04963","description":"<p>Learned image compression has achieved great success due to its excellent\nmodeling capacity, but seldom further considers the Rate-Distortion\nOptimization (RDO) of each input image. To explore this potential in the\nlearned codec, we make the first attempt to build a neural data-dependent\ntransform and introduce a continuous online mode decision mechanism to jointly\noptimize the coding efficiency for each individual image. Specifically, apart\nfrom the image content stream, we employ an additional model stream to generate\nthe transform parameters at the decoder side. The presence of a model stream\nenables our model to learn more abstract neural-syntax, which helps cluster the\nlatent representations of images more compactly. Beyond the transform stage, we\nalso adopt neural-syntax based post-processing for the scenarios that require\nhigher quality reconstructions regardless of extra decoding overhead. Moreover,\nthe involvement of the model stream further makes it possible to optimize both\nthe representation and the decoder in an online way, i.e. RDO at the testing\ntime. It is equivalent to a continuous online mode decision, like coding modes\nin the traditional codecs, to improve the coding efficiency based on the\nindividual input image. The experimental results show the effectiveness of the\nproposed neural-syntax design and the continuous online mode decision\nmechanism, demonstrating the superiority of our method in coding efficiency\ncompared to the latest conventional standard Versatile Video Coding (VVC) and\nother state-of-the-art learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dezhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yueyu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06386","description":"<p>The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06558","description":"<p>Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06717","description":"<p>We revisit large kernel design in modern convolutional neural networks\n(CNNs). Inspired by recent advances in vision transformers (ViTs), in this\npaper, we demonstrate that using a few large convolutional kernels instead of a\nstack of small kernels could be a more powerful paradigm. We suggested five\nguidelines, e.g., applying re-parameterized large depth-wise convolutions, to\ndesign efficient high-performance large-kernel CNNs. Following the guidelines,\nwe propose RepLKNet, a pure CNN architecture whose kernel size is as large as\n31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the\nperformance gap between CNNs and ViTs, e.g., achieving comparable or superior\nresults than Swin Transformer on ImageNet and a few typical downstream tasks,\nwith lower latency. RepLKNet also shows nice scalability to big data and large\nmodels, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,\nwhich is very competitive among the state-of-the-arts with similar model sizes.\nOur study further reveals that, in contrast to small-kernel CNNs, large-kernel\nCNNs have much larger effective receptive fields and higher shape bias rather\nthan texture bias. Code &amp; models at\nhttps://github.com/megvii-research/RepLKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yizhuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.07772","description":"<p>The numerical wavefront backpropagation principle of digital holography\nconfers unique extended focus capabilities, without mechanical displacements\nalong z-axis. However, the determination of the correct focusing distance is a\nnon-trivial and time consuming issue. A deep learning (DL) solution is proposed\nto cast the autofocusing as a regression problem and tested over both\nexperimental and simulated holograms. Single wavelength digital holograms were\nrecorded by a Digital Holographic Microscope (DHM) with a 10$\\mathrm{x}$\nmicroscope objective from a patterned target moving in 3D over an axial range\nof 92 $\\mu$m. Tiny DL models are proposed and compared such as a tiny Vision\nTransformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The\nexperiments show that the predicted focusing distance $Z_R^{\\mathrm{Pred}}$ is\naccurately inferred with an accuracy of 1.2 $\\mu$m in average in comparison\nwith the DHM depth of field of 15 $\\mu$m. Numerical simulations show that all\ntiny models give the $Z_R^{\\mathrm{Pred}}$ with an error below 0.3 $\\mu$m. Such\na prospect would significantly improve the current capabilities of computer\nvision position sensing in applications such as 3D microscopy for life sciences\nor micro-robotics. Moreover, all models reach state of the art inference time\non CPU, less than 25 ms per inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andr&#xe9;oli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andre_A/0/1/0/all/0/1\">Antoine N. Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandoz_P/0/1/0/all/0/1\">Patrick Sandoz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laurent_G/0/1/0/all/0/1\">Guillaume J. Laurent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. (arXiv:2203.09418v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09418","description":"<p>Establishing correspondences from image to 3D has been a key task of 6DoF\nobject pose estimation for a long time. To predict pose more accurately, deeply\nlearned dense maps replaced sparse templates. Dense methods also improved pose\nestimation in the presence of occlusion. More recently researchers have shown\nimprovements by learning object fragments as segmentation. In this work, we\npresent a discrete descriptor, which can represent the object surface densely.\nBy incorporating a hierarchical binary grouping, we can encode the object\nsurface very efficiently. Moreover, we propose a coarse to fine training\nstrategy, which enables fine-grained correspondence prediction. Finally, by\nmatching predicted codes with object surface and using a PnP solver, we\nestimate the 6DoF pose. Results on the public LM-O and YCB-V datasets show\nmajor improvement over the state of the art w.r.t. ADD(-S) metric, even\nsurpassing RGB-D based methods in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yongzhi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mahdi Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzer_T/0/1/0/all/0/1\">Torben Fetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer. (arXiv:2203.10062v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10062","description":"<p>MET is a proto-oncogene whose somatic activation in non-small cell lung\ncancer leads to increased cell growth and tumor progression. The two major\nclasses of MET alterations are gene amplification and exon 14 deletion, both of\nwhich are therapeutic targets and detectable using existing molecular assays.\nHowever, existing tests are limited by their consumption of valuable tissue,\ncost and complexity that prevent widespread use. MET alterations could have an\neffect on cell morphology, and quantifying these associations could open new\navenues for research and development of morphology-based screening tools. Using\nH&amp;E-stained whole slide images (WSIs), we investigated the association of\ndistinct cell-morphological features with MET amplifications and MET exon 14\ndeletions. We found that cell shape, color, grayscale intensity and\ntexture-based features from both tumor infiltrating lymphocytes and tumor cells\ndistinguished MET wild-type from MET amplified or MET exon 14 deletion cases.\nThe association of individual cell features with MET alterations suggested a\npredictive model could distinguish MET wild-type from MET amplification or MET\nexon 14 deletion. We therefore developed an L1-penalized logistic regression\nmodel, achieving a mean Area Under the Receiver Operating Characteristic Curve\n(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent\nholdout test set. A sparse set of 43 features differentiated these classes,\nwhich included features similar to what was found in the univariate analysis as\nwell as the percent of tumor cells in the tissue. Our study demonstrates that\nMET alterations result in a detectable morphological signal in tumor cells and\nlymphocytes. These results suggest that development of low-cost predictive\nmodels based on H&amp;E-stained WSIs may improve screening for MET altered tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rohan P. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Boles&#x142;aw L. Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beig_N/0/1/0/all/0/1\">Niha Beig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lingdao Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingale_K/0/1/0/all/0/1\">Kshitij Ingale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking. (arXiv:2203.10541v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10541","description":"<p>Previous advances in object tracking mostly reported on favorable\nillumination circumstances while neglecting performance at nighttime, which\nsignificantly impeded the development of related aerial robot applications.\nThis work instead develops a novel unsupervised domain adaptation framework for\nnighttime aerial tracking (named UDAT). Specifically, a unique object discovery\napproach is provided to generate training patches from raw nighttime tracking\nvideos. To tackle the domain discrepancy, we employ a Transformer-based\nbridging layer post to the feature extractor to align image features from both\ndomains. With a Transformer day/night feature discriminator, the daytime\ntracking model is adversarially trained to track at night. Moreover, we\nconstruct a pioneering benchmark namely NAT2021 for unsupervised domain\nadaptive nighttime tracking, which comprises a test set of 180 manually\nannotated tracking sequences and a train set of over 276k unlabelled nighttime\ntracking frames. Exhaustive experiments demonstrate the robustness and domain\nadaptability of the proposed framework in nighttime aerial tracking. The code\nand benchmark are available at https://github.com/vision4robotics/UDAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization. (arXiv:2203.11471v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11471","description":"<p>In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute\nhuman pose estimation with calibrated camera. Accurate and generalizable\nabsolute 3D human pose estimation from monocular 2D pose input is an ill-posed\nproblem. To address this challenge, we convert the input from pixel space to 3D\nnormalized rays. This conversion makes our approach robust to camera intrinsic\nparameter changes. To deal with the in-the-wild camera extrinsic parameter\nvariations, Ray3D explicitly takes the camera extrinsic parameters as an input\nand jointly models the distribution between the 3D pose rays and camera\nextrinsic parameters. This novel network design is the key to the outstanding\ngeneralizability of Ray3D approach. To have a comprehensive understanding of\nhow the camera intrinsic and extrinsic parameter variations affect the accuracy\nof absolute 3D key-point localization, we conduct in-depth systematic\nexperiments on three single person 3D benchmarks as well as one synthetic\nbenchmark. These experiments demonstrate that our method significantly\noutperforms existing state-of-the-art models. Our code and the synthetic\ndataset are available at https://github.com/YxZhxn/Ray3D .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenghai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Renliang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Wongun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images. (arXiv:2203.11490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11490","description":"<p>Skin cancer is one of the most common types of malignancy, affecting a large\npopulation and causing a heavy economic burden worldwide. Over the last few\nyears, computer-aided diagnosis has been rapidly developed and make great\nprogress in healthcare and medical practices due to the advances in artificial\nintelligence. However, most studies in skin cancer detection keep pursuing high\nprediction accuracies without considering the limitation of computing resources\non portable devices. In this case, knowledge distillation (KD) has been proven\nas an efficient tool to help improve the adaptability of lightweight models\nunder limited resources, meanwhile keeping a high-level representation\ncapability. To bridge the gap, this study specifically proposes a novel method,\ntermed SSD-KD, that unifies diverse knowledge into a generic KD framework for\nskin diseases classification. Our method models an intra-instance relational\nfeature representation and integrates it with existing KD research. A dual\nrelational knowledge distillation architecture is self-supervisedly trained\nwhile the weighted softened outputs are also exploited to enable the student\nmodel to capture richer knowledge from the teacher model. To demonstrate the\neffectiveness of our method, we conduct experiments on ISIC 2019, a large-scale\nopen-accessed benchmark of skin diseases dermoscopic images. Experiments show\nthat our distilled lightweight model can achieve an accuracy as high as 85% for\nthe classification tasks of 8 different skin diseases with minimal parameters\nand computing requirements. Ablation studies confirm the effectiveness of our\nintra- and inter-instance relational knowledge integration strategy. Compared\nwith state-of-the-art knowledge distillation techniques, the proposed method\ndemonstrates improved performances for multi-diseases classification on the\nlarge-scale dermoscopy database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tim K. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Emotion Recognition using Visual-audio-linguistic information: A Technical Report for ABAW3. (arXiv:2203.13031v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.13031","description":"<p>We propose a cross-modal co-attention model for continuous emotion\nrecognition using visual-audio-linguistic information. The model consists of\nfour blocks. The visual, audio, and linguistic blocks are used to learn the\nspatial-temporal features of the multi-modal input. A co-attention block is\ndesigned to fuse the learned features with the multi-head co-attention\nmechanism. The visual encoding from the visual block is concatenated with the\nattention feature to emphasize the visual information. To make full use of the\ndata and alleviate over-fitting, cross-validation is carried out on the\ntraining and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. The achieved CCC on the\ntest set is $0.520$ for valence and $0.602$ for arousal, which significantly\noutperforms the baseline method with the corresponding CCC of 0.180 and 0.170\nfor valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1\">Ruyi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Monocular Depth Estimation Provide Better Pre-training than Classification for Semantic Segmentation?. (arXiv:2203.13987v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13987","description":"<p>Training a deep neural network for semantic segmentation is labor-intensive,\nso it is common to pre-train it for a different task, and then fine-tune it\nwith a small annotated dataset. State-of-the-art methods use image\nclassification for pre-training, which introduces uncontrolled biases. We test\nthe hypothesis that depth estimation from unlabeled videos may provide better\npre-training. Despite the absence of any semantic information, we argue that\nestimating scene geometry is closer to the task of semantic segmentation than\nclassifying whole images into semantic classes. Since analytical validation is\nintractable, we test the hypothesis empirically by introducing a pre-training\nscheme that yields an improvement of 5.7% mIoU and 4.1% pixel accuracy over\nclassification-based pre-training. While annotation is not needed for\npre-training, it is needed for testing the hypothesis. We use the KITTI\n(outdoor) and NYU-V2 (indoor) benchmarks to that end, and provide an extensive\ndiscussion of the benefits and limitations of the proposed scheme in relation\nto existing unsupervised, self-supervised, and semi-supervised pre-training\nprotocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1\">Dong Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds. (arXiv:2203.14048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14048","description":"<p>Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape\nrepresentation. To capture geometry details, current methods usually learn DIF\nusing local latent codes, which discretize the space into a regular 3D grid (or\noctree) and store local codes in grid points (or octree nodes). Given a query\npoint, the local feature is computed by interpolating its neighboring local\ncodes with their positions. However, the local codes are constrained at\ndiscrete and regular positions like grid points, which makes the code positions\ndifficult to be optimized and limits their representation ability. To solve\nthis problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF.\nOur method explicitly associates local codes with learnable position vectors,\nand the position vectors are continuous and can be dynamically optimized, which\nimproves the representation ability. In addition, we propose a novel code\nposition loss to optimize the code positions, which heuristically guides more\nlocal codes to be distributed around complex geometric details. In contrast to\nprevious methods, our DCC-DIF represents 3D shapes more efficiently with a\nsmall amount of local codes, and improves the reconstruction quality.\nExperiments demonstrate that DCC-DIF achieves better performance over previous\nmethods. Code and data are available at https://github.com/lity20/DCCDIF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hua Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Thermal Screening for COVID-19 using Machine Learning. (arXiv:2203.14128v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14128","description":"<p>In the last two years, millions of lives have been lost due to COVID-19.\nDespite the vaccination programmes for a year, hospitalization rates and deaths\nare still high due to the new variants of COVID-19. Stringent guidelines and\nCOVID-19 screening measures such as temperature check and mask check at all\npublic places are helping reduce the spread of COVID-19. Visual inspections to\nensure these screening measures can be taxing and erroneous. Automated\ninspection ensures an effective and accurate screening. Traditional approaches\ninvolve identification of faces and masks from visual camera images followed by\nextraction of temperature values from thermal imaging cameras. Use of visual\nimaging as a primary modality limits these applications only for good-lighting\nconditions. The use of thermal imaging alone for these screening measures makes\nthe system invariant to illumination. However, lack of open source datasets is\nan issue to develop such systems. In this paper, we discuss our work on using\nmachine learning over thermal video streams for face and mask detection and\nsubsequent temperature screening in a passive non-invasive way that enables an\neffective automated COVID-19 screening method in public places. We open source\nour NTIC dataset that was used for training our models and was collected at 8\ndifferent locations. Our results show that the use of thermal imaging is as\neffective as visual imaging in the presence of high illumination. This\nperformance stays the same for thermal images even under low-lighting\nconditions, whereas the performance with visual trained classifiers show more\nthan 50% degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katte_P/0/1/0/all/0/1\">Pratik Katte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakileti_S/0/1/0/all/0/1\">Siva Teja Kakileti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhu_H/0/1/0/all/0/1\">Himanshu J. Madhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_G/0/1/0/all/0/1\">Geetha Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Stream Multi-Level Alignment for Vision-Language Pretraining. (arXiv:2203.14395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14395","description":"<p>Recent progress in large-scale vision-language pre-training has shown the\nimportance of aligning the visual and text modalities for downstream\nvision-language tasks. Many methods use a dual-stream architecture that fuses\nvisual tokens and language tokens after representation learning, which aligns\nonly at a global level and cannot extract finer-scale semantics. In contrast,\nwe propose a single stream model that aligns the modalities at multiple levels:\ni) instance level, ii) fine-grained patch level, iii) conceptual semantic\nlevel. We achieve this using two novel tasks: symmetric cross-modality\nreconstruction and a pseudo-labeled key word prediction. In the former part, we\nmask the input tokens from one of the modalities and use the cross-modal\ninformation to reconstruct the masked token, thus improving fine-grained\nalignment between the two modalities. In the latter part, we parse the caption\nto select a few key words and feed it together with the momentum encoder pseudo\nsignal to self-supervise the visual encoder, enforcing it to learn rich\nsemantic concepts that are essential for grounding a textual token to an image\nregion. We demonstrate top performance on a set of Vision-Language downstream\ntasks such as zero-shot/fine-tuned image/text retrieval, referring expression,\nand VQA. We also demonstrate how the proposed models can align the modalities\nat multiple levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BG_V/0/1/0/all/0/1\">Vijay Kumar BG</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1\">Samuel Schulter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectFormer for Image Manipulation Detection and Localization. (arXiv:2203.14681v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14681","description":"<p>Recent advances in image editing techniques have posed serious challenges to\nthe trustworthiness of multimedia data, which drives the research of image\ntampering detection. In this paper, we propose ObjectFormer to detect and\nlocalize image manipulations. To capture subtle manipulation traces that are no\nlonger visible in the RGB domain, we extract high-frequency features of the\nimages and combine them with RGB features as multimodal patch embeddings.\nAdditionally, we use a set of learnable object prototypes as mid-level\nrepresentations to model the object-level consistencies among different\nregions, which are further used to refine patch embeddings to capture the\npatch-level consistencies. We conduct extensive experiments on various datasets\nand the results verify the effectiveness of the proposed method, outperforming\nstate-of-the-art tampering detection and localization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xintong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WSEBP: A Novel Width-depth Synchronous Extension-based Basis Pursuit Algorithm for Multi-Layer Convolutional Sparse Coding. (arXiv:2203.14856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14856","description":"<p>The pursuit algorithms integrated in multi-layer convolutional sparse coding\n(ML-CSC) can interpret the convolutional neural networks (CNNs). However, many\ncurrent state-of-art (SOTA) pursuit algorithms require multiple iterations to\noptimize the solution of ML-CSC, which limits their applications to deeper CNNs\ndue to high computational cost and large number of resources for getting very\ntiny gain of performance. In this study, we focus on the 0th iteration in\npursuit algorithm by introducing an effective initialization strategy for each\nlayer, by which the solution for ML-CSC can be improved. Specifically, we first\npropose a novel width-depth synchronous extension-based basis pursuit (WSEBP)\nalgorithm which solves the ML-CSC problem without the limitation of the number\nof iterations compared to the SOTA algorithms and maximizes the performance by\nan effective initialization in each layer. Then, we propose a simple and\nunified ML-CSC-based classification network (ML-CSC-Net) which consists of an\nML-CSC-based feature encoder and a fully-connected layer to validate the\nperformance of WSEBP on image classification task. The experimental results\nshow that our proposed WSEBP outperforms SOTA algorithms in terms of accuracy\nand consumption resources. In addition, the WSEBP integrated in CNNs can\nimprove the performance of deeper CNNs and make them interpretable. Finally,\ntaking VGG as an example, we propose WSEBP-VGG13 to enhance the performance of\nVGG13, which achieves competitive results on four public datasets, i.e., 87.79%\nvs. 86.83% on Cifar-10 dataset, 58.01% vs. 54.60% on Cifar-100 dataset, 91.52%\nvs. 89.58% on COVID-19 dataset, and 99.88% vs. 99.78% on Crack dataset,\nrespectively. The results show the effectiveness of the proposed WSEBP, the\nimproved performance of ML-CSC with WSEBP, and interpretation of the CNNs or\ndeeper CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1\">Lingbin Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15216","description":"<p>Affine registration is indispensable in a comprehensive medical image\nregistration pipeline. However, only a few studies focus on fast and robust\naffine registration algorithms. Most of these studies utilize convolutional\nneural networks (CNNs) to learn joint affine and non-parametric registration,\nwhile the standalone performance of the affine subnetwork is less explored.\nMoreover, existing CNN-based affine registration approaches focus either on the\nlocal misalignment or the global orientation and position of the input to\npredict the affine transformation matrix, which are sensitive to spatial\ninitialization and exhibit limited generalizability apart from the training\ndataset. In this paper, we present a fast and robust learning-based algorithm,\nCoarse-to-Fine Vision Transformer (C2FViT), for 3D affine medical image\nregistration. Our method naturally leverages the global connectivity and\nlocality of the convolutional vision transformer and the multi-resolution\nstrategy to learn the global affine registration. We evaluate our method on 3D\nbrain atlas registration and template-matching normalization. Comprehensive\nresults demonstrate that our method is superior to the existing CNNs-based\naffine registration methods in terms of registration accuracy, robustness and\ngeneralizability while preserving the runtime advantage of the learning-based\nmethods. The source code is available at https://github.com/cwmok/C2FViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15221","description":"<p>Recently, transformer-based methods have achieved promising progresses in\nobject detection, as they can eliminate the post-processes like NMS and enrich\nthe deep representations. However, these methods cannot well cope with scene\ntext due to its extreme variance of scales and aspect ratios. In this paper, we\npresent a simple yet effective transformer-based architecture for scene text\ndetection. Different from previous approaches that learn robust deep\nrepresentations of scene text in a holistic manner, our method performs scene\ntext detection based on a few representative features, which avoids the\ndisturbance by background and reduces the computational cost. Specifically, we\nfirst select a few representative features at all scales that are highly\nrelevant to foreground text. Then, we adopt a transformer for modeling the\nrelationship of the sampled features, which effectively divides them into\nreasonable groups. As each feature group corresponds to a text instance, its\nbounding box can be easily obtained without any post-processing operation.\nUsing the basic feature pyramid network for feature extraction, our method\nconsistently achieves state-of-the-art results on several popular datasets for\nscene text detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingqun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">MingKun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guanglong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15243","description":"<p>In this paper we propose augmenting Vision Transformer models with learnable\nmemory tokens. Our approach allows the model to adapt to new tasks, using few\nparameters, while optionally preserving its capabilities on previously learned\ntasks. At each layer we introduce a set of learnable embedding vectors that\nprovide contextual information useful for specific datasets. We call these\n\"memory tokens\". We show that augmenting a model with just a handful of such\ntokens per layer significantly improves accuracy when compared to conventional\nhead-only fine-tuning, and performs only slightly below the significantly more\nexpensive full fine-tuning. We then propose an attention-masking approach that\nenables extension to new downstream tasks, with a computation reuse. In this\nsetup in addition to being parameters efficient, models can execute both old\nand new tasks as a part of single inference at a small incremental cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_A/0/1/0/all/0/1\">Andrew Jackson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15270","description":"<p>Recent studies have shown the importance of modeling long-range interactions\nin the inpainting problem. To achieve this goal, existing approaches exploit\neither standalone attention techniques or transformers, but usually under a low\nresolution in consideration of computational cost. In this paper, we present a\nnovel transformer-based model for large hole inpainting, which unifies the\nmerits of transformers and convolutions to efficiently process high-resolution\nimages. We carefully design each component of our framework to guarantee the\nhigh fidelity and diversity of recovered images. Specifically, we customize an\ninpainting-oriented transformer block, where the attention module aggregates\nnon-local information only from partial valid tokens, indicated by a dynamic\nmask. Extensive experiments demonstrate the state-of-the-art performance of the\nnew model on multiple benchmark datasets. Code is released at\nhttps://github.com/fenglinglwb/MAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15353","description":"<p>Object detection under imperfect data receives great attention recently.\nWeakly supervised object detection (WSOD) suffers from severe localization\nissues due to the lack of instance-level annotation, while semi-supervised\nobject detection (SSOD) remains challenging led by the inter-image discrepancy\nbetween labeled and unlabeled data. In this study, we propose the Single\nInstance annotated Object Detection (SIOD), requiring only one instance\nannotation for each existing category in an image. Degraded from inter-task\n(WSOD) or inter-image (SSOD) discrepancies to the intra-image discrepancy, SIOD\nprovides more reliable and rich prior knowledge for mining the rest of\nunlabeled instances and trades off the annotation cost and performance. Under\nthe SIOD setting, we propose a simple yet effective framework, termed\nDual-Mining (DMiner), which consists of a Similarity-based Pseudo Label\nGenerating module (SPLG) and a Pixel-level Group Contrastive Learning module\n(PGCL). SPLG firstly mines latent instances from feature representation space\nto alleviate the annotation missing problem. To avoid being misled by\ninaccurate pseudo labels, we propose PGCL to boost the tolerance to false\npseudo labels. Extensive experiments on MS COCO verify the feasibility of the\nSIOD setting and the superiority of the proposed method, which obtains\nconsistent and significant improvements compared to baseline methods and\nachieves comparable results with fully supervised object detection (FSOD)\nmethods with only 40% instances annotated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15355","description":"<p>Learning under a continuously changing data distribution with incorrect\nlabels is a desirable real-world problem yet challenging. A large body of\ncontinual learning (CL) methods, however, assumes data streams with clean\nlabels, and online learning scenarios under noisy data streams are yet\nunderexplored. We consider a more practical CL task setup of an online learning\nfrom blurry data stream with corrupted labels, where existing CL methods\nstruggle. To address the task, we first argue the importance of both diversity\nand purity of examples in the episodic memory of continual learning models. To\nbalance diversity and purity in the episodic memory, we propose a novel\nstrategy to manage and use the memory by a unified approach of label noise\naware diverse sampling and robust learning with semi-supervised learning. Our\nempirical validations on four real-world or synthetic noise datasets (CIFAR10\nand 100, mini-WebVision, and Food-101N) exhibit that our method significantly\noutperforms prior arts in this realistic and challenging continual learning\nscenario. Code and data splits are available in\nhttps://github.com/clovaai/puridiver.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Jihwan Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Hyunseo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seulki Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransductGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.15406","description":"<p>Novelty detection, a widely studied problem in machine learning, is the\nproblem of detecting a novel class of data that has not been previously\nobserved. A common setting for novelty detection is inductive whereby only\nexamples of the negative class are available during training time. Transductive\nnovelty detection on the other hand has only witnessed a recent surge in\ninterest, it not only makes use of the negative class during training but also\nincorporates the (unlabeled) test set to detect novel examples. Several studies\nhave emerged under the transductive setting umbrella that have demonstrated its\nadvantage over its inductive counterpart. Depending on the assumptions about\nthe data, these methods go by different names (e.g. transductive novelty\ndetection, semi-supervised novelty detection, positive-unlabeled learning,\nout-of-distribution detection). With the use of generative adversarial networks\n(GAN), a segment of those studies have adopted a transductive setup in order to\nlearn how to generate examples of the novel class. In this study, we propose\nTransductGAN, a transductive generative adversarial network that attempts to\nlearn how to generate image examples from both the novel and negative classes\nby using a mixture of two Gaussians in the latent space. It achieves that by\nincorporating an adversarial autoencoder with a GAN network, the ability to\ngenerate examples of novel data points offers not only a visual representation\nof novelties, but also overcomes the hurdle faced by many inductive methods of\nhow to tune the model hyperparameters at the decision rule level. Our model has\nshown superior performance over state-of-the-art inductive and transductive\nmethods. Our study is fully reproducible with the code available publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toron_N/0/1/0/all/0/1\">Najiba Toron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourao_Miranda_J/0/1/0/all/0/1\">Janaina Mourao-Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15533","description":"<p>We present a novel one-shot method for object detection and 6 DoF pose\nestimation, that does not require training on target objects. At test time, it\ntakes as input a target image and a textured 3D query model. The core idea is\nto represent a 3D model with a number of 2D templates rendered from different\nviewpoints. This enables CNN-based direct dense feature extraction and\nmatching. The object is first localized in 2D, then its approximate viewpoint\nis estimated, followed by dense 2D-3D correspondence prediction. The final pose\nis computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed,\nYCB-V and TLESS datasets and report very competitive performance in comparison\nto the state-of-the-art methods trained on synthetic data, even though our\nmethod is not trained on the object models used for testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shugurov_I/0/1/0/all/0/1\">Ivan Shugurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15547","description":"<p>Convolutional Neural Networks need the construction of informative features,\nwhich are determined by channel-wise and spatial-wise information at the\nnetwork's layers. In this research, we focus on bringing in a novel solution\nthat uses sophisticated optimization for enhancing both the spatial and channel\ncomponents inside each layer's receptive field. Capsule Networks were used to\nunderstand the spatial association between features in the feature map.\nStandalone capsule networks have shown good results on comparatively simple\ndatasets than on complex datasets as a result of the inordinate amount of\nfeature information. Thus, to tackle this issue, we have proposed ME-CapsNet by\nintroducing deeper convolutional layers to extract important features before\npassing through modules of capsule layers strategically to improve the\nperformance of the network significantly. The deeper convolutional layer\nincludes blocks of Squeeze-Excitation networks which uses a soft-pooling\napproach for progressively reducing the spatial size thereby dynamically\nrecalibrating the channels by reconstructing their interdependencies without\nmuch loss of important feature information. Extensive experimentation was done\nusing commonly used datasets demonstrating the efficiency of the proposed\nME-CapsNet, which clearly outperforms various research works by achieving\nhigher accuracy with minimal model complexity in complex datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1\">Jerrin Bright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_S/0/1/0/all/0/1\">Suryaprakash Rajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_A/0/1/0/all/0/1\">Arockia Selvakumar Arockia Doss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}