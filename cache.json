{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Reducing the Need for Speech Training Data To Build Spoken Language Understanding Systems. (arXiv:2203.00006v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00006","description":"<p>The lack of speech data annotated with labels required for spoken language\nunderstanding (SLU) is often a major hurdle in building end-to-end (E2E)\nsystems that can directly process speech inputs. In contrast, large amounts of\ntext data with suitable labels are usually available. In this paper, we propose\na novel text representation and training methodology that allows E2E SLU\nsystems to be effectively constructed using these text resources. With very\nlimited amounts of additional speech, we show that these models can be further\nimproved to perform at levels close to similar systems built on the full speech\ndatasets. The efficacy of our proposed approach is demonstrated on both intent\nand entity tasks using three different SLU datasets. With text-only training,\nthe proposed system achieves up to 90% of the performance possible with full\nspeech training. With just an additional 10% of speech data, these models\nsignificantly improve further to 97% of full performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LISA: Learning Interpretable Skill Abstractions from Language. (arXiv:2203.00054v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00054","description":"<p>Learning policies that effectually utilize language instructions in complex,\nmulti-task environments is an important problem in imitation learning. While it\nis possible to condition on the entire language instruction directly, such an\napproach could suffer from generalization issues. To encode complex\ninstructions into skills that can generalize to unseen instructions, we propose\nLearning Interpretable Skill Abstractions (LISA), a hierarchical imitation\nlearning framework that can learn diverse, interpretable skills from\nlanguage-conditioned demonstrations. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA is able to outperform a strong non-hierarchical baseline in\nthe low data regime and compose learned skills to solve tasks containing unseen\nlong-range instructions. Our method demonstrates a more natural way to\ncondition on language in sequential decision-making problems and achieve\ninterpretable and controllable behavior with the learned skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Divyansh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidyanath_S/0/1/0/all/0/1\">Skanda Vaidyanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuno Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Explanations in Out-of-Domain Settings. (arXiv:2203.00056v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00056","description":"<p>Recent work in Natural Language Processing has focused on developing\napproaches that extract faithful explanations, either via identifying the most\nimportant tokens in the input (i.e. post-hoc explanations) or by designing\ninherently faithful models that first select the most important tokens and then\nuse them to predict the correct label (i.e. select-then-predict models).\nCurrently, these approaches are largely evaluated on in-domain settings. Yet,\nlittle is known about how post-hoc explanations and inherently faithful models\nperform in out-of-domain settings. In this paper, we conduct an extensive\nempirical study that examines: (1) the out-of-domain faithfulness of post-hoc\nexplanations, generated by five feature attribution methods; and (2) the\nout-of-domain performance of two inherently faithful models over six datasets.\nContrary to our expectations, results show that in many cases out-of-domain\npost-hoc explanation faithfulness measured by sufficiency and comprehensiveness\nis higher compared to in-domain. We find this misleading and suggest using a\nrandom baseline as a yardstick for evaluating post-hoc explanation\nfaithfulness. Our findings also show that select-then predict models\ndemonstrate comparable predictive performance in out-of-domain settings to\nfull-text trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Choosing on Sequences. (arXiv:2203.00070v1 [econ.TH])","link":"http://arxiv.org/abs/2203.00070","description":"<p>The standard economic model of choice assumes that a decision maker chooses\nfrom sets of alternatives. A new branch of literature has considered the\nproblem of choosing from lists i.e. ordered sets. In this paper, we propose a\nnew framework that considers choice from infinite sequences. Our framework\nprovides a natural way to model decision making in settings where choice relies\non a string of recommendations. We introduce three broad classes of choice\nrules in this framework. Our main result shows that bounded attention is due to\nthe continuity of the choice functions with respect to a natural topology. We\nintroduce some natural choice rules in this framework and provide their\naxiomatic characterizations. Finally, we introduce the notion of computability\nof a choice function using Turing machines and show that computable choice\nrules can be implemented by a finite automaton.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Bhardwaj_B/0/1/0/all/0/1\">Bhavook Bhardwaj</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Chatterjee_S/0/1/0/all/0/1\">Siddharth Chatterjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure Extraction in Task-Oriented Dialogues with Slot Clustering. (arXiv:2203.00073v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00073","description":"<p>Extracting structure information from dialogue data can help us better\nunderstand user and system behaviors. In task-oriented dialogues, dialogue\nstructure has often been considered as transition graphs among dialogue states.\nHowever, annotating dialogue states manually is expensive and time-consuming.\nIn this paper, we propose a simple yet effective approach for structure\nextraction in task-oriented dialogues. We first detect and cluster possible\nslot tokens with a pre-trained model to approximate dialogue ontology for a\ntarget domain. Then we track the status of each identified token group and\nderive a state transition structure. Empirical results show that our approach\noutperforms unsupervised baseline models by far in dialogue structure\nextraction. In addition, we show that data augmentation based on extracted\nstructures enriches the surface formats of training data and can achieve a\nsignificant performance boost in dialogue response generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing. (arXiv:2203.00130v1 [cs.HC])","link":"http://arxiv.org/abs/2203.00130","description":"<p>When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1\">Tal August</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1\">Andrew Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Sentence Composition Reasoning for Multi-Hop Question Answering. (arXiv:2203.00160v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00160","description":"<p>Due to the lack of insufficient data, existing multi-hop open domain question\nanswering systems require to effectively find out relevant supporting facts\naccording to each question. To alleviate the challenges of semantic factual\nsentences retrieval and multi-hop context expansion, we present a semantic\nsentence composition reasoning approach for a multi-hop question answering\ntask, which consists of two key modules: a multi-stage semantic matching module\n(MSSM) and a factual sentence composition module (FSC). With the combination of\nfactual sentences and multi-stage semantic retrieval, our approach can provide\nmore comprehensive contextual information for model training and reasoning.\nExperimental results demonstrate our model is able to incorporate existing\npre-trained language models and outperform the existing SOTA method on the QASC\ntask with an improvement of about 9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformers use variable binding?. (arXiv:2203.00162v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00162","description":"<p>Increasing the explainability of deep neural networks (DNNs) requires\nevaluating whether they implement symbolic computation. One central symbolic\ncapacity is variable binding: linking an input value to an abstract variable\nheld in system-internal memory. Prior work on the computational abilities of\nDNNs has not resolved the question of whether their internal processes involve\nvariable binding. We argue that the reason for this is fundamental, inherent in\nthe way experiments in prior work were designed. We provide the first\nsystematic evaluation of the variable binding capacities of the\nstate-of-the-art Transformer networks BERT and RoBERTa. Our experiments are\ndesigned such that the model must generalize a rule across disjoint subsets of\nthe input vocabulary, and cannot rely on associative pattern matching alone.\nThe results show a clear discrepancy between classification and\nsequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or\nreverse strings even when trained on task-specific vocabularies that are\nswitched in the test set; but both models completely fail to generalize across\nvocabularies in similar sequence classification tasks. These findings indicate\nthat the effectiveness of Transformers in sequence modelling may lie in their\nextensive use of the input itself as an external \"memory\" rather than\nnetwork-internal symbolic operations involving variable binding. Therefore, we\npropose a novel direction for future work: augmenting the inputs available to\ncircumvent the lack of network-internal variable binding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grondahl_T/0/1/0/all/0/1\">Tommi Gr&#xf6;ndahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPPAC: Entity Pre-typing Relation Classification with Prompt AnswerCentralizing. (arXiv:2203.00193v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00193","description":"<p>Relation classification (RC) aims to predict the relationship between a pair\nof subject and object in a given context. Recently, prompt tuning approaches\nhave achieved high performance in RC. However, existing prompt tuning\napproaches have the following issues: (1) numerous categories decrease RC\nperformance; (2) manually designed prompts require intensive labor. To address\nthese issues, a novel paradigm, Entity Pre-typing Relation Classification with\nPrompt Answer Centralizing(EPPAC) is proposed in this paper. The entity\npre-tying in EPPAC is presented to address the first issue using a double-level\nframework that pre-types entities before RC and prompt answer centralizing is\nproposed to address the second issue. Extensive experiments show that our\nproposed EPPAC outperformed state-of-the-art approaches on TACRED and TACREV by\n14.4% and 11.1%, respectively. The code is provided in the Supplementary\nMaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jiejun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">WeiWei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RMBR: A Regularized Minimum Bayes Risk Reranking Framework for Machine Translation. (arXiv:2203.00201v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00201","description":"<p>Beam search is the most widely used decoding method for neural machine\ntranslation (NMT). In practice, the top-1 candidate with the highest\nlog-probability among the n candidates is selected as the preferred one.\nHowever, this top-1 candidate may not be the best overall translation among the\nn-best list. Recently, Minimum Bayes Risk (MBR) decoding has been proposed to\nimprove the quality for NMT, which seeks for a consensus translation that is\nclosest on average to other candidates from the n-best list. We argue that MBR\nstill suffers from the following problems: The utility function only considers\nthe lexical-level similarity between candidates; The expected utility considers\nthe entire n-best list which is time-consuming and inadequate candidates in the\ntail list may hurt the performance; Only the relationship between candidates is\nconsidered. To solve these issues, we design a regularized MBR reranking\nframework (RMBR), which considers semantic-based similarity and computes the\nexpected utility for each candidate by truncating the list. We expect the\nproposed framework to further consider the translation quality and model\nuncertainty of each candidate. Thus the proposed quality regularizer and\nuncertainty regularizer are incorporated into the framework. Extensive\nexperiments on multiple translation tasks demonstrate the effectiveness of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yidan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings. (arXiv:2203.00211v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00211","description":"<p>In order to equip NLP systems with selective prediction capability, several\ntask-specific approaches have been proposed. However, which approaches work\nbest across tasks or even if they consistently outperform the simplest baseline\n'MaxProb' remains to be explored. To this end, we systematically study\n'selective prediction' in a large-scale setup of 17 datasets across several NLP\ntasks. Through comprehensive experiments under in-domain (IID), out-of-domain\n(OOD), and adversarial (ADV) settings, we show that despite leveraging\nadditional resources (held-out data/computation), none of the existing\napproaches consistently and considerably outperforms MaxProb in all three\nsettings. Furthermore, their performance does not translate well across tasks.\nFor instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate\nDetection datasets but does not fare well on NLI datasets, especially in the\nOOD setting. Thus, we recommend that future selective prediction approaches\nshould be evaluated across tasks and settings for reliable estimation of their\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR. (arXiv:2203.00232v1 [cs.SD])","link":"http://arxiv.org/abs/2203.00232","description":"<p>Graph-based temporal classification (GTC), a generalized form of the\nconnectionist temporal classification loss, was recently proposed to improve\nautomatic speech recognition (ASR) systems using graph-based supervision. For\nexample, GTC was first used to encode an N-best list of pseudo-label sequences\ninto a graph for semi-supervised learning. In this paper, we propose an\nextension of GTC to model the posteriors of both labels and label transitions\nby a neural network, which can be applied to a wider range of tasks. As an\nexample application, we use the extended GTC (GTC-e) for the multi-speaker\nspeech recognition task. The transcriptions and speaker information of\nmulti-speaker speech are represented by a graph, where the speaker information\nis associated with the transitions and ASR outputs with the nodes. Using GTC-e,\nmulti-speaker ASR modelling becomes very similar to single-speaker ASR\nmodeling, in that tokens by multiple speakers are recognized as a single merged\nsequence in chronological order. For evaluation, we perform experiments on a\nsimulated multi-speaker speech dataset derived from LibriSpeech, obtaining\npromising results with performance close to classical benchmarks for the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRILLsson: Distilled Universal Paralinguistic Speech Representations. (arXiv:2203.00236v1 [eess.AS])","link":"http://arxiv.org/abs/2203.00236","description":"<p>Recent advances in self-supervision have dramatically improved the quality of\nspeech representations. However, deployment of state-of-the-art embedding\nmodels on devices has been restricted due to their limited public availability\nand large resource footprint. Our work addresses these issues by publicly\nreleasing a collection of paralinguistic speech models that are small and near\nstate-of-the-art performance. Our approach is based on knowledge distillation,\nand our models are distilled on public data only. We explore different\narchitectures and thoroughly evaluate our models on the Non-Semantic Speech\n(NOSS) benchmark. Our largest distilled model is less than 15% the size of the\noriginal model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7\ntasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)\nand achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the\nopen source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model\noutperforms the open source Wav2Vec 2.0 on both emotion recognition tasks\ndespite being 7% the size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment. (arXiv:2203.00242v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00242","description":"<p>Vision-and-Language (V+L) pre-training models have achieved tremendous\nsuccess in recent years on various multi-modal benchmarks. However, the\nmajority of existing models require pre-training on a large set of parallel\nimage-text data, which is costly to collect, compared to image-only or\ntext-only data. In this paper, we explore unsupervised Vision-and-Language\npre-training (UVLP) to learn the cross-modal representation from non-parallel\nimage and text datasets. We found two key factors that lead to good\nunsupervised V+L pre-training without parallel data: (i) joint image-and-text\ninput (ii) overall image-text alignment (even for non-parallel data).\nAccordingly, we propose a novel unsupervised V+L pre-training curriculum for\nnon-parallel texts and images. We first construct a weakly aligned image-text\ncorpus via a retrieval-based approach, then apply a set of multi-granular\nalignment pre-training tasks, including region-to-tag, region-to-phrase, and\nimage-to-sentence alignment, to bridge the gap between the two modalities. A\ncomprehensive ablation study shows each granularity is helpful to learn a\nstronger pre-trained model. We adapt our pre-trained model to a set of V+L\ndownstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our\nmodel achieves the state-of-art performance in all these tasks under the\nunsupervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring and Adapting Chinese GPT to Pinyin Input Method. (arXiv:2203.00249v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00249","description":"<p>While GPT has become the de-facto method for text generation tasks, its\napplication to pinyin input method remains unexplored. In this work, we make\nthe first exploration to leverage Chinese GPT for pinyin input method. We find\nthat a frozen GPT achieves state-of-the-art performance on perfect pinyin.\nHowever, the performance drops dramatically when the input includes abbreviated\npinyin. A reason is that an abbreviated pinyin can be mapped to many perfect\npinyin, which links to even larger number of Chinese characters. We mitigate\nthis issue with two strategies, including enriching the context with pinyin and\noptimizing the training process to help distinguish homophones. To further\nfacilitate the evaluation of pinyin input method, we create a dataset\nconsisting of 270K instances from 15 domains. Results show that our approach\nimproves performance on abbreviated pinyin across all domains. Model analysis\ndemonstrates that both strategies contribute to the performance boost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs. (arXiv:2203.00255v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00255","description":"<p>Question answering over temporal knowledge graphs (KGs) efficiently uses\nfacts contained in a temporal KG, which records entity relations and when they\noccur in time, to answer natural language questions (e.g., \"Who was the\npresident of the US before Obama?\"). These questions often involve three\ntime-related challenges that previous work fail to adequately address: 1)\nquestions often do not specify exact timestamps of interest (e.g., \"Obama\"\ninstead of 2000); 2) subtle lexical differences in time relations (e.g.,\n\"before\" vs \"after\"); 3) off-the-shelf temporal KG embeddings that previous\nwork builds on ignore the temporal order of timestamps, which is crucial for\nanswering temporal-order related questions. In this paper, we propose a\ntime-sensitive question answering (TSQA) framework to tackle these problems.\nTSQA features a timestamp estimation module to infer the unwritten timestamp\nfrom the question. We also employ a time-sensitive KG encoder to inject\nordering information into the temporal KG embeddings that TSQA is based on.\nWith the help of techniques to reduce the search space for potential answers,\nTSQA significantly outperforms the previous state of the art on a new benchmark\nfor question answering over temporal KGs, especially achieving a 32% (absolute)\nerror reduction on complex questions that require multiple steps of reasoning\nover facts in the temporal KG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1\">Chao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors. (arXiv:2203.00257v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00257","description":"<p>Multimodal sentiment analysis has attracted increasing attention and lots of\nmodels have been proposed. However, the performance of the state-of-the-art\nmodels decreases sharply when they are deployed in the real world. We find that\nthe main reason is that real-world applications can only access the text\noutputs by the automatic speech recognition (ASR) models, which may be with\nerrors because of the limitation of model capacity. Through further analysis of\nthe ASR outputs, we find that in some cases the sentiment words, the key\nsentiment elements in the textual modality, are recognized as other words,\nwhich makes the sentiment of the text change and hurts the performance of\nmultimodal sentiment models directly. To address this problem, we propose the\nsentiment word aware multimodal refinement model (SWRM), which can dynamically\nrefine the erroneous sentiment words by leveraging multimodal sentiment clues.\nSpecifically, we first use the sentiment word position detection module to\nobtain the most possible position of the sentiment word in the text and then\nutilize the multimodal sentiment word refinement module to dynamically refine\nthe sentiment word embeddings. The refined embeddings are taken as the textual\ninputs of the multimodal feature fusion module to predict the sentiment labels.\nWe conduct extensive experiments on the real-world datasets including\nMOSI-Speechbrain, MOSI-IBM, and MOSI-iFlytek and the results demonstrate the\neffectiveness of our model, which surpasses the current state-of-the-art models\non three datasets. Furthermore, our approach can be adapted for other\nmultimodal feature fusion models easily. Data and code are available at\nhttps://github.com/albertwy/SWRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaohuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArabGend: Gender Analysis and Inference on Arabic Twitter. (arXiv:2203.00271v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00271","description":"<p>Gender analysis of Twitter can reveal important socio-cultural differences\nbetween male and female users. There has been a significant effort to analyze\nand automatically infer gender in the past for most widely spoken languages'\ncontent, however, to our knowledge very limited work has been done for Arabic.\nIn this paper, we perform an extensive analysis of differences between male and\nfemale users on the Arabic Twitter-sphere. We study differences in user\nengagement, topics of interest, and the gender gap in professions. Along with\ngender analysis, we also propose a method to infer gender by utilizing\nusernames, profile pictures, tweets, and networks of friends. In order to do\nso, we manually annotated gender and locations for ~166K Twitter accounts\nassociated with ~92K user location, which we plan to make publicly available at\n<a href=\"http://anonymous.com.\">this http URL</a> Our proposed gender inference method achieve an F1 score\nof 82.1%, which is 47.3% higher than majority baseline. In addition, we also\ndeveloped a demo and made it publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding. (arXiv:2203.00274v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00274","description":"<p>Understanding tables is an important aspect of natural language\nunderstanding. Existing models for table understanding require linearization of\nthe table structure, where row or column order is encoded as an unwanted bias.\nSuch spurious biases make the model vulnerable to row and column order\nperturbations. Additionally, prior work has not thoroughly modeled the table\nstructures or table-text alignments, hindering the table-text understanding\nability. In this work, we propose a robust and structurally aware table-text\nencoding architecture TableFormer, where tabular structural biases are\nincorporated completely through learnable attention biases. TableFormer is (1)\nstrictly invariant to row and column orders, and, (2) could understand tables\nbetter due to its tabular inductive biases. Our evaluations showed that\nTableFormer outperforms strong baselines in all settings on SQA, WTQ and\nTabFact table reasoning datasets, and achieves state-of-the-art performance on\nSQA, especially when facing answer-invariant row and column order perturbations\n(6% improvement over the best baseline), because previous SOTA models'\nperformance drops by 4% - 6% when facing such perturbations while TableFormer\nis not affected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shachi Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. (arXiv:2203.00281v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00281","description":"<p>Recently CKY-based models show great potential in unsupervised grammar\ninduction thanks to their human-like encoding paradigm, which runs recursively\nand hierarchically, but requires $O(n^3)$ time-complexity. Recursive\nTransformer based on Differentiable Trees (R2D2) makes it possible to scale to\nlarge language model pre-training even with complex tree encoder by introducing\na heuristic pruning method. However, the rule-based pruning approach suffers\nfrom local optimum and slow inference issues. In this paper, we fix those\nissues in a unified method. We propose to use a top-down parser as a\nmodel-based pruning method, which also enables parallel encoding during\ninference. Typically, our parser casts parsing as a split point scoring task,\nwhich first scores all split points for a given sentence, and then recursively\nsplits a span into two by picking a split point with the highest score in the\ncurrent span. The reverse order of the splits is considered as the order of\npruning in R2D2 encoder. Beside the bi-directional language model loss, we also\noptimize the parser by minimizing the KL distance between tree probabilities\nfrom parser and R2D2. Our experiments show that our Fast-R2D2 improves\nperformance significantly in grammar induction and achieves competitive results\nin downstream classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Is Whole Word Masking Always Better for Chinese BERT?\": Probing on Chinese Grammatical Error Correction. (arXiv:2203.00286v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00286","description":"<p>Whole word masking (WWM), which masks all subwords corresponding to a word at\nonce, makes a better English BERT model. For the Chinese language, however,\nthere is no subword because each token is an atomic character. The meaning of a\nword in Chinese is different in that a word is a compositional unit consisting\nof multiple characters. Such difference motivates us to investigate whether WWM\nleads to better context understanding ability for Chinese BERT. To achieve\nthis, we introduce two probing tasks related to grammatical error correction\nand ask pretrained models to revise or insert tokens in a masked language\nmodeling manner. We construct a dataset including labels for 19,075 tokens in\n10,448 sentences. We train three Chinese BERT models with standard\ncharacter-level masking (CLM), WWM, and a combination of CLM and WWM,\nrespectively. Our major findings are as follows: First, when one character\nneeds to be inserted or replaced, the model trained with CLM performs the best.\nSecond, when more than one character needs to be handled, WWM is the key to\nbetter performance. Finally, when being fine-tuned on sentence-level downstream\ntasks, models trained with different masking strategies perform comparably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Enbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VScript: Controllable Script Generation with Audio-Visual Presentation. (arXiv:2203.00314v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00314","description":"<p>Automatic script generation could save a considerable amount of resources and\noffer inspiration to professional scriptwriters. We present VScript, a\ncontrollable pipeline that generates complete scripts including dialogues and\nscene descriptions, and presents visually using video retrieval and aurally\nusing text-to-speech for spoken dialogue. With an interactive interface, our\nsystem allows users to select genres and input starting words that control the\ntheme and development of the generated script. We adopt a hierarchical\nstructure, which generates the plot, then the script and its audio-visual\npresentation. We also introduce a novel approach to plot-guided dialogue\ngeneration by treating it as an inverse dialogue summarization. Experiment\nresults show that our approach outperforms the baselines on both automatic and\nhuman evaluations, especially in terms of genre control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_I/0/1/0/all/0/1\">I-Tsun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-LID: Leveraging BERT to Improve Spoken Language Identification. (arXiv:2203.00328v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00328","description":"<p>Language identification is a task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances (&gt;3s), the\nperformance on short utterances (&lt;=1s) is still far from satisfactory. We\npropose an effective BERT-based language identification system (BERT-LID) to\nimprove language identification performance, especially on short-duration\nspeech segments. To adapt BERT into the LID pipeline, we drop in a conjunction\nnetwork prior to BERT to accommodate the frame-level Phonetic\nPosteriorgrams(PPG) derived from the frontend phone recognizer and then\nfine-tune the conjunction network and BERT pre-trained model together. We\nevaluate several variations within this piped framework, including combining\nBERT with CNN, LSTM, DPCNN, and RCNN. The experimental results demonstrate that\nthe best-performing model is RCNN-BERT. Compared with the prior works, our\nRCNN-BERT model can improve the accuracy by about 5% on long-segment\nidentification and 18% on short-segment identification. The outperformance of\nour model, especially on the short-segment task, demonstrates the applicability\nof our proposed BERT-based approach on language identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuting Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read before Generate! Faithful Long Form Question Answering with Machine Reading. (arXiv:2203.00343v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00343","description":"<p>Long-form question answering (LFQA) aims to generate a paragraph-length\nanswer for a given question. While current work on LFQA using large pre-trained\nmodel for generation are effective at producing fluent and somewhat relevant\ncontent, one primary challenge lies in how to generate a faithful answer that\nhas less hallucinated content. We propose a new end-to-end framework that\njointly models answer generation and machine reading. The key idea is to\naugment the generation model with fine-grained, answer-related salient\ninformation which can be viewed as an emphasis on faithful facts.\nState-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate\nthe effectiveness of our method, in comparison with strong baselines on\nautomatic and human evaluation metrics. A detailed analysis further proves the\ncompetency of our methods in generating fluent, relevant, and more faithful\nanswers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Performance of Automated Essay Scoring by using back-translation essays and adjusted scores. (arXiv:2203.00354v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00354","description":"<p>Automated essay scoring plays an important role in judging students' language\nabilities in education. Traditional approaches use handcrafted features to\nscore and are time-consuming and complicated. Recently, neural network\napproaches have improved performance without any feature engineering. Unlike\nother natural language processing tasks, only a small number of datasets are\npublicly available for automated essay scoring, and the size of the dataset is\nnot sufficiently large. Considering that the performance of a neural network is\nclosely related to the size of the dataset, the lack of data limits the\nperformance improvement of the automated essay scoring model. In this paper, we\nproposed a method to increase the number of essay-score pairs using\nback-translation and score adjustment and applied it to the Automated Student\nAssessment Prize dataset for augmentation. We evaluated the effectiveness of\nthe augmented data using models from prior work. In addition, performance was\nevaluated in a model using long short-term memory, which is widely used for\nautomated essay scoring. The performance of the models was improved by using\naugmented data to train the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_Y/0/1/0/all/0/1\">You-Jin Jong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yong-Jin Kim</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ri_O/0/1/0/all/0/1\">Ok-Chol Ri</a> (1) ((1) Kum Sung Middle School Number 2, Pyongyang, D.P.R of Korea, (2) Faculty of Mathematics, KIM IL SUNG University, Pyongyang, D.P.R of Korea)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. (arXiv:2203.00357v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00357","description":"<p>Logical reasoning is of vital importance to natural language understanding.\nPrevious studies either employ graph-based models to incorporate prior\nknowledge about logical relations, or introduce symbolic logic into neural\nmodels through data augmentation. These methods, however, heavily depend on\nannotated training data, and thus suffer from over-fitting and poor\ngeneralization problems due to the dataset sparsity. To address these two\nproblems, in this paper, we propose MERIt, a MEta-path guided contrastive\nlearning method for logical ReasonIng of text, to perform self-supervised\npre-training on abundant unlabeled text data. Two novel strategies serve as\nindispensable components of our method. In particular, a strategy based on\nmeta-path is devised to discover the logical structure in natural texts,\nfollowed by a counterfactual data augmentation strategy to eliminate the\ninformation shortcut induced by pre-training. The experimental results on two\nchallenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate\nthat our method outperforms the SOTA baselines with significant improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00545","description":"<p>The MultiCoNER shared task aims at detecting semantically ambiguous and\ncomplex named entities in short and low-context settings for multiple\nlanguages. The lack of contexts makes the recognition of ambiguous named\nentities challenging. To alleviate this issue, our team DAMO-NLP proposes a\nknowledge-based system, where we build a multilingual knowledge base based on\nWikipedia to provide related context information to the named entity\nrecognition (NER) model. Given an input sentence, our system effectively\nretrieves related contexts from the knowledge base. The original input\nsentences are then augmented with such context information, allowing\nsignificantly better contextualized token representations to be captured. Our\nsystem wins 10 out of 13 tracks in the MultiCoNER shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepNet: Scaling Transformers to 1,000 Layers. (arXiv:2203.00555v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00555","description":"<p>In this paper, we propose a simple yet effective method to stabilize\nextremely deep Transformers. Specifically, we introduce a new normalization\nfunction (DeepNorm) to modify the residual connection in Transformer,\naccompanying with theoretically derived initialization. In-depth theoretical\nanalysis shows that model updates can be bounded in a stable way. The proposed\nmethod combines the best of two worlds, i.e., good performance of Post-LN and\nstable training of Pre-LN, making DeepNorm a preferred alternative. We\nsuccessfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and\nfeed-forward network sublayers) without difficulty, which is one order of\nmagnitude deeper than previous deep Transformers. Remarkably, on a multilingual\nbenchmark with 7,482 translation directions, our 200-layer model with 3.2B\nparameters significantly outperforms the 48-layer state-of-the-art model with\n12B parameters by 5 BLEU points, which indicates a promising scaling direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Data Analysis for Word Sense Disambiguation. (arXiv:2203.00565v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00565","description":"<p>We develop and test a novel unsupervised algorithm for word sense induction\nand disambiguation which uses topological data analysis. Typical approaches to\nthe problem involve clustering, based on simple low level features of distance\nin word embeddings. Our approach relies on advanced mathematical concepts in\nthe field of topology which provides a richer conceptualization of clusters for\nthe word sense induction tasks. We use a persistent homology barcode algorithm\non the SemCor dataset and demonstrate that our approach gives low relative\nerror on word sense induction. This shows the promise of topological algorithms\nfor natural language processing and we advocate for future work in this\npromising area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rawson_M/0/1/0/all/0/1\">Michael Rawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_M/0/1/0/all/0/1\">Mithun Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_R/0/1/0/all/0/1\">Rishabh Choudhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural invariants and semantic fingerprints in the \"ego network\" of words. (arXiv:2203.00588v1 [cs.SI])","link":"http://arxiv.org/abs/2203.00588","description":"<p>Well-established cognitive models coming from anthropology have shown that,\ndue to the cognitive constraints that limit our \"bandwidth\" for social\ninteractions, humans organize their social relations according to a regular\nstructure. In this work, we postulate that similar regularities can be found in\nother cognitive processes, such as those involving language production. In\norder to investigate this claim, we analyse a dataset containing tweets of a\nheterogeneous group of Twitter users (regular users and professional writers).\nLeveraging a methodology similar to the one used to uncover the\nwell-established social cognitive constraints, we find regularities at both the\nstructural and semantic level. At the former, we find that a concentric layered\nstructure (which we call ego network of words, in analogy to the ego network of\nsocial relationships) very well captures how individuals organise the words\nthey use. The size of the layers in this structure regularly grows\n(approximately 2-3 times with respect to the previous one) when moving\noutwards, and the two penultimate external layers consistently account for\napproximately 60% and 30% of the used words, irrespective of the number of the\ntotal number of layers of the user. For the semantic analysis, each ring of\neach ego network is described by a semantic profile, which captures the topics\nassociated with the words in the ring. We find that ring #1 has a special role\nin the model. It is semantically the most dissimilar and the most diverse among\nthe rings. We also show that the topics that are important in the innermost\nring also have the characteristic of being predominant in each of the other\nrings, as well as in the entire ego network. In this respect, ring #1 can be\nseen as the semantic fingerprint of the ego network of words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollivier_K/0/1/0/all/0/1\">Kilian Ollivier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boldrini_C/0/1/0/all/0/1\">Chiara Boldrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1\">Andrea Passarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Marco Conti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Common Speech Analysis Engine. (arXiv:2203.00613v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00613","description":"<p>Recent innovations in self-supervised representation learning have led to\nremarkable advances in natural language processing. That said, in the speech\nprocessing domain, self-supervised representation learning-based systems are\nnot yet considered state-of-the-art. We propose leveraging recent advances in\nself-supervised-based speech processing to create a common speech analysis\nengine. Such an engine should be able to handle multiple speech processing\ntasks, using a single architecture, to obtain state-of-the-art accuracy. The\nengine must also enable support for new tasks with small training datasets.\nBeyond that, a common engine should be capable of supporting distributed\ntraining with client in-house private data. We present the architecture for a\ncommon speech analysis engine based on the HuBERT self-supervised speech\nrepresentation. Based on experiments, we report our results for language\nidentification and emotion recognition on the standard evaluations NIST-LRE 07\nand IEMOCAP. Our results surpass the state-of-the-art performance reported so\nfar on these tasks. We also analyzed our engine on the emotion recognition task\nusing reduced amounts of training data and show how to achieve improved\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aronowitz_H/0/1/0/all/0/1\">Hagai Aronowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morais_E/0/1/0/all/0/1\">Edmilson Morais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weizhong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoory_R/0/1/0/all/0/1\">Ron Hoory</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale. (arXiv:2203.00633v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00633","description":"<p>Transformer language models that are trained on vast amounts of data have\nachieved remarkable success at various NLP benchmarks. Intriguingly, this\nsuccess is achieved by models that lack an explicit modeling of hierarchical\nsyntactic structures, which were hypothesized by decades of linguistic research\nto be necessary for good generalization. This naturally leaves a question: to\nwhat extent can we further improve the performance of Transformer language\nmodels, through an inductive bias that encourages the model to explain the data\nthrough the lens of recursive syntactic compositions? Although the benefits of\nmodeling recursive syntax have been shown at the small data and model scales,\nit remains an open question whether -- and to what extent -- a similar design\nprinciple is still beneficial in the case of powerful Transformer language\nmodels that work well at scale. To answer these questions, we introduce\nTransformer Grammars -- a novel class of Transformer language models that\ncombine: (i) the expressive power, scalability, and strong performance of\nTransformers, and (ii) recursive syntactic compositions, which here are\nimplemented through a special attention mask. We find that Transformer Grammars\noutperform various strong baselines on multiple syntax-sensitive language\nmodeling evaluation metrics, in addition to sentence-level language modeling\nperplexity. Nevertheless, we find that the recursive syntactic composition\nbottleneck harms perplexity on document-level modeling, providing evidence that\na different kind of memory mechanism -- that works independently of syntactic\nstructures -- plays an important role in the processing of long-form text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_S/0/1/0/all/0/1\">Samuel Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00648","description":"<p>Human speech data comprises a rich set of domain factors such as accent,\nsyntactic and semantic variety, or acoustic environment. Previous work explores\nthe effect of domain mismatch in automatic speech recognition between\npre-training and fine-tuning as a whole but does not dissect the contribution\nof individual factors. In this paper, we present a controlled study to better\nunderstand the effect of such factors on the performance of pre-trained\nrepresentations. To do so, we pre-train models either on modified natural\nspeech or synthesized audio, with a single domain factor modified, and then\nmeasure performance on automatic speech recognition after fine tuning. Results\nshow that phonetic domain factors play an important role during pre-training\nwhile grammatical and syntactic factors are far less important. To our\nknowledge, this is the first study to better understand the domain\ncharacteristics in self-supervised pre-training for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing an Interdisciplinary Science of Conversation: Insights from a Large Multimodal Corpus of Human Speech. (arXiv:2203.00674v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00674","description":"<p>People spend a substantial portion of their lives engaged in conversation,\nand yet our scientific understanding of conversation is still in its infancy.\nIn this report we advance an interdisciplinary science of conversation, with\nfindings from a large, novel, multimodal corpus of 1,656 recorded conversations\nin spoken English. This 7+ million word, 850 hour corpus totals over 1TB of\naudio, video, and transcripts, with moment-to-moment measures of vocal, facial,\nand semantic expression, along with an extensive survey of speaker post\nconversation reflections. We leverage the considerable scope of the corpus to\n(1) extend key findings from the literature, such as the cooperativeness of\nhuman turn-taking; (2) define novel algorithmic procedures for the segmentation\nof speech into conversational turns; (3) apply machine learning insights across\nvarious textual, auditory, and visual features to analyze what makes\nconversations succeed or fail; and (4) explore how conversations are related to\nwell-being across the lifespan. We also report (5) a comprehensive mixed-method\nreport, based on quantitative analysis and qualitative review of each\nrecording, that showcases how individuals from diverse backgrounds alter their\ncommunication patterns and find ways to connect. We conclude with a discussion\nof how this large-scale public dataset may offer new directions for future\nresearch, especially across disciplinary boundaries, as scholars from a variety\nof fields appear increasingly interested in the study of conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reece_A/0/1/0/all/0/1\">Andrew Reece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooney_G/0/1/0/all/0/1\">Gus Cooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_P/0/1/0/all/0/1\">Peter Bull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Christine Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_B/0/1/0/all/0/1\">Bryn Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_C/0/1/0/all/0/1\">Casey Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glazer_T/0/1/0/all/0/1\">Tamara Glazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knox_D/0/1/0/all/0/1\">Dean Knox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebscher_A/0/1/0/all/0/1\">Alex Liebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_S/0/1/0/all/0/1\">Sebastian Marin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.04293","description":"<p>Humans are able to perceive, understand and reason about causal events.\nDeveloping models with similar physical and causal understanding capabilities\nis a long-standing goal of artificial intelligence. As a step towards this\ndirection, we introduce CRAFT, a new video question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories in CRAFT\ninclude previously studied descriptive and counterfactual questions.\nAdditionally, inspired by the Force Dynamics Theory in cognitive linguistics,\nwe introduce a new causal question category that involves understanding the\ncausal interactions between objects through notions like cause, enable, and\nprevent. Our results show that even though the questions in CRAFT are easy for\nhumans, the tested baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1\">Tayfun Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1\">M. Samil Atesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1\">Cagatay Yigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">Ilker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1\">Mert Kobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1\">Tilbe Goksun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariance, encodings, and generalization: learning identity effects with neural networks. (arXiv:2101.08386v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08386","description":"<p>Often in language and other areas of cognition, whether two components of an\nobject are identical or not determines if it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na framework in which we can rigorously prove that algorithms satisfying simple\ncriteria cannot make the correct inference. We then show that a broad class of\nlearning algorithms including deep feedforward neural networks trained via\ngradient-based algorithms (such as stochastic gradient descent or the Adam\nmethod) satisfy our criteria, dependent on the encoding of inputs. In some\nbroader circumstances we are able to provide adversarial examples that the\nnetwork necessarily classifies incorrectly. Finally, we demonstrate our theory\nwith computational experiments in which we explore the effect of different\ninput encodings on the ability of algorithms to generalize to novel inputs.\nThis allows us to show similar effects to those predicted by theory for more\nrealistic methods that violate some of the conditions of our theoretical\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brugiapaglia_S/0/1/0/all/0/1\">S. Brugiapaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">M. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tupper_P/0/1/0/all/0/1\">P. Tupper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating Columns with Pre-trained Language Models. (arXiv:2104.01785v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2104.01785","description":"<p>Inferring meta information about tables, such as column headers or\nrelationships between columns, is an active research topic in data management\nas we find many tables are missing some of this information. In this paper, we\nstudy the problem of annotating table columns (i.e., predicting column types\nand the relationships between columns) using only information from the table\nitself. We develop a multi-task learning framework (called Doduo) based on\npre-trained language models, which takes the entire table as input and predicts\ncolumn types/relations using a single model. Experimental results show that\nDoduo establishes new state-of-the-art performance on two benchmarks for the\ncolumn type prediction and column relation prediction tasks with up to 4.0% and\n11.9% improvements, respectively. We report that Doduo can already outperform\nthe previous state-of-the-art performance with a minimal number of tokens, only\n8 tokens per column. We release a toolbox\n(https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo\non a real-world data science problem through a case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport-based Adaptation in Dysarthric Speech Tasks. (arXiv:2104.02535v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.02535","description":"<p>In many real-world applications, the mismatch between distributions of\ntraining data (source) and test data (target) significantly degrades the\nperformance of machine learning algorithms. In speech data, causes of this\nmismatch include different acoustic environments or speaker characteristics. In\nthis paper, we address this issue in the challenging context of dysarthric\nspeech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we\npropose the use of an optimal-transport based approach, called MSDA via\nWeighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem\nin dysarthria detection for which the proposed approach outperforms both the\nBaseline and the state-of-the-art MSDA models, improving the detection accuracy\nof 0.9% over the best competitor method. We then employ MSDA-WJDOT for\ndysarthric speaker adaptation in command speech recognition. This provides a\nCommand Error Rate relative reduction of 16% and 7% over the baseline and the\nbest competitor model, respectively. Interestingly, MSDA-WJDOT provides a\nsimilarity score between the source and the target, i.e. between speakers in\nthis case. We leverage this similarity measure to define a Dysarthric and\nHealthy score of the target speaker and diagnose the dysarthria with an\naccuracy of 95%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turrisi_R/0/1/0/all/0/1\">Rosanna Turrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badino_L/0/1/0/all/0/1\">Leonardo Badino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Inference Attacks on Knowledge Graphs. (arXiv:2104.08273v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.08273","description":"<p>Membership inference attacks (MIAs) infer whether a specific data record is\nused for target model training. MIAs have provoked many discussions in the\ninformation security community since they give rise to severe data privacy\nissues, especially for private and sensitive datasets. Knowledge Graphs (KGs),\nwhich describe domain-specific subjects and relationships among them, are\nvaluable and sensitive, such as medical KGs constructed from electronic health\nrecords. However, the privacy threat to knowledge graphs is critical but rarely\nexplored. In this paper, we conduct the first empirical evaluation of privacy\nthreats to knowledge graphs triggered by knowledge graph embedding methods\n(KGEs). We propose three types of membership inference attacks: transfer\nattacks (TAs), prediction loss-based attacks (PLAs), and prediction\ncorrectness-based attacks (PCAs), according to attack difficulty levels. In the\nexperiments, we conduct three inference attacks against four standard KGE\nmethods over three benchmark datasets. In addition, we also propose the attacks\nagainst medical KG and financial KG. The results demonstrate that the proposed\nattack methods can easily explore the privacy leakage of knowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring diachronic sense change: new models and Monte Carlo methods for Bayesian inference. (arXiv:2105.00819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00819","description":"<p>In a bag-of-words model, the senses of a word with multiple meanings, e.g.\n\"bank\" (used either in a river-bank or an institution sense), are represented\nas probability distributions over context words, and sense prevalence is\nrepresented as a probability distribution over senses. Both of these may change\nwith time. Modelling and measuring this kind of sense change is challenging due\nto the typically high-dimensional parameter space and sparse datasets. A\nrecently published corpus of ancient Greek texts contains expert-annotated\nsense labels for selected target words. Automatic sense-annotation for the word\n\"kosmos\" (meaning decoration, order or world) has been used as a test case in\nrecent work with related generative models and Monte Carlo methods. We adapt an\nexisting generative sense change model to develop a simpler model for the main\neffects of sense and time, and give MCMC methods for Bayesian inference on all\nthese models that are more efficient than existing methods. We carry out\nautomatic sense-annotation of snippets containing \"kosmos\" using our model, and\nmeasure the time-evolution of its three senses and their prevalence. As far as\nwe are aware, ours is the first analysis of this data, within the class of\ngenerative models we consider, that quantifies uncertainty and returns credible\nsets for evolving sense prevalence in good agreement with those given by expert\nannotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1\">Schyan Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholls_G/0/1/0/all/0/1\">Geoff Nicholls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Training for Cross-Task Learning. (arXiv:2105.14095v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14095","description":"<p>In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted\ntraining algorithm for cross-task learning based on minimizing a\nrepresentation-based task distance between the source and target tasks. We show\nthat TAWT is easy to implement, is computationally efficient, requires little\nhyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees.\nThe effectiveness of TAWT is corroborated through extensive experiments with\nBERT on four sequence tagging tasks in natural language processing (NLP),\nincluding part-of-speech (PoS) tagging, chunking, predicate detection, and\nnamed entity recognition (NER). As a byproduct, the proposed\nrepresentation-based task distance allows one to reason in a theoretically\nprincipled way about several critical aspects of cross-task learning, such as\nthe choice of the source data and the impact of fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crammer_K/0/1/0/all/0/1\">Koby Crammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Temperature Matters in Abstractive Summarization Distillation. (arXiv:2106.03441v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03441","description":"<p>Recent progress of abstractive text summarization largely relies on large\npre-trained sequence-to-sequence Transformer models, which are computationally\nexpensive. This paper aims to distill these large models into smaller ones for\nfaster inference and minimal performance loss. Pseudo-labeling based methods\nare popular in sequence-to-sequence model distillation. In this paper, we find\nsimply manipulating attention temperatures in Transformers can make pseudo\nlabels easier to learn for student models. Our experiments on three\nsummarization datasets show our proposed method consistently improves over\nvanilla pseudo-labeling based methods. We also find that both the pseudo labels\nand summaries produced by our students are shorter and more abstractive. Our\ncode is available at \\url{https://github.com/Shengqiang-Zhang/plate}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02352","description":"<p>Aspect-level sentiment classification (ASC) aims to predict the fine-grained\nsentiment polarity towards a given aspect mentioned in a review. Despite recent\nadvances in ASC, enabling machines to preciously infer aspect sentiments is\nstill challenging. This paper tackles two challenges in ASC: (1) due to lack of\naspect knowledge, aspect representation derived in prior works is inadequate to\nrepresent aspect's exact meaning and property information; (2) prior works only\ncapture either local syntactic information or global relational information,\nthus missing either one of them leads to insufficient syntactic information. To\ntackle these challenges, we propose a novel ASC model which not only end-to-end\nembeds and leverages aspect knowledge but also marries the two kinds of\nsyntactic information and lets them compensate for each other. Our model\nincludes three key components: (1) a knowledge-aware gated recurrent memory\nnetwork recurrently integrates dynamically summarized aspect knowledge; (2) a\ndual syntax graph network combines both kinds of syntactic information to\ncomprehensively capture sufficient syntactic information; (3) a knowledge\nintegrating gate re-enhances the final representation with further needed\naspect knowledge; (4) an aspect-to-context attention mechanism aggregates the\naspect-related semantics from all hidden states into the final representation.\nExperimental results on several benchmark datasets demonstrate the\neffectiveness of our model, which overpass previous state-of-the-art models by\nlarge margins in terms of both Accuracy and Macro-F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues. (arXiv:2109.00430v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, treatment and\nconsultation. The development of MDSs is hindered because of a lack of\nresources. In particular. (1) there is no dataset with large-scale medical\ndialogues that covers multiple medical services and contains fine-grained\nmedical labels (i.e., intents, actions, slots, values), and (2) there is no set\nof established benchmarks for MDSs for multi-domain, multi-service medical\ndialogues. In this paper, we present ReMeDi, a set of resource for medical\ndialogues. ReMeDi consists of two parts, the ReMeDi dataset and the ReMeDi\nbenchmarks. The ReMeDi dataset contains 96,965 conversations between doctors\nand patients, including 1,557 conversations with fine-gained labels. It covers\n843 types of diseases, 5,228 medical entities, and 3 specialties of medical\nservices across 40 domains. To the best of our knowledge, the ReMeDi dataset is\nthe only medical dialogue dataset that covers multiple domains and services,\nand has fine-grained medical labels. The second part of the ReMeDi resources\nconsists of a set of state-of-the-art models for (medical) dialogue generation.\nThe ReMeDi benchmark has the following methods: (1) pretrained models (i.e.,\nBERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ReMeDi\ndataset, and (2) a self-supervised contrastive learning(SCL) method to expand\nthe ReMeDi dataset and enhance the training of the state-of-the-art pretrained\nmodels. We describe the creation of the ReMeDi dataset, the ReMeDi benchmarking\nmethods, and establish experimental results using the ReMeDi benchmarking\nmethods on the ReMeDi dataset for future research to compare against. With this\npaper, we share the dataset, implementations of the benchmarks, and evaluation\nscripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14739","description":"<p>Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval. (arXiv:2110.07524v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07524","description":"<p>Training dense passage representations via contrastive learning has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Existing studies\nfocus on further optimizing by improving negative sampling strategy or extra\npretraining. However, these studies keep unknown in capturing passage with\ninternal representation conflicts from improper modeling granularity. This work\nthus presents a refined model on the basis of a smaller granularity, contextual\nsentences, to alleviate the concerned conflicts. In detail, we introduce an\nin-passage negative sampling strategy to encourage a diverse generation of\nsentence representations within the same passage. Experiments on three\nbenchmark datasets verify the efficacy of our method, especially on datasets\nwhere conflicts are severe. Extensive experiments further present good\ntransferability of our method across datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework. (arXiv:2110.08544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08544","description":"<p>Open-domain questions are likely to be open-ended and ambiguous, leading to\nmultiple valid answers. Existing approaches typically adopt the\nrerank-then-read framework, where a reader reads top-ranking evidence to\npredict answers. According to our empirical analysis, this framework faces\nthree problems: first, to leverage a large reader under a memory constraint,\nthe reranker should select only a few relevant passages to cover diverse\nanswers, while balancing relevance and diversity is non-trivial; second, the\nsmall reading budget prevents the reader from accessing valuable retrieved\nevidence filtered out by the reranker; third, when using a generative reader to\npredict answers all at once based on all selected evidence, whether a valid\nanswer will be predicted also pathologically depends on the evidence of some\nother valid answer(s). To address these issues, we propose to answer\nopen-domain multi-answer questions with a recall-then-verify framework, which\nseparates the reasoning process of each answer so that we can make better use\nof retrieved evidence while also leveraging large models under the same memory\nconstraint. Our framework achieves state-of-the-art results on two multi-answer\ndatasets, and predicts significantly more gold answers than a rerank-then-read\nsystem that uses an oracle reranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAIR: Data Augmented Invariant Regularization. (arXiv:2110.11205v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11205","description":"<p>While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM\ngeneralizes poorly to distribution shift. This is partly explained by\noverfitting to spurious features such as background in images or named entities\nin natural language. Synthetic data augmentation followed by empirical risk\nminimization (DA-ERM) is a simple and widely used solution to remedy this\nproblem. In addition, consistency regularization could be applied to further\npromote model performance to be consistent on the augmented sample and the\noriginal one. In this paper, we propose data augmented invariant regularization\n(DAIR), a simple form of consistency regularization that is applied directly on\nthe loss function rather than intermediate features, making it widely\napplicable regardless of network architecture or problem setup. We apply DAIR\nto multiple real-world learning problems, namely robust regression, visual\nquestion answering, robust deep neural network training, and neural\ntask-oriented dialog modeling. Our experiments show that DAIR consistently\noutperforms ERM and DA-ERM with little marginal cost and sets new\nstate-of-the-art results in several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halbe_S/0/1/0/all/0/1\">Shaunak Halbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientWord-Net: An Open Source Hotword Detection Engine based on One-shot Learning. (arXiv:2111.00379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00379","description":"<p>Voice assistants like Siri, Google Assistant, Alexa etc. are used widely\nacross the globe for home automation, these require the use of special phrases\nalso known as hotwords to wake it up and perform an action like \"Hey Alexa!\",\n\"Ok Google!\" and \"Hey Siri!\" etc. These hotwords are detected with lightweight\nreal-time engines whose purpose is to detect the hotwords uttered by the user.\nThis paper presents the design and implementation of a hotword detection engine\nbased on one-shot learning which detects the hotword uttered by the user in\nreal-time with just one or few training samples of the hotword. This approach\nis efficient when compared to existing implementations because the process of\nadding a new hotword in the existing systems requires enormous amounts of\npositive and negative training samples and the model needs to retrain for every\nhotword. This makes the existing implementations inefficient in terms of\ncomputation and cost. The architecture proposed in this paper has achieved an\naccuracy of 94.51%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_C/0/1/0/all/0/1\">Chidhambararajan R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangapur_A/0/1/0/all/0/1\">Aman Rangapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethuraman_S/0/1/0/all/0/1\">Sibi Chakkaravarthy Sethuraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis. (arXiv:2111.03133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03133","description":"<p>Generating images that fit a given text description using machine learning\nhas improved greatly with the release of technologies such as the CLIP\nimage-text encoder model; however, current methods lack artistic control of the\nstyle of image to be generated. We introduce StyleCLIPDraw which adds a style\nloss to the CLIPDraw text-to-drawing synthesis model to allow artistic control\nof the synthesized drawings in addition to control of the content via text.\nWhereas performing decoupled style transfer on a generated image only affects\nthe texture, our proposed coupled approach is able to capture a style in both\ntexture and shape, suggesting that the style of the drawing is coupled with the\ndrawing process itself. More results and our code are available at\nhttps://github.com/pschaldenbrand/StyleCLIPDraw\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaldenbrand_P/0/1/0/all/0/1\">Peter Schaldenbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2112.09427","description":"<p>Adapting Automatic Speech Recognition (ASR) models to new domains leads to a\ndeterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.05409","description":"<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weihao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typical Decoding for Natural Language Generation. (arXiv:2202.00666v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00666","description":"<p>Despite achieving incredibly low perplexities on myriad natural language\ncorpora, today's language models still often underperform when used to generate\ntext. This dichotomy has puzzled the language generation community for the last\nfew years. In this work, we posit that the abstraction of natural language as a\ncommunication channel (\\`a la Shannon, 1948) can provide new insights into the\nbehaviors of probabilistic language generators, e.g., why high-probability\ntexts can be dull or repetitive. Humans use language as a means of\ncommunicating information, and do so in a simultaneously efficient and\nerror-minimizing manner; they choose each word in a string with this (perhaps\nsubconscious) goal in mind. We propose that generation from probabilistic\nmodels should mimic this behavior. Rather than always choosing words from the\nhigh-probability region of the distribution--which have a low Shannon\ninformation content--we sample from the set of words with information content\nclose to the conditional entropy of our model, i.e., close to the expected\ninformation content. This decision criterion can be realized through a simple\nand efficient implementation, which we call typical sampling. Automatic and\nhuman evaluations show that, in comparison to nucleus and top-k sampling,\ntypical sampling offers competitive performance in terms of quality while\nconsistently reducing the number of degenerate repetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01279","description":"<p>PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1\">Srulik Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almubarak_K/0/1/0/all/0/1\">Khalid Almubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mike Tian-Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding. (arXiv:2202.13093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13093","description":"<p>Contrastive learning is emerging as a powerful technique for extracting\nknowledge from unlabeled data. This technique requires a balanced mixture of\ntwo ingredients: positive (similar) and negative (dissimilar) samples. This is\ntypically achieved by maintaining a queue of negative samples during training.\nPrior works in the area typically uses a fixed-length negative sample queue,\nbut how the negative sample size affects the model performance remains unclear.\nThe opaque impact of the number of negative samples on performance when\nemploying contrastive learning aroused our in-depth exploration. This paper\npresents a momentum contrastive learning model with negative sample queue for\nsentence embedding, namely MoCoSE. We add the prediction layer to the online\nbranch to make the model asymmetric and together with EMA update mechanism of\nthe target branch to prevent model from collapsing. We define a maximum\ntraceable distance metric, through which we learn to what extent the text\ncontrastive learning benefits from the historical information of negative\nsamples. Our experiments find that the best results are obtained when the\nmaximum traceable distance is at a certain range, demonstrating that there is\nan optimal range of historical information for a negative sample queue. We\nevaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)\ntask and obtain an average Spearman's correlation of $77.27\\%$. Source code is\navailable at https://github.com/xbdxwyh/mocose\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Modular Skills in Multitask Learning. (arXiv:2202.13914v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13914","description":"<p>A modular design encourages neural models to disentangle and recombine\ndifferent facets of knowledge to generalise more systematically to new tasks.\nIn this work, we assume that each task is associated with a subset of latent\ndiscrete skills from a (potentially small) inventory. In turn, skills\ncorrespond to parameter-efficient (sparse / low-rank) model parameterisations.\nBy jointly learning these and a task-skill allocation matrix, the network for\neach task is instantiated as the average of the parameters of active skills. To\nfavour non-trivial soft partitions of skills across tasks, we experiment with a\nseries of inductive biases, such as an Indian Buffet Process prior and a\ntwo-speed learning rate. We evaluate our latent-skill model on two main\nsettings: 1) multitask reinforcement learning for grounded instruction\nfollowing on 8 levels of the BabyAI platform; and 2) few-shot adaptation of\npre-trained text-to-text generative models on CrossFit, a benchmark comprising\n160 NLP tasks. We find that the modular design of a network significantly\nincreases sample efficiency in reinforcement learning and few-shot\ngeneralisation in supervised learning, compared to baselines with fully shared,\ntask-specific, or conditionally generated parameters where knowledge is\nentangled across tasks. In addition, we show how discrete skills help\ninterpretability, as they yield an explicit hierarchy of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Spatio-temporal Vision Transformer for Super-resolution Microscopy. (arXiv:2203.00030v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00030","description":"<p>Structured illumination microscopy (SIM) is an optical super-resolution\ntechnique that enables live-cell imaging beyond the diffraction limit.\nReconstruction of SIM data is prone to artefacts, which becomes problematic\nwhen imaging highly dynamic samples because previous methods rely on the\nassumption that samples are static. We propose a new transformer-based\nreconstruction method, VSR-SIM, that uses shifted 3-dimensional window\nmulti-head attention in addition to channel attention mechanism to tackle the\nproblem of video super-resolution (VSR) in SIM. The attention mechanisms are\nfound to capture motion in sequences without the need for common motion\nestimation techniques such as optical flow. We take an approach to training the\nnetwork that relies solely on simulated data using videos of natural scenery\nwith a model for SIM image formation. We demonstrate a use case enabled by\nVSR-SIM referred to as rolling SIM imaging, which increases temporal resolution\nin SIM by a factor of 9. Our method can be applied to any SIM setup enabling\nprecise recordings of dynamic processes in biomedical research with high\ntemporal resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Christensen_C/0/1/0/all/0/1\">Charles N. Christensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Meng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ward_E/0/1/0/all/0/1\">Edward N. Ward</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaminski_C/0/1/0/all/0/1\">Clemens F. Kaminski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxelmorph++ Going beyond the cranial vault with keypoint supervision and multi-channel instance optimisation. (arXiv:2203.00046v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00046","description":"<p>The majority of current research in deep learning based image registration\naddresses inter-patient brain registration with moderate deformation\nmagnitudes. The recent Learn2Reg medical registration benchmark has\ndemonstrated that single-scale U-Net architectures, such as VoxelMorph that\ndirectly employ a spatial transformer loss, often do not generalise well beyond\nthe cranial vault and fall short of state-of-the-art performance for abdominal\nor intra-patient lung registration. Here, we propose two straightforward steps\nthat greatly reduce this gap in accuracy. First, we employ keypoint\nself-supervision with a novel network head that predicts a discretised heatmap\nand robustly reduces large deformations for better robustness. Second, we\nreplace multiple learned fine-tuning steps by a single instance optimisation\nwith hand-crafted features and the Adam optimiser. Different to other related\nwork, including FlowNet or PDD-Net, our approach does not require a fully\ndiscretised architecture with correlation layer. Our ablation study\ndemonstrates the importance of keypoints in both self-supervised and\nunsupervised (using only a MIND metric) settings. On a multi-centric\ninspiration-exhale lung CT dataset, including very challenging COPD scans, our\nmethod outperforms VoxelMorph by improving nonlinear alignment by 77% compared\nto 19% - reaching target registration errors of 2 mm that outperform all but\none learning methods published to date. Extending the method to semantic\nfeatures sets new stat-of-the-art performance on inter-subject abdominal CT\nregistration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias P. Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local and Global GANs with Semantic-Aware Upsampling for Image Generation. (arXiv:2203.00047v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00047","description":"<p>In this paper, we address the task of semantic-guided image generation. One\nchallenge common to most existing image-level generation methods is the\ndifficulty in generating small objects and detailed local textures. To address\nthis, in this work we consider generating images using local context. As such,\nwe design a local class-specific generative network using semantic maps as\nguidance, which separately constructs and learns subgenerators for different\nclasses, enabling it to capture finer details. To learn more discriminative\nclass-specific feature representations for the local generation, we also\npropose a novel classification module. To combine the advantages of both global\nimage-level and local class-specific generation, a joint generation network is\ndesigned with an attention fusion module and a dual-discriminator structure\nembedded. Lastly, we propose a novel semantic-aware upsampling method, which\nhas a larger receptive field and can take far-away pixels that are semantically\nrelated for feature upsampling, enabling it to better preserve semantic\nconsistency for instances with the same semantic labels. Extensive experiments\non two image generation tasks show the superior performance of the proposed\nmethod. State-of-the-art results are established by large margins on both tasks\nand on nine challenging public benchmarks. The source code and trained models\nare available at https://github.com/Ha0Tang/LGGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00048","description":"<p>Aligning signals from different modalities is an important step in\nvision-language representation learning as it affects the performance of later\nstages such as cross-modality fusion. Since image and text typically reside in\ndifferent regions of the feature space, directly aligning them at instance\nlevel is challenging especially when features are still evolving during\ntraining. In this paper, we propose to align at a higher and more stable level\nusing cluster representation. Specifically, we treat image and text as two\n\"views\" of the same entity, and encode them into a joint vision-language coding\nspace spanned by a dictionary of cluster centers (codebook). We contrast\npositive and negative samples via their cluster assignments while\nsimultaneously optimizing the cluster centers. To further smooth out the\nlearning process, we adopt a teacher-student distillation paradigm, where the\nmomentum teacher of one view guides the student learning of the other. We\nevaluated our approach on common vision language benchmarks and obtain new SoTA\non zero-shot cross modality retrieval while being competitive on various other\ntransfer tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping. (arXiv:2203.00049v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00049","description":"<p>In this paper we develop a method for mapping forest mortality in the\nforest-tundra ecotone using satellite data from heterogeneous sensors. We use\nmedium resolution imagery in order to provide the complex pattern of forest\nmortality in this sparsely forested area, which has been induced by an outbreak\nof geometrid moths. Specifically, Landsat-5 Thematic Mapper images from before\nthe event are used, with RADARSAT-2 providing the post-event images. We obtain\nthe difference images for both multispectral optical and synthetic aperture\nradar (SAR) by using a recently developed deep learning method for translating\nbetween the two domains. These differences are stacked with the original pre-\nand post-event images in order to let our algorithm also learn how the areas\nappear before and after the change event. By doing this, and focusing on\nlearning only the changes of interest with one-class classification (OCC), we\nobtain good results with very little training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agersborg_J/0/1/0/all/0/1\">J&#xf8;rgen A. Agersborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luppino_L/0/1/0/all/0/1\">Luigi T. Luppino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jepsen_J/0/1/0/all/0/1\">Jane Uhd Jepsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERF: Explicit Radiance Field Reconstruction From Scratch. (arXiv:2203.00051v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00051","description":"<p>We propose a novel explicit dense 3D reconstruction approach that processes a\nset of images of a scene with sensor poses and calibrations and estimates a\nphoto-real digital model. One of the key innovations is that the underlying\nvolumetric representation is completely explicit in contrast to neural\nnetwork-based (implicit) alternatives. We encode scenes explicitly using clear\nand understandable mappings of optimization variables to scene geometry and\ntheir outgoing surface radiance. We represent them using hierarchical\nvolumetric fields stored in a sparse voxel octree. Robustly reconstructing such\na volumetric scene model with millions of unknown variables from registered\nscene images only is a highly non-convex and complex optimization problem. To\nthis end, we employ stochastic gradient descent (Adam) which is steered by an\ninverse differentiable renderer.\n</p>\n<p>We demonstrate that our method can reconstruct models of high quality that\nare comparable to state-of-the-art implicit methods. Importantly, we do not use\na sequential reconstruction pipeline where individual steps suffer from\nincomplete or unreliable information from previous stages, but start our\noptimizations from uniformed initial solutions with scene geometry and radiance\nthat is far off from the ground truth. We show that our method is general and\npractical. It does not require a highly controlled lab setup for capturing, but\nallows for reconstructing scenes with a vast variety of objects, including\nchallenging ones, such as outdoor plants or furry toys. Finally, our\nreconstructed scene models are versatile thanks to their explicit design. They\ncan be edited interactively which is computationally too costly for implicit\nalternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aroudj_S/0/1/0/all/0/1\">Samir Aroudj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovegrove_S/0/1/0/all/0/1\">Steven Lovegrove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1\">Eddy Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_T/0/1/0/all/0/1\">Tanner Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goesele_M/0/1/0/all/0/1\">Michael Goesele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_R/0/1/0/all/0/1\">Richard Newcombe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport-based Graph Matching for 3D retinal OCT image registration. (arXiv:2203.00069v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00069","description":"<p>Registration of longitudinal optical coherence tomography (OCT) images\nassists disease monitoring and is essential in image fusion applications. Mouse\nretinal OCT images are often collected for longitudinal study of eye disease\nmodels such as uveitis, but their quality is often poor compared with human\nimaging. This paper presents a novel but efficient framework involving an\noptimal transport based graph matching (OT-GM) method for 3D mouse OCT image\nregistration. We first perform registration of fundus-like images obtained by\nprojecting all b-scans of a volume on a plane orthogonal to them, hereafter\nreferred to as the x-y plane. We introduce Adaptive Weighted Vessel Graph\nDescriptors (AWVGD) and 3D Cube Descriptors (CD) to identify the correspondence\nbetween nodes of graphs extracted from segmented vessels within the OCT\nprojection images. The AWVGD comprises scaling, translation and rotation, which\nare computationally efficient, whereas CD exploits 3D spatial and frequency\ndomain information. The OT-GM method subsequently performs the correct\nalignment in the x-y plane. Finally, registration along the direction\northogonal to the x-y plane (the z-direction) is guided by the segmentation of\ntwo important anatomical features peculiar to mouse b-scans, the Internal\nLimiting Membrane (ILM) and the hyaloid remnant (HR). Both subjective and\nobjective evaluation results demonstrate that our framework outperforms other\nwell-established methods on mouse OCT images within a reasonable execution\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholson_L/0/1/0/all/0/1\">Lindsay Nicholson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achim_A/0/1/0/all/0/1\">Alin Achim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model is All You Need: Multi-Task Learning Enables Simultaneous Histology Image Segmentation and Classification. (arXiv:2203.00077v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00077","description":"<p>The recent surge in performance for image analysis of digitised pathology\nslides can largely be attributed to the advance of deep learning. Deep models\ncan be used to initially localise various structures in the tissue and hence\nfacilitate the extraction of interpretable features for biomarker discovery.\nHowever, these models are typically trained for a single task and therefore\nscale poorly as we wish to adapt the model for an increasing number of\ndifferent tasks. Also, supervised deep learning models are very data hungry and\ntherefore rely on large amounts of training data to perform well. In this paper\nwe present a multi-task learning approach for segmentation and classification\nof nuclei, glands, lumen and different tissue regions that leverages data from\nmultiple independent data sources. While ensuring that our tasks are aligned by\nthe same tissue type and resolution, we enable simultaneous prediction with a\nsingle network. As a result of feature sharing, we also show that the learned\nrepresentation can be used to improve downstream tasks, including nuclear\nclassification and signet ring cell detection. As part of this work, we use a\nlarge dataset consisting of over 600K objects for segmentation and 440K patches\nfor classification and make the data publicly available. We use our approach to\nprocess the colorectal subset of TCGA, consisting of 599 whole-slide images, to\nlocalise 377 million, 900K and 2.1 million nuclei, glands and lumen\nrespectively. We make this resource available to remove a major barrier in the\ndevelopment of explainable models for computational pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_Q/0/1/0/all/0/1\">Quoc Dang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Camera Pose Regression Using Pseudo-LiDAR. (arXiv:2203.00080v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00080","description":"<p>An accurate and robust large-scale localization system is an integral\ncomponent for active areas of research such as autonomous vehicles and\naugmented reality. To this end, many learning algorithms have been proposed\nthat predict 6DOF camera pose from RGB or RGB-D images. However, previous\nmethods that incorporate depth typically treat the data the same way as RGB\nimages, often adding depth maps as additional channels to RGB images and\npassing them through convolutional neural networks (CNNs). In this paper, we\nshow that converting depth maps into pseudo-LiDAR signals, previously shown to\nbe useful for 3D object detection, is a better representation for camera\nlocalization tasks by projecting point clouds that can accurately determine\n6DOF camera pose. This is demonstrated by first comparing localization\naccuracies of a network operating exclusively on pseudo-LiDAR representations,\nwith networks operating exclusively on depth maps. We then propose FusionLoc, a\nnovel architecture that uses pseudo-LiDAR to regress a 6DOF camera pose.\nFusionLoc is a dual stream neural network, which aims to remedy common issues\nwith typical 2D CNNs operating on RGB-D images. The results from this\narchitecture are compared against various other state-of-the-art deep pose\nregression implementations using the 7 Scenes dataset. The findings are that\nFusionLoc performs better than a number of other camera localization methods,\nwith a notable improvement being, on average, 0.33m and 4.35{\\deg} more\naccurate than RGB-D PoseNet. By proving the validity of using pseudo-LiDAR\nsignals over depth maps for localization, there are new considerations when\nimplementing large-scale localization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Ali Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lolic_L/0/1/0/all/0/1\">Lazar Lolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhter_S/0/1/0/all/0/1\">Shahmir Akhter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_A/0/1/0/all/0/1\">Alfonso Dela Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liut_M/0/1/0/all/0/1\">Michael Liut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment. (arXiv:2203.00108v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00108","description":"<p>DeepFakes are synthetic videos generated by swapping a face of an original\nimage with the face of somebody else. In this paper, we describe our work to\ndevelop general, deep learning-based models to classify DeepFake content. We\npropose a novel framework for using Generative Adversarial Network (GAN)-based\nmodels, we call MRI-GAN, that utilizes perceptual differences in images to\ndetect synthesized videos. We test our MRI-GAN approach and a\nplain-frames-based model using the DeepFake Detection Challenge Dataset. Our\nplain frames-based-model achieves 91% test accuracy and a model which uses our\nMRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the\nperceptual differences achieves 74% test accuracy. The results of MRI-GAN are\npreliminary and may be improved further by modifying the choice of loss\nfunction, tuning hyper-parameters, or by using a more advanced perceptual\nsimilarity metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_P/0/1/0/all/0/1\">Pratikkumar Prajapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollett_C/0/1/0/all/0/1\">Chris Pollett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Right Spin: Learning Object Motion from Rotation-Compensated Flow Fields. (arXiv:2203.00115v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00115","description":"<p>Both a good understanding of geometrical concepts and a broad familiarity\nwith objects lead to our excellent perception of moving objects. The human\nability to detect and segment moving objects works in the presence of multiple\nobjects, complex background geometry, motion of the observer and even\ncamouflage. How humans perceive moving objects so reliably is a longstanding\nresearch question in computer vision and borrows findings from related areas\nsuch as psychology, cognitive science and physics. One approach to the problem\nis to teach a deep network to model all of these effects. This contrasts with\nthe strategy used by human vision, where cognitive processes and body design\nare tightly coupled and each is responsible for certain aspects of correctly\nidentifying moving objects. Similarly from the computer vision perspective,\nthere is evidence that classical, geometry-based techniques are better suited\nto the \"motion-based\" parts of the problem, while deep networks are more\nsuitable for modeling appearance. In this work, we argue that the coupling of\ncamera rotation and camera translation can create complex motion fields that\nare difficult for a deep network to untangle directly. We present a novel\nprobabilistic model to estimate the camera's rotation given the motion field.\nWe then rectify the flow field to obtain a rotation-compensated motion field\nfor subsequent segmentation. This strategy of first estimating camera motion,\nand then allowing a network to learn the remaining parts of the problem, yields\nimproved results on the widely used DAVIS benchmark as well as the recently\npublished motion segmentation data set MoCA (Moving Camouflaged Animals).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bideau_P/0/1/0/all/0/1\">Pia Bideau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1\">Erik Learned-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Delivered Information Trade Study. (arXiv:2203.00116v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00116","description":"<p>The sensor to shooter timeline is affected by two main variables: satellite\npositioning and asset positioning. Speeding up satellite positioning by adding\nmore sensors or by decreasing processing time is important only if there is a\nprepared shooter, otherwise the main source of time is getting the shooter into\nposition. However, the intelligence community should work towards the\nexploitation of sensors to the highest speed and effectiveness possible.\nAchieving a high effectiveness while keeping speed high is a tradeoff that must\nbe considered in the sensor to shooter timeline. In this paper we investigate\ntwo main ideas, increasing the effectiveness of satellite imagery through image\nmanipulation and how on-board image manipulation would affect the sensor to\nshooter timeline. We cover these ideas in four scenarios: Discrete Event\nSimulation of onboard processing versus ground station processing, quality of\ninformation with cloud cover removal, information improvement with super\nresolution, and data reduction with image to caption. This paper will show how\nimage manipulation techniques such as Super Resolution, Cloud Removal, and\nImage to Caption will improve the quality of delivered information in addition\nto showing how those processes effect the sensor to shooter timeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rectifying homographies for stereo vision: analytical solution for minimal distortion. (arXiv:2203.00123v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00123","description":"<p>Stereo rectification is the determination of two image transformations (or\nhomographies) that map corresponding points on the two images, projections of\nthe same point in the 3D space, onto the same horizontal line in the\ntransformed images. Rectification is used to simplify the subsequent stereo\ncorrespondence problem and speeding up the matching process. Rectifying\ntransformations, in general, introduce perspective distortion on the obtained\nimages, which shall be minimised to improve the accuracy of the following\nalgorithm dealing with the stereo correspondence problem. The search for the\noptimal transformations is usually carried out relying on numerical\noptimisation. This work proposes a closed-form solution for the rectifying\nhomographies that minimise perspective distortion. The experimental comparison\nconfirms its capability to solve the convergence issues of the previous\nformulation. Its Python implementation is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lafiosca_P/0/1/0/all/0/1\">Pasquale Lafiosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccaroni_M/0/1/0/all/0/1\">Marta Ceccaroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlazeNeo: Blazing fast polyp segmentation and neoplasm detection. (arXiv:2203.00129v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00129","description":"<p>In recent years, computer-aided automatic polyp segmentation and neoplasm\ndetection have been an emerging topic in medical image analysis, providing\nvaluable support to colonoscopy procedures. Attentions have been paid to\nimproving the accuracy of polyp detection and segmentation. However, not much\nfocus has been given to latency and throughput for performing these tasks on\ndedicated devices, which can be crucial for practical applications. This paper\nintroduces a novel deep neural network architecture called BlazeNeo, for the\ntask of polyp segmentation and neoplasm detection with an emphasis on\ncompactness and speed while maintaining high accuracy. The model leverages the\nhighly efficient HarDNet backbone alongside lightweight Receptive Field Blocks\nfor computational efficiency, and an auxiliary training mechanism to take full\nadvantage of the training data for the segmentation quality. Our experiments on\na challenging dataset show that BlazeNeo achieves improvements in latency and\nmodel size while maintaining comparable accuracy against state-of-the-art\nmethods. When deploying on the Jetson AGX Xavier edge device in INT8 precision,\nour BlazeNeo achieves over 155 fps while yielding the best accuracy among all\ncompared methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+An_N/0/1/0/all/0/1\">Nguyen Sy An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1\">Phan Ngoc Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1\">Dao Viet Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1\">Dao Van Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trung_T/0/1/0/all/0/1\">Tran Quang Trung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks. (arXiv:2203.00131v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00131","description":"<p>Transformers have emerged to be successful in a number of natural language\nprocessing and vision tasks, but their potential applications to medical\nimaging remain largely unexplored due to the unique difficulties of this field.\nIn this study, we present UTNetV2, a simple yet powerful backbone model that\ncombines the strengths of the convolutional neural network and Transformer for\nenhancing performance and efficiency in medical image segmentation. The\ncritical design of UTNetV2 includes three innovations: (1) We used a hybrid\nhierarchical architecture by introducing depthwise separable convolution to\nprojection and feed-forward network in the Transformer block, which brings\nlocal relationship modeling and desirable properties of CNNs (translation\ninvariance) to Transformer, thus eliminate the requirement of large-scale\npre-training. (2) We proposed efficient bidirectional attention (B-MHA) that\nreduces the quadratic computation complexity of self-attention to linear by\nintroducing an adaptively updated semantic map. The efficient attention makes\nit possible to capture long-range relationship and correct the fine-grained\nerrors in high-resolution token maps. (3) The semantic maps in the B-MHA allow\nus to perform semantically and spatially global multi-scale feature fusion\nwithout introducing much computational overhead. Furthermore, we provide a fair\ncomparison codebase of CNN-based and Transformer-based on various medical image\nsegmentation tasks to evaluate the merits and defects of both architectures.\nUTNetV2 demonstrated state-of-the-art performance across various settings,\nincluding large-scale datasets, small-scale datasets, 2D and 3D settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Video Neural Representations for High-Quality Frame Interpolation. (arXiv:2203.00137v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00137","description":"<p>This paper considers the problem of temporal video interpolation, where the\ngoal is to synthesize a new video frame given its two neighbors. We propose\nCross-Video Neural Representation (CURE) as the first video interpolation\nmethod based on neural fields (NF). NF refers to the recent class of methods\nfor the neural representation of complex 3D scenes that has seen widespread\nsuccess and application across computer vision. CURE represents the video as a\ncontinuous function parameterized by a coordinate-based neural network, whose\ninputs are the spatiotemporal coordinates and outputs are the corresponding RGB\nvalues. CURE introduces a new architecture that conditions the neural network\non the input frames for imposing space-time consistency in the synthesized\nvideo. This not only improves the final interpolation quality, but also enables\nCURE to learn a prior across multiple videos. Experimental evaluations show\nthat CURE achieves the state-of-the-art performance on video interpolation on\nseveral benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shangguan_W/0/1/0/all/0/1\">Wentao Shangguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Weijie Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint Segmentation and Motion Prediction in Point Cloud. (arXiv:2203.00138v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00138","description":"<p>Environment perception including detection, classification, tracking, and\nmotion prediction are key enablers for automated driving systems and\nintelligent transportation applications. Fueled by the advances in sensing\ntechnologies and machine learning techniques, LiDAR-based sensing systems have\nbecome a promising solution. The current challenges of this solution are how to\neffectively combine different perception tasks into a single backbone and how\nto efficiently learn the spatiotemporal features directly from point cloud\nsequences. In this research, we propose a novel spatiotemporal attention\nnetwork based on a transformer self-attention mechanism for joint semantic\nsegmentation and motion prediction within a point cloud at the voxel level. The\nnetwork is trained to simultaneously outputs the voxel level class and\npredicted motion by learning directly from a sequence of point cloud datasets.\nThe proposed backbone includes both a temporal attention module (TAM) and a\nspatial attention module (SAM) to learn and extract the complex spatiotemporal\nfeatures. This approach has been evaluated with the nuScenes dataset, and\npromising performance has been achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhensong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Saswat Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_P/0/1/0/all/0/1\">Peng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers. (arXiv:2203.00156v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00156","description":"<p>As technology advances, the need for safe, efficient, and collaborative\nhuman-robot-teams has become increasingly important. One of the most\nfundamental collaborative tasks in any setting is the object handover.\nHuman-to-robot handovers can take either of two approaches: (1) direct\nhand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach\nensures minimal contact between the human and robot but can also result in\nincreased idle time due to having to wait for the object to first be placed\ndown on a surface. To minimize such idle time, the robot must preemptively\npredict the human intent of where the object will be placed. Furthermore, for\nthe robot to preemptively act in any sort of productive manner, predictions and\nmotion planning must occur in real-time. We introduce a novel\nprediction-planning pipeline that allows the robot to preemptively move towards\nthe human agent's intended placement location using gaze and gestures as model\ninputs. In this paper, we investigate the performance and drawbacks of our\nearly intent predictor-planner as well as the practical benefits of using such\na pipeline through a human-robot case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_A/0/1/0/all/0/1\">Andrew Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawed_M/0/1/0/all/0/1\">Mohammad Khalid Jawed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Semantic and Instance Segmentation for Colon Nuclei Identification and Counting. (arXiv:2203.00157v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00157","description":"<p>We address the problem of automated nuclear segmentation, classification, and\nquantification from Haematoxylin and Eosin stained histology images, which is\nof great relevance for several downstream computational pathology applications.\nIn this work, we present a solution framed as a simultaneous semantic and\ninstance segmentation framework. Our solution is part of the Colon Nuclei\nIdentification and Counting (CoNIC) Challenge. We first train a semantic and\ninstance segmentation model separately. Our framework uses as backbone HoverNet\nand Cascade Mask-RCNN models. We then ensemble the results with a custom\nNon-Maximum Suppression embedding (NMS). In our framework, the semantic model\ncomputes a class prediction for the cells whilst the instance model provides a\nrefined segmentation. We demonstrate, through our experimental results, that\nour model outperforms the provided baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Chenyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nuclear Segmentation and Classification Model with Imbalanced Classes for CoNiC Challenge. (arXiv:2203.00171v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00171","description":"<p>Nuclear segmentation and classification is an essential step for\ncomputational pathology. TIA lab from Warwick University organized a nuclear\nsegmentation and classification challenge (CoNiC) for H&amp;E stained\nhistopathology images in colorectal cancer based on the Lizard dataset. In this\nchallenge, computer algorithms should be able to segment and recognize six\ntypes of nuclei, including Epithelial, Lymphocyte, Plasma, Eosinophil,\nNeutrophil, Connective tissue. This challenge introduces two highly correlated\ntasks, nuclei segmentation and classification task and prediction of cellular\ncomposition task. There are a few obstacles we have to address in this\nchallenge, 1) imbalanced annotations with few training samples on minority\nclasses, 2) color variation of the images from multiple centers or scanners, 3)\nlimited training samples, 4) similar morphological appearance among classes. To\ndeal with these challenges, we proposed a systematic pipeline for nuclear\nsegmentation and classification. First, we built a GAN-based model to\nautomatically generate pseudo images for data augmentation. Then we trained a\nself-supervised stain normalization model to solve the color variation problem.\nNext we constructed a baseline model HoVer-Net with cost-sensitive loss to\nencourage the model pay more attention on the minority classes. According to\nthe results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+\n(Rank 33rd) and 0.62199 r2 (Rank 4th) in the preliminary test phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jijun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_F/0/1/0/all/0/1\">Feihu Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenbing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention. (arXiv:2203.00172v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00172","description":"<p>We present a simple but effective attention named the unary-pairwise\nattention (UPA) for modeling the relationship between 3D point clouds. Our idea\nis motivated by the analysis that the standard self-attention (SA) that\noperates globally tends to produce almost the same attention maps for different\nquery positions, revealing difficulties for learning query-independent and\nquery-dependent information jointly. Therefore, we reformulate the SA and\npropose query-independent (Unary) and query-dependent (Pairwise) components to\nfacilitate the learning of both terms. In contrast to the SA, the UPA ensures\nquery dependence via operating locally. Extensive experiments show that the UPA\noutperforms the SA consistently on various point cloud understanding tasks\nincluding shape classification, part segmentation, and scene segmentation.\nMoreover, simply equipping the popular PointNet++ method with the UPA even\noutperforms or is on par with the state-of-the-art attention-based approaches.\nIn addition, the UPA systematically boosts the performance of both standard and\nmodern networks when it is integrated into them as a compositional module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiu_H/0/1/0/all/0/1\">Haoyi Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyoung-Sook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinohara_T/0/1/0/all/0/1\">Takayuki Shinohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qiong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuoka_M/0/1/0/all/0/1\">Masashi Matsuoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACTIVE:Augmentation-Free Graph Contrastive Learning for Partial Multi-View Clustering. (arXiv:2203.00186v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00186","description":"<p>In this paper, we propose an augmentation-free graph contrastive learning\nframework, namely ACTIVE, to solve the problem of partial multi-view\nclustering. Notably, we suppose that the representations of similar samples\n(i.e., belonging to the same cluster) and their multiply views features should\nbe similar. This is distinct from the general unsupervised contrastive learning\nthat assumes an image and its augmentations share a similar representation.\nSpecifically, relation graphs are constructed using the nearest neighbours to\nidentify existing similar samples, then the constructed inter-instance relation\ngraphs are transferred to the missing views to build graphs on the\ncorresponding missing data. Subsequently, two main components, within-view\ngraph contrastive learning (WGC) and cross-view graph consistency learning\n(CGC), are devised to maximize the mutual information of different views within\na cluster. The proposed approach elevates instance-level contrastive learning\nand missing data inference to the cluster-level, effectively mitigating the\nimpact of individual missing data on clustering. Experiments on several\nchallenging datasets demonstrate the superiority of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongxia Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiqiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations. (arXiv:2203.00187v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00187","description":"<p>Robotic detection of people in crowded and/or cluttered human-centered\nenvironments including hospitals, long-term care, stores and airports is\nchallenging as people can become occluded by other people or objects, and\ndeform due to variations in clothing or pose. There can also be loss of\ndiscriminative visual features due to poor lighting. In this paper, we present\na novel multimodal person detection architecture to address the mobile robot\nproblem of person detection under intraclass variations. We present a two-stage\ntraining approach using 1) a unique pretraining method we define as Temporal\nInvariant Multimodal Contrastive Learning (TimCLR), and 2) a Multimodal Faster\nR-CNN (MFRCNN) detector. TimCLR learns person representations that are\ninvariant under intraclass variations through unsupervised learning. Our\napproach is unique in that it generates image pairs from natural variations\nwithin multimodal image sequences, in addition to synthetic data augmentation,\nand contrasts crossmodal features to transfer invariances between different\nmodalities. These pretrained features are used by the MFRCNN detector for\nfinetuning and person detection from RGB-D images. Extensive experiments\nvalidate the performance of our DL architecture in both human-centered crowded\nand cluttered environments. Results show that our method outperforms existing\nunimodal and multimodal person detection approaches in terms of detection\naccuracy in detecting people with body occlusions and pose deformations in\ndifferent lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fung_A/0/1/0/all/0/1\">Angus Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhabib_B/0/1/0/all/0/1\">Beno Benhabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejat_G/0/1/0/all/0/1\">Goldie Nejat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00190","description":"<p>Deep learning methodologies have been employed in several different fields,\nwith an outstanding success in image recognition applications, such as material\nquality control, medical imaging, autonomous driving, etc. Deep learning models\nrely on the abundance of labelled observations to train a prospective model.\nThese models are composed of millions of parameters to estimate, increasing the\nneed of more training observations. Frequently it is expensive to gather\nlabelled observations of data, making the usage of deep learning models not\nideal, as the model might over-fit data. In a semi-supervised setting,\nunlabelled data is used to improve the levels of accuracy and generalization of\na model with small labelled datasets. Nevertheless, in many situations\ndifferent unlabelled data sources might be available. This raises the risk of a\nsignificant distribution mismatch between the labelled and unlabelled datasets.\nSuch phenomena can cause a considerable performance hit to typical\nsemi-supervised deep learning frameworks, which often assume that both labelled\nand unlabelled datasets are drawn from similar distributions. Therefore, in\nthis paper we study the latest approaches for semi-supervised deep learning for\nimage recognition. Emphasis is made in semi-supervised deep learning models\ndesigned to deal with a distribution mismatch between the labelled and\nunlabelled datasets. We address open challenges with the aim to encourage the\ncommunity to tackle them, and overcome the high data demand of traditional deep\nlearning pipelines under real-world usage settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Challenges When 3D Semantic Segmentation Faces Class Imbalanced and OOD Data. (arXiv:2203.00214v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00214","description":"<p>3D semantic segmentation (3DSS) is an essential process in the creation of a\nsafe autonomous driving system. However, deep learning models for 3D semantic\nsegmentation often suffer from the class imbalance problem and\nout-of-distribution (OOD) data. In this study, we explore how the class\nimbalance problem affects 3DSS performance and whether the model can detect the\ncategory prediction correctness, or whether data is ID (in-distribution) or\nOOD. For these purposes, we conduct two experiments using three representative\n3DSS models and five trust scoring methods, and conduct both a confusion and\nfeature analysis of each class. Furthermore, a data augmentation method for the\n3D LiDAR dataset is proposed to create a new dataset based on SemanticKITTI and\nSemanticPOSS, called AugKITTI. We propose the wPre metric and TSD for a more\nin-depth analysis of the results, and follow are proposals with an insightful\ndiscussion. Based on the experimental results, we find that: (1) the classes\nare not only imbalanced in their data size but also in the basic properties of\neach semantic category. (2) The intraclass diversity and interclass ambiguity\nmake class learning difficult and greatly limit the models' performance,\ncreating the challenges of semantic and data gaps. (3) The trust scores are\nunreliable for classes whose features are confused with other classes. For 3DSS\nmodels, those misclassified ID classes and OODs may also be given high trust\nscores, making the 3DSS predictions unreliable, and leading to the challenges\nin judging 3DSS result trustworthiness. All of these outcomes point to several\nresearch directions for improving the performance and reliability of the 3DSS\nmodels used for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yancheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huijing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How certain are your uncertainties?. (arXiv:2203.00238v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00238","description":"<p>Having a measure of uncertainty in the output of a deep learning method is\nuseful in several ways, such as in assisting with interpretation of the\noutputs, helping build confidence with end users, and for improving the\ntraining and performance of the networks. Therefore, several different methods\nhave been proposed to capture various types of uncertainty, including epistemic\n(relating to the model used) and aleatoric (relating to the data) sources, with\nthe most commonly used methods for estimating these being test-time dropout for\nepistemic uncertainty and test-time augmentation for aleatoric uncertainty.\nHowever, these methods are parameterised (e.g. amount of dropout or type and\nlevel of augmentation) and so there is a whole range of possible uncertainties\nthat could be calculated, even with a fixed network and dataset. This work\ninvestigates the stability of these uncertainty measurements, in terms of both\nmagnitude and spatial pattern. In experiments using the well characterised\nBraTS challenge, we demonstrate substantial variability in the magnitude and\nspatial pattern of these uncertainties, and discuss the implications for\ninterpretability, repeatability and confidence in results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitbread_L/0/1/0/all/0/1\">Luke Whitbread</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment. (arXiv:2203.00242v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00242","description":"<p>Vision-and-Language (V+L) pre-training models have achieved tremendous\nsuccess in recent years on various multi-modal benchmarks. However, the\nmajority of existing models require pre-training on a large set of parallel\nimage-text data, which is costly to collect, compared to image-only or\ntext-only data. In this paper, we explore unsupervised Vision-and-Language\npre-training (UVLP) to learn the cross-modal representation from non-parallel\nimage and text datasets. We found two key factors that lead to good\nunsupervised V+L pre-training without parallel data: (i) joint image-and-text\ninput (ii) overall image-text alignment (even for non-parallel data).\nAccordingly, we propose a novel unsupervised V+L pre-training curriculum for\nnon-parallel texts and images. We first construct a weakly aligned image-text\ncorpus via a retrieval-based approach, then apply a set of multi-granular\nalignment pre-training tasks, including region-to-tag, region-to-phrase, and\nimage-to-sentence alignment, to bridge the gap between the two modalities. A\ncomprehensive ablation study shows each granularity is helpful to learn a\nstronger pre-trained model. We adapt our pre-trained model to a set of V+L\ndownstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our\nmodel achieves the state-of-art performance in all these tasks under the\nunsupervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When A Conventional Filter Meets Deep Learning: Basis Composition Learning on Image Filters. (arXiv:2203.00258v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00258","description":"<p>Image filters are fast, lightweight and effective, which make these\nconventional wisdoms preferable as basic tools in vision tasks. In practical\nscenarios, users have to tweak parameters multiple times to obtain satisfied\nresults. This inconvenience heavily discounts the efficiency and user\nexperience. We propose basis composition learning on single image filters to\nautomatically determine their optimal formulas. The feasibility is based on a\ntwo-step strategy: first, we build a set of filtered basis (FB) consisting of\napproximations under selected parameter configurations; second, a dual-branch\ncomposition module is proposed to learn how the candidates in FB are combined\nto better approximate the target image. Our method is simple yet effective in\npractice; it renders filters to be user-friendly and benefits fundamental\nlow-level vision problems including denoising, deraining and texture removal.\nExtensive experiments demonstrate that our method achieves an appropriate\nbalance among the performance, time complexity and memory efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu Lee Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yidan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gary Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection. (arXiv:2203.00259v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00259","description":"<p>Density-based and classification-based methods have ruled unsupervised\nanomaly detection in recent years, while reconstruction-based methods are\nrarely mentioned for the poor reconstruction ability and low performance.\nHowever, the latter requires no costly extra training samples for the\nunsupervised training that is more practical, so this paper focuses on\nimproving this kind of method and proposes a novel Omni-frequency\nChannel-selection Reconstruction (OCR-GAN) network to handle anomaly detection\ntask in a perspective of frequency. Concretely, we propose a Frequency\nDecoupling (FD) module to decouple the input image into different frequency\ncomponents and model the reconstruction process as a combination of parallel\nomni-frequency image restorations, as we observe a significant difference in\nthe frequency distribution of normal and abnormal images. Given the correlation\namong multiple frequencies, we further propose a Channel Selection (CS) module\nthat performs frequency interaction among different encoders by adaptively\nselecting different channels. Abundant experiments demonstrate the\neffectiveness and superiority of our approach over different kinds of methods,\ne.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD\ndataset without extra training data that markedly surpasses the\nreconstruction-based baseline by +38.1 and the current SOTA method by +0.3.\nSource code will be available at https://github.com/zhangzjn/OCR-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yufei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Runze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shuwen Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separable-HoverNet and Instance-YOLO for Colon Nuclei Identification and Counting. (arXiv:2203.00262v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00262","description":"<p>Nuclear segmentation, classification and quantification within Haematoxylin &amp;\nEosin stained histology images enables the extraction of interpretable\ncell-based features that can be used in downstream explainable models in\ncomputational pathology (CPath). However, automatic recognition of different\nnuclei is faced with a major challenge in that there are several different\ntypes of nuclei, some of them exhibiting large intraclass variability. In this\nwork, we propose an approach that combine Separable-HoverNet and\nInstance-YOLOv5 to indentify colon nuclei small and unbalanced. Our approach\ncan achieve mPQ+ 0.389 on the Segmentation and Classification-Preliminary Test\nDataset and r2 0.599 on the Cellular Composition-Preliminary Test Dataset on\nISBI 2022 CoNIC Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chunhui Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liukun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_L/0/1/0/all/0/1\">Lijian Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception. (arXiv:2203.00283v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00283","description":"<p>Visual perception tasks often require vast amounts of labelled data,\nincluding 3D poses and image space segmentation masks. The process of creating\nsuch training data sets can prove difficult or time-intensive to scale up to\nefficacy for general use. Consider the task of pose estimation for rigid\nobjects. Deep neural network based approaches have shown good performance when\ntrained on large, public datasets. However, adapting these networks for other\nnovel objects, or fine-tuning existing models for different environments,\nrequires significant time investment to generate newly labelled instances.\nTowards this end, we propose ProgressLabeller as a method for more efficiently\ngenerating large amounts of 6D pose training data from color images sequences\nfor custom scenes in a scalable manner. ProgressLabeller is intended to also\nsupport transparent or translucent objects, for which the previous methods\nbased on depth dense reconstruction will fail. We demonstrate the effectiveness\nof ProgressLabeller by rapidly create a dataset of over 1M samples with which\nwe fine-tune a state-of-the-art pose estimation network in order to markedly\nimprove the downstream robotic grasp success rates. ProgressLabeller will be\nmade publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_S/0/1/0/all/0/1\">Stanley Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles. (arXiv:2203.00291v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00291","description":"<p>The motion of planar ground vehicles is often non-holonomic, and as a result\nmay be modelled by the 2 DoF Ackermann steering model. We analyse the\nfeasibility of estimating such motion with a downward facing camera that exerts\nfronto-parallel motion with respect to the ground plane. This turns the motion\nestimation into a simple image registration problem in which we only have to\nidentify a 2-parameter planar homography. However, one difficulty that arises\nfrom this setup is that ground-plane features are indistinctive and thus hard\nto match between successive views. We encountered this difficulty by\nintroducing the first globally-optimal, correspondence-less solution to\nplane-based Ackermann motion estimation. The solution relies on the\nbranch-and-bound optimisation technique. Through the low-dimensional\nparametrisation, a derivation of tight bounds, and an efficient implementation,\nwe demonstrate how this technique is eventually amenable to accurate real-time\nmotion estimation. We prove its property of global optimality and analyse the\nimpact of assuming a locally constant centre of rotation. Our results on real\ndata finally demonstrate a significant advantage over the more traditional,\ncorrespondence-based hypothesise-and-test schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Junyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiadi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FP-Loc: Lightweight and Drift-free Floor Plan-assisted LiDAR Localization. (arXiv:2203.00292v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00292","description":"<p>We present a novel framework for floor plan-based, full six degree-of-freedom\nLiDAR localization. Our approach relies on robust ceiling and ground plane\ndetection, which solves part of the pose and supports the segmentation of\nvertical structure elements such as walls and pillars. Our core contribution is\na novel nearest neighbour data structure for an efficient look-up of nearest\nvertical structure elements from the floor plan. The registration is realized\nas a pair-wise regularized windowed pose graph optimization. Highly efficient,\naccurate and drift-free long-term localization is demonstrated on multiple\nscenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial samples for deep monocular 6D object pose estimation. (arXiv:2203.00302v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00302","description":"<p>Estimating object 6D pose from an RGB image is important for many real-world\napplications such as autonomous driving and robotic grasping, where robustness\nof the estimation is crucial. In this work, for the first time, we study\nadversarial samples that can fool state-of-the-art (SOTA) deep learning based\n6D pose estimation models. In particular, we propose a Unified 6D pose\nestimation Attack, namely U6DA, which can successfully attack all the three\nmain categories of models for 6D pose estimation. The key idea of our U6DA is\nto fool the models to predict wrong results for object shapes that are\nessential for correct 6D pose estimation. Specifically, we explore a\ntransfer-based black-box attack to 6D pose estimation. By shifting the\nsegmentation attention map away from its original position, adversarial samples\nare crafted. We show that such adversarial samples are not only effective for\nthe direct 6D pose estimation models, but also able to attack the two-stage\nbased models regardless of their robust RANSAC modules. Extensive experiments\nwere conducted to demonstrate the effectiveness of our U6DA with large-scale\npublic benchmarks. We also introduce a new U6DA-Linemod dataset for robustness\nstudy of the 6D pose estimation task. Our codes and dataset will be available\nat \\url{https://github.com/cuge1995/U6DA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Analysis of the Object Detection Pipeline on UAVs. (arXiv:2203.00306v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00306","description":"<p>An object detection pipeline comprises a camera that captures the scene and\nan object detector that processes these images. The quality of the images\ndirectly affects the performance of the object detector. Many works nowadays\nfocus either on improving the image quality or improving the object detection\nmodels independently, but neglect the importance of joint optimization of the\ntwo subsystems. In this paper, we first empirically analyze the influence of\nseven parameters (quantization, compression, resolution, color model, image\ndistortion, gamma correction, additional channels) in remote sensing\napplications. For our experiments, we utilize three UAV data sets from\ndifferent domains and a mixture of large and small state-of-the-art object\ndetector models to provide an extensive evaluation of the influence of the\npipeline parameters. Additionally, we realize an object detection pipeline\nprototype on an embedded platform for an UAV and give a best practice\nrecommendation for building object detection pipelines based on our findings.\nWe show that not all parameters have an equal impact on detection accuracy and\ndata throughput, and that by using a suitable compromise between parameters we\nare able to improve detection accuracy for lightweight object detection models,\nwhile keeping the same data throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1\">Leon Amadeus Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Sebastian Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection. (arXiv:2203.00307v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00307","description":"<p>Generic Boundary Detection (GBD) aims at locating general boundaries that\ndivide videos into semantically coherent and taxonomy-free units, and could\nserver as an important pre-processing step for long-form video understanding.\nPrevious research separately handle these different-level generic boundaries\nwith specific designs of complicated deep networks from simple CNN to LSTM.\nInstead, in this paper, our objective is to develop a general yet simple\narchitecture for arbitrary boundary detection in videos. To this end, we\npresent Temporal Perceiver, a general architecture with Transformers, offering\na unified solution to the detection of arbitrary generic boundaries. The core\ndesign is to introduce a small set of latent feature queries as anchors to\ncompress the redundant input into fixed dimension via cross-attention blocks.\nThanks to this fixed number of latent units, it reduces the quadratic\ncomplexity of attention operation to a linear form of input frames.\nSpecifically, to leverage the coherence structure of videos, we construct two\ntypes of latent feature queries: boundary queries and context queries, which\nhandle the semantic incoherence and coherence regions accordingly. Moreover, to\nguide the learning of latent feature queries, we propose an alignment loss on\ncross-attention to explicitly encourage the boundary queries to attend on the\ntop possible boundaries. Finally, we present a sparse detection head on the\ncompressed representations and directly output the final boundary detection\nresults without any post-processing module. We test our Temporal Perceiver on a\nvariety of detection benchmarks, ranging from shot-level, event-level, to\nscene-level GBD. Our method surpasses the previous state-of-the-art methods on\nall benchmarks, demonstrating the generalization ability of our temporal\nperceiver.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards IID representation learning and its application on biomedical data. (arXiv:2203.00332v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00332","description":"<p>Due to the heterogeneity of real-world data, the widely accepted independent\nand identically distributed (IID) assumption has been criticized in recent\nstudies on causality. In this paper, we argue that instead of being a\nquestionable assumption, IID is a fundamental task-relevant property that needs\nto be learned. Consider $k$ independent random vectors $\\mathsf{X}^{i = 1,\n\\ldots, k}$, we elaborate on how a variety of different causal questions can be\nreformulated to learning a task-relevant function $\\phi$ that induces IID among\n$\\mathsf{Z}^i := \\phi \\circ \\mathsf{X}^i$, which we term IID representation\nlearning.\n</p>\n<p>For proof of concept, we examine the IID representation learning on\nOut-of-Distribution (OOD) generalization tasks. Concretely, by utilizing the\nrepresentation obtained via the learned function that induces IID, we conduct\nprediction of molecular characteristics (molecular prediction) on two\nbiomedical datasets with real-world distribution shifts introduced by a)\npreanalytical variation and b) sampling protocol. To enable reproducibility and\nfor comparison to the state-of-the-art (SOTA) methods, this is done by\nfollowing the OOD benchmarking guidelines recommended from WILDS. Compared to\nthe SOTA baselines supported in WILDS, the results confirm the superior\nperformance of IID representation learning on OOD tasks. The code is publicly\naccessible via https://github.com/CTPLab/IID_representation_learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlobec_I/0/1/0/all/0/1\">Inti Zlobec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafarge_M/0/1/0/all/0/1\">Maxime Lafarge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yukun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koelzer_V/0/1/0/all/0/1\">Viktor H. Koelzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affordance Learning from Play for Sample-Efficient Policy Learning. (arXiv:2203.00352v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00352","description":"<p>Robots operating in human-centered environments should have the ability to\nunderstand how objects function: what can be done with each object, where this\ninteraction may occur, and how the object is used to achieve a goal. To this\nend, we propose a novel approach that extracts a self-supervised visual\naffordance model from human teleoperated play data and leverages it to enable\nefficient policy learning and motion planning. We combine model-based planning\nwith model-free deep reinforcement learning (RL) to learn policies that favor\nthe same object regions favored by people, while requiring minimal robot\ninteractions with the environment. We evaluate our algorithm, Visual\nAffordance-guided Policy Optimization (VAPO), with both diverse simulation\nmanipulation tasks and real world robot tidy-up experiments to demonstrate the\neffectiveness of our affordance-guided policies. We find that our policies\ntrain 4x faster than the baselines and generalize better to novel objects\nbecause our visual affordance model can anticipate their affordance regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borja_Diaz_J/0/1/0/all/0/1\">Jessica Borja-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalweit_G/0/1/0/all/0/1\">Gabriel Kalweit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1\">Joschka Boedecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation. (arXiv:2203.00355v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00355","description":"<p>Assessing the structure and function of the right ventricle (RV) is important\nin the diagnosis of several cardiac pathologies. However, it remains more\nchallenging to segment the RV than the left ventricle (LV). In this paper, we\nfocus on segmenting the RV in both short (SA) and long-axis (LA) cardiac MR\nimages simultaneously. For this task, we propose a new multi-input/output\narchitecture, hybrid 2D/3D geometric spatial TransformEr Multi-Pass fEature\npyRAmid (Tempera). Our feature pyramid extends current designs by allowing not\nonly a multi-scale feature output but multi-scale SA and LA input images as\nwell. Tempera transfers learned features between SA and LA images via layer\nweight sharing and incorporates a geometric target transformer to map the\npredicted SA segmentation to LA space. Our model achieves an average Dice score\nof 0.836 and 0.798 for the SA and LA, respectively, and 26.31 mm and 31.19 mm\nHausdorff distances. This opens up the potential for the incorporation of RV\nsegmentation models into clinical workflows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Galazis_C/0/1/0/all/0/1\">Christoforos Galazis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huiyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petri_C/0/1/0/all/0/1\">Camille Petri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bharath_A/0/1/0/all/0/1\">Anil A. Bharath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varela_M/0/1/0/all/0/1\">Marta Varela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wilderness Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00379","description":"<p>Wilderness areas offer important ecological and social benefits, and\ntherefore warrant monitoring and preservation. Yet, what makes a place \"wild\"\nis vaguely defined, making the detection and monitoring of wilderness areas via\nremote sensing techniques a challenging task. In this article, we explore the\ncharacteristics and appearance of the vague concept of wilderness areas via\nmultispectral satellite imagery. For this, we apply a novel explainable machine\nlearning technique on a curated dataset, which is sophisticated for the task to\ninvestigate wild and anthropogenic areas in Fennoscandia. The dataset contains\nSentinel-2 images of areas representing 1) protected areas with the aim of\npreserving and retaining the natural character and 2) anthropogenic areas\nconsisting of artificial and agricultural landscapes. With our technique, we\npredict continuous, detailed and high-resolution sensitivity maps of unseen\nremote sensing data in regards to wild and anthropogenic characteristics. Our\nneural network provides an interpretable activation space in which regions are\nsemantically arranged in regards to wild and anthropogenic characteristics and\ncertain land cover classes. This increases confidence in the method and allows\nfor new explanations in regards to the investigated concept. Our model advances\nexplainable machine learning for remote sensing, offers opportunities for\ncomprehensive analyses of existing wilderness, and practical relevance for\nconservation efforts. Code and data are available at\n<a href=\"http://rs.ipb.uni-bonn.de/data\">this http URL</a> and\nhttps://gitlab.jsc.fz-juelich.de/kiste/wilderness, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stomberg_T/0/1/0/all/0/1\">Timo T. Stomberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_T/0/1/0/all/0/1\">Taylor Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1\">Johannes Leonhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP. (arXiv:2203.00386v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00386","description":"<p>Training a text-to-image generator in the general domain (e.g., Dall.e,\nCogView) requires huge amounts of paired text-image data, which is too\nexpensive to collect. In this paper, we propose a self-supervised scheme named\nas CLIP-GEN for general text-to-image generation with the language-image priors\nextracted with a pre-trained CLIP model. In our approach, we only require a set\nof unlabeled images in the general domain to train a text-to-image generator.\nSpecifically, given an image without text labels, we first extract the\nembedding of the image in the united language-vision embedding space with the\nimage encoder of CLIP. Next, we convert the image into a sequence of discrete\ntokens in the VQGAN codebook space (the VQGAN model can be trained with the\nunlabeled image dataset in hand). Finally, we train an autoregressive\ntransformer that maps the image tokens from its unified language-vision\nrepresentation. Once trained, the transformer can generate coherent image\ntokens based on the text embedding extracted from the text encoder of CLIP upon\nan input text. Such a strategy enables us to train a strong and general\ntext-to-image generator with large text-free image dataset such as ImageNet.\nQualitative and quantitative evaluations verify that our method significantly\noutperforms optimization-based text-to-image methods in terms of image quality\nwhile not compromising the text-image matching. Our method can even achieve\ncomparable performance as flagship supervised models like CogView.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinglong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing. (arXiv:2203.00387v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00387","description":"<p>Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture\nsequential video frames and compresses them into a single measurement. Various\nreconstruction methods have been developed to recover the high-speed video\nframes from the snapshot measurement. However, most existing reconstruction\nmethods are incapable of capturing long-range spatial and temporal\ndependencies, which are critical for video processing. In this paper, we\npropose a flexible and robust approach based on graph neural network (GNN) to\nefficiently model non-local interactions between pixels in space as well as\ntime regardless of the distance. Specifically, we develop a motion-aware\ndynamic GNN for better video representation, i.e., represent each pixel as the\naggregation of relative nodes under the guidance of frame-by-frame motions,\nwhich consists of motion-aware dynamic sampling, cross-scale node sampling and\ngraph aggregation. Extensive results on both simulation and real data\ndemonstrate both the effectiveness and efficiency of the proposed approach, and\nthe visualization clearly illustrates the intrinsic dynamic sampling operations\nof our proposed model for boosting the video SCI reconstruction results. The\ncode and models will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ruiying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam-Shape Effects and Noise Removal from THz Time-Domain Images in Reflection Geometry in the 0.25-6 THz Range. (arXiv:2203.00417v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00417","description":"<p>The increasing need of restoring high-resolution Hyper-Spectral (HS) images\nis determining a growing reliance on Computer Vision-based processing to\nenhance the clarity of the image content. HS images can, in fact, suffer from\ndegradation effects or artefacts caused by instrument limitations. This paper\nfocuses on a procedure aimed at reducing the degradation effects,\nfrequency-dependent blur and noise, in Terahertz Time-Domain Spectroscopy\n(THz-TDS) images in reflection geometry. It describes the application of a\njoint deblurring and denoising approach that had been previously proved to be\neffective for the restoration of THz-TDS images in transmission geometry, but\nthat had never been tested in reflection modality. This mode is often the only\none that can be effectively used in most cases, for example when analyzing\nobjects that are either opaque in the THz range, or that cannot be displaced\nfrom their location (e.g., museums), such as those of cultural interest.\nCompared to transmission mode, reflection geometry introduces, however, further\ndistortion to THz data, neglected in existing literature. In this work, we\nsuccessfully implement image deblurring and denoising of both uniform-shape\nsamples (a contemporary 1 Euro cent coin and an inlaid pendant) and samples\nwith the uneven reliefs and corrosion products on the surface which make the\nanalysis of the object particularly complex (an ancient Roman silver coin). The\nstudy demonstrates the ability of image processing to restore data in the 0.25\n- 6 THz range, spanning over more than four octaves, and providing the\nfoundation for future analytical approaches of cultural heritage using the\nfar-infrared spectrum still not sufficiently investigated in literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ljubenovic_M/0/1/0/all/0/1\">Marina Ljubenovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Artesani_A/0/1/0/all/0/1\">Alessia Artesani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonetti_S/0/1/0/all/0/1\">Stefano Bonetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Traviglia_A/0/1/0/all/0/1\">Arianna Traviglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"se-Shweshwe Inspired Fashion Generation. (arXiv:2203.00435v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00435","description":"<p>Fashion is one of the ways in which we show ourselves to the world. It is a\nreflection of our personal decisions and one of the ways in which people\ndistinguish and represent themselves. In this paper, we focus on the fashion\ndesign process and expand computer vision for fashion beyond its current focus\non western fashion. We discuss the history of Southern African se-Shweshwe\nfabric fashion, the collection of a se-Shweshwe dataset, and the application of\nsketch-to-design image generation for affordable fashion-design. The\napplication to fashion raises both technical questions of training with small\namounts of data, and also important questions for computer vision beyond\nfairness, in particular ethical considerations on creating and employing\nfashion datasets, and how computer vision supports cultural representation and\nmight avoid algorithmic cultural appropriation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malobola_L/0/1/0/all/0/1\">Lindiwe Brigitte Malobola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1\">Negar Rostamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1\">Shakir Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary Corrected Multi-scale Fusion Network for Real-time Semantic Segmentation. (arXiv:2203.00436v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00436","description":"<p>Image semantic segmentation aims at the pixel-level classification of images,\nwhich has requirements for both accuracy and speed in practical application.\nExisting semantic segmentation methods mainly rely on the high-resolution input\nto achieve high accuracy and do not meet the requirements of inference time.\nAlthough some methods focus on high-speed scene parsing with lightweight\narchitectures, they can not fully mine semantic features under low computation\nwith relatively low performance. To realize the real-time and high-precision\nsegmentation, we propose a new method named Boundary Corrected Multi-scale\nFusion Network, which uses the designed Low-resolution Multi-scale Fusion\nModule to extract semantic information. Moreover, to deal with boundary errors\ncaused by low-resolution feature map fusion, we further design an additional\nBoundary Corrected Loss to constrain overly smooth features. Extensive\nexperiments show that our method achieves a state-of-the-art balance of\naccuracy and speed for the real-time semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianjiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tengfei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realtime strategy for image data labelling using binary models and active sampling. (arXiv:2203.00439v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00439","description":"<p>Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data.\nMost of the ML and DL applications involve supervised learning which requires\nlabelled data. In the initial phases of ML realm lack of data used to be a\nproblem, now we are in a new era of big data. The supervised ML algorithms\nrequire data to be labelled and of good quality. Labelling task requires a\nlarge amount of money and time investment. Data labelling require a skilled\nperson who will charge high for this task, consider the case of the medical\nfield or the data is in bulk that requires a lot of people assigned to label\nit. The amount of data that is well enough for training needs to be known,\nmoney and time can not be wasted to label the whole data. This paper mainly\naims to propose a strategy that helps in labelling the data along with oracle\nin real-time. With balancing on model contribution for labelling is 89 and 81.1\nfor furniture type and intel scene image data sets respectively. Further with\nbalancing being kept off model contribution is found to be 83.47 and 78.71 for\nfurniture type and flower data sets respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Ankush Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_B/0/1/0/all/0/1\">Bhargava B C</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhadhan_A/0/1/0/all/0/1\">A V Narasimhadhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification. (arXiv:2203.00441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00441","description":"<p>Unsupervised learning technology has caught up with or even surpassed\nsupervised learning technology in general object classification (GOC) and\nperson re-identification (re-ID). However, it is found that the unsupervised\nlearning of fine-grained visual classification (FGVC) is more challenging than\nGOC and person re-ID. In order to bridge the gap between unsupervised and\nsupervised learning for FGVC, we investigate the essential factors (including\nfeature extraction, clustering, and contrastive learning) for the performance\ngap between supervised and unsupervised FGVC. Furthermore, we propose a simple,\neffective, and practical method, termed as UFCL, to alleviate the gap. Three\nkey issues are concerned and improved: First, we introduce a robust and\npowerful backbone, ResNet50-IBN, which has an ability of domain adaptation when\nwe transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to\nintroduce HDBSCAN instead of DBSCAN to do clustering, which can generate better\nclusters for adjacent categories with fewer hyper-parameters. Finally, we\npropose a weighted feature agent and its updating mechanism to do contrastive\nlearning by using the pseudo labels with inevitable noise, which can improve\nthe optimization process of learning the parameters of the network. The\neffectiveness of our UFCL is verified on CUB-200-2011, Oxford-Flowers,\nOxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the\nunsupervised FGVC setting, we achieve state-of-the-art results, and analyze the\nkey factors and the important parameters to provide a practical guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhuang Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technological evaluation of two AFIS systems. (arXiv:2203.00447v1 [cs.CR])","link":"http://arxiv.org/abs/2203.00447","description":"<p>This paper provides a technological evaluation of two Automatic Fingerprint\nIdentification Systems (AFIS) used in forensic applications. Both of them are\ninstalled and working in Spanish police premises. The first one is a Printrak\nAFIS 2000 system with a database of more than 450,000 fingerprints, while the\nsecond one is a NEC AFIS 21 SAID NT-LEXS Release 2.4.4 with a database of more\nthan 15 million fingerprints. Our experiments reveal that although both systems\ncan manage inkless fingerprints, the latest one offers better experimental\nresults\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning based Prediction of MSI in Colorectal Cancer via Prediction of the Status of MMR Markers. (arXiv:2203.00449v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.00449","description":"<p>An accurate diagnosis and profiling of tumour are critical to the best\ntreatment choices for cancer patients. In addition to the cancer type and its\naggressiveness, molecular heterogeneity also plays a vital role in treatment\nselection. MSI or MMR deficiency is one of the well-studied aberrations in\nterms of molecular changes. Colorectal cancer patients with MMR deficiency\nrespond well to immunotherapy, hence assessment of the relevant molecular\nmarkers can assist clinicians in making optimal treatment selections for\npatients. Immunohistochemistry is one of the ways for identifying these\nmolecular changes which requires additional sections of tumour tissue.\nIntroduction of automated methods that can predict MSI or MMR status from a\ntarget image without the need for additional sections can substantially reduce\nthe cost associated with it. In this work, we present our work on predicting\nMSI status in a two-stage process using a single target slide either stained\nwith CK818 or H\\&amp;E. First, we train a multi-headed convolutional neural network\nmodel where each head is responsible for predicting one of the MMR protein\nexpressions. To this end, we perform registration of MMR slides to the target\nslide as a pre-processing step. In the second stage, statistical features\ncomputed from the MMR prediction maps are used for the final MSI prediction.\nOur results demonstrate that MSI classification can be improved on\nincorporating fine-grained MMR labels in comparison to the previous approaches\nin which coarse labels (MSI/MSS) are utilised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nimir_M/0/1/0/all/0/1\">Mohammed Nimir</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robison_A/0/1/0/all/0/1\">Andrew Robison</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Tailed Classification with Gradual Balanced Loss and Adaptive Feature Generation. (arXiv:2203.00452v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00452","description":"<p>The real-world data distribution is essentially long-tailed, which poses\ngreat challenge to the deep model. In this work, we propose a new method,\nGradual Balanced Loss and Adaptive Feature Generator (GLAG) to alleviate\nimbalance. GLAG first learns a balanced and robust feature model with Gradual\nBalanced Loss, then fixes the feature model and augments the under-represented\ntail classes on the feature level with the knowledge from well-represented head\nclasses. And the generated samples are mixed up with real training samples\nduring training epochs. Gradual Balanced Loss is a general loss and it can\ncombine with different decoupled training methods to improve the original\nperformance. State-of-the-art results have been achieved on long-tail datasets\nsuch as CIFAR100-LT, ImageNetLT, and iNaturalist, which demonstrates the\neffectiveness of GLAG for long-tailed visual recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JOINED : Prior Guided Multi-task Learning for Joint Optic Disc/Cup Segmentation and Fovea Detection. (arXiv:2203.00461v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00461","description":"<p>Fundus photography has been routinely used to document the presence and\nseverity of various retinal degenerative diseases such as age-related macula\ndegeneration, glaucoma, and diabetic retinopathy, for which the fovea, optic\ndisc (OD), and optic cup (OC) are important anatomical landmarks.\nIdentification of those anatomical landmarks is of great clinical importance.\nHowever, the presence of lesions, drusen, and other abnormalities during\nretinal degeneration severely complicates automatic landmark detection and\nsegmentation. Most existing works treat the identification of each landmark as\na single task and typically do not make use of any clinical prior information.\nIn this paper, we present a novel method, named JOINED, for prior guided\nmulti-task learning for joint OD/OC segmentation and fovea detection. An\nauxiliary branch for distance prediction, in addition to a segmentation branch\nand a detection branch, is constructed to effectively utilize the distance\ninformation from each image pixel to landmarks of interest. Our proposed JOINED\npipeline consists of a coarse stage and a fine stage. At the coarse stage, we\nobtain the OD/OC coarse segmentation and the heatmap localization of fovea\nthrough a joint segmentation and detection module. Afterwards, we crop the\nregions of interest for subsequent fine processing and use predictions obtained\nat the coarse stage as additional information for better performance and faster\nconvergence. Experimental results reveal that our proposed JOINED outperforms\nexisting state-of-the-art approaches on the publicly-available GAMMA, PALM, and\nREFUGE datasets of fundus images. Furthermore, JOINED ranked the 5th on the\nOD/OC segmentation and fovea detection tasks in the GAMMA challenge hosted by\nthe MICCAI2021 workshop OMIA8.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiyuan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compliance Challenges in Forensic Image Analysis Under the Artificial Intelligence Act. (arXiv:2203.00469v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00469","description":"<p>In many applications of forensic image analysis, state-of-the-art results are\nnowadays achieved with machine learning methods. However, concerns about their\nreliability and opaqueness raise the question whether such methods can be used\nin criminal investigations. So far, this question of legal compliance has\nhardly been discussed, also because legal regulations for machine learning\nmethods were not defined explicitly. To this end, the European Commission\nrecently proposed the artificial intelligence (AI) act, a regulatory framework\nfor the trustworthy use of AI. Under the draft AI act, high-risk AI systems for\nuse in law enforcement are permitted but subject to compliance with mandatory\nrequirements. In this paper, we review why the use of machine learning in\nforensic image analysis is classified as high-risk. We then summarize the\nmandatory requirements for high-risk AI systems and discuss these requirements\nin light of two forensic applications, license plate recognition and deep fake\ndetection. The goal of this paper is to raise awareness of the upcoming legal\nrequirements and to point out avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorch_B/0/1/0/all/0/1\">Benedikt Lorch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheler_N/0/1/0/all/0/1\">Nicole Scheler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Recurrent Fusion for Indoor Localization. (arXiv:2203.00510v1 [eess.SP])","link":"http://arxiv.org/abs/2203.00510","description":"<p>This paper considers indoor localization using multi-modal wireless signals\nincluding Wi-Fi, inertial measurement unit (IMU), and ultra-wideband (UWB). By\nformulating the localization as a multi-modal sequence regression problem, a\nmulti-stream recurrent fusion method is proposed to combine the current hidden\nstate of each modality in the context of recurrent neural networks while\naccounting for the modality uncertainty which is directly learned from its own\nimmediate past states. The proposed method was evaluated on the large-scale\nSPAWC2021 multi-modal localization dataset and compared with a wide range of\nbaseline methods including the trilateration method, traditional fingerprinting\nmethods, and convolution network-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jianyuan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koike_Akino_T/0/1/0/all/0/1\">Toshiaki Koike-Akino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlik_P/0/1/0/all/0/1\">Philip V. Orlik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00523","description":"<p>Deep generative models, such as Variational Autoencoders (VAEs) and\nGenerative Adversarial Networks (GANs), have been employed widely in\ncomputational creativity research. However, such models discourage\nout-of-distribution generation to avoid spurious sample generation, thereby\nlimiting their creativity. Thus, incorporating research on human creativity\ninto generative deep learning techniques presents an opportunity to make their\noutputs more compelling and human-like. As we see the emergence of generative\nmodels directed toward creativity research, a need for machine learning-based\nsurrogate metrics to characterize creative output from these models is\nimperative. We propose group-based subset scanning to identify, quantify, and\ncharacterize creative processes by detecting a subset of anomalous\nnode-activations in the hidden layers of the generative models. Our experiments\non the standard image benchmarks, and their \"creatively generated\" variants,\nreveal that the proposed subset scores distribution is more useful for\ndetecting creative processes in the activation space rather than the pixel\nspace. Further, we found that creative samples generate larger subsets of\nanomalies than normal or non-creative samples across datasets. The node\nactivations highlighted during the creative decoding process are different from\nthose responsible for the normal sample generation. Lastly, we assess if the\nimages from the subsets selected by our method were also found creative by\nhuman evaluators, presenting a link between creativity perception in humans and\nnode activations within deep neural nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1\">Brian Quanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction. (arXiv:2203.00531v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00531","description":"<p>An important limitation to the development of Artificial Intelligence\n(AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a\npublic reference benchmark to train and evaluate deep learning (DL) models. In\nthis work, we describe a fully annotated dataset of 756 videos of developing\nembryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D\narchitectures to our dataset and demonstrate that they overperform algorithmic\napproaches to automatically annotate stage development phases. Altogether, we\npropose the first public benchmark that will allow the community to evaluate\nmorphokinetic models. This is the first step towards deep learning-powered IVF.\nOf note, we propose highly detailed annotations with 16 different development\nphases, including early cell division phases, but also late cell divisions,\nphases after morulation, and very early phases, which have never been used\nbefore. We postulate that this original approach will help improve the overall\nperformance of deep learning approaches on time-lapse videos of embryo\ndevelopment, ultimately benefiting infertile patients with improved clinical\nsuccess rates (Code and data are available at\nhttps://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feyeux_M/0/1/0/all/0/1\">Magalie Feyeux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Normand_N/0/1/0/all/0/1\">Nicolas Normand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_L/0/1/0/all/0/1\">Laurent David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paul_Gilloteaux_P/0/1/0/all/0/1\">Perrine Paul-Gilloteaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Descriptellation: Deep Learned Constellation Descriptors for SLAM. (arXiv:2203.00567v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00567","description":"<p>Current global localization descriptors in Simultaneous Localization and\nMapping (SLAM) often fail under vast viewpoint or appearance changes. Adding\ntopological information of semantic objects into the descriptors ameliorates\nthe problem. However, hand-crafted topological descriptors extract limited\ninformation and they are not robust to environmental noise, drastic perspective\nchanges, or object occlusion or misdetections. To solve this problem, we\nformulate a learning-based approach by constructing constellations from\nsemantically meaningful objects and use Deep Graph Convolution Networks to map\nthe constellation representation to a descriptor. We demonstrate the\neffectiveness of our Deep Learned Constellation Descriptor (Descriptellation)\non the Paris-Rue-Lille and IQmulus datasets. Although Descriptellation is\ntrained on randomly generated simulation datasets, it shows good generalization\nabilities on real-world datasets. Descriptellation outperforms the PointNet and\nhandcrafted constellation descriptors for global localization, and shows\nrobustness against different types of noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chunwei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cramariuc_A/0/1/0/all/0/1\">Andrei Cramariuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gull_S/0/1/0/all/0/1\">Samuel Gull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jen Jen Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschopp_F/0/1/0/all/0/1\">Florian Tschopp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a unified view of unsupervised non-local methods for image denoising: the NL-Ridge approach. (arXiv:2203.00570v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00570","description":"<p>We propose a unified view of unsupervised non-local methods for image\ndenoising that linearily combine noisy image patches. The best methods,\nestablished in different modeling and estimation frameworks, are two-step\nalgorithms. Leveraging Stein's unbiased risk estimate (SURE) for the first step\nand the \"internal adaptation\", a concept borrowed from deep learning theory,\nfor the second one, we show that our NL-Ridge approach enables to reconcile\nseveral patch aggregation methods for image denoising. In the second step, our\nclosed-form aggregation weights are computed through multivariate Ridge\nregressions. Experiments on artificially noisy images demonstrate that NL-Ridge\nmay outperform well established state-of-the-art unsupervised denoisers such as\nBM3D and NL-Bayes, as well as recent unsupervised deep learning methods, while\nbeing simpler conceptually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Herbreteau_S/0/1/0/all/0/1\">S&#xe9;bastien Herbreteau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kervrann_C/0/1/0/all/0/1\">Charles Kervrann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology. (arXiv:2203.00585v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00585","description":"<p>Tissue phenotyping is a fundamental task in learning objective\ncharacterizations of histopathologic biomarkers within the tumor-immune\nmicroenvironment in cancer pathology. However, whole-slide imaging (WSI) is a\ncomplex computer vision in which: 1) WSIs have enormous image resolutions with\nprecludes large-scale pixel-level efforts in data curation, and 2) diversity of\nmorphological phenotypes results in inter- and intra-observer variability in\ntissue labeling. To address these limitations, current efforts have proposed\nusing pretrained image encoders (transfer learning from ImageNet,\nself-supervised pretraining) in extracting morphological features from\npathology, but have not been extensively validated. In this work, we conduct a\nsearch for good representations in pathology by training a variety of\nself-supervised models with validation on a variety of weakly-supervised and\npatch-level tasks. Our key finding is in discovering that Vision Transformers\nusing DINO-based knowledge distillation are able to learn data-efficient and\ninterpretable features in histology images wherein the different attention\nheads learn distinct morphological phenotypes. We make evaluation code and\npretrained weights publicly-available at:\nhttps://github.com/Richarizardd/Self-Supervised-ViT-Path.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rahul G. Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwitchHit: A Probabilistic, Complementarity-Based Switching System for Improved Visual Place Recognition in Changing Environments. (arXiv:2203.00591v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00591","description":"<p>Visual place recognition (VPR), a fundamental task in computer vision and\nrobotics, is the problem of identifying a place mainly based on visual\ninformation. Viewpoint and appearance changes, such as due to weather and\nseasonal variations, make this task challenging. Currently, there is no\nuniversal VPR technique that can work in all types of environments, on a\nvariety of robotic platforms, and under a wide range of viewpoint and\nappearance changes. Recent work has shown the potential of combining different\nVPR methods intelligently by evaluating complementarity for some specific VPR\ndatasets to achieve better performance. This, however, requires ground truth\ninformation (correct matches) which is not available when a robot is deployed\nin a real-world scenario. Moreover, running multiple VPR techniques in parallel\nmay be prohibitive for resource-constrained embedded platforms. To overcome\nthese limitations, this paper presents a probabilistic complementarity based\nswitching VPR system, SwitchHit. Our proposed system consists of multiple VPR\ntechniques, however, it does not simply run all techniques at once, rather\npredicts the probability of correct match for an incoming query image and\ndynamically switches to another complementary technique if the probability of\ncorrectly matching the query is below a certain threshold. This innovative use\nof multiple VPR techniques allow our system to be more efficient and robust\nthan other combined VPR approaches employing brute force and running multiple\nVPR techniques at once. Thus making it more suitable for resource constrained\nembedded systems and achieving an overall superior performance from what any\nindividual VPR method in the system could have by achieved running\nindependently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waheed_M/0/1/0/all/0/1\">Maria Waheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A unified 3D framework for Organs at Risk Localization and Segmentation for Radiation Therapy Planning. (arXiv:2203.00624v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00624","description":"<p>Automatic localization and segmentation of organs-at-risk (OAR) in CT are\nessential pre-processing steps in medical image analysis tasks, such as\nradiation therapy planning. For instance, the segmentation of OAR surrounding\ntumors enables the maximization of radiation to the tumor area without\ncompromising the healthy tissues. However, the current medical workflow\nrequires manual delineation of OAR, which is prone to errors and is\nannotator-dependent. In this work, we aim to introduce a unified 3D pipeline\nfor OAR localization-segmentation rather than novel localization or\nsegmentation architectures. To the best of our knowledge, our proposed\nframework fully enables the exploitation of 3D context information inherent in\nmedical imaging. In the first step, a 3D multi-variate regression network\npredicts organs' centroids and bounding boxes. Secondly, 3D organ-specific\nsegmentation networks are leveraged to generate a multi-organ segmentation map.\nOur method achieved an overall Dice score of $0.9260\\pm 0.18 \\%$ on the\nVISCERAL dataset containing CT scans with varying fields of view and multiple\norgans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_F/0/1/0/all/0/1\">Fernando Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasahara_G/0/1/0/all/0/1\">Guido Sasahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peeken_J/0/1/0/all/0/1\">Jan C. Peeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Combs_S/0/1/0/all/0/1\">Stephanie E. Combs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern H. Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full RGB Just Noticeable Difference (JND) Modelling. (arXiv:2203.00629v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00629","description":"<p>Just Noticeable Difference (JND) has many applications in multimedia signal\nprocessing, especially for visual data processing up to date. It's generally\ndefined as the minimum visual content changes that the human can perspective,\nwhich has been studied for decades. However, most of the existing methods only\nfocus on the luminance component of JND modelling and simply regard chrominance\ncomponents as scaled versions of luminance. In this paper, we propose a JND\nmodel to generate the JND by taking the characteristics of full RGB channels\ninto account, termed as the RGB-JND. To this end, an RGB-JND-NET is proposed,\nwhere the visual content in full RGB channels is used to extract features for\nJND generation. To supervise the JND generation, an adaptive image quality\nassessment combination (AIC) is developed. Besides, the RDB-JND-NET also takes\nthe visual attention into account by automatically mining the underlying\nrelationship between visual attention and the JND, which is further used to\nconstrain the JND spatial distribution. To the best of our knowledge, this is\nthe first work on careful investigation of JND modelling for full-color space.\nExperimental results demonstrate that the RGB-JND-NET model outperforms the\nrelevant state-of-the-art JND models. Besides, the JND of the red and blue\nchannels are larger than that of the green one according to the experimental\nresults of the proposed model, which demonstrates that more changes can be\ntolerated in the red and blue channels, in line with the well-known fact that\nthe human visual system is more sensitive to the green channel in comparison\nwith the red and blue ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lili Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huaxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Multi-Scale Learning For Outcome Prediction in 3D PET Images. (arXiv:2203.00641v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00641","description":"<p>Background and Objectives: Predicting patient response to treatment and\nsurvival in oncology is a prominent way towards precision medicine. To that\nend, radiomics was proposed as a field of study where images are used instead\nof invasive methods. The first step in radiomic analysis is the segmentation of\nthe lesion. However, this task is time consuming and can be physician\nsubjective. Automated tools based on supervised deep learning have made great\nprogress to assist physicians. However, they are data hungry, and annotated\ndata remains a major issue in the medical field where only a small subset of\nannotated images is available. Methods: In this work, we propose a multi-task\nlearning framework to predict patient's survival and response. We show that the\nencoder can leverage multiple tasks to extract meaningful and powerful features\nthat improve radiomics performance. We show also that subsidiary tasks serve as\nan inductive bias so that the model can better generalize. Results: Our model\nwas tested and validated for treatment response and survival in lung and\nesophageal cancers, with an area under the ROC curve of 77% and 71%\nrespectively, outperforming single task learning methods. Conclusions: We show\nthat, by using a multi-task learning approach, we can boost the performance of\nradiomic analysis by extracting rich information of intratumoral and\nperitumoral regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Amyar_A/0/1/0/all/0/1\">Amine Amyar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modzelewski_R/0/1/0/all/0/1\">Romain Modzelewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vera_P/0/1/0/all/0/1\">Pierre Vera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morard_V/0/1/0/all/0/1\">Vincent Morard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Autoencoders Without the Variation. (arXiv:2203.00645v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00645","description":"<p>Variational autoencdoers (VAE) are a popular approach to generative\nmodelling. However, exploiting the capabilities of VAEs in practice can be\ndifficult. Recent work on regularised and entropic autoencoders have begun to\nexplore the potential, for generative modelling, of removing the variational\napproach and returning to the classic deterministic autoencoder (DAE) with\nadditional novel regularisation methods. In this paper we empirically explore\nthe capability of DAEs for image generation without additional novel methods\nand the effect of the implicit regularisation and smoothness of large networks.\nWe find that DAEs can be used successfully for image generation without\nadditional loss terms, and that many of the useful properties of VAEs can arise\nimplicitly from sufficiently large convolutional encoders and decoders when\ntrained on CIFAR-10 and CelebA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daly_G/0/1/0/all/0/1\">Gregory A. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieldsend_J/0/1/0/all/0/1\">Jonathan E. Fieldsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_G/0/1/0/all/0/1\">Gavin Tabor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks. (arXiv:2203.00667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00667","description":"<p>Generative Adversarial Networks (GANs) are very popular frameworks for\ngenerating high-quality data, and are immensely used in both the academia and\nindustry in many domains. Arguably, their most substantial impact has been in\nthe area of computer vision, where they achieve state-of-the-art image\ngeneration. This chapter gives an introduction to GANs, by discussing their\nprinciple mechanism and presenting some of their inherent problems during\ntraining and evaluation. We focus on these three issues: (1) mode collapse, (2)\nvanishing gradients, and (3) generation of low-quality images. We then list\nsome architecture-variant and loss-variant GANs that remedy the above\nchallenges. Lastly, we present two utilization examples of GANs for real-world\napplications: Data augmentation and face images generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gilad Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption. (arXiv:2203.00672v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00672","description":"<p>In this paper, we investigate the generalization problem of person\nre-identification (re-id), whose major challenge is the distribution shift on\nan unseen domain. As an important tool of regularizing the distribution, batch\nnormalization (BN) has been widely used in existing methods. However, they\nneglect that BN is severely biased to the training domain and inevitably\nsuffers the performance drop if directly generalized without being updated. To\ntackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel\nre-id framework that applies the self-supervised strategy to update BN\nparameters adaptively. Specifically, BNTA quickly explores the domain-aware\ninformation within unlabeled target data before inference, and accordingly\nmodulates the feature distribution normalized by BN to adapt to the target\ndomain. This is accomplished by two designed self-supervised auxiliary tasks,\nnamely part positioning and part nearest neighbor matching, which help the\nmodel mine the domain-aware information with respect to the structure and\nidentity of body parts, respectively. To demonstrate the effectiveness of our\nmethod, we conduct extensive experiments on three re-id datasets and confirm\nthe superior performance to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Ke Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00680","description":"<p>Manual annotation of large-scale point cloud dataset for varying tasks such\nas 3D object classification, segmentation and detection is often laborious\nowing to the irregular structure of point clouds. Self-supervised learning,\nwhich operates without any human labeling, is a promising approach to address\nthis issue. We observe in the real world that humans are capable of mapping the\nvisual concepts learnt from 2D images to understand the 3D world. Encouraged by\nthis insight, we propose CrossPoint, a simple cross-modal contrastive learning\napproach to learn transferable 3D point cloud representations. It enables a\n3D-2D correspondence of objects by maximizing agreement between point clouds\nand the corresponding rendered 2D image in the invariant space, while\nencouraging invariance to transformations in the point cloud modality. Our\njoint training objective combines the feature correspondences within and across\nmodalities, thus ensembles a rich learning signal from both 3D point cloud and\n2D image modalities in a self-supervised fashion. Experimental results show\nthat our approach outperforms the previous unsupervised learning methods on a\ndiverse range of downstream tasks including 3D object classification and\nsegmentation. Further, the ablation studies validate the potency of our\napproach for a better point cloud understanding. Code and pretrained models are\navailable at <a href=\"http://github.com/MohamedAfham/CrossPoint.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afham_M/0/1/0/all/0/1\">Mohamed Afham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_I/0/1/0/all/0/1\">Isuru Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_D/0/1/0/all/0/1\">Dinithi Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmasiri_A/0/1/0/all/0/1\">Amaya Dharmasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1\">Kanchana Thilakarathna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't miss the Mismatch: Investigating the Objective Function Mismatch for Unsupervised Representation Learning. (arXiv:2009.02383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.02383","description":"<p>Finding general evaluation metrics for unsupervised representation learning\ntechniques is a challenging open research question, which recently has become\nmore and more necessary due to the increasing interest in unsupervised methods.\nEven though these methods promise beneficial representation characteristics,\nmost approaches currently suffer from the objective function mismatch. This\nmismatch states that the performance on a desired target task can decrease when\nthe unsupervised pretext task is learned too long - especially when both tasks\nare ill-posed. In this work, we build upon the widely used linear evaluation\nprotocol and define new general evaluation metrics to quantitatively capture\nthe objective function mismatch and the more generic metrics mismatch. We\ndiscuss the usability and stability of our protocols on a variety of pretext\nand target tasks and study mismatches in a wide range of experiments. Thereby\nwe disclose dependencies of the objective function mismatch across several\npretext and target tasks with respect to the pretext model's representation\nsize, target model complexity, pretext and target augmentations as well as\npretext and target task types. In our experiments, we find that the objective\nfunction mismatch reduces performance by ~0.1-5.0% for Cifar10, Cifar100 and\nPCam in many setups, and up to ~25-59% in extreme cases for the 3dshapes\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stuhr_B/0/1/0/all/0/1\">Bonifaz Stuhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauer_J/0/1/0/all/0/1\">J&#xfc;rgen Brauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Grassmannians for Dimensionality Reduction with Applications. (arXiv:2010.14589v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.14589","description":"<p>In the recent past, nested structures in Riemannian manifolds has been\nstudied in the context of dimensionality reduction as an alternative to the\npopular principal geodesic analysis (PGA) technique, for example, the principal\nnested spheres. In this paper, we propose a novel framework for constructing a\nnested sequence of homogeneous Riemannian manifolds. Common examples of\nhomogeneous Riemannian manifolds include the $n$-sphere, the Stiefel manifold,\nthe Grassmann manifold and many others. In particular, we focus on applying the\nproposed framework to the Grassmann manifold, giving rise to the nested\nGrassmannians (NG). An important application in which Grassmann manifolds are\nencountered is planar shape analysis. Specifically, each planar (2D) shape can\nbe represented as a point in the complex projective space which is a complex\nGrass-mann manifold. Some salient features of our framework are: (i) it\nexplicitly exploits the geometry of the homogeneous Riemannian manifolds and\n(ii) the nested lower-dimensional submanifolds need not be geodesic. With the\nproposed NG structure, we develop algorithms for the supervised and\nunsupervised dimensionality reduction problems respectively. The proposed\nalgorithms are compared with PGA via simulation studies and real data\nexperiments and are shown to achieve a higher ratio of expressed variance\ncompared to PGA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun-Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1\">Baba C. Vemuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.04293","description":"<p>Humans are able to perceive, understand and reason about causal events.\nDeveloping models with similar physical and causal understanding capabilities\nis a long-standing goal of artificial intelligence. As a step towards this\ndirection, we introduce CRAFT, a new video question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories in CRAFT\ninclude previously studied descriptive and counterfactual questions.\nAdditionally, inspired by the Force Dynamics Theory in cognitive linguistics,\nwe introduce a new causal question category that involves understanding the\ncausal interactions between objects through notions like cause, enable, and\nprevent. Our results show that even though the questions in CRAFT are easy for\nhumans, the tested baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1\">Tayfun Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1\">M. Samil Atesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1\">Cagatay Yigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">Ilker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1\">Mert Kobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1\">Tilbe Goksun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Computer Vision Methodology for Quantitative Metallography. (arXiv:2104.11159v3 [cond-mat.mtrl-sci] UPDATED)","link":"http://arxiv.org/abs/2104.11159","description":"<p>Metallography is crucial for a proper assessment of material's properties. It\ninvolves mainly the investigation of spatial distribution of grains and the\noccurrence and characteristics of inclusions or precipitates. This work\npresents an holistic artificial intelligence model for Anomaly Detection that\nautomatically quantifies the degree of anomaly of impurities in alloys. We\nsuggest the following examination process: (1) Deep semantic segmentation is\nperformed on the inclusions (based on a suitable metallographic database of\nalloys and corresponding tags of inclusions), producing inclusions masks that\nare saved into a separated database. (2) Deep image inpainting is performed to\nfill the removed inclusions parts, resulting in 'clean' metallographic images,\nwhich contain the background of grains. (3) Grains' boundaries are marked using\ndeep semantic segmentation (based on another metallographic database of\nalloys), producing boundaries that are ready for further inspection on the\ndistribution of grains' size. (4) Deep anomaly detection and pattern\nrecognition is performed on the inclusions masks to determine spatial, shape\nand area anomaly detection of the inclusions. Finally, the system recommends to\nan expert on areas of interests for further examination. The performance of the\nmodel is presented and analyzed based on few representative cases. Although the\nmodels presented here were developed for metallography analysis, most of them\ncan be generalized to a wider set of problems in which anomaly detection of\ngeometrical objects is desired. All models as well as the data-sets that were\ncreated for this work, are publicly available at\nhttps://github.com/Scientific-Computing-Lab-NRCN/MLography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Rusanovsky_M/0/1/0/all/0/1\">Matan Rusanovsky</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Beeri_O/0/1/0/all/0/1\">Ofer Beeri</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation. (arXiv:2105.00402v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.00402","description":"<p>Colorectal cancer is among the most common malignancies and can develop from\nhigh-risk colon polyps. Colonoscopy is an effective screening tool to detect\nand remove polyps, especially in the case of precancerous lesions. However, the\nmissing rate in clinical practice is relatively high due to many factors. The\nprocedure could benefit greatly from using AI models for automatic polyp\nsegmentation, which provide valuable insights for improving colon polyp\ndetection. However, precise segmentation is still challenging due to variations\nof polyps in size, shape, texture, and color. This paper proposes a novel\nneural network architecture called AG-CUResNeSt, which enhances Coupled UNets\nusing the robust ResNeSt backbone and attention gates. The network is capable\nof effectively combining multi-level features to yield accurate polyp\nsegmentation. Experimental results on five popular benchmark datasets show that\nour proposed method achieves state-of-the-art accuracy compared to existing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_T/0/1/0/all/0/1\">Tran Quang Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1\">Phan Ngoc Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1\">Dao Viet Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1\">Dao Van Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttDLNet: Attention-based DL Network for 3D LiDAR Place Recognition. (arXiv:2106.09637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09637","description":"<p>LiDAR-based place recognition is one of the key components of SLAM and global\nlocalization in autonomous vehicles and robotics applications. With the success\nof DL approaches in learning useful information from 3D LiDARs, place\nrecognition has also benefited from this modality, which has led to higher\nre-localization and loop-closure detection performance, particularly, in\nenvironments with significant changing conditions. Despite the progress in this\nfield, the extraction of proper and efficient descriptors from 3D LiDAR data\nthat are invariant to changing conditions and orientation is still an unsolved\nchallenge. To address this problem, this work proposes a novel 3D LiDAR-based\ndeep learning network (named AttDLNet) that uses a range-based proxy\nrepresentation for point clouds and an attention network with stacked attention\nlayers to selectively focus on long-range context and inter-feature\nrelationships. The proposed network is trained and validated on the KITTI\ndataset and an ablation study is presented to assess the novel attention\nnetwork. Results show that adding attention to the network improves\nperformance, leading to efficient loop closures, and outperforming an\nestablished 3D LiDAR-based place recognition approach. From the ablation study,\nresults indicate that the middle encoder layers have the highest mean\nperformance, while deeper layers are more robust to orientation change. The\ncode is publicly available at https://github.com/Cybonic/AttDLNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">Tiago Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1\">Lu&#xed;s Garrote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1\">Ricardo Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">Urbano J. Nunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Place recognition survey: An update on deep learning approaches. (arXiv:2106.10458v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10458","description":"<p>Autonomous Vehicles (AV) are becoming more capable of navigating in complex\nenvironments with dynamic and changing conditions. A key component that enables\nthese intelligent vehicles to overcome such conditions and become more\nautonomous is the sophistication of the perception and localization systems. As\npart of the localization system, place recognition has benefited from recent\ndevelopments in other perception tasks such as place categorization or object\nrecognition, namely with the emergence of deep learning (DL) frameworks. This\npaper surveys recent approaches and methods used in place recognition,\nparticularly those based on deep learning. The contributions of this work are\ntwofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in\nplace recognition; and categorizing the various DL-based place recognition\nworks into supervised, unsupervised, semi-supervised, parallel, and\nhierarchical categories. First, this survey introduces key place recognition\nconcepts to contextualize the reader. Then, sensor characteristics are\naddressed. This survey proceeds by elaborating on the various DL-based works,\npresenting summaries for each framework. Some lessons learned from this survey\ninclude: the importance of NetVLAD for supervised end-to-end learning; the\nadvantages of unsupervised approaches in place recognition, namely for\ncross-domain applications; or the increasing tendency of recent works to seek,\nnot only for higher performance but also for higher efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">Tiago Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1\">Ricardo Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1\">Lu&#xed;s Garrote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">Urbano J. Nunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Fusion Vision Transformer for Fine-Grained Visual Categorization. (arXiv:2107.02341v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02341","description":"<p>The core for tackling the fine-grained visual categorization (FGVC) is to\nlearn subtle yet discriminative features. Most previous works achieve this by\nexplicitly selecting the discriminative parts or integrating the attention\nmechanism via CNN-based approaches.However, these methods enhance the\ncomputational complexity and make the modeldominated by the regions containing\nthe most of the objects. Recently, vision trans-former (ViT) has achieved SOTA\nperformance on general image recognition tasks. Theself-attention mechanism\naggregates and weights the information from all patches to the classification\ntoken, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation\ntoken in the deep layer pays more attention to the global information, lacking\nthe local and low-level features that are essential for FGVC. In this work, we\nproposea novel pure transformer-based framework Feature Fusion Vision\nTransformer (FFVT)where we aggregate the important tokens from each transformer\nlayer to compensate thelocal, low-level and middle-level information. We design\na novel token selection mod-ule called mutual attention weight selection (MAWS)\nto guide the network effectively and efficiently towards selecting\ndiscriminative tokens without introducing extra param-eters. We verify the\neffectiveness of FFVT on three benchmarks where FFVT achieves the\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multispectral Vineyard Segmentation: A Deep Learning approach. (arXiv:2108.01200v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01200","description":"<p>Digital agriculture has evolved significantly over the last few years due to\nthe technological developments in automation and computational intelligence\napplied to the agricultural sector, including vineyards which are a relevant\ncrop in the Mediterranean region. In this work, a study is presented of\nsemantic segmentation for vine detection in real-world vineyards by exploring\nstate-of-the-art deep segmentation networks and conventional unsupervised\nmethods. Camera data have been collected on vineyards using an Unmanned Aerial\nSystem (UAS) equipped with a dual imaging sensor payload, namely a\nhigh-definition RGB camera and a five-band multispectral and thermal camera.\nExtensive experiments using deep-segmentation networks and unsupervised methods\nhave been performed on multimodal datasets representing four distinct vineyards\nlocated in the central region of Portugal. The reported results indicate that\nSegNet, U-Net, and ModSegNet have equivalent overall performance in vine\nsegmentation. The results also show that multimodality slightly improves the\nperformance of vine segmentation, but the NIR spectrum alone generally is\nsufficient on most of the datasets. Furthermore, results suggest that\nhigh-definition RGB images produce equivalent or higher performance than any\nlower resolution multispectral band combination. Lastly, Deep Learning (DL)\nnetworks have higher overall performance than classical methods. The code and\ndataset are publicly available at\nhttps://github.com/Cybonic/DL_vineyard_segmentation_study.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">T. Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_P/0/1/0/all/0/1\">P. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_G/0/1/0/all/0/1\">G. Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">C. Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">M. Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1\">C.S.S. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">U.J. Nunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05080","description":"<p>While the significant advancements have made in the generation of deepfakes\nusing deep learning technologies, its misuse is a well-known issue now.\nDeepfakes can cause severe security and privacy issues as they can be used to\nimpersonate a person's identity in a video by replacing his/her face with\nanother person's face. Recently, a new problem of generating synthesized human\nvoice of a person is emerging, where AI-based deep learning models can\nsynthesize any person's voice requiring just a few seconds of audio. With the\nemerging threat of impersonation attacks using deepfake audios and videos, a\nnew generation of deepfake detectors is needed to focus on both video and audio\ncollectively. To develop a competent deepfake detector, a large amount of\nhigh-quality data is typically required to capture real-world (or practical)\nscenarios. Existing deepfake datasets either contain deepfake videos or audios,\nwhich are racially biased as well. As a result, it is critical to develop a\nhigh-quality video and audio deepfake dataset that can be used to detect both\naudio and video deepfakes simultaneously. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\nvideos but also respective synthesized lip-synced fake audios. We generate this\ndataset using the most popular deepfake generation methods. We selected real\nYouTube videos of celebrities with four ethnic backgrounds to develop a more\nrealistic multimodal dataset that addresses racial bias, and further help\ndevelop multimodal deepfake detectors. We performed several experiments using\nstate-of-the-art detection methods to evaluate our deepfake dataset and\ndemonstrate the challenges and usefulness of our multimodal Audio-Video\ndeepfake dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07058","description":"<p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark\nsuite. It offers 3,670 hours of daily-life activity video spanning hundreds of\nscenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique\ncamera wearers from 74 worldwide locations and 9 different countries. The\napproach to collection is designed to uphold rigorous privacy and ethics\nstandards with consenting participants and robust de-identification procedures\nwhere relevant. Ego4D dramatically expands the volume of diverse egocentric\nvideo footage publicly available to the research community. Portions of the\nvideo are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,\nand/or synchronized videos from multiple egocentric cameras at the same event.\nFurthermore, we present a host of new benchmark challenges centered around\nunderstanding the first-person visual experience in the past (querying an\nepisodic memory), present (analyzing hand-object manipulation, audio-visual\nconversation, and social interactions), and future (forecasting activities). By\npublicly sharing this massive annotated dataset and benchmark suite, we aim to\npush the frontier of first-person perception. Project page:\nhttps://ego4d-data.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1\">Andrew Westbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1\">Eugene Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1\">Zachary Chavis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamburger_J/0/1/0/all/0/1\">Jackson Hamburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1\">Miguel Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1\">Tushar Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1\">Ilija Radosavovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1\">Fiona Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jayant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Siddhant Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartillier_V/0/1/0/all/0/1\">Vincent Cartillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1\">Sean Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulaty_M/0/1/0/all/0/1\">Morrie Doulaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erapalli_A/0/1/0/all/0/1\">Akshay Erapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragomeni_A/0/1/0/all/0/1\">Adriano Fragomeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1\">Abrham Gebreselasie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristina Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillis_J/0/1/0/all/0/1\">James Hillis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenqi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1\">Weslie Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolar_J/0/1/0/all/0/1\">Jachym Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landini_F/0/1/0/all/0/1\">Federico Landini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhugu_R/0/1/0/all/0/1\">Raghava Modhugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munro_J/0/1/0/all/0/1\">Jonathan Munro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrell_T/0/1/0/all/0/1\">Tullie Murrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiyasu_T/0/1/0/all/0/1\">Takumi Nishiyasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puentes_P/0/1/0/all/0/1\">Paola Ruiz Puentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramazanova_M/0/1/0/all/0/1\">Merey Ramazanova</a>, et al. (33 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAIR: Data Augmented Invariant Regularization. (arXiv:2110.11205v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11205","description":"<p>While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM\ngeneralizes poorly to distribution shift. This is partly explained by\noverfitting to spurious features such as background in images or named entities\nin natural language. Synthetic data augmentation followed by empirical risk\nminimization (DA-ERM) is a simple and widely used solution to remedy this\nproblem. In addition, consistency regularization could be applied to further\npromote model performance to be consistent on the augmented sample and the\noriginal one. In this paper, we propose data augmented invariant regularization\n(DAIR), a simple form of consistency regularization that is applied directly on\nthe loss function rather than intermediate features, making it widely\napplicable regardless of network architecture or problem setup. We apply DAIR\nto multiple real-world learning problems, namely robust regression, visual\nquestion answering, robust deep neural network training, and neural\ntask-oriented dialog modeling. Our experiments show that DAIR consistently\noutperforms ERM and DA-ERM with little marginal cost and sets new\nstate-of-the-art results in several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halbe_S/0/1/0/all/0/1\">Shaunak Halbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration. (arXiv:2110.15250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15250","description":"<p>Even though considerable progress has been made in deep learning-based 3D\npoint cloud processing, how to obtain accurate correspondences for robust\nregistration remains a major challenge because existing hard assignment methods\ncannot deal with outliers naturally. Alternatively, the soft matching-based\nmethods have been proposed to learn the matching probability rather than hard\nassignment. However, in this paper, we prove that these methods have an\ninherent ambiguity causing many deceptive correspondences. To address the above\nchallenges, we propose to learn a partial permutation matching matrix, which\ndoes not assign corresponding points to outliers, and implements hard\nassignment to prevent ambiguity. However, this proposal poses two new problems,\ni.e., existing hard assignment algorithms can only solve a full rank\npermutation matrix rather than a partial permutation matrix, and this desired\nmatrix is defined in the discrete space, which is non-differentiable. In\nresponse, we design a dedicated soft-to-hard (S2H) matching procedure within\nthe registration pipeline consisting of two steps: solving the soft matching\nmatrix (S-step) and projecting this soft matrix to the partial permutation\nmatrix (H-step). Specifically, we augment the profit matrix before the hard\nassignment to solve an augmented permutation matrix, which is cropped to\nachieve the final partial permutation matrix. Moreover, to guarantee end-to-end\nlearning, we supervise the learned partial permutation matrix but propagate the\ngradient to the soft matrix instead. Our S2H matching procedure can be easily\nintegrated with existing registration frameworks, which has been verified in\nrepresentative frameworks including DCP, RPMNet, and DGR. Extensive experiments\nhave validated our method, which creates a new state-of-the-art performance for\nrobust 3D point cloud registration. The code will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis. (arXiv:2111.03133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03133","description":"<p>Generating images that fit a given text description using machine learning\nhas improved greatly with the release of technologies such as the CLIP\nimage-text encoder model; however, current methods lack artistic control of the\nstyle of image to be generated. We introduce StyleCLIPDraw which adds a style\nloss to the CLIPDraw text-to-drawing synthesis model to allow artistic control\nof the synthesized drawings in addition to control of the content via text.\nWhereas performing decoupled style transfer on a generated image only affects\nthe texture, our proposed coupled approach is able to capture a style in both\ntexture and shape, suggesting that the style of the drawing is coupled with the\ndrawing process itself. More results and our code are available at\nhttps://github.com/pschaldenbrand/StyleCLIPDraw\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaldenbrand_P/0/1/0/all/0/1\">Peter Schaldenbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction. (arXiv:2111.06636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06636","description":"<p>This work proposes a new computational framework for learning a structured\ngenerative model for real-world datasets. In particular, we propose to learn a\nclosed-loop transcription between a multi-class multi-dimensional data\ndistribution and a linear discriminative representation (LDR) in the feature\nspace that consists of multiple independent multi-dimensional linear subspaces.\nIn particular, we argue that the optimal encoding and decoding mappings sought\ncan be formulated as the equilibrium point of a two-player minimax game between\nthe encoder and decoder. A natural utility function for this game is the\nso-called rate reduction, a simple information-theoretic measure for distances\nbetween mixtures of subspace-like Gaussians in the feature space. Our\nformulation draws inspiration from closed-loop error feedback from control\nsystems and avoids expensive evaluating and minimizing approximated distances\nbetween arbitrary distributions in either the data space or the feature space.\nTo a large extent, this new formulation unifies the concepts and benefits of\nAuto-Encoding and GAN and naturally extends them to the settings of learning a\nboth discriminative and generative representation for multi-class and\nmulti-dimensional real-world data. Our extensive experiments on many benchmark\nimagery datasets demonstrate tremendous potential of this new closed-loop\nformulation: under fair comparison, visual quality of the learned decoder and\nclassification performance of the encoder is competitive and often better than\nexisting methods based on GAN, VAE, or a combination of both. Unlike existing\ngenerative models, the so learned features of the multiple classes are\nstructured: different classes are explicitly mapped onto corresponding\nindependent principal subspaces in the feature space. Source code can be found\nat https://github.com/Delay-Xili/LDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xili Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Pengyuan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psenka_M/0/1/0/all/0/1\">Michael Psenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung Yeung Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Artifacts Removal for Geometry-based Point Cloud Compression. (arXiv:2112.00560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00560","description":"<p>Geometry-based point cloud compression (G-PCC) can achieve remarkable\ncompression efficiency for point clouds. However, it still leads to serious\nattribute compression artifacts, especially under low bitrate scenarios. In\nthis paper, we propose a Multi-Scale Graph Attention Network (MS-GAT) to remove\nthe artifacts of point cloud attributes compressed by G-PCC. We first construct\na graph based on point cloud geometry coordinates and then use the Chebyshev\ngraph convolutions to extract features of point cloud attributes. Considering\nthat one point may be correlated with points both near and far away from it, we\npropose a multi-scale scheme to capture the short- and long-range correlations\nbetween the current point and its neighboring and distant points. To address\nthe problem that various points may have different degrees of artifacts caused\nby adaptive quantization, we introduce the quantization step per point as an\nextra input to the proposed network. We also incorporate a weighted graph\nattentional layer into the network to pay special attention to the points with\nmore attribute artifacts. To the best of our knowledge, this is the first\nattribute artifacts removal method for G-PCC. We validate the effectiveness of\nour method over various point clouds. Objective comparison results show that\nour proposed method achieves an average of 9.74% BD-rate reduction compared\nwith Predlift and 10.13% BD-rate reduction compared with RAHT. Subjective\ncomparison results present that visual artifacts such as color shifting,\nblurring, and quantization noise are reduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xihua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer. (arXiv:2112.04894v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04894","description":"<p>Recently, deep learning with Convolutional Neural Networks (CNNs) and\nTransformers has shown encouraging results in fully supervised medical image\nsegmentation. However, it is still challenging for them to achieve good\nperformance with limited annotations for training. In this work, we present a\nvery simple yet efficient framework for semi-supervised medical image\nsegmentation by introducing the cross teaching between CNN and Transformer.\nSpecifically, we simplify the classical deep co-training from consistency\nregularization to cross teaching, where the prediction of a network is used as\nthe pseudo label to supervise the other network directly end-to-end.\nConsidering the difference in learning paradigm between CNN and Transformer, we\nintroduce the Cross Teaching between CNN and Transformer rather than just using\nCNNs. Experiments on a public benchmark show that our method outperforms eight\nexisting semi-supervised learning methods just with a simpler framework.\nNotably, this work may be the first attempt to combine CNN and transformer for\nsemi-supervised medical image segmentation and achieve promising results on a\npublic benchmark. The code will be released at:\nhttps://github.com/HiLab-git/SSL4MIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_M/0/1/0/all/0/1\">Minhao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_T/0/1/0/all/0/1\">Tao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.01367","description":"<p>Increasing the performance of tactile sensing in robots enables versatile,\nin-hand manipulation. Vision-based tactile sensors have been widely used as\nrich tactile feedback has been shown to be correlated with increased\nperformance in manipulation tasks. Existing tactile sensor solutions with high\nresolution have limitations that include low accuracy, expensive components, or\nlack of scalability. In this paper, an inexpensive, scalable, and compact\ntactile sensor with high-resolution surface deformation modeling for surface\nreconstruction of the 3D sensor surface is proposed. By measuring the image\nfrom the fisheye camera, it is shown that the sensor can successfully estimate\nthe surface deformation in real-time (1.8ms) by using deep convolutional neural\nnetworks. This sensor in its design and sensing abilities represents a\nsignificant step toward better object in-hand localization, classification, and\nsurface estimation all enabled by high-resolution shape reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_W/0/1/0/all/0/1\">Won Kyung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_M/0/1/0/all/0/1\">Monroe Kennedy III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis. (arXiv:2201.08953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08953","description":"<p>Utilizing multi-modal neuroimaging data has been proved to be effective to\ninvestigate human cognitive activities and certain pathologies. However, it is\nnot practical to obtain the full set of paired neuroimaging data centrally\nsince the collection faces several constraints, e.g., high examination cost,\nlong acquisition time, and image corruption. In addition, these data are\ndispersed into different medical institutions and thus cannot be aggregated for\ncentralized training considering the privacy issues. There is a clear need to\nlaunch a federated learning and facilitate the integration of the dispersed\ndata from different institutions. In this paper, we propose a new benchmark for\nfederated domain translation on unsupervised brain image synthesis (termed as\nFedMed-GAN) to bridge the gap between federated learning and medical GAN.\nFedMed-GAN mitigates the mode collapse without sacrificing the performance of\ngenerators, and is widely applied to different proportions of unpaired and\npaired data with variation adaptation property. We treat the gradient penalties\nby federally averaging algorithm and then leveraging differential privacy\ngradient descent to regularize the training dynamics. A comprehensive\nevaluation is provided for comparing FedMed-GAN and other centralized methods,\nwhich shows the new state-of-the-art performance by our FedMed-GAN. Our code\nhas been released on the website: https://github.com/M-3LAB/FedMed-GAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SafePicking: Learning Safe Object Extraction via Object-Level Mapping. (arXiv:2202.05832v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.05832","description":"<p>Robots need object-level scene understanding to manipulate objects while\nreasoning about contact, support, and occlusion among objects. Given a pile of\nobjects, object recognition and reconstruction can identify the boundary of\nobject instances, giving important cues as to how the objects form and support\nthe pile. In this work, we present a system, SafePicking, that integrates\nobject-level mapping and learning-based motion planning to generate a motion\nthat safely extracts occluded target objects from a pile. Planning is done by\nlearning a deep Q-network that receives observations of predicted poses and a\ndepth-based heightmap to output a motion trajectory, trained to maximize a\nsafety metric reward. Our results show that the observation fusion of poses and\ndepth-sensing gives both better performance and robustness to the model. We\nevaluate our methods using the YCB objects in both simulation and the real\nworld, achieving safe object extraction from piles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Functional Connectivity Based Classification of ADHD Using Different Atlases. (arXiv:2202.08953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08953","description":"<p>These days, computational diagnosis strategies of neuropsychiatric disorders\nare gaining attention day by day. It's critical to determine the brain's\nfunctional connectivity based on Functional-Magnetic-Resonance-Imaging(fMRI) to\ndiagnose the disorder. It's known as a chronic disease, and millions of\nchildren amass the symptoms of this disease, so there is much vacuum for the\nresearcher to formulate a model to improve the accuracy to diagnose ADHD\naccurately. In this paper, we consider the functional connectivity of a brain\nextracted using various time templates/Atlases. Local-Binary Encoding-Method\n(LBEM) algorithm is utilized for feature extraction, while Hierarchical-\nExtreme-Learning-Machine (HELM) is used to classify the extracted features. To\nvalidate our approach, fMRI data of 143 normal and 100 ADHD affected children\nis used for experimental purpose. Our experimental results are based on\ncomparing various Atlases given as CC400, CC200, and AAL. Our model achieves\nhigh performance with CC400 as compared to other Atlases\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_S/0/1/0/all/0/1\">Sartaj Ahmed Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1\">Marva Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuduo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2202.12154","description":"<p>Trojan attacks on deep neural networks are both dangerous and surreptitious.\nOver the past few years, Trojan attacks have advanced from using only a simple\ntrigger and targeting only one class to using many sophisticated triggers and\ntargeting multiple classes. However, Trojan defenses have not caught up with\nthis development. Most defense methods still make out-of-date assumptions about\nTrojan triggers and target classes, thus, can be easily circumvented by modern\nTrojan attacks. In this paper, we advocate general defenses that are effective\nand robust against various Trojan attacks and propose two novel \"filtering\"\ndefenses with these characteristics called Variational Input Filtering (VIF)\nand Adversarial Input Filtering (AIF). VIF and AIF leverage variational\ninference and adversarial training respectively to purify all potential Trojan\ntriggers in the input at run time without making any assumption about their\nnumbers and forms. We further extend \"filtering\" to\n\"filtering-then-contrasting\" - a new defense mechanism that helps avoid the\ndrop in classification accuracy on clean data caused by filtering. Extensive\nexperimental results show that our proposed defenses significantly outperform 4\nwell-known defenses in mitigating 5 different Trojan attacks including the two\nstate-of-the-art which defeat many strong defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_W/0/1/0/all/0/1\">Willy Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12785","description":"<p>Calibrated confidence estimates obtained from neural networks are crucial,\nparticularly for safety-critical applications such as autonomous driving or\nmedical image diagnosis. However, although the task of confidence calibration\nhas been investigated on classification problems, thorough investigations on\nobject detection and segmentation problems are still missing. Therefore, we\nfocus on the investigation of confidence calibration for object detection and\nsegmentation models in this chapter. We introduce the concept of multivariate\nconfidence calibration that is an extension of well-known calibration methods\nto the task of object detection and segmentation. This allows for an extended\nconfidence calibration that is also aware of additional features such as\nbounding box/pixel position, shape information, etc. Furthermore, we extend the\nexpected calibration error (ECE) to measure miscalibration of object detection\nand segmentation models. We examine several network architectures on MS COCO as\nwell as on Cityscapes and show that especially object detection as well as\ninstance segmentation models are intrinsically miscalibrated given the\nintroduced definition of calibration. Using our proposed calibration methods,\nwe have been able to improve calibration so that it also has a positive impact\non the quality of segmentation masks as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuppers_F/0/1/0/all/0/1\">Fabian K&#xfc;ppers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselhoff_A/0/1/0/all/0/1\">Anselm Haselhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronenberger_J/0/1/0/all/0/1\">Jan Kronenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jonas Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient End-to-End 3D Model Reconstruction based on Neural Architecture Search. (arXiv:2202.13313v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13313","description":"<p>Using neural networks to represent 3D objects has become popular. However,\nmany previous works employ neural networks with fixed architecture and size to\nrepresent different 3D objects, which lead to excessive network parameters for\nsimple objects and limited reconstruction accuracy for complex objects. For\neach 3D model, it is desirable to have an end-to-end neural network with as few\nparameters as possible to achieve high-fidelity reconstruction. In this paper,\nwe propose an efficient model reconstruction method utilizing neural\narchitecture search (NAS) and binary classification. Taking the number of\nlayers, the number of nodes in each layer, and the activation function of each\nlayer as the search space, a specific network architecture can be obtained\nbased on reinforcement learning technology. Furthermore, to get rid of the\ntraditional surface reconstruction algorithms (e.g., marching cube) used after\nnetwork inference, we complete the end-to-end network by classifying binary\nvoxels. Compared to other signed distance field (SDF) prediction or binary\nclassification networks, our method achieves significantly higher\nreconstruction accuracy using fewer network parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongdong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xulong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Ting Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial Training. (arXiv:2202.13755v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13755","description":"<p>Capsule networks (CapsNets) are new neural networks that classify images\nbased on the spatial relationships of features. By analyzing the pose of\nfeatures and their relative positions, it is more capable to recognize images\nafter affine transformation. The stacked capsule autoencoder (SCAE) is a\nstate-of-the-art CapsNet, and achieved unsupervised classification of CapsNets\nfor the first time. However, the security vulnerabilities and the robustness of\nthe SCAE has rarely been explored. In this paper, we propose an evasion attack\nagainst SCAE, where the attacker can generate adversarial perturbations based\non reducing the contribution of the object capsules in SCAE related to the\noriginal category of the image. The adversarial perturbations are then applied\nto the original images, and the perturbed images will be misclassified.\nFurthermore, we propose a defense method called Hybrid Adversarial Training\n(HAT) against such evasion attacks. HAT makes use of adversarial training and\nadversarial distillation to achieve better robustness and stability. We\nevaluate the defense method and the experimental results show that the refined\nSCAE model can achieve 82.14% classification accuracy under evasion attack. The\nsource code is available at https://github.com/FrostbiteXSW/SCAE_Defense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jiazhu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1\">Siwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background Mixup Data Augmentation for Hand and Object-in-Contact Detection. (arXiv:2202.13941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13941","description":"<p>Detecting the positions of human hands and objects-in-contact (hand-object\ndetection) in each video frame is vital for understanding human activities from\nvideos. For training an object detector, a method called Mixup, which overlays\ntwo training images to mitigate data bias, has been empirically shown to be\neffective for data augmentation. However, in hand-object detection, mixing two\nhand-manipulation images produces unintended biases, e.g., the concentration of\nhands and objects in a specific region degrades the ability of the hand-object\ndetector to identify object boundaries. We propose a data-augmentation method\ncalled Background Mixup that leverages data-mixing regularization while\nreducing the unintended effects in hand-object detection. Instead of mixing two\nimages where a hand and an object in contact appear, we mix a target training\nimage with background images without hands and objects-in-contact extracted\nfrom external image sources, and use the mixed images for training the\ndetector. Our experiments demonstrated that the proposed method can effectively\nreduce false positives and improve the performance of hand-object detection in\nboth supervised and semi-supervised learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tango_K/0/1/0/all/0/1\">Koya Tango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}