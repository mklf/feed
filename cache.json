{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.4","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents. (arXiv:2110.10150v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10150","description":"<p>Text summarization is an essential task to help readers capture salient\ninformation from documents, news, interviews, and meetings. However, most\nstate-of-the-art pretrained language models are unable to efficiently process\nlong text commonly seen in the summarization problem domain. In this paper, we\npropose Summ^N, a simple, flexible, and effective multi-stage framework for\ninput texts that are longer than the maximum context lengths of typical\npretrained LMs. Summ^N first generates the coarse summary in multiple stages\nand then produces the final fine-grained summary based on them. The framework\ncan process input text of arbitrary length by adjusting the number of stages\nwhile keeping the LM context size fixed. Moreover, it can deal with both\ndocuments and dialogues and can be used on top of any underlying backbone\nabstractive summarization model. Our experiments demonstrate that Summ^N\nsignificantly outperforms previous state-of-the-art methods by improving ROUGE\nscores on three long meeting summarization datasets AMI, ICSI, and QMSum, two\nlong TV series datasets from SummScreen, and a newly proposed long document\nsummarization dataset GovReport. Our data and code are available at\nhttps://github.com/chatc/Summ-N.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenNI: Human-AI Collaboration for Data-Backed Text Generation. (arXiv:2110.10185v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10185","description":"<p>Table2Text systems generate textual output based on structured data utilizing\nmachine learning. These systems are essential for fluent natural language\ninterfaces in tools such as virtual assistants; however, left to generate\nfreely these ML systems often produce misleading or unexpected outputs. GenNI\n(Generation Negotiation Interface) is an interactive visual system for\nhigh-level human-AI collaboration in producing descriptive text. The tool\nutilizes a deep learning model designed with explicit control states. These\ncontrols allow users to globally constrain model generations, without\nsacrificing the representation power of the deep learning models. The visual\ninterface makes it possible for users to interact with AI systems following a\nRefine-Forecast paradigm to ensure that the generation system acts in a manner\nhuman users find suitable. We report multiple use cases on two experiments that\nimprove over uncontrolled generation approaches, while at the same time\nproviding fine-grained control. A demo and source code are available at\nhttps://genni.vizhub.ai .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinley_J/0/1/0/all/0/1\">Jambay Kinley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_R/0/1/0/all/0/1\">Robert Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_J/0/1/0/all/0/1\">Johanna Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects. (arXiv:2110.10189v1 [cs.RO])","link":"http://arxiv.org/abs/2110.10189","description":"<p>Geometric organization of objects into semantically meaningful arrangements\npervades the built world. As such, assistive robots operating in warehouses,\noffices, and homes would greatly benefit from the ability to recognize and\nrearrange objects into these semantically meaningful structures. To be useful,\nthese robots must contend with previously unseen objects and receive\ninstructions without significant programming. While previous works have\nexamined recognizing pairwise semantic relations and sequential manipulation to\nchange these simple relations none have shown the ability to arrange objects\ninto complex structures such as circles or table settings. To address this\nproblem we propose a novel transformer-based neural network, StructFormer,\nwhich takes as input a partial-view point cloud of the current object\narrangement and a structured language command encoding the desired object\nconfiguration. We show through rigorous experiments that StructFormer enables a\nphysical robot to rearrange novel objects into semantically meaningful\nstructures with multi-object relational constraints inferred from the language\ncommand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Medication Extraction: A Comparison of Recent Models in Supervised and Semi-supervised Learning Settings. (arXiv:2110.10213v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10213","description":"<p>Drug prescriptions are essential information that must be encoded in\nelectronic medical records. However, much of this information is hidden within\nfree-text reports. This is why the medication extraction task has emerged. To\ndate, most of the research effort has focused on small amount of data and has\nonly recently considered deep learning methods. In this paper, we present an\nindependent and comprehensive evaluation of state-of-the-art neural\narchitectures on the I2B2 medical prescription extraction task both in the\nsupervised and semi-supervised settings. The study shows the very competitive\nperformance of simple DNN models on the task as well as the high interest of\npre-trained models. Adapting the latter models on the I2B2 dataset enables to\npush medication extraction performances above the state-of-the-art. Finally,\nthe study also confirms that semi-supervised techniques are promising to\nleverage large amounts of unlabeled data in particular in low resource setting\nwhen labeled data is too costly to acquire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocabiyikoglu_A/0/1/0/all/0/1\">Ali Can Kocabiyikoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qader_R/0/1/0/all/0/1\">Raheel Qader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babouchkine_J/0/1/0/all/0/1\">Jean-Marc Babouchkine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Domain Specific Language Models for Automatic Speech Recognition through Machine Translation. (arXiv:2110.10261v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10261","description":"<p>Automatic Speech Recognition (ASR) systems have been gaining popularity in\nthe recent years for their widespread usage in smart phones and speakers.\nBuilding ASR systems for task-specific scenarios is subject to the availability\nof utterances that adhere to the style of the task as well as the language in\nquestion. In our work, we target such a scenario wherein task-specific text\ndata is available in a language that is different from the target language in\nwhich an ASR Language Model (LM) is expected. We use Neural Machine Translation\n(NMT) as an intermediate step to first obtain translations of the task-specific\ntext data. We then train LMs on the 1-best and N-best translations and study\nways to improve on such a baseline LM. We develop a procedure to derive word\nconfusion networks from NMT beam search graphs and evaluate LMs trained on\nthese confusion networks. With experiments on the WMT20 chat translation task\ndataset, we demonstrate that NMT confusion networks can help to reduce the\nperplexity of both n-gram and recurrent neural network LMs compared to those\ntrained only on N-best translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Saurav Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction. (arXiv:2110.10318v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10318","description":"<p>We evaluate a simple approach to improving zero-shot multilingual transfer of\nmBERT on social media corpus by adding a pretraining task called translation\npair prediction (TPP), which predicts whether a pair of cross-lingual texts are\na valid translation. Our approach assumes access to translations (exact or\napproximate) between source-target language pairs, where we fine-tune a model\non source language task data and evaluate the model in the target language. In\nparticular, we focus on language pairs where transfer learning is difficult for\nmBERT: those where source and target languages are different in script,\nvocabulary, and linguistic typology. We show improvements from TPP pretraining\nover mBERT alone in zero-shot transfer from English to Hindi, Arabic, and\nJapanese on two social media tasks: NER (a 37% average relative improvement in\nF1 across target languages) and sentiment classification (12% relative\nimprovement in F1) on social media text, while also benchmarking on a\nnon-social media task of Universal Dependency POS tagging (6.7% relative\nimprovement in accuracy). Our results are promising given the lack of social\nmedia bitext corpus. Our code can be found at:\nhttps://github.com/twitter-research/multilingual-alignment-tpp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1\">Aria Haghighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMSOC: An Approach for Socially Sensitive Pretraining. (arXiv:2110.10319v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10319","description":"<p>While large-scale pretrained language models have been shown to learn\neffective linguistic representations for many NLP tasks, there remain many\nreal-world contextual aspects of language that current approaches do not\ncapture. For instance, consider a cloze-test \"I enjoyed the ____ game this\nweekend\": the correct answer depends heavily on where the speaker is from, when\nthe utterance occurred, and the speaker's broader social milieu and\npreferences. Although language depends heavily on the geographical, temporal,\nand other social contexts of the speaker, these elements have not been\nincorporated into modern transformer-based language models. We propose a simple\nbut effective approach to incorporate speaker social context into the learned\nrepresentations of large-scale language models. Our method first learns dense\nrepresentations of social contexts using graph representation learning\nalgorithms and then primes language model pretraining with these social context\nrepresentations. We evaluate our approach on geographically-sensitive\nlanguage-modeling tasks and show a substantial improvement (more than 100%\nrelative lift on MRR) compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Vivek Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1\">Aria Haghighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R$^3$Net:Relation-embedded Representation Reconstruction Network for Change Captioning. (arXiv:2110.10328v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10328","description":"<p>Change captioning is to use a natural language sentence to describe the\nfine-grained disagreement between two similar images. Viewpoint change is the\nmost typical distractor in this task, because it changes the scale and location\nof the objects and overwhelms the representation of real change. In this paper,\nwe propose a Relation-embedded Representation Reconstruction Network (R$^3$Net)\nto explicitly distinguish the real change from the large amount of clutter and\nirrelevant changes. Specifically, a relation-embedded module is first devised\nto explore potential changed objects in the large amount of clutter. Then,\nbased on the semantic similarities of corresponding locations in the two\nimages, a representation reconstruction module (RRM) is designed to learn the\nreconstruction representation and further model the difference representation.\nBesides, we introduce a syntactic skeleton predictor (SSP) to enhance the\nsemantic interaction between change localization and caption generation.\nExtensive experiments show that the proposed method achieves the\nstate-of-the-art results on two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yunbin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shengxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training. (arXiv:2110.10329v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10329","description":"<p>Unsupervised pre-training is now the predominant approach for both text and\nspeech understanding. Self-attention models pre-trained on large amounts of\nunannotated data have been hugely successful when fine-tuned on downstream\ntasks from a variety of domains and languages. This paper takes the\nuniversality of unsupervised language pre-training one step further, by\nunifying speech and text pre-training within a single model. We build a single\nencoder with the BERT objective on unlabeled text together with the w2v-BERT\nobjective on unlabeled speech. To further align our model representations\nacross modalities, we leverage alignment losses, specifically Translation\nLanguage Modeling (TLM) and Speech Text Matching (STM) that make use of\nsupervised speech-text recognition data. We demonstrate that incorporating both\nspeech and text data during pre-training can significantly improve downstream\nquality on CoVoST~2 speech translation, by around 1 BLEU compared to\nsingle-modality pre-trained models, while retaining close to SotA performance\non LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and\ntext-normalization, we observe evidence of capacity limitations and\ninterference between the two modalities, leading to degraded performance\ncompared to an equivalent text-only model, while still being competitive with\nBERT. Through extensive empirical analysis we also demonstrate the importance\nof the choice of objective function for speech pre-training, and the beneficial\neffect of adding additional supervised signals on the quality of the learned\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-an Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"News-based Business Sentiment and its Properties as an Economic Index. (arXiv:2110.10340v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10340","description":"<p>This paper presents an approach to measuring business sentiment based on\ntextual data. Business sentiment has been measured by traditional surveys,\nwhich are costly and time-consuming to conduct. To address the issues, we take\nadvantage of daily newspaper articles and adopt a self-attention-based model to\ndefine a business sentiment index, named S-APIR, where outlier detection models\nare investigated to properly handle various genres of news articles. Moreover,\nwe propose a simple approach to temporally analyzing how much any given event\ncontributed to the predicted business sentiment index. To demonstrate the\nvalidity of the proposed approach, an extensive analysis is carried out on 12\nyears' worth of newspaper articles. The analysis shows that the S-APIR index is\nstrongly and positively correlated with established survey-based index (up to\ncorrelation coefficient r=0.937) and that the outlier detection is effective\nespecially for a general newspaper. Also, S-APIR is compared with a variety of\neconomic indices, revealing the properties of S-APIR that it reflects the trend\nof the macroeconomy as well as the economic outlook and sentiment of economic\nagents. Moreover, to illustrate how S-APIR could benefit economists and\npolicymakers, several events are analyzed with respect to their impacts on\nbusiness sentiment over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seki_K/0/1/0/all/0/1\">Kazuhiro Seki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikuta_Y/0/1/0/all/0/1\">Yusuke Ikuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubayashi_Y/0/1/0/all/0/1\">Yoichi Matsubayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Aspect-guided Explanation Generation for Explainable Recommendation. (arXiv:2110.10358v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10358","description":"<p>Explainable recommendation systems provide explanations for recommendation\nresults to improve their transparency and persuasiveness. The existing\nexplainable recommendation methods generate textual explanations without\nexplicitly considering the user's preferences on different aspects of the item.\nIn this paper, we propose a novel explanation generation framework, named\nHierarchical Aspect-guided explanation Generation (HAG), for explainable\nrecommendation. Specifically, HAG employs a review-based syntax graph to\nprovide a unified view of the user/item details. An aspect-guided graph pooling\noperator is proposed to extract the aspect-relevant information from the\nreview-based syntax graphs to model the user's preferences on an item at the\naspect level. Then, a hierarchical explanation decoder is developed to generate\naspects and aspect-relevant explanations based on the attention mechanism. The\nexperimental results on three real datasets indicate that HAG outperforms\nstate-of-the-art explanation generation methods in both single-aspect and\nmulti-aspect explanation generation tasks, and also achieves comparable or even\nbetter preference prediction accuracy than strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yidan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Gongqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yuan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Classifiers in Sentiment Analysis. (arXiv:2110.10372v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10372","description":"<p>In this paper, we propose sentiment classification models based on BERT\nintegrated with DRO (Distributionally Robust Classifiers) to improve model\nperformance on datasets with distributional shifts. We added 2-Layer Bi-LSTM,\nprojection layer (onto simplex or Lp ball), and linear layer on top of BERT to\nachieve distributionally robustness. We considered one form of distributional\nshift (from IMDb dataset to Rotten Tomatoes dataset). We have confirmed through\nexperiments that our DRO model does improve performance on our test set with\ndistributional shift from the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shilun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Renee Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Carina Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach. (arXiv:2110.10429v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10429","description":"<p>The remarkable performance of the pre-trained language model (LM) using\nself-supervised learning has led to a major paradigm shift in the study of\nnatural language processing. In line with these changes, leveraging the\nperformance of speech recognition systems with massive deep learning-based LMs\nis a major topic of speech recognition research. Among the various methods of\napplying LMs to speech recognition systems, in this paper, we focus on a\ncross-modal knowledge distillation method that transfers knowledge between two\ntypes of deep neural networks with different modalities. We propose an acoustic\nmodel structure with multiple auxiliary output layers for cross-modal\ndistillation and demonstrate that the proposed method effectively compensates\nfor the shortcomings of the existing label-interpolation-based distillation\nmethod. In addition, we extend the proposed method to a hierarchical\ndistillation method using LMs trained in different units (senones, monophones,\nand subwords) and reveal the effectiveness of the hierarchical distillation\nmethod through an ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mun-Hak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Joon-Hyuk Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discontinuous Grammar as a Foreign Language. (arXiv:2110.10431v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10431","description":"<p>In order to achieve deep natural language understanding, syntactic\nconstituent parsing is a vital step, highly demanded by many artificial\nintelligence systems to process both text and speech. One of the most recent\nproposals is the use of standard sequence-to-sequence models to perform\nconstituent parsing as a machine translation task, instead of applying\ntask-specific parsers. While they show a competitive performance, these\ntext-to-parse transducers are still lagging behind classic techniques in terms\nof accuracy, coverage and speed. To close the gap, we here extend the framework\nof sequence-to-sequence models for constituent parsing, not only by providing a\nmore powerful neural architecture for improving their performance, but also by\nenlarging their coverage to handle the most complex syntactic phenomena:\ndiscontinuous structures. To that end, we design several novel linearizations\nthat can fully produce discontinuities and, for the first time, we test a\nsequence-to-sequence model on the main discontinuous benchmarks, obtaining\ncompetitive results on par with task-specific discontinuous constituent parsers\nand achieving state-of-the-art scores on the (discontinuous) English Penn\nTreebank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph informed Fake News Classification via Heterogeneous Representation Ensembles. (arXiv:2110.10457v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10457","description":"<p>Increasing amounts of freely available data both in textual and relational\nform offers exploration of richer document representations, potentially\nimproving the model performance and robustness. An emerging problem in the\nmodern era is fake news detection -- many easily available pieces of\ninformation are not necessarily factually correct, and can lead to wrong\nconclusions or are used for manipulation. In this work we explore how different\ndocument representations, ranging from simple symbolic bag-of-words, to\ncontextual, neural language model-based ones can be used for efficient fake\nnews identification. One of the key contributions is a set of novel document\nrepresentation learning methods based solely on knowledge graphs, i.e.\nextensive collections of (grounded) subject-predicate-object triplets. We\ndemonstrate that knowledge graph-based representations already achieve\ncompetitive performance to conventionally accepted representation learners.\nFurthermore, when combined with existing, contextual representations, knowledge\ngraph-based document representations can achieve state-of-the-art performance.\nTo our knowledge this is the first larger-scale evaluation of how knowledge\ngraph-based representations can be systematically incorporated into the process\nof fake news classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepisnik_Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik-Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Deep Learning Models in Natural Language Processing: A Review. (arXiv:2110.10470v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10470","description":"<p>Neural network models have achieved state-of-the-art performances in a wide\nrange of natural language processing (NLP) tasks. However, a long-standing\ncriticism against neural network models is the lack of interpretability, which\nnot only reduces the reliability of neural NLP systems but also limits the\nscope of their applications in areas where interpretability is essential (e.g.,\nhealth care applications). In response, the increasing interest in interpreting\nneural NLP models has spurred a diverse array of interpretation methods over\nrecent years. In this survey, we provide a comprehensive review of various\ninterpretation methods for neural models in NLP. We first stretch out a\nhigh-level taxonomy for interpretation methods in NLP, i.e., training-based\napproaches, test-based approaches, and hybrid approaches. Next, we describe\nsub-categories in each category in detail, e.g., influence-function based\nmethods, KNN-based methods, attention-based models, saliency-based methods,\nperturbation-based methods, etc. We point out deficiencies of current methods\nand suggest some avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Unsupervised Neural Machine Translation with Denoising Adapters. (arXiv:2110.10472v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10472","description":"<p>We consider the problem of multilingual unsupervised machine translation,\ntranslating to and from languages that only have monolingual data by using\nauxiliary parallel language pairs. For this problem the standard procedure so\nfar to leverage the monolingual data is back-translation, which is\ncomputationally costly and hard to tune.\n</p>\n<p>In this paper we propose instead to use denoising adapters, adapter layers\nwith a denoising objective, on top of pre-trained mBART-50. In addition to the\nmodularity and flexibility of such an approach we show that the resulting\ntranslations are on-par with back-translating as measured by BLEU, and\nfurthermore it allows adding unseen languages incrementally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre B&#xe9;rard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning in Multilingual NMT via Language-Specific Embeddings. (arXiv:2110.10478v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10478","description":"<p>This paper proposes a technique for adding a new source or target language to\nan existing multilingual NMT model without re-training it on the initial set of\nlanguages. It consists in replacing the shared vocabulary with a small\nlanguage-specific vocabulary and fine-tuning the new embeddings on the new\nlanguage's parallel data. Some additional language-specific components may be\ntrained to improve performance (e.g., Transformer layers or adapter modules).\nBecause the parameters of the original model are not modified, its performance\non the initial languages does not degrade. We show on two sets of experiments\n(small-scale on TED Talks, and large-scale on ParaCrawl) that this approach\nperforms as well or better as the more costly alternatives; and that it has\nexcellent zero-shot performance: training on English-centric data is enough to\ntranslate between the new language and any of the initial languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialVisTUM: An Interactive Visualization Toolkit for Correlated Neural Topic Models on Social Media Opinion Mining. (arXiv:2110.10575v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10575","description":"<p>Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_M/0/1/0/all/0/1\">Martin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesch_R/0/1/0/all/0/1\">Robert Pesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1\">Mainak Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Archishman Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaxi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the 2021 Key Point Analysis Shared Task. (arXiv:2110.10577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10577","description":"<p>We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point\nanalysis that we organized as a part of the 8th Workshop on Argument Mining\n(ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the\nresults of the shared task. We expect the task and the findings reported in\nthis paper to be relevant for researchers working on text summarization and\nargument mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1\">Roni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark. (arXiv:2110.10661v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10661","description":"<p>Existing work in language grounding typically study single environments. How\ndo we build unified models that apply across multiple environments? We propose\nthe multi-environment Symbolic Interactive Language Grounding benchmark (SILG),\nwhich unifies a collection of diverse grounded language learning environments\nunder a common interface. SILG consists of grid-world environments that require\ngeneralization to new dynamics, entities, and partially observed worlds (RTFM,\nMessenger, NetHack), as well as symbolic counterparts of visual worlds that\nrequire interpreting rich natural language with respect to complex scenes\n(ALFWorld, Touchdown). Together, these environments provide diverse grounding\nchallenges in richness of observation space, action space, language\nspecification, and plan complexity. In addition, we propose the first shared\nmodel architecture for RL on these environments, and evaluate recent advances\nsuch as egocentric local convolution, recurrent state-tracking, entity-centric\nattention, and pretrained LM using SILG. Our shared architecture achieves\ncomparable performance to environment-specific architectures. Moreover, we find\nthat many recent modelling advances do not result in significant gains on\nenvironments other than the one they were designed for. This highlights the\nneed for a multi-environment benchmark. Finally, the best models significantly\nunderperform humans on SILG, which suggests ample room for future work. We hope\nSILG enables the community to quickly identify new methodologies for language\ngrounding that generalize to a diverse set of environments and their associated\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer. (arXiv:2110.10668v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10668","description":"<p>While the field of style transfer (ST) has been growing rapidly, it has been\nhampered by a lack of standardized practices for automatic evaluation. In this\npaper, we evaluate leading ST automatic metrics on the oft-researched task of\nformality style transfer. Unlike previous evaluations, which focus solely on\nEnglish, we expand our focus to Brazilian-Portuguese, French, and Italian,\nmaking this work the first multilingual evaluation of metrics in ST. We outline\nbest practices for automatic evaluation in (formality) style transfer and\nidentify several models that correlate well with human judgments and are robust\nacross languages. We hope that this work will help accelerate development in\nST, where human evaluation is often challenging to collect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1\">Eleftheria Briakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sweta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Explainable Stylish Image Captioning Framework via Multi-References. (arXiv:2110.10704v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10704","description":"<p>In this paper, we propose to build a stylish image captioning model through a\nMulti-style Multi modality mechanism (2M). We demonstrate that with 2M, we can\nbuild an effective stylish captioner and that multi-references produced by the\nmodel can also support explaining the model through identifying erroneous input\nfeatures on faulty examples. We show how this 2M mechanism can be used to build\nstylish captioning models and show how these models can be utilized to provide\nexplanations of likely errors in the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1\">Brent Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better than Average: Paired Evaluation of NLP Systems. (arXiv:2110.10746v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10746","description":"<p>Evaluation in NLP is usually done by comparing the scores of competing\nsystems independently averaged over a common set of test instances. In this\nwork, we question the use of averages for aggregating evaluation scores into a\nfinal number used to decide which system is best, since the average, as well as\nalternatives such as the median, ignores the pairing arising from the fact that\nsystems are evaluated on the same test instances. We illustrate the importance\nof taking the instance-level pairing of evaluation scores into account and\ndemonstrate, both theoretically and empirically, the advantages of aggregation\nmethods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a\nmechanism based on the estimated probability that a given system scores better\nthan another on the test set. By re-evaluating 296 real NLP evaluation setups\nacross four tasks and 18 evaluation metrics, we show that the choice of\naggregation mechanism matters and yields different conclusions as to which\nsystems are state of the art in about 30% of the setups. To facilitate the\nadoption of pairwise evaluation, we release a practical tool for performing the\nfull analysis of evaluation scores with the mean, median, BT, and two variants\nof BT (Elo and TrueSkill), alongside functionality for appropriate statistical\ntesting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation. (arXiv:2110.10774v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10774","description":"<p>Generating texts in scientific papers requires not only capturing the content\ncontained within the given input but also frequently acquiring the external\ninformation called \\textit{context}. We push forward the scientific text\ngeneration by proposing a new task, namely \\textbf{context-aware text\ngeneration} in the scientific domain, aiming at exploiting the contributions of\ncontext in generated texts. To this end, we present a novel challenging\nlarge-scale \\textbf{Sci}entific Paper Dataset for Conte\\textbf{X}t-Aware Text\n\\textbf{Gen}eration (SciXGen), consisting of well-annotated 205,304 papers with\nfull references to widely-used objects (e.g., tables, figures, algorithms) in a\npaper. We comprehensively benchmark, using state-of-the-arts, the efficacy of\nour newly constructed SciXGen dataset in generating description and paragraph.\nOur dataset and benchmarks will be made publicly available to hopefully\nfacilitate the scientific text generation research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamura_H/0/1/0/all/0/1\">Hiroya Takamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Document Representation Learning with Graph Attention Networks. (arXiv:2110.10778v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10778","description":"<p>Recent progress in pretrained Transformer-based language models has shown\ngreat success in learning contextual representation of text. However, due to\nthe quadratic self-attention complexity, most of the pretrained Transformers\nmodels can only handle relatively short text. It is still a challenge when it\ncomes to modeling very long documents. In this work, we propose to use a graph\nattention network on top of the available pretrained Transformers model to\nlearn document embeddings. This graph attention network allows us to leverage\nthe high-level semantic structure of the document. In addition, based on our\ngraph document model, we design a simple contrastive learning strategy to\npretrain our models on a large amount of unlabeled corpus. Empirically, we\ndemonstrate the effectiveness of our approaches in document classification and\ndocument retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C). (arXiv:2110.10780v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10780","description":"<p>While we pay attention to the latest advances in clinical natural language\nprocessing (NLP), we can notice some resistance in the clinical and\ntranslational research community to adopt NLP models due to limited\ntransparency, Interpretability and usability. Built upon our previous work, in\nthis study, we proposed an open natural language processing development\nframework and evaluated it through the implementation of NLP algorithms for the\nNational COVID Cohort Collaborative (N3C). Based on the interests in\ninformation extraction from COVID-19 related clinical notes, our work includes\n1) an open data annotation process using COVID-19 signs and symptoms as the use\ncase, 2) a community-driven ruleset composing platform, and 3) a synthetic text\ndata generation workflow to generate texts for information extraction tasks\nwithout involving human subjects. The generated corpora derived out of the\ntexts from multiple intuitions and gold standard annotation are tested on a\nsingle institution's rule set has the performances in F1 score of 0.876, 0.706\nand 0.694, respectively. The study as a consortium effort of the N3C NLP\nsubgroup demonstrates the feasibility of creating a federated NLP algorithm\ndevelopment and benchmarking platform to enhance multi-institution clinical NLP\nstudy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1\">Andrew Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Sunyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1\">Robert Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Andrew Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_D/0/1/0/all/0/1\">Daniel Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_el_rub_N/0/1/0/all/0/1\">Noor Abu-el-rub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_J/0/1/0/all/0/1\">John D. Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_M/0/1/0/all/0/1\">Masoud Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfaff_E/0/1/0/all/0/1\">Emily Pfaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chute_C/0/1/0/all/0/1\">Christopher G. Chute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Tim Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1\">Melissa A. Haendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_R/0/1/0/all/0/1\">Rafael Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szolovits_P/0/1/0/all/0/1\">Peter Szolovits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a> (N3C Natural Language Processing (NLP) Subgroup)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The R package sentometrics to compute, aggregate and predict with textual sentiment. (arXiv:2110.10817v1 [stat.ML])","link":"http://arxiv.org/abs/2110.10817","description":"<p>We provide a hands-on introduction to optimized textual sentiment indexation\nusing the R package sentometrics. Textual sentiment analysis is increasingly\nused to unlock the potential information value of textual data. The\nsentometrics package implements an intuitive framework to efficiently compute\nsentiment scores of numerous texts, to aggregate the scores into multiple time\nseries, and to use these time series to predict other variables. The workflow\nof the package is illustrated with a built-in corpus of news articles from two\nmajor U.S. journals to forecast the CBOE Volatility Index.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ardia_D/0/1/0/all/0/1\">David Ardia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bluteau_K/0/1/0/all/0/1\">Keven Bluteau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Borms_S/0/1/0/all/0/1\">Samuel Borms</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boudt_K/0/1/0/all/0/1\">Kris Boudt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization. (arXiv:2110.10834v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10834","description":"<p>While much research has been done in text-to-image synthesis, little work has\nbeen done to explore the usage of linguistic structure of the input text. Such\ninformation is even more important for story visualization since its inputs\nhave an explicit narrative structure that needs to be translated into an image\nsequence (or visual story). Prior work in this domain has shown that there is\nample room for improvement in the generated image sequence in terms of visual\nquality, consistency and relevance. In this paper, we first explore the use of\nconstituency parse trees using a Transformer-based recurrent architecture for\nencoding structured input. Second, we augment the structured input with\ncommonsense information and study the impact of this external knowledge on the\ngeneration of visual story. Third, we also incorporate visual structure via\nbounding boxes and dense captioning to provide feedback about the\ncharacters/objects in generated images within a dual learning setup. We show\nthat off-the-shelf dense-captioning models trained on Visual Genome can improve\nthe spatial structure of images from a different target domain without needing\nfine-tuning. We train the model end-to-end using intra-story contrastive loss\n(between words and image sub-regions) and show significant improvements in\nseveral metrics (and human evaluation) for multiple datasets. Finally, we\nprovide an analysis of the linguistic and visuo-spatial information. Code and\ndata: https://github.com/adymaharana/VLCStoryGan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1\">Adyasha Maharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer. (arXiv:1908.11017v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.11017","description":"<p>Aspect-category sentiment analysis (ACSA) aims to predict the aspect\ncategories mentioned in texts and their corresponding sentiment polarities.\nSome joint models have been proposed to address this task. Given a text, these\njoint models detect all the aspect categories mentioned in the text and predict\nthe sentiment polarities toward them at once. Although these joint models\nobtain promising performances, they train separate parameters for each aspect\ncategory and therefore suffer from data deficiency of some aspect categories.\nTo solve this problem, we propose a novel joint model which contains a shared\nsentiment prediction layer. The shared sentiment prediction layer transfers\nsentiment knowledge between aspect categories and alleviates the problem caused\nby data deficiency. Experiments conducted on SemEval-2016 Datasets demonstrate\nthe effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Cunxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lunan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Ting Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualSem: A High-quality Knowledge Graph for Vision and Language. (arXiv:2008.09150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09150","description":"<p>An exciting frontier in natural language understanding (NLU) and generation\n(NLG) calls for (vision-and-) language models that can efficiently access\nexternal structured knowledge repositories. However, many existing knowledge\nbases only cover limited domains, or suffer from noisy data, and most of all\nare typically hard to integrate into neural language pipelines. To fill this\ngap, we release VisualSem: a high-quality knowledge graph (KG) which includes\nnodes with multilingual glosses, multiple illustrative images, and visually\nrelevant relations. We also release a neural multi-modal retrieval model that\ncan use images or sentences as inputs and retrieves entities in the KG. This\nmulti-modal retrieval model can be integrated into any (neural network) model\npipeline. We encourage the research community to use VisualSem for data\naugmentation and/or as a source of grounding, among other possible uses.\nVisualSem as well as the multi-modal retrieval models are publicly available\nand can be downloaded in this URL: https://github.com/iacercalixto/visualsem\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_H/0/1/0/all/0/1\">Houda Alberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Teresa Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_Y/0/1/0/all/0/1\">Yash Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1\">Clara Vania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Alignment Structure with Gated Graph Attention Networks for Sentence Matching. (arXiv:2010.07668v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.07668","description":"<p>Sentence matching is a fundamental task of natural language processing with\nvarious applications. Most recent approaches adopt attention-based neural\nmodels to build word- or phrase-level alignment between two sentences. However,\nthese models usually ignore the inherent structure within the sentences and\nfail to consider various dependency relationships among text units. To address\nthese issues, this paper proposes a graph-based approach for sentence matching.\nFirst, we represent a sentence pair as a graph with several carefully design\nstrategies. We then employ a novel gated graph attention network to encode the\nconstructed graph for sentence matching. Experimental results demonstrate that\nour method substantially achieves state-of-the-art performance on two datasets\nacross tasks of natural language and paraphrase identification. Further\ndiscussions show that our model can learn meaningful graph structure,\nindicating its superiority on improved interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Le Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanchao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10216","description":"<p>Popular dialog datasets such as MultiWOZ are created by providing crowd\nworkers an instruction, expressed in natural language, that describes the task\nto be accomplished. Crowd workers play the role of a user and an agent to\ngenerate dialogs to accomplish tasks involving booking restaurant tables,\ncalling a taxi etc. In this paper, we present a data creation strategy that\nuses the pre-trained language model, GPT2, to simulate the interaction between\ncrowd workers by creating a user bot and an agent bot. We train the simulators\nusing a smaller percentage of actual crowd-generated conversations and their\ncorresponding instructions. We demonstrate that by using the simulated data, we\nachieve significant improvements in low-resource settings on two publicly\navailable datasets - the MultiWOZ dataset and the Persona chat dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_B/0/1/0/all/0/1\">Biswesh Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora. (arXiv:2010.14649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.14649","description":"<p>We propose a new approach for learning contextualised cross-lingual word\nembeddings based on a small parallel corpus (e.g. a few hundred sentence\npairs). Our method obtains word embeddings via an LSTM encoder-decoder model\nthat simultaneously translates and reconstructs an input sentence. Through\nsharing model parameters among different languages, our model jointly trains\nthe word embeddings in a common cross-lingual space. We also propose to combine\nword and subword embeddings to make use of orthographic similarities across\ndifferent languages. We base our experiments on real-world data from endangered\nlanguages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on\nbilingual lexicon induction and word alignment tasks show that our model\noutperforms existing methods by a large margin for most language pairs. These\nresults demonstrate that, contrary to common belief, an encoder-decoder\ntranslation model is beneficial for learning cross-lingual representations even\nin extremely low-resource conditions. Furthermore, our model also works well on\nhigh-resource conditions, achieving state-of-the-art performance on a\nGerman-English word-alignment task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takashi Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements. (arXiv:2101.06053v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2101.06053","description":"<p>Truly real-life data presents a strong, but exciting challenge for sentiment\nand emotion research. The high variety of possible `in-the-wild' properties\nmakes large datasets such as these indispensable with respect to building\nrobust machine learning models. A sufficient quantity of data covering a deep\nvariety in the challenges of each modality to force the exploratory analysis of\nthe interplay of all modalities has not yet been made available in this\ncontext. In this contribution, we present MuSe-CaR, a first of its kind\nmultimodal dataset. The data is publicly available as it recently served as the\ntesting bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on\nthe tasks of emotion, emotion-target engagement, and trustworthiness\nrecognition by means of comprehensively integrating the audio-visual and\nlanguage modalities. Furthermore, we give a thorough overview of the dataset in\nterms of collection and annotation, including annotation tiers not used in this\nyear's MuSe 2020. In addition, for one of the sub-challenges - predicting the\nlevel of trustworthiness - no participant outperformed the baseline model, and\nso we propose a simple, but highly efficient Multi-Head-Attention network that\nexceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %\nimprovement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximating How Single Head Attention Learns. (arXiv:2103.07601v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07601","description":"<p>Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness. We end by discussing the limitation of our approximation\nframework and suggest future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models have a Moral Dimension. (arXiv:2103.11790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11790","description":"<p>Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning\nthem for specific tasks, researchers have extended state of the art for many\nNLP tasks and shown that they capture not only linguistic knowledge but also\nretain general knowledge implicitly present in the data. Unfortunately, LMs\ntrained on unfiltered text corpora suffer from degenerated and biased\nbehaviour. While this is well established, we show that recent improvements of\nLMs also store ethical and moral norms of the society and actually bring a\n\"moral direction\" to surface. In this study, we show that these norms can be\ncaptured geometrically by a direction, which can be computed, e.g., by a PCA,\nin the embedding space, reflecting well the agreement of phrases to social\nnorms implicitly expressed in the training texts. Furthermore, this provides a\npath for attenuating or even preventing toxic degeneration in LMs. Being able\nto rate the (non-)normativity of arbitrary phrases without explicitly training\nthe LM for this task, we demonstrate the capabilities of the moral direction\nfor guiding (even other) LMs towards producing normative text and showcase it\non RealToxicityPrompts testbed, preventing the neural toxic degeneration in\nGPT-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turan_C/0/1/0/all/0/1\">Cigdem Turan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_N/0/1/0/all/0/1\">Nico Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin Rothkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Models. (arXiv:2104.04580v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2104.04580","description":"<p>In recent years, significant effort has been invested verifying the\nreproducibility and robustness of research claims in social and behavioral\nsciences (SBS), much of which has involved resource-intensive replication\nprojects. In this paper, we investigate prediction of the reproducibility of\nSBS papers using machine learning methods based on a set of features. We\npropose a framework that extracts five types of features from scholarly work\nthat can be used to support assessments of reproducibility of published\nresearch claims. Bibliometric features, venue features, and author features are\ncollected from public APIs or extracted using open source machine learning\nlibraries with customized parsers. Statistical features, such as p-values, are\nextracted by recognizing patterns in the body text. Semantic features, such as\nfunding information, are obtained from public APIs or are extracted using\nnatural language processing models. We analyze pairwise correlations between\nindividual features and their importance for predicting a set of human-assessed\nground truth labels. In doing so, we identify a subset of 9 top features that\nplay relatively more important roles in predicting the reproducibility of SBS\npapers in our corpus. Results are verified by comparing performances of 10\nsupervised predictive classifiers trained on different sets of features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nivargi_R/0/1/0/all/0/1\">Rajal Nivargi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanka_S/0/1/0/all/0/1\">Sree Sai Teja Lanka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Arjun Manoj Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modukuri_S/0/1/0/all/0/1\">Sai Ajay Modukuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakshatri_N/0/1/0/all/0/1\">Nishanth Nakshatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuoer Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajtmajer_S/0/1/0/all/0/1\">Sarah M. Rajtmajer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C. Lee Giles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2104.08663","description":"<p>Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations. (arXiv:2104.08667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08667","description":"<p>Next generation task-oriented dialog systems need to understand\nconversational contexts with their perceived surroundings, to effectively help\nusers in the real-world multimodal environment. Existing task-oriented dialog\ndatasets aimed towards virtual assistance fall short and do not situate the\ndialog in the user's multimodal context. To overcome, we present a new dataset\nfor Situated and Interactive Multimodal Conversations, SIMMC 2.0, which\nincludes 11K task-oriented user&lt;-&gt;assistant dialogs (117K utterances) in the\nshopping domain, grounded in immersive and photo-realistic scenes.\n</p>\n<p>The dialogs are collected using a two-phase pipeline: (1) A novel multimodal\ndialog simulator generates simulated dialog flows, with an emphasis on\ndiversity and richness of interactions, (2) Manual paraphrasing of the\ngenerated utterances to collect diverse referring expressions. We provide an\nin-depth analysis of the collected dataset, and describe in detail the four\nmain benchmark tasks we propose. Our baseline model, powered by the\nstate-of-the-art language model, shows promising results, and highlights new\nchallenges and directions for the community to study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1\">Babak Damavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09608","description":"<p>World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox. (arXiv:2107.11757v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11757","description":"<p>We introduce the MuSe-Toolbox - a Python-based open-source toolkit for\ncreating a variety of continuous and discrete emotion gold standards. In a\nsingle framework, we unify a wide range of fusion methods and propose the novel\nRater Aligned Annotation Weighting (RAAW), which aligns the annotations in a\ntranslation-invariant way before weighting and fusing them based on the\ninter-rater agreements between the annotations. Furthermore, discrete\ncategories tend to be easier for humans to interpret than continuous signals.\nWith this in mind, the MuSe-Toolbox provides the functionality to run\nexhaustive searches for meaningful class clusters in the continuous gold\nstandards. To our knowledge, this is the first toolkit that provides a wide\nselection of state-of-the-art emotional gold standard methods and their\ntransformation to discrete classes. Experimental results indicate that\nMuSe-Toolbox can provide promising and novel class formations which can be\nbetter predicted than hard-coded classes boundaries with minimal human\nintervention. The implementation (1) is out-of-the-box available with all\ndependencies using a Docker container (2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertolli_B/0/1/0/all/0/1\">Benjamin Sertolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigel_B/0/1/0/all/0/1\">Benjamin Weigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06130","description":"<p>The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique or doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11034","description":"<p>Beam search is the default decoding strategy for many sequence generation\ntasks in NLP. The set of approximate K-best items returned by the algorithm is\na useful summary of the distribution for many applications; however, the\ncandidates typically exhibit high overlap and may give a highly biased estimate\nfor expectations under our model. These problems can be addressed by instead\nusing stochastic decoding strategies. In this work, we propose a new method for\nturning beam search into a stochastic process: Conditional Poisson stochastic\nbeam search. Rather than taking the maximizing set at each iteration, we sample\nK candidates without replacement according to the conditional Poisson sampling\ndesign. We view this as a more natural alternative to Kool et. al. 2019's\nstochastic beam search (SBS). Furthermore, we show how samples generated under\nthe CPSBS design can be used to build consistent estimators and sample diverse\nsets from sequence models. In our experiments, we observe CPSBS produces lower\nvariance and more efficient estimators than SBS, even showing improvements in\nhigh entropy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viera_T/0/1/0/all/0/1\">Tim Viera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12068","description":"<p>Transfer learning with a unified Transformer framework (T5) that converts all\nlanguage problems into a text-to-text format has recently been proposed as a\nsimple, yet effective, transfer learning approach. Although a multilingual\nversion of the T5 model (mT5) has been introduced, it is not clear how well it\ncan fare on non-English tasks involving diverse data. To investigate this\nquestion, we apply mT5 on a language with a wide variety of dialects--Arabic.\nFor evaluation, we use an existing benchmark for Arabic language understanding\nand introduce a new benchmark for Arabic language generation (ARGEN). We also\npre-train three powerful Arabic-specific text-to-text Transformer based models\nand evaluate them on the two benchmarks. Our new models perform significantly\nbetter than mT5 and exceed MARBERT, the current state-of-the-art Arabic\nBERT-based model, on Arabic language understanding. The models also set new\nSOTA on the generation benchmark. Our new models and are publicly released at\nhttps://github.com/UBC-NLP/araT5 and ARGEN will be released through the same\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSG@Dravidian-CodeMix-HASOC2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02852","description":"<p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in\nhttps://github.com/seanbenhur/tanglish-offensive-language-identification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05856","description":"<p>Monitoring progress on the United Nations Sustainable Development Goals\n(SDGs) is important for both academic and non-academic organizations. Existing\napproaches to monitoring SDGs have focused on specific data types, namely,\npublications listed in proprietary research databases. We present the text2sdg\nR package, a user-friendly, open-source package that detects SDGs in any kind\nof text data using several different query systems from any text source. The\ntext2sdg package thereby facilitates the monitoring of SDGs for a wide array of\ntext sources and provides a much-needed basis for validating and improving\nextant methods to detect SDGs from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1\">Dominik S. Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_R/0/1/0/all/0/1\">Rui Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1\">Dirk U. Wulff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization. (arXiv:2110.06388v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06388","description":"<p>To capture the semantic graph structure from raw text, most existing\nsummarization approaches are built on GNNs with a pre-trained model. However,\nthese methods suffer from cumbersome procedures and inefficient computations\nfor long-text documents. To mitigate these issues, this paper proposes\nHETFORMER, a Transformer-based pre-trained model with multi-granularity sparse\nattentions for long-text extractive summarization. Specifically, we model\ndifferent types of semantic nodes in raw text as a potential heterogeneous\ngraph and directly learn heterogeneous relationships (edges) among nodes by\nTransformer. Extensive experiments on both single- and multi-document\nsummarization tasks show that HETFORMER achieves state-of-the-art performance\nin Rouge F1 while using less memory and fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian-Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindsight: Posterior-guided training of retrievers for improved open-ended generation. (arXiv:2110.07752v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07752","description":"<p>Many text generation systems benefit from using a retriever to retrieve\npassages from a textual knowledge corpus (e.g., Wikipedia) which are then\nprovided as additional context to the generator. For open-ended generation\ntasks (like generating informative utterances in conversations) many varied\npassages may be equally relevant and we find that existing methods that jointly\ntrain the retriever and generator underperform: the retriever may not find\nrelevant passages even amongst the top-10 and hence the generator may not learn\na preference to ground its generated output in them. We propose using an\nadditional guide retriever that is allowed to use the target output and \"in\nhindsight\" retrieve relevant passages during training. We model the guide\nretriever after the posterior distribution Q of passages given the input and\nthe target output and train it jointly with the standard retriever and the\ngenerator by maximizing the evidence lower bound (ELBo) in expectation over Q.\nFor informative conversations from the Wizard of Wikipedia dataset, with\nposterior-guided training, the retriever finds passages with higher relevance\nin the top-10 (23% relative improvement), the generator's responses are more\ngrounded in the retrieved passage (19% relative improvement) and the end-to-end\nsystem produces better overall output (6.4% relative improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"n-stage Latent Dirichlet Allocation: A Novel Approach for LDA. (arXiv:2110.08591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08591","description":"<p>Nowadays, data analysis has become a problem as the amount of data is\nconstantly increasing. In order to overcome this problem in textual data, many\nmodels and methods are used in natural language processing. The topic modeling\nfield is one of these methods. Topic modeling allows determining the semantic\nstructure of a text document. Latent Dirichlet Allocation (LDA) is the most\ncommon method among topic modeling methods. In this article, the proposed\nn-stage LDA method, which can enable the LDA method to be used more\neffectively, is explained in detail. The positive effect of the method has been\ndemonstrated by the applied English and Turkish studies. Since the method\nfocuses on reducing the word count in the dictionary, it can be used\nlanguage-independently. You can access the open-source code of the method and\nthe example: https://github.com/anil1055/n-stage_LDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guven_Z/0/1/0/all/0/1\">Zekeriya Anil Guven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diri_B/0/1/0/all/0/1\">Banu Diri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakaloglu_T/0/1/0/all/0/1\">Tolgahan Cakaloglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. (arXiv:2110.09702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.09702","description":"<p>Existing text- and image-based multimodal dialogue systems use the\ntraditional Hierarchical Recurrent Encoder-Decoder (HRED) framework, which has\nan utterance-level encoder to model utterance representation and a\ncontext-level encoder to model context representation. Although pioneer efforts\nhave shown promising performances, they still suffer from the following\nchallenges: (1) the interaction between textual features and visual features is\nnot fine-grained enough. (2) the context representation can not provide a\ncomplete representation for the context. To address the issues mentioned above,\nwe propose a non-hierarchical attention network with modality dropout, which\nabandons the HRED framework and utilizes attention modules to encode each\nutterance and model the context representation. To evaluate our proposed model,\nwe conduct comprehensive experiments on a public multimodal dialogue dataset.\nAutomatic and human evaluation demonstrate that our proposed model outperforms\nthe existing methods and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Borun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">YunBo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2105.09142","description":"<p>The automatic detection of humor poses a grand challenge for natural language\nprocessing. Transformer-based systems have recently achieved remarkable results\non this task, but they usually (1)~were evaluated in setups where serious vs\nhumorous texts came from entirely different sources, and (2)~focused on\nbenchmarking performance without providing insights into how the models work.\nWe make progress in both respects by training and analyzing transformer-based\nhumor recognition models on a recently introduced dataset consisting of minimal\npairs of aligned sentences, one serious, the other humorous. We find that,\nalthough our aligned dataset is much harder than previous datasets,\ntransformer-based models recognize the humorous sentence in an aligned pair\nwith high accuracy (78%). In a careful error analysis, we characterize easy vs\nhard instances. Finally, by analyzing attention weights, we obtain important\ninsights into the mechanisms by which transformers recognize humor. Most\nremarkably, we find clear evidence that one single attention head learns to\nrecognize the words that make a test sentence humorous, even without access to\nthis information at training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1\">Kristina Gligori&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-Sim-NGF: FFT-Based Global Rigid Multimodal Alignment of Image Volumes using Normalized Gradient Fields. (arXiv:2110.10156v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10156","description":"<p>Multimodal image alignment involves finding spatial correspondences between\nvolumes varying in appearance and structure. Automated alignment methods are\noften based on local optimization that can be highly sensitive to their\ninitialization. We propose a global optimization method for rigid multimodal 3D\nimage alignment, based on a novel efficient algorithm for computing similarity\nof normalized gradient fields (NGF) in the frequency domain. We validate the\nmethod experimentally on a dataset comprised of 20 brain volumes acquired in\nfour modalities (T1w, Flair, CT, [18F] FDG PET), synthetically displaced with\nknown transformations. The proposed method exhibits excellent performance on\nall six possible modality combinations, and outperforms all four reference\nmethods by a large margin. The method is fast; a 3.4Mvoxel global rigid\nalignment requires approximately 40 seconds of computation, and the proposed\nalgorithm outperforms a direct algorithm for the same task by more than three\norders of magnitude. Open-source implementation is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ofverstedt_J/0/1/0/all/0/1\">Johan &#xd6;fverstedt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindblad_J/0/1/0/all/0/1\">Joakim Lindblad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sladoje_N/0/1/0/all/0/1\">Nata&#x161;a Sladoje</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction. (arXiv:2110.10174v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10174","description":"<p>Every hand-object interaction begins with contact. Despite predicting the\ncontact state between hands and objects is useful in understanding hand-object\ninteractions, prior methods on hand-object analysis have assumed that the\ninteracting hands and objects are known, and were not studied in detail. In\nthis study, we introduce a video-based method for predicting contact between a\nhand and an object. Specifically, given a video and a pair of hand and object\ntracks, we predict a binary contact state (contact or no-contact) for each\nframe. However, annotating a large number of hand-object tracks and contact\nlabels is costly. To overcome the difficulty, we propose a semi-supervised\nframework consisting of (i) automatic collection of training data with\nmotion-based pseudo-labels and (ii) guided progressive label correction (gPLC),\nwhich corrects noisy pseudo-labels with a small amount of trusted data. We\nvalidated our framework's effectiveness on a newly built benchmark dataset for\nhand-object contact prediction and showed superior performance against existing\nbaseline methods. Code and data are available at\nhttps://github.com/takumayagi/hand_object_contact_prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Tasnimul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Cross MLP-Mixer GANs for Cross-View Image Translation. (arXiv:2110.10183v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10183","description":"<p>It is hard to generate an image at target view well for previous cross-view\nimage translation methods that directly adopt a simple encoder-decoder or U-Net\nstructure, especially for drastically different views and severe deformation\ncases. To ease this problem, we propose a novel two-stage framework with a new\nCascaded Cross MLP-Mixer (CrossMLP) sub-network in the first stage and one\nrefined pixel-level loss in the second stage. In the first stage, the CrossMLP\nsub-network learns the latent transformation cues between image code and\nsemantic map code via our novel CrossMLP blocks. Then the coarse results are\ngenerated progressively under the guidance of those cues. Moreover, in the\nsecond stage, we design a refined pixel-level loss that eases the noisy\nsemantic label problem with more reasonable regularization in a more compact\nfashion for better optimization. Extensive experimental results on\nDayton~\\cite{vo2016localizing} and CVUSA~\\cite{workman2015wide} datasets show\nthat our method can generate significantly better results than state-of-the-art\nmethods. The source code and trained models are available at\nhttps://github.com/Amazingren/CrossMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects. (arXiv:2110.10189v1 [cs.RO])","link":"http://arxiv.org/abs/2110.10189","description":"<p>Geometric organization of objects into semantically meaningful arrangements\npervades the built world. As such, assistive robots operating in warehouses,\noffices, and homes would greatly benefit from the ability to recognize and\nrearrange objects into these semantically meaningful structures. To be useful,\nthese robots must contend with previously unseen objects and receive\ninstructions without significant programming. While previous works have\nexamined recognizing pairwise semantic relations and sequential manipulation to\nchange these simple relations none have shown the ability to arrange objects\ninto complex structures such as circles or table settings. To address this\nproblem we propose a novel transformer-based neural network, StructFormer,\nwhich takes as input a partial-view point cloud of the current object\narrangement and a structured language command encoding the desired object\nconfiguration. We show through rigorous experiments that StructFormer enables a\nphysical robot to rearrange novel objects into semantically meaningful\nstructures with multi-object relational constraints inferred from the language\ncommand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoFi: Coarse-to-Fine ICP for LiDAR Localization in an Efficient Long-lasting Point Cloud Map. (arXiv:2110.10194v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10194","description":"<p>LiDAR odometry and localization has attracted increasing research interest in\nrecent years. In the existing works, iterative closest point (ICP) is widely\nused since it is precise and efficient. Due to its non-convexity and its local\niterative strategy, however, ICP-based method easily falls into local optima,\nwhich in turn calls for a precise initialization. In this paper, we propose\nCoFi, a Coarse-to-Fine ICP algorithm for LiDAR localization. Specifically, the\nproposed algorithm down-samples the input point sets under multiple voxel\nresolution, and gradually refines the transformation from the coarse point sets\nto the fine-grained point sets. In addition, we propose a map based LiDAR\nlocalization algorithm that extracts semantic feature points from the LiDAR\nframes and apply CoFi to estimate the pose on an efficient point cloud map.\nWith the help of the Cylinder3D algorithm for LiDAR scan semantic segmentation,\nthe proposed CoFi localization algorithm demonstrates the state-of-the-art\nperformance on the KITTI odometry benchmark, with significant improvement over\nthe literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yecheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Come Again? Re-Query in Referring Expression Comprehension. (arXiv:2110.10206v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10206","description":"<p>To build a shared perception of the world, humans rely on the ability to\nresolve misunderstandings by requesting and accepting clarifications. However,\nwhen evaluating visiolinguistic models, metrics such as accuracy enforce the\nassumption that a decision must be made based on a single piece of evidence. In\nthis work, we relax this assumption for the task of referring expression\ncomprehension by allowing the model to request help when its confidence is low.\nWe consider two ways in which this help can be provided: multimodal re-query,\nwhere the user is allowed to point or click to provide additional information\nto the model, and rephrase re-query, where the user is only allowed to provide\nanother referring expression. We demonstrate the importance of re-query by\nshowing that providing the best referring expression for all objects can\nincrease accuracy by up to 21.9% and that this accuracy can be matched by\nre-querying only 12% of initial referring expressions. We further evaluate\nre-query functions for both multimodal and rephrase re-query across three\nmodern approaches and demonstrate combined replacement for rephrase re-query,\nwhich improves average single-query performance by up to 6.5% and converges to\nas close as 1.6% of the upper bound of single-query performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemmer_S/0/1/0/all/0/1\">Stephan J. Lemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Equivariances and Partial Equivariances from Data. (arXiv:2110.10211v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10211","description":"<p>Group equivariant Convolutional Neural Networks (G-CNNs) constrain features\nto respect the chosen symmetries, and lead to better generalization when these\nsymmetries appear in the data. However, if the chosen symmetries are not\npresent, group equivariant architectures lead to overly constrained models and\nworse performance. Frequently, the distribution of the data can be better\nrepresented by a subset of a group than by the group as a whole, e.g.,\nrotations in $[-90^{\\circ}, 90^{\\circ}]$. In such cases, a model that respects\nequivariance partially is better suited to represent the data. Moreover,\nrelevant symmetries may differ for low and high-level features, e.g., edge\norientations in a face, and face poses relative to the camera. As a result, the\noptimal level of equivariance may differ per layer. In this work, we introduce\nPartial G-CNNs: a family of equivariant networks able to learn partial and full\nequivariances from data at every layer end-to-end. Partial G-CNNs retain full\nequivariance whenever beneficial, e.g., for rotated MNIST, but are able to\nrestrict it whenever it becomes harmful, e.g., for 6~/~9 or natural image\nclassification. Partial G-CNNs perform on par with G-CNNs when full\nequivariance is necessary, and outperform them otherwise. Our method is\napplicable to discrete groups, continuous groups and combinations thereof.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohit_S/0/1/0/all/0/1\">Suhas Lohit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks. (arXiv:2110.10217v1 [cs.NE])","link":"http://arxiv.org/abs/2110.10217","description":"<p>Current state-of-the-art methods of image classification using convolutional\nneural networks are often constrained by both latency and power consumption.\nThis places a limit on the devices, particularly low-power edge devices, that\ncan employ these methods. Spiking neural networks (SNNs) are considered to be\nthe third generation of artificial neural networks which aim to address these\nlatency and power constraints by taking inspiration from biological neuronal\ncommunication processes. Before data such as images can be input into an SNN,\nhowever, they must be first encoded into spike trains. Herein, we propose a\nmethod for encoding static images into temporal spike trains using edge\ndetection and an adaptive signal sampling method for use in SNNs. The edge\ndetection process consists of first performing Canny edge detection on the 2D\nstatic images and then converting the edge detected images into two X and Y\nsignals using an image-to-signal conversion method. The adaptive signaling\napproach consists of sampling the signals such that the signals maintain enough\ndetail and are sensitive to abrupt changes in the signal. Temporal encoding\nmechanisms such as threshold-based representation (TBR) and step-forward (SF)\nare then able to be used to convert the sampled signals into spike trains. We\nuse various error and indicator metrics to optimize and evaluate the efficiency\nand precision of the proposed image encoding approach. Comparison results\nbetween the original and reconstructed signals from spike trains generated\nusing edge-detection and adaptive temporal encoding mechanism exhibit 18x and\n7x reduction in average root mean square error (RMSE) compared to the\nconventional SF and TBR encoding, respectively, while used for encoding MNIST\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandarana_P/0/1/0/all/0/1\">Peyton Chandarana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Junlin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zand_R/0/1/0/all/0/1\">Ramtin Zand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test time Adaptation through Perturbation Robustness. (arXiv:2110.10232v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10232","description":"<p>Data samples generated by several real world processes are dynamic in nature\n\\textit{i.e.}, their characteristics vary with time. Thus it is not possible to\ntrain and tackle all possible distributional shifts between training and\ninference, using the host of transfer learning methods in literature. In this\npaper, we tackle this problem of adapting to domain shift at inference time\n\\textit{i.e.}, we do not change the training process, but quickly adapt the\nmodel at test-time to handle any domain shift. For this, we propose to enforce\nconsistency of predictions of data sampled in the vicinity of test sample on\nthe image manifold. On a host of test scenarios like dealing with corruptions\n(CIFAR-10-C and CIFAR-100-C), and domain adaptation (VisDA-C), our method is at\npar or significantly outperforms previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivaprasad_P/0/1/0/all/0/1\">Prabhu Teja Sivaprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1st Place Solution for the UVO Challenge on Image-based Open-World Segmentation 2021. (arXiv:2110.10239v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10239","description":"<p>We describe our two-stage instance segmentation framework we use to compete\nin the challenge. The first stage of our framework consists of an object\ndetector, which generates object proposals in the format of bounding boxes.\nThen, the images and the detected bounding boxes are fed to the second stage,\nwhere a segmentation network is applied to segment the objects in the bounding\nboxes. We train all our networks in a class-agnostic way. Our approach achieves\nthe first place in the UVO 2021 Image-based Open-World Segmentation Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Automatic Change Detection Frame-work Based on Region Growing and Weighted Local Mutual Information: Analysis of Breast Tumor Response to Chemotherapy in Serial MR Images. (arXiv:2110.10242v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10242","description":"<p>The automatic analysis of subtle changes between longitudinal MR images is an\nimportant task as it is still a challenging issue in scope of the breast\nmedical image processing. In this paper we propose an effective automatic\nchange detection framework composed of two phases since previously used methods\nhave features with low distinctive power. First, in the preprocessing phase an\nintensity normalization method is suggested based on Hierarchical Histogram\nMatching (HHM) that is more robust to noise than previous methods. To eliminate\nundesirable changes and extract the regions containing significant changes the\nproposed Extraction Region of Changes (EROC) method is applied based on\nintensity distribution and Hill-Climbing algorithm. Second, in the detection\nphase a region growing-based approach is suggested to differentiate significant\nchanges from unreal ones. Due to using proposed Weighted Local Mutual\nInformation (WLMI) method to extract high level features and also utilizing the\nprinciple of the local consistency of changes, the proposed approach enjoys\nreasonable performance. The experimental results on both simulated and real\nlongitudinal Breast MR Images confirm the effectiveness of the proposed\nframework. Also, this framework outperforms the human expert in some cases\nwhich can detect many lesion evolutions that are missed by expert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_N/0/1/0/all/0/1\">Narges Norouzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azmi_R/0/1/0/all/0/1\">Reza Azmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noshiri_N/0/1/0/all/0/1\">Nooshin Noshiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anbiaee_R/0/1/0/all/0/1\">Robab Anbiaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early- and in-season crop type mapping without current-year ground truth: generating labels from historical information via a topology-based approach. (arXiv:2110.10275v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10275","description":"<p>Land cover classification in remote sensing is often faced with the challenge\nof limited ground truth. Incorporating historical information has the potential\nto significantly lower the expensive cost associated with collecting ground\ntruth and, more importantly, enable early- and in-season mapping that is\nhelpful to many pre-harvest decisions. In this study, we propose a new approach\nthat can effectively transfer knowledge about the topology (i.e. relative\nposition) of different crop types in the spectral feature space (e.g. the\nhistogram of SWIR1 vs RDEG1 bands) to generate labels, thereby support crop\nclassification in a different year. Importantly, our approach does not attempt\nto transfer classification decision boundaries that are susceptible to\ninter-annual variations of weather and management, but relies on the more\nrobust and shift-invariant topology information. We tested this approach for\nmapping corn/soybeans in the US Midwest and paddy rice/corn/soybeans in\nNortheast China using Landsat-8 and Sentinel-2 data. Results show that our\napproach automatically generates high-quality labels for crops in the target\nyear immediately after each image becomes available. Based on these generated\nlabels from our approach, the subsequent crop type mapping using a random\nforest classifier reach the F1 score as high as 0.887 for corn as early as the\nsilking stage and 0.851 for soybean as early as the flowering stage and the\noverall accuracy of 0.873 in Iowa. In Northeast China, F1 scores of paddy rice,\ncorn and soybeans and the overall accuracy can exceed 0.85 two and half months\nahead of harvest. Overall, these results highlight unique advantages of our\napproach in transferring historical knowledge and maximizing the timeliness of\ncrop maps. Our approach supports a general paradigm shift towards learning\ntransferrable and generalizable knowledge to facilitate land cover\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Liheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiao-Peng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B.Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Control of Artistic Styles in Image Generation. (arXiv:2110.10278v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10278","description":"<p>Recent advances in generative models and adversarial training have enabled\nartificially generating artworks in various artistic styles. It is highly\ndesirable to gain more control over the generated style in practice. However,\nartistic styles are unlike object categories -- there are a continuous spectrum\nof styles distinguished by subtle differences. Few works have been explored to\ncapture the continuous spectrum of styles and apply it to a style generation\ntask. In this paper, we propose to achieve this by embedding original artwork\nexamples into a continuous style space. The style vectors are fed to the\ngenerator and discriminator to achieve fine-grained control. Our method can be\nused with common generative adversarial networks (such as StyleGAN).\nExperiments show that our method not only precisely controls the fine-grained\nartistic style but also improves image quality over vanilla StyleGAN as\nmeasured by FID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhenyu Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Coordinate Decoding for Keypoint Estimation Tasks. (arXiv:2110.10289v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10289","description":"<p>A series of 2D (and 3D) keypoint estimation tasks are built upon heatmap\ncoordinate representation, i.e. a probability map that allows for learnable and\nspatially aware encoding and decoding of keypoint coordinates on grids, even\nallowing for sub-pixel coordinate accuracy. In this report, we aim to reproduce\nthe findings of DARK that investigated the 2D heatmap representation by\nhighlighting the importance of the encoding of the ground truth heatmap and the\ndecoding of the predicted heatmap to keypoint coordinates. The authors claim\nthat a) a more principled distribution-aware coordinate decoding method\novercomes the limitations of the standard techniques widely used in the\nliterature, and b), that the reconstruction of heatmaps from ground-truth\ncoordinates by generating accurate and continuous heatmap distributions lead to\nunbiased model training, contrary to the standard coordinate encoding process\nthat quantizes the keypoint coordinates on the resolution of the input image\ngrid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzitofis_A/0/1/0/all/0/1\">Anargyros Chatzitofis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1\">Georgios Nikolaos Albanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Rich Nearest Neighbor Representations from Self-supervised Ensembles. (arXiv:2110.10293v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10293","description":"<p>Pretraining convolutional neural networks via self-supervision, and applying\nthem in transfer learning, is an incredibly fast-growing field that is rapidly\nand iteratively improving performance across practically all image domains.\nMeanwhile, model ensembling is one of the most universally applicable\ntechniques in supervised learning literature and practice, offering a simple\nsolution to reliably improve performance. But how to optimally combine\nself-supervised models to maximize representation quality has largely remained\nunaddressed. In this work, we provide a framework to perform self-supervised\nmodel ensembling via a novel method of learning representations directly\nthrough gradient descent at inference time. This technique improves\nrepresentation quality, as measured by k-nearest neighbors, both on the\nin-domain dataset and in the transfer setting, with models transferable from\nthe former setting to the latter. Additionally, this direct learning of feature\nthrough backpropagation improves representations from even a single model,\nechoing the improvements found in self-distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Bram Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1\">Devansh Arpit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Contrastive Autoencoder: Using Contrastive Learning for Latent Space Distribution Matching in WAE. (arXiv:2110.10303v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10303","description":"<p>Wasserstein autoencoder (WAE) shows that matching two distributions is\nequivalent to minimizing a simple autoencoder (AE) loss under the constraint\nthat the latent space of this AE matches a pre-specified prior distribution.\nThis latent space distribution matching is a core component of WAE, and a\nchallenging task. In this paper, we propose to use the contrastive learning\nframework that has been shown to be effective for self-supervised\nrepresentation learning, as a means to resolve this problem. We do so by\nexploiting the fact that contrastive learning objectives optimize the latent\nspace distribution to be uniform over the unit hyper-sphere, which can be\neasily sampled from. We show that using the contrastive learning framework to\noptimize the WAE loss achieves faster convergence and more stable optimization\ncompared with existing popular algorithms for WAE. This is also reflected in\nthe FID scores on CelebA and CIFAR-10 datasets, and the realistic generated\nimage quality on the CelebA-HQ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1\">Devansh Arpit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aadyot/0/1/0/all/0/1\">Aadyot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar/0/1/0/all/0/1\">Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Mean Shift for Representation Learning. (arXiv:2110.10309v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10309","description":"<p>We are interested in representation learning from labeled or unlabeled data.\nInspired by recent success of self-supervised learning (SSL), we develop a\nnon-contrastive representation learning method that can exploit additional\nknowledge. This additional knowledge may come from annotated labels in the\nsupervised setting or an SSL model from another modality in the SSL setting.\nOur main idea is to generalize the mean-shift algorithm by constraining the\nsearch space of nearest neighbors, resulting in semantically purer\nrepresentations. Our method simply pulls the embedding of an instance closer to\nits nearest neighbors in a search space that is constrained using the\nadditional knowledge. By leveraging this non-contrastive loss, we show that the\nsupervised ImageNet-1k pretraining with our method results in better transfer\nperformance as compared to the baselines. Further, we demonstrate that our\nmethod is relatively robust to label noise. Finally, we show that it is\npossible to use the noisy constraint across modalities to train self-supervised\nvideo models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R$^3$Net:Relation-embedded Representation Reconstruction Network for Change Captioning. (arXiv:2110.10328v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10328","description":"<p>Change captioning is to use a natural language sentence to describe the\nfine-grained disagreement between two similar images. Viewpoint change is the\nmost typical distractor in this task, because it changes the scale and location\nof the objects and overwhelms the representation of real change. In this paper,\nwe propose a Relation-embedded Representation Reconstruction Network (R$^3$Net)\nto explicitly distinguish the real change from the large amount of clutter and\nirrelevant changes. Specifically, a relation-embedded module is first devised\nto explore potential changed objects in the large amount of clutter. Then,\nbased on the semantic similarities of corresponding locations in the two\nimages, a representation reconstruction module (RRM) is designed to learn the\nreconstruction representation and further model the difference representation.\nBesides, we introduce a syntactic skeleton predictor (SSP) to enhance the\nsemantic interaction between change localization and caption generation.\nExtensive experiments show that the proposed method achieves the\nstate-of-the-art results on two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yunbin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shengxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence-Based Detection, Classification and Prediction/Prognosis in PET Imaging: Towards Radiophenomics. (arXiv:2110.10332v1 [physics.med-ph])","link":"http://arxiv.org/abs/2110.10332","description":"<p>Artificial intelligence (AI) techniques have significant potential to enable\neffective, robust, and automated image phenotyping including identification of\nsubtle patterns. AI-based detection searches the image space to find the\nregions of interest based on patterns and features. There is a spectrum of\ntumor histologies from benign to malignant that can be identified by AI-based\nclassification approaches using image features. The extraction of minable\ninformation from images gives way to the field of radiomics and can be explored\nvia explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics\nanalysis has the potential to be utilized as a noninvasive technique for the\naccurate characterization of tumors to improve diagnosis and treatment\nmonitoring. This work reviews AI-based techniques, with a special focus on\noncological PET and PET/CT imaging, for different detection, classification,\nand prediction/prognosis tasks. We also discuss needed efforts to enable the\ntranslation of AI techniques to routine clinical workflows, and potential\nimprovements and complementary techniques such as the use of natural language\nprocessing on electronic health records and neuro-symbolic AI techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Decasez_P/0/1/0/all/0/1\">Pierre Decasez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amyar_A/0/1/0/all/0/1\">Amine Amyar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Accurate and Reliable Iris Segmentation Using Uncertainty Learning. (arXiv:2110.10334v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10334","description":"<p>As an upstream task of iris recognition, iris segmentation plays a vital role\nin multiple subsequent tasks, including localization and matching. A slight\nbias in iris segmentation often results in obvious performance degradation of\nthe iris recognition system. In the paper, we propose an Iris U-transformer\n(IrisUsformer) for accurate and reliable iris segmentation. For better\naccuracy, we elaborately design IrisUsformer by adopting position-sensitive\noperation and re-packaging transformer block to raise the spatial perception\nability of the model. For better reliability, IrisUsformer utilizes an\nauxiliary head to distinguishes the high- and low-uncertainty regions of\nsegmentation predictions and then adopts a weighting scheme to guide model\noptimization. Experimental results on three publicly available databases\ndemonstrate that IrisUsformer achieves better segmentation accuracy using 35%\nMACs of the SOTA IrisParseNet. More importantly, our method estimates the\nuncertainty map corresponding to the segmentation prediction for subsequent\nprocessing in iris recognition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianze Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler Does It: Generating Semantic Labels with Objectness Guidance. (arXiv:2110.10335v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10335","description":"<p>Existing weakly or semi-supervised semantic segmentation methods utilize\nimage or box-level supervision to generate pseudo-labels for weakly labeled\nimages. However, due to the lack of strong supervision, the generated\npseudo-labels are often noisy near the object boundaries, which severely\nimpacts the network's ability to learn strong representations. To address this\nproblem, we present a novel framework that generates pseudo-labels for training\nimages, which are then used to train a segmentation model. To generate\npseudo-labels, we combine information from: (i) a class agnostic objectness\nnetwork that learns to recognize object-like regions, and (ii) either\nimage-level or bounding box annotations. We show the efficacy of our approach\nby demonstrating how the objectness network can naturally be leveraged to\ngenerate object-like regions for unseen categories. We then propose an\nend-to-end multi-task learning strategy, that jointly learns to segment\nsemantics and objectness using the generated pseudo-labels. Extensive\nexperiments demonstrate the high quality of our generated pseudo-labels and\neffectiveness of the proposed framework in a variety of domains. Our approach\nachieves better or competitive performance compared to existing\nweakly-supervised and semi-supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Amirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1\">Matthew Kowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1\">Neil D. B. Bruce</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBJR: Energy-Based Joint Reasoning for Adaptive Inference. (arXiv:2110.10343v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10343","description":"<p>State-of-the-art deep learning models have achieved significant performance\nlevels on various benchmarks. However, the excellent performance comes at a\ncost of inefficient computational cost. Light-weight architectures, on the\nother hand, achieve moderate accuracies, but at a much more desirable latency.\nThis paper presents a new method of jointly using the large accurate models\ntogether with the small fast ones. To this end, we propose an Energy-Based\nJoint Reasoning (EBJR) framework that adaptively distributes the samples\nbetween shallow and deep models to achieve an accuracy close to the deep model,\nbut latency close to the shallow one. Our method is applicable to\nout-of-the-box pre-trained models as it does not require an architecture change\nnor re-training. Moreover, it is easy to use and deploy, especially for cloud\nservices. Through a comprehensive set of experiments on different down-stream\ntasks, we show that our method outperforms strong state-of-the-art approaches\nwith a considerable margin. In addition, we propose specialized EBJR, an\nextension of our method where we create a smaller specialized side model that\nperforms the target task only partially, but yields an even higher accuracy and\nfaster inference. We verify the strengths of our methods with both theoretical\nand experimental evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTM: Gray Temporal Model for Video Recognition. (arXiv:2110.10348v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10348","description":"<p>Data input modality plays an important role in video action recognition.\nNormally, there are three types of input: RGB, flow stream and compressed data.\nIn this paper, we proposed a new input modality: gray stream. Specifically,\ntaken the stacked consecutive 3 gray images as input, which is the same size of\nRGB, can not only skip the conversion process from video decoding data to RGB,\nbut also improve the spatio-temporal modeling ability at zero computation and\nzero parameters. Meanwhile, we proposed a 1D Identity Channel-wise\nSpatio-temporal Convolution(1D-ICSC) which captures the temporal relationship\nat channel-feature level within a controllable computation budget(by parameters\nG &amp; R). Finally, we confirm its effectiveness and efficiency on several action\nrecognition benchmarks, such as Kinetics, Something-Something, HMDB-51 and\nUCF-101, and achieve impressive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yongxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Gradient Scaling for Few-Shot Learning. (arXiv:2110.10353v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10353","description":"<p>Model-agnostic meta-learning (MAML) is a well-known optimization-based\nmeta-learning algorithm that works well in various computer vision tasks, e.g.,\nfew-shot classification. MAML is to learn an initialization so that a model can\nadapt to a new task in a few steps. However, since the gradient norm of a\nclassifier (head) is much bigger than those of backbone layers, the model\nfocuses on learning the decision boundary of the classifier with similar\nrepresentations. Furthermore, gradient norms of high-level layers are small\nthan those of the other layers. So, the backbone of MAML usually learns\ntask-generic features, which results in deteriorated adaptation performance in\nthe inner-loop. To resolve or mitigate this problem, we propose contextual\ngradient scaling (CxGrad), which scales gradient norms of the backbone to\nfacilitate learning task-specific knowledge in the inner-loop. Since the\nscaling factors are generated from task-conditioned parameters, gradient norms\nof the backbone can be scaled in a task-wise fashion. Experimental results show\nthat CxGrad effectively encourages the backbone to learn task-specific\nknowledge in the inner-loop and improves the performance of MAML up to a\nsignificant margin in both same- and cross-domain few-shot classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Backdoor Attacks Against Point Cloud Classifiers. (arXiv:2110.10354v1 [cs.CR])","link":"http://arxiv.org/abs/2110.10354","description":"<p>Backdoor attacks (BA) are an emerging threat to deep neural network\nclassifiers. A classifier being attacked will predict to the attacker's target\nclass when a test sample from a source class is embedded with the backdoor\npattern (BP). Recently, the first BA against point cloud (PC) classifiers was\nproposed, creating new threats to many important applications including\nautonomous driving. Such PC BAs are not detectable by existing BA defenses due\nto their special BP embedding mechanism. In this paper, we propose a\nreverse-engineering defense that infers whether a PC classifier is backdoor\nattacked, without access to its training set or to any clean classifiers for\nreference. The effectiveness of our defense is demonstrated on the benchmark\nModeNet40 dataset for PCs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhen Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-Person Mesh Recovery From Uncalibrated Multi-View Cameras. (arXiv:2110.10355v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10355","description":"<p>Dynamic multi-person mesh recovery has been a hot topic in 3D vision\nrecently. However, few works focus on the multi-person motion capture from\nuncalibrated cameras, which mainly faces two challenges: the one is that\ninter-person interactions and occlusions introduce inherent ambiguities for\nboth camera calibration and motion capture; The other is that a lack of dense\ncorrespondences can be used to constrain sparse camera geometries in a dynamic\nmulti-person scene. Our key idea is incorporating motion prior knowledge into\nsimultaneous optimization of extrinsic camera parameters and human meshes from\nnoisy human semantics. First, we introduce a physics-geometry consistency to\nreduce the low and high frequency noises of the detected human semantics. Then\na novel latent motion prior is proposed to simultaneously optimize extrinsic\ncamera parameters and coherent human motions from slightly noisy inputs.\nExperimental results show that accurate camera parameters and human motions can\nbe obtained through one-stage optimization. The codes will be publicly\navailable at~\\url{https://www.yangangwang.com}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Buzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yuan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset. (arXiv:2110.10364v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10364","description":"<p>Recent work indicates that, besides being a challenge in producing\nperceptually pleasing images, low light proves more difficult for machine\ncognition than previously thought. In our work, we take a closer look at object\ndetection in low light. First, to support the development and evaluation of new\nmethods in this domain, we present a high-quality large-scale Night Object\nDetection (NOD) dataset showing dynamic scenes captured on the streets at\nnight. Next, we directly link the lighting conditions to perceptual difficulty\nand identify what makes low light problematic for machine cognition.\nAccordingly, we provide instance-level annotation for a subset of the dataset\nfor an in-depth evaluation of future methods. We also present an analysis of\nthe baseline model performance to highlight opportunities for future research\nand show that low light is a non-trivial problem that requires special\nattention from the researchers. Further, to address the issues caused by low\nlight, we propose to incorporate an image enhancement module into the object\ndetection framework and two novel data augmentation techniques. Our image\nenhancement module is trained under the guidance of the object detector to\nlearn image representation optimal for machine cognition rather than for the\nhuman visual system. Finally, experimental results confirm that the proposed\nmethod shows consistent improvement of the performance on low-light datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morawski_I/0/1/0/all/0/1\">Igor Morawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples. (arXiv:2110.10366v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10366","description":"<p>Convolutional Neural Networks (CNNs) for visual tasks are believed to learn\nboth the low-level textures and high-level object attributes, throughout the\nnetwork depth. This paper further investigates the `texture bias' in CNNs. To\nthis end, we regenerate multiple instances of training examples from each\noriginal image, through a process we call `repainting'. The repainted examples\npreserve the shape and structure of the regions and objects within the scenes,\nbut diversify their texture and color. Our method can regenerate a same image\nat different daylight, season, or weather conditions, can have colorization or\nde-colorization effects, or even bring back some texture information from\nblacked-out areas. The in-place repaint allows us to further use these\nrepainted examples for improving the generalization of CNNs. Through an\nextensive set of experiments, we demonstrate the usefulness of the repainted\nexamples in training, for the tasks of image classification (ImageNet) and\nobject detection (COCO), over several state-of-the-art network architectures at\ndifferent capacities, and across different data availability regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning. (arXiv:2110.10368v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10368","description":"<p>Existing semi-supervised learning (SSL) algorithms typically assume\nclass-balanced datasets, although the class distributions of many real-world\ndatasets are imbalanced. In general, classifiers trained on a class-imbalanced\ndataset are biased toward the majority classes. This issue becomes more\nproblematic for SSL algorithms because they utilize the biased prediction of\nunlabeled data for training. However, traditional class-imbalanced learning\ntechniques, which are designed for labeled data, cannot be readily combined\nwith SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that\ncan effectively use unlabeled data, while mitigating class imbalance by\nintroducing an auxiliary balanced classifier (ABC) of a single layer, which is\nattached to a representation layer of an existing SSL algorithm. The ABC is\ntrained with a class-balanced loss of a minibatch, while using high-quality\nrepresentations learned from all data points in the minibatch using the\nbackbone SSL algorithm to avoid overfitting and information loss.Moreover, we\nuse consistency regularization, a recent SSL technique for utilizing unlabeled\ndata in a modified way, to train the ABC to be balanced among the classes by\nselecting unlabeled data with the same probability for each class. The proposed\nalgorithm achieves state-of-the-art performance in various class-imbalanced SSL\nexperiments using four benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyuck Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungjae Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?. (arXiv:2110.10369v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10369","description":"<p>The diversity of deep learning applications, datasets, and neural network\narchitectures necessitates a careful selection of the architecture and data\nthat match best to a target application. As an attempt to mitigate this\ndilemma, this paper investigates the idea of combining multiple trained neural\nnetworks using unlabeled data. In addition, combining multiple models into one\ncan speed up the inference, result in stronger, more capable models, and allows\nus to select efficient device-friendly target network architectures. To this\nend, the proposed method makes use of generation, filtering, and aggregation of\nreliable pseudo-labels collected from unlabeled data. Our method supports using\nan arbitrary number of input models with arbitrary architectures and\ncategories. Extensive performance evaluations demonstrated that our method is\nvery effective. For example, for the task of object detection and without using\nany ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an\nEfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model,\nwith a similar mAP as the supervised training. If fine-tuned in a\nsemi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1%\nmAP improvements over supervised training with 1%, 5%, and 10% of labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images. (arXiv:2110.10381v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10381","description":"<p>Elbow fractures are one of the most common fracture types. Diagnoses on elbow\nfractures often need the help of radiographic imaging to be read and analyzed\nby a specialized radiologist with years of training. Thanks to the recent\nadvances of deep learning, a model that can classify and detect different types\nof bone fractures needs only hours of training and has shown promising results.\nHowever, most existing deep learning models are purely data-driven, lacking\nincorporation of known domain knowledge from human experts. In this work, we\npropose a novel deep learning method to diagnose elbow fracture from elbow\nX-ray images by integrating domain-specific medical knowledge into a curriculum\nlearning framework. In our method, the training data are permutated by sampling\nwithout replacement at the beginning of each training epoch. The sampling\nprobability of each training sample is guided by a scoring criterion\nconstructed based on clinically known knowledge from human experts, where the\nscoring indicates the diagnosis difficultness of different elbow fracture\nsubtypes. We also propose an algorithm that updates the sampling probabilities\nat each epoch, which is applicable to other sampling-based curriculum learning\nframeworks. We design an experiment with 1865 elbow X-ray images for a\nfracture/normal binary classification task and compare our proposed method to a\nbaseline method and a previous method using multiple metrics. Our results show\nthat the proposed method achieves the highest classification performance. Also,\nour proposed probability update algorithm boosts the performance of the\nprevious method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitamura_G/0/1/0/all/0/1\">Gene Kitamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doganay_E/0/1/0/all/0/1\">Emine Doganay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arefan_D/0/1/0/all/0/1\">Dooman Arefan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Shandong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Guided Multiview Deep Curriculum Learning for Elbow Fracture Classification. (arXiv:2110.10383v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10383","description":"<p>Elbow fracture diagnosis often requires patients to take both frontal and\nlateral views of elbow X-ray radiographs. In this paper, we propose a multiview\ndeep learning method for an elbow fracture subtype classification task. Our\nstrategy leverages transfer learning by first training two single-view models,\none for frontal view and the other for lateral view, and then transferring the\nweights to the corresponding layers in the proposed multiview network\narchitecture. Meanwhile, quantitative medical knowledge was integrated into the\ntraining process through a curriculum learning framework, which enables the\nmodel to first learn from \"easier\" samples and then transition to \"harder\"\nsamples to reach better performance. In addition, our multiview network can\nwork both in a dual-view setting and with a single view as input. We evaluate\nour method through extensive experiments on a classification task of elbow\nfracture with a dataset of 1,964 images. Results show that our method\noutperforms two related methods on bone fracture study in multiple settings,\nand our technique is able to boost the performance of the compared methods. The\ncode is available at https://github.com/ljaiverson/multiview-curriculum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitamura_G/0/1/0/all/0/1\">Gene Kitamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arefan_D/0/1/0/all/0/1\">Dooman Arefan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doganay_E/0/1/0/all/0/1\">Emine Doganay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panigrahy_A/0/1/0/all/0/1\">Ashok Panigrahy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Shandong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias. (arXiv:2110.10389v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10389","description":"<p>Contextual information is a valuable cue for Deep Neural Networks (DNNs) to\nlearn better representations and improve accuracy. However, co-occurrence bias\nin the training dataset may hamper a DNN model's generalizability to unseen\nscenarios in the real world. For example, in COCO, many object categories have\na much higher co-occurrence with men compared to women, which can bias a DNN's\nprediction in favor of men. Recent works have focused on task-specific training\nstrategies to handle bias in such scenarios, but fixing the available data is\noften ignored. In this paper, we propose a novel and more generic solution to\naddress the contextual bias in the datasets by selecting a subset of the\nsamples, which is fair in terms of the co-occurrence with various classes for a\nprotected attribute. We introduce a data repair algorithm using the coefficient\nof variation, which can curate fair and contextually balanced data for a\nprotected class(es). This helps in training a fair model irrespective of the\ntask, architecture or training methodology. Our proposed solution is simple,\neffective, and can even be used in an active learning setting where the data\nlabels are not present or being generated incrementally. We demonstrate the\neffectiveness of our algorithm for the task of object detection and multi-label\nimage classification across different datasets. Through a series of\nexperiments, we validate that curating contextually fair data helps make model\npredictions fair by balancing the true positive rate for the protected class\nacross groups without compromising on the model's overall performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sharat Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muku_S/0/1/0/all/0/1\">Sumanyu Muku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Saket Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for HDR Imaging: State-of-the-Art and Future Trends. (arXiv:2110.10394v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10394","description":"<p>High dynamic range (HDR) imaging is a technique that allows an extensive\ndynamic range of exposures, which is important in image processing, computer\ngraphics, and computer vision. In recent years, there has been a significant\nadvancement in HDR imaging using deep learning (DL). This study conducts a\ncomprehensive and insightful survey and analysis of recent developments in deep\nHDR imaging methodologies. We hierarchically and structurally group existing\ndeep HDR imaging methods into five categories based on (1) number/domain of\ninput exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel\nlearning strategies, and (5) applications. Importantly, we provide a\nconstructive discussion on each category regarding its potential and\nchallenges. Moreover, we review some crucial aspects of deep HDR imaging, such\nas datasets and evaluation metrics. Finally, we highlight some open problems\nand point out future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DFaceFill: An Analysis-By-Synthesis Approach to Face Completion. (arXiv:2110.10395v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10395","description":"<p>Existing face completion solutions are primarily driven by end-to-end models\nthat directly generate 2D completions of 2D masked faces. By having to\nimplicitly account for geometric and photometric variations in facial shape and\nappearance, such approaches result in unrealistic completions, especially under\nlarge variations in pose, shape, illumination and mask sizes. To alleviate\nthese limitations, we introduce 3DFaceFill, an analysis-by-synthesis approach\nfor face completion that explicitly considers the image formation process. It\ncomprises three components, (1) an encoder that disentangles the face into its\nconstituent 3D mesh, 3D pose, illumination and albedo factors, (2) an\nautoencoder that inpaints the UV representation of facial albedo, and (3) a\nrenderer that resynthesizes the completed face. By operating on the UV\nrepresentation, 3DFaceFill affords the power of correspondence and allows us to\nnaturally enforce geometrical priors (e.g. facial symmetry) more effectively.\nQuantitatively, 3DFaceFill improves the state-of-the-art by up to 4dB higher\nPSNR and 25% better LPIPS for large masks. And, qualitatively, it leads to\ndemonstrably more photorealistic face completions over a range of masks and\nocclusions while preserving consistency in global and component-wise shape,\npose, illumination and eye-gaze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1\">Rahul Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Boddeti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation. (arXiv:2110.10403v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10403","description":"<p>Recent advances in transformer-based models have drawn attention to exploring\nthese techniques in medical image segmentation, especially in conjunction with\nthe U-Net model (or its variants), which has shown great success in medical\nimage segmentation, under both 2D and 3D settings. Current 2D based methods\neither directly replace convolutional layers with pure transformers or consider\na transformer as an additional intermediate encoder between the encoder and\ndecoder of U-Net. However, these approaches only consider the attention\nencoding within one single slice and do not utilize the axial-axis information\nnaturally provided by a 3D volume. In the 3D setting, convolution on volumetric\ndata and transformers both consume large GPU memory. One has to either\ndownsample the image or use cropped local patches to reduce GPU memory usage,\nwhich limits its performance. In this paper, we propose Axial Fusion\nTransformer UNet (AFTer-UNet), which takes both advantages of convolutional\nlayers' capability of extracting detailed features and transformers' strength\non long sequence modeling. It considers both intra-slice and inter-slice\nlong-range cues to guide the segmentation. Meanwhile, it has fewer parameters\nand takes less GPU memory to train than the previous transformer-based models.\nExtensive experiments on three multi-organ segmentation datasets demonstrate\nthat our method outperforms current state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_X/0/1/0/all/0/1\">Xiangyi Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARTS: Eliminating Inconsistency between Text Detection and Recognition with Auto-Rectification Text Spotter. (arXiv:2110.10405v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10405","description":"<p>Recent approaches for end-to-end text spotting have achieved promising\nresults. However, most of the current spotters were plagued by the\ninconsistency problem between text detection and recognition. In this work, we\nintroduce and prove the existence of the inconsistency problem and analyze it\nfrom two aspects: (1) inconsistency of text recognition features between\ntraining and testing, and (2) inconsistency of optimization targets between\ntext detection and recognition. To solve the aforementioned issues, we propose\na differentiable Auto-Rectification Module (ARM) together with a new training\nstrategy to enable propagating recognition loss back into detection branch, so\nthat our detection branch can be jointly optimized by detection and recognition\ntargets, which largely alleviates the inconsistency problem between text\ndetection and recognition. Based on these designs, we present a simple yet\nrobust end-to-end text spotting framework, termed Auto-Rectification Text\nSpotter (ARTS), to detect and recognize arbitrarily-shaped text in natural\nscenes. Extensive experiments demonstrate the superiority of our method. In\nparticular, our ARTS-S achieves 77.1% end-to-end text spotting F-measure on\nTotal-Text at a competitive speed of 10.5 FPS, which significantly outperforms\nprevious methods in both accuracy and inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Humen Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Cong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth360: Monocular Depth Estimation using Learnable Axisymmetric Camera Model for Spherical Camera Image. (arXiv:2110.10415v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10415","description":"<p>Self-supervised monocular depth estimation has been widely investigated to\nestimate depth images and relative poses from RGB images. This framework is\nattractive for researchers because the depth and pose networks can be trained\nfrom just time sequence images without the need for the ground truth depth and\nposes.\n</p>\n<p>In this work, we estimate the depth around a robot (360 degree view) using\ntime sequence spherical camera images, from a camera whose parameters are\nunknown. We propose a learnable axisymmetric camera model which accepts\ndistorted spherical camera images with two fisheye camera images. In addition,\nwe trained our models with a photo-realistic simulator to generate ground truth\ndepth images to provide supervision. Moreover, we introduced loss functions to\nprovide floor constraints to reduce artifacts that can result from reflective\nfloor surfaces. We demonstrate the efficacy of our method using the spherical\ncamera images from the GO Stanford dataset and pinhole camera images from the\nKITTI dataset to compare our method's performance with that of baseline method\nin learning the camera parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1\">Noriaki Hirose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahara_K/0/1/0/all/0/1\">Kosuke Tahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A unifying framework for $n$-dimensional quasi-conformal mappings. (arXiv:2110.10437v1 [cs.CG])","link":"http://arxiv.org/abs/2110.10437","description":"<p>With the advancement of computer technology, there is a surge of interest in\neffective mapping methods for objects in higher-dimensional spaces. To\nestablish a one-to-one correspondence between objects, higher-dimensional\nquasi-conformal theory can be utilized for ensuring the bijectivity of the\nmappings. In addition, it is often desirable for the mappings to satisfy\ncertain prescribed geometric constraints and possess low distortion in\nconformality or volume. In this work, we develop a unifying framework for\ncomputing $n$-dimensional quasi-conformal mappings. More specifically, we\npropose a variational model that integrates quasi-conformal distortion,\nvolumetric distortion, landmark correspondence, intensity mismatch and volume\nprior information to handle a large variety of deformation problems. We further\nprove the existence of a minimizer for the proposed model and devise efficient\nnumerical methods to solve the optimization problem. We demonstrate the\neffectiveness of the proposed framework using various experiments in two- and\nthree-dimensions, with applications to medical image registration, adaptive\nremeshing and shape modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moir\\'e Attack (MA): A New Potential Risk of Screen Photos. (arXiv:2110.10444v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10444","description":"<p>Images, captured by a camera, play a critical role in training Deep Neural\nNetworks (DNNs). Usually, we assume the images acquired by cameras are\nconsistent with the ones perceived by human eyes. However, due to the different\nphysical mechanisms between human-vision and computer-vision systems, the final\nperceived images could be very different in some cases, for example shooting on\ndigital monitors. In this paper, we find a special phenomenon in digital image\nprocessing, the moir\\'e effect, that could cause unnoticed security threats to\nDNNs. Based on it, we propose a Moir\\'e Attack (MA) that generates the\nphysical-world moir\\'e pattern adding to the images by mimicking the shooting\nprocess of digital devices. Extensive experiments demonstrate that our proposed\ndigital Moir\\'e Attack (MA) is a perfect camouflage for attackers to tamper\nwith DNNs with a high success rate ($100.0\\%$ for untargeted and $97.0\\%$ for\ntargeted attack with the noise budget $\\epsilon=4$), high transferability rate\nacross different models, and high robustness under various defenses.\nFurthermore, MA owns great stealthiness because the moir\\'e effect is\nunavoidable due to the camera's inner physical structure, which therefore\nhardly attracts the awareness of humans. Our code is available at\nhttps://github.com/Dantong88/Moire_Attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Annotation Refinement for Object Detection. (arXiv:2110.10456v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10456","description":"<p>Supervised training of object detectors requires well-annotated large-scale\ndatasets, whose production is costly. Therefore, some efforts have been made to\nobtain annotations in economical ways, such as cloud sourcing. However,\ndatasets obtained by these methods tend to contain noisy annotations such as\ninaccurate bounding boxes and incorrect class labels. In this study, we propose\na new problem setting of training object detectors on datasets with entangled\nnoises of annotations of class labels and bounding boxes. Our proposed method\nefficiently decouples the entangled noises, corrects the noisy annotations, and\nsubsequently trains the detector using the corrected annotations. We verified\nthe effectiveness of our proposed method and compared it with the baseline on\nnoisy datasets with different noise levels. The experimental results show that\nour proposed method significantly outperforms the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiafeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamakata_Y/0/1/0/all/0/1\">Yoko Yamakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Style Transfer. (arXiv:2110.10481v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10481","description":"<p>Currently, it is hard to compare and evaluate different style transfer\nalgorithms due to chaotic definitions of style and the absence of agreed\nobjective validation methods in the study of style transfer. In this paper, a\nnovel approach, the Unified Style Transfer (UST) model, is proposed. With the\nintroduction of a generative model for internal style representation, UST can\ntransfer images in two approaches, i.e., Domain-based and Image-based,\nsimultaneously. At the same time, a new philosophy based on the human sense of\nart and style distributions for evaluating the transfer model is presented and\ndemonstrated, called Statistical Style Analysis. It provides a new path to\nvalidate style transfer models' feasibility by validating the general\nconsistency between internal style representation and art facts. Besides, the\ntranslation-invariance of AdaIN features is also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of augmentation methods in classifying autism spectrum disorders from fMRI data with 3D convolutional neural networks. (arXiv:2110.10489v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10489","description":"<p>Classifying subjects as healthy or diseased using neuroimaging data has\ngained a lot of attention during the last 10 years. Here we apply deep learning\nto derivatives from resting state fMRI data, and investigate how different 3D\naugmentation techniques affect the test accuracy. Specifically, we use resting\nstate derivatives from 1,112 subjects in ABIDE preprocessed to train a 3D\nconvolutional neural network (CNN) to perform the classification. Our results\nshow that augmentation only provide minor improvements to the test accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jonemo_J/0/1/0/all/0/1\">Johan J&#xf6;nemo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abramian_D/0/1/0/all/0/1\">David Abramian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1\">Anders Eklund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Point Cloud Normal Estimation via Triplet Learning. (arXiv:2110.10494v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10494","description":"<p>Normal estimation on 3D point clouds is a fundamental problem in 3D vision\nand graphics. Current methods often show limited accuracy in predicting normals\nat sharp features (e.g., edges and corners) and less robustness to noise. In\nthis paper, we propose a novel normal estimation method for point clouds. It\nconsists of two phases: (a) feature encoding which learns representations of\nlocal patches, and (b) normal estimation that takes the learned representation\nas input and regresses the normal vector. We are motivated that local patches\non isotropic and anisotropic surfaces have similar or distinct normals, and\nthat separable features or representations can be learned to facilitate normal\nestimation. To realise this, we first construct triplets of local patches on 3D\npoint cloud data, and design a triplet network with a triplet loss for feature\nencoding. We then design a simple network with several MLPs and a loss function\nto regress the normal vector. Despite having a smaller network size compared to\nmost other methods, experimental results show that our method preserves sharp\nfeatures and achieves better normal estimation results on CAD-like shapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edirimuni_D/0/1/0/all/0/1\">Dasith de Silva Edirimuni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robles_Kelly_A/0/1/0/all/0/1\">Antonio Robles-Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STALP: Style Transfer with Auxiliary Limited Pairing. (arXiv:2110.10501v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10501","description":"<p>We present an approach to example-based stylization of images that uses a\nsingle pair of a source image and its stylized counterpart. We demonstrate how\nto train an image translation network that can perform real-time semantically\nmeaningful style transfer to a set of target images with similar content as the\nsource image. A key added value of our approach is that it considers also\nconsistency of target images during training. Although those have no stylized\ncounterparts, we constrain the translation to keep the statistics of neural\nresponses compatible with those extracted from the stylized source. In contrast\nto concurrent techniques that use a similar input, our approach better\npreserves important visual characteristics of the source style and can deliver\ntemporally stable results without the need to explicitly handle temporal\nconsistency. We demonstrate its practical utility on various applications\nincluding video stylization, style transfer to panoramas, faces, and 3D models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futschik_D/0/1/0/all/0/1\">David Futschik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucera_M/0/1/0/all/0/1\">Michal Ku&#x10d;era</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukac_M/0/1/0/all/0/1\">Michal Luk&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sykora_D/0/1/0/all/0/1\">Daniel S&#xfd;kora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Guided Depth Sensing. (arXiv:2110.10505v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10505","description":"<p>Active depth sensors like structured light, lidar, and time-of-flight systems\nsample the depth of the entire scene uniformly at a fixed scan rate. This leads\nto limited spatio-temporal resolution where redundant static information is\nover-sampled and precious motion information might be under-sampled. In this\npaper, we present an efficient bio-inspired event-camera-driven depth\nestimation algorithm. In our approach, we dynamically illuminate areas of\ninterest densely, depending on the scene activity detected by the event camera,\nand sparsely illuminate areas in the field of view with no motion. The depth\nestimation is achieved by an event-based structured light system consisting of\na laser point projector coupled with a second event-based sensor tuned to\ndetect the reflection of the laser from the scene. We show the feasibility of\nour approach in a simulated autonomous driving scenario and real indoor\nsequences using our prototype. We show that, in natural scenes like autonomous\ndriving and indoor environments, moving edges correspond to less than 10% of\nthe scene on average. Thus our setup requires the sensor to scan only 10% of\nthe scene, which could lead to almost 90% less power consumption by the\nillumination source. While we present the evaluation and proof-of-concept for\nan event-based structured-light system, the ideas presented here are applicable\nfor a wide range of depth-sensing modalities like LIDAR, time-of-flight, and\nstandard stereo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeys_D/0/1/0/all/0/1\">Diederik Paul Moeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development and accuracy evaluation of Coded Phase-shift 3D scanner. (arXiv:2110.10520v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10520","description":"<p>In this paper, we provide an overview of development of a structured light\n3D-scanner based on combination of binary-coded patterns and sinusoidal\nphase-shifted fringe patterns called Coded Phase-shift technique. Further, we\ndescribe the experiments performed to evaluate measurement accuracy and\nprecision of the developed system. A study of this kind is expected to be\nhelpful in understanding the basic working of current structured-light 3D\nscanners and the approaches followed for their performance assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gaur_P/0/1/0/all/0/1\">Pranav Kant Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarode_D/0/1/0/all/0/1\">D.M.Sarode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bose_S/0/1/0/all/0/1\">S.K.Bose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Identifying Optical Signal Attacks on Autonomous Driving Systems. (arXiv:2110.10523v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10523","description":"<p>For autonomous driving, an essential task is to detect surrounding objects\naccurately. To this end, most existing systems use optical devices, including\ncameras and light detection and ranging (LiDAR) sensors, to collect environment\ndata in real time. In recent years, many researchers have developed advanced\nmachine learning models to detect surrounding objects. Nevertheless, the\naforementioned optical devices are vulnerable to optical signal attacks, which\ncould compromise the accuracy of object detection. To address this critical\nissue, we propose a framework to detect and identify sensors that are under\nattack. Specifically, we first develop a new technique to detect attacks on a\nsystem that consists of three sensors. Our main idea is to: 1) use data from\nthree sensors to obtain two versions of depth maps (i.e., disparity) and 2)\ndetect attacks by analyzing the distribution of disparity errors. In our study,\nwe use real data sets and the state-of-the-art machine learning model to\nevaluate our attack detection scheme and the results confirm the effectiveness\nof our detection method. Based on the detection scheme, we further develop an\nidentification model that is capable of identifying up to n-2 attacked sensors\nin a system with one LiDAR and n cameras. We prove the correctness of our\nidentification scheme and conduct experiments to show the accuracy of our\nidentification method. Finally, we investigate the overall sensitivity of our\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AniFormer: Data-driven 3D Animation with Transformer. (arXiv:2110.10533v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10533","description":"<p>We present a novel task, i.e., animating a target 3D object through the\nmotion of a raw driving sequence. In previous works, extra auxiliary\ncorrelations between source and target meshes or intermedia factors are\ninevitable to capture the motions in the driving sequences. Instead, we\nintroduce AniFormer, a novel Transformer-based architecture, that generates\nanimated 3D sequences by directly taking the raw driving sequences and\narbitrary same-type target meshes as inputs. Specifically, we customize the\nTransformer architecture for 3D animation that generates mesh sequences by\nintegrating styles from target meshes and motions from the driving meshes.\nBesides, instead of the conventional single regression head in the vanilla\nTransformer, AniFormer generates multiple frames as outputs to preserve the\nsequential consistency of the generated meshes. To achieve this, we carefully\ndesign a pair of regression constraints, i.e., motion and appearance\nconstraints, that can provide strong regularization on the generated mesh\nsequences. Our AniFormer achieves high-fidelity, realistic, temporally coherent\nanimated results and outperforms compared start-of-the-art methods on\nbenchmarks of diverse categories. Code is available:\nhttps://github.com/mikecheninoulu/AniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Model Generalization by Agreement of Learned Representations from Data Augmentation. (arXiv:2110.10536v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10536","description":"<p>Data augmentation reduces the generalization error by forcing a model to\nlearn invariant representations given different transformations of the input\nimage. In computer vision, on top of the standard image processing functions,\ndata augmentation techniques based on regional dropout such as CutOut, MixUp,\nand CutMix and policy-based selection such as AutoAugment demonstrated\nstate-of-the-art (SOTA) results. With an increasing number of data augmentation\nalgorithms being proposed, the focus is always on optimizing the input-output\nmapping while not realizing that there might be an untapped value in the\ntransformed images with the same label. We hypothesize that by forcing the\nrepresentations of two transformations to agree, we can further reduce the\nmodel generalization error. We call our proposed method Agreement Maximization\nor simply AgMax. With this simple constraint applied during training, empirical\nresults show that data augmentation algorithms can further improve the\nclassification accuracy of ResNet50 on ImageNet by up to 1.5%, WideResNet40-2\non CIFAR10 by up to 0.7%, WideResNet40-2 on CIFAR100 by up to 1.6%, and LeNet5\non Speech Commands Dataset by up to 1.4%. Experimental results further show\nthat unlike other regularization terms such as label smoothing, AgMax can take\nadvantage of the data augmentation to consistently improve model generalization\nby a significant margin. On downstream tasks such as object detection and\nsegmentation on PascalVOC and COCO, AgMax pre-trained models outperforms other\ndata augmentation methods by as much as 1.0mAP (box) and 0.5mAP (mask). Code is\navailable at https://github.com/roatienza/agmax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning. (arXiv:2110.10538v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10538","description":"<p>Access to 3D point cloud representations has been widely facilitated by LiDAR\nsensors embedded in various mobile devices. This has led to an emerging need\nfor fast and accurate point cloud processing techniques. In this paper, we\nrevisit and dive deeper into PointNet++, one of the most influential yet\nunder-explored networks, and develop faster and more accurate variants of the\nmodel. We first present a novel Separable Set Abstraction (SA) module that\ndisentangles the vanilla SA module used in PointNet++ into two separate\nlearning stages: (1) learning channel correlation and (2) learning spatial\ncorrelation. The Separable SA module is significantly faster than the vanilla\nversion, yet it achieves comparable performance. We then introduce a new\nAnisotropic Reduction function into our Separable SA module and propose an\nAnisotropic Separable SA (ASSA) module that substantially increases the\nnetwork's accuracy. We later replace the vanilla SA modules in PointNet++ with\nthe proposed ASSA module, and denote the modified network as ASSANet. Extensive\nexperiments on point cloud classification, semantic segmentation, and part\nsegmentation show that ASSANet outperforms PointNet++ and other methods,\nachieving much higher accuracy and faster speeds. In particular, ASSANet\noutperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6\n\\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled\nASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more\nthan $54 \\times$ faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guocheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation. (arXiv:2110.10546v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10546","description":"<p>Single image reflection separation (SIRS), as a representative blind source\nseparation task, aims to recover two layers, $\\textit{i.e.}$, transmission and\nreflection, from one mixed observation, which is challenging due to the highly\nill-posed nature. Existing deep learning based solutions typically restore the\ntarget layers individually, or with some concerns at the end of the output,\nbarely taking into account the interaction across the two streams/branches. In\norder to utilize information more efficiently, this work presents a general yet\nsimple interactive strategy, namely $\\textit{your trash is my treasure}$\n(YTMT), for constructing dual-stream decomposition networks. To be specific, we\nexplicitly enforce the two streams to communicate with each other block-wisely.\nInspired by the additive property between the two components, the interactive\npath can be easily built via transferring, instead of discarding, deactivated\ninformation by the ReLU rectifier from one stream to the other. Both ablation\nstudies and experimental results on widely-used SIRS datasets are conducted to\ndemonstrate the efficacy of YTMT, and reveal its superiority over other\nstate-of-the-art alternatives. The implementation is quite simple and our code\nis publicly available at\n$\\href{https://github.com/mingcv/YTMT-Strategy}{\\textit{https://github.com/mingcv/YTMT-Strategy}}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Temporal Action Localization with Query Adaptive Transformer. (arXiv:2110.10552v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10552","description":"<p>Existing temporal action localization (TAL) works rely on a large number of\ntraining videos with exhaustive segment-level annotation, preventing them from\nscaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL)\naims to adapt a model to a new class represented by as few as a single video.\nExiting FS-TAL methods assume trimmed training videos for new classes. However,\nthis setting is not only unnatural actions are typically captured in untrimmed\nvideos, but also ignores background video segments containing vital contextual\ncues for foreground action segmentation. In this work, we first propose a new\nFS-TAL setting by proposing to use untrimmed training videos. Further, a novel\nFS-TAL model is proposed which maximizes the knowledge transfer from training\nclasses whilst enabling the model to be dynamically adapted to both the new\nclass and each video of that class simultaneously. This is achieved by\nintroducing a query adaptive Transformer in the model. Extensive experiments on\ntwo action localization benchmarks demonstrate that our method can outperform\nall the state of the art alternatives significantly in both single-domain and\ncross-domain scenarios. The source code can be found in\nhttps://github.com/sauradip/fewshotQAT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sauradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task Uncertainty Estimation. (arXiv:2110.10563v1 [cs.RO])","link":"http://arxiv.org/abs/2110.10563","description":"<p>Robust localization in dense urban scenarios using a low-cost sensor setup\nand sparse HD maps is highly relevant for the current advances in autonomous\ndriving, but remains a challenging topic in research. We present a novel\nmonocular localization approach based on a sliding-window pose graph that\nleverages predicted uncertainties for increased precision and robustness\nagainst challenging scenarios and per frame failures. To this end, we propose\nan efficient multi-task uncertainty-aware perception module, which covers\nsemantic segmentation, as well as bounding box detection, to enable the\nlocalization of vehicles in sparse maps, containing only lane borders and\ntraffic lights. Further, we design differentiable cost maps that are directly\ngenerated from the estimated uncertainties. This opens up the possibility to\nminimize the reprojection loss of amorphous map elements in an association free\nand uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows\nthat, despite the sparsity of the map, our approach enables robust and accurate\n6D localization in challenging urban scenarios\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petek_K/0/1/0/all/0/1\">K&#xfc;rsat Petek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1\">Kshitij Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1\">Daniel B&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fingerprint recognition with embedded presentation attacks detection: are we ready?. (arXiv:2110.10567v1 [cs.CR])","link":"http://arxiv.org/abs/2110.10567","description":"<p>The diffusion of fingerprint verification systems for security applications\nmakes it urgent to investigate the embedding of software-based presentation\nattack detection algorithms (PAD) into such systems. Companies and institutions\nneed to know whether such integration would make the system more \"secure\" and\nwhether the technology available is ready, and, if so, at what operational\nworking conditions. Despite significant improvements, especially by adopting\ndeep learning approaches to fingerprint PAD, current research did not state\nmuch about their effectiveness when embedded in fingerprint verification\nsystems. We believe that the lack of works is explained by the lack of\ninstruments to investigate the problem, that is, modeling the cause-effect\nrelationships when two non-zero error-free systems work together. Accordingly,\nthis paper explores the fusion of PAD into verification systems by proposing a\nnovel investigation instrument: a performance simulator based on the\nprobabilistic modeling of the relationships among the Receiver Operating\nCharacteristics (ROC) of the two individual systems when PAD and verification\nstages are implemented sequentially. As a matter of fact, this is the most\nstraightforward, flexible, and widespread approach. We carry out simulations on\nthe PAD algorithms' ROCs submitted to the most recent editions of LivDet\n(2017-2019), the state-of-the-art NIST Bozorth3, and the top-level Veryfinger\n12 matchers. Reported experiments explore significant scenarios to get the\nconditions under which fingerprint matching with embedded PAD can improve,\nrather than degrade, the overall personal verification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micheletto_M/0/1/0/all/0/1\">Marco Micheletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcialis_G/0/1/0/all/0/1\">Gian Luca Marcialis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orru_G/0/1/0/all/0/1\">Giulia Orr&#xf9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference Graphs for CNN Interpretation. (arXiv:2110.10568v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10568","description":"<p>Convolutional neural networks (CNNs) have achieved superior accuracy in many\nvisual related tasks. However, the inference process through intermediate\nlayers is opaque, making it difficult to interpret such networks or develop\ntrust in their operation. We propose to model the network hidden layers\nactivity using probabilistic models. The activity patterns in layers of\ninterest are modeled as Gaussian mixture models, and transition probabilities\nbetween clusters in consecutive modeled layers are estimated. Based on\nmaximum-likelihood considerations, nodes and paths relevant for network\nprediction are chosen, connected, and visualized as an inference graph. We show\nthat such graphs are useful for understanding the general inference process of\na class, as well as explaining decisions the network makes regarding specific\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konforti_Y/0/1/0/all/0/1\">Yael Konforti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpigler_A/0/1/0/all/0/1\">Alon Shpigler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Hillel_B/0/1/0/all/0/1\">Boaz Lernerand Aharon Bar-Hillel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Learning Framework for Diffeomorphic Image Registration based on Quasi-conformal Geometry. (arXiv:2110.10580v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10580","description":"<p>Image registration, the process of defining meaningful correspondences\nbetween images, is essential for various image analysis tasks, especially\nmedical imaging. Numerous learning-based methods, notably convolutional neural\nnetworks (CNNs), for deformable image registration proposed in recent years\nhave demonstrated the feasibility and superiority of deep learning techniques\nfor registration problems. Besides, compared to traditional algorithms'\noptimization scheme of the objective function for each image pair,\nlearning-based algorithms are several orders of magnitude faster. However,\nthese data-driven methods without proper constraint on the deformation field\nwill easily lead to topological foldings.\n</p>\n<p>To tackle this problem, We propose the quasi-conformal registration network\n(QCRegNet), an unsupervised learning framework, to obtain diffeomorphic 2D\nimage registrations with large deformations based on quasi-conformal (QC) map,\nan orientation-preserving homeomorphism between two manifolds.\n</p>\n<p>The basic idea is to design a CNN mapping image pairs to deformation fields.\nQCRegNet consists of the estimator network and the Beltrami solver network\n(BSNet). The estimator network takes image pair as input and outputs the\nBeltrami coefficient (BC). The BC, which captures conformal distortion of a QC\nmap and guarantees the bijectivity, will then be input to the BSNet, a\ntask-independent network which reconstructs the desired QC map.\n</p>\n<p>Furthermore, we reduce the number of network parameters and computational\ncomplexity by utilizing Fourier approximation to compress BC. Experiments have\nbeen carried out on different data such as underwater and medical images.\nRegistration results show that the registration accuracy is comparable to\nstate-of-the-art methods and diffeomorphism is to a great extent guaranteed\ncompared to other diffeomorphic registration algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos. (arXiv:2110.10596v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10596","description":"<p>We introduce the task of spatially localizing narrated interactions in\nvideos. Key to our approach is the ability to learn to spatially localize\ninteractions with self-supervision on a large corpus of videos with\naccompanying transcribed narrations. To achieve this goal, we propose a\nmultilayer cross-modal attention network that enables effective optimization of\na contrastive loss during training. We introduce a divided strategy that\nalternates between computing inter- and intra-modal attention across the visual\nand natural language modalities, which allows effective training via directly\ncontrasting the two modalities' representations. We demonstrate the\neffectiveness of our approach by self-training on the HowTo100M instructional\nvideo dataset and evaluating on a newly collected dataset of localized\ndescribed interactions in the YouCook2 dataset. We show that our approach\noutperforms alternative baselines, including shallow co-attention and full\ncross-modal attention. We also apply our approach to grounding phrases in\nimages with weak supervision on Flickr30K and show that stacking multiple\nattention layers is effective and, when combined with a word-to-region loss,\nachieves state of the art on recall-at-one and pointing hand accuracies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Reuben Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Instance Segmentation by Instance Flow Assembly. (arXiv:2110.10599v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10599","description":"<p>Instance segmentation is a challenging task aiming at classifying and\nsegmenting all object instances of specific classes. While two-stage box-based\nmethods achieve top performances in the image domain, they cannot easily extend\ntheir superiority into the video domain. This is because they usually deal with\nfeatures or images cropped from the detected bounding boxes without alignment,\nfailing to capture pixel-level temporal consistency. We embrace the observation\nthat bottom-up methods dealing with box-free features could offer accurate\nspacial correlations across frames, which can be fully utilized for object and\npixel level tracking. We first propose our bottom-up framework equipped with a\ntemporal context fusion module to better encode inter-frame correlations.\nIntra-frame cues for semantic segmentation and object localization are\nsimultaneously extracted and reconstructed by corresponding decoders after a\nshared backbone. For efficient and robust tracking among instances, we\nintroduce an instance-level correspondence across adjacent frames, which is\nrepresented by a center-to-center flow, termed as instance flow, to assemble\nmessy dense temporal correspondences. Experiments demonstrate that the proposed\nmethod outperforms the state-of-the-art online methods (taking image-level\ninput) on the challenging Youtube-VIS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinglu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Domain Adaptation for Semantic Segmentation. (arXiv:2110.10639v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10639","description":"<p>Deep learning approaches for semantic segmentation rely primarily on\nsupervised learning approaches and require substantial efforts in producing\npixel-level annotations. Further, such approaches may perform poorly when\napplied to unseen image domains. To cope with these limitations, both\nunsupervised domain adaptation (UDA) with full source supervision but without\ntarget supervision and semi-supervised learning (SSL) with partial supervision\nhave been proposed. While such methods are effective at aligning different\nfeature distributions, there is still a need to efficiently exploit unlabeled\ndata to address the performance gap with respect to fully-supervised methods.\nIn this paper we address semi-supervised domain adaptation (SSDA) for semantic\nsegmentation, where a large amount of labeled source data as well as a small\namount of labeled target data are available. We propose a novel and effective\ntwo-step semi-supervised dual-domain adaptation (SSDDA) approach to address\nboth cross- and intra-domain gaps in semantic segmentation. The proposed\nframework is comprised of two mixing modules. First, we conduct a cross-domain\nadaptation via an image-level mixing strategy, which learns to align the\ndistribution shift of features between the source data and target data. Second,\nintra-domain adaptation is achieved using a separate student-teacher network\nwhich is built to generate category-level data augmentation by mixing unlabeled\ntarget data in a way that respects predicted object boundaries. We demonstrate\nthat the proposed approach outperforms state-of-the-art methods on two common\nsynthetic-to-real semantic segmentation benchmarks. An extensive ablation study\nis provided to further validate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaiyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agam_G/0/1/0/all/0/1\">Gady Agam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data. (arXiv:2110.10640v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10640","description":"<p>Convolutional neural networks (CNNs) are the current state-of-the-art\nmeta-algorithm for volumetric segmentation of medical data, for example, to\nlocalize COVID-19 infected tissue on computer tomography scans or the detection\nof tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on\nvoxelised data is that the memory consumption grows cubically with the training\ndata resolution. Occupancy networks (O-Nets) are an alternative for which the\ndata is represented continuously in a function space and 3D shapes are learned\nas a continuous decision boundary. While O-Nets are significantly more memory\nefficient than 3D CNNs, they are limited to simple shapes, are relatively slow\nat inference, and have not yet been adapted for 3D semantic segmentation of\nmedical data. Here, we propose Occupancy Networks for Semantic Segmentation\n(OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We\nbuild upon the original O-Net with modifications for increased expressiveness\nleading to improved segmentation performance comparable to 3D CNNs, as well as\nmodifications for faster inference. We leverage local observations to represent\ncomplex shapes and prior encoder predictions to expedite inference. We showcase\nOSS-Net's performance on 3D brain tumour and liver segmentation against a\nfunction space baseline (O-Net), a performance baseline (3D residual U-Net),\nand an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation\nresults similar to the performance baseline and superior to the function space\nand efficiency baselines. In terms of memory efficiency, OSS-Net consumes\ncomparable amounts of memory as the function space baseline, somewhat more\nmemory than the efficiency baseline and significantly less than the performance\nbaseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic\nsegmentation that can scale to high resolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cetin_O/0/1/0/all/0/1\">&#xd6;zdemir Cetin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Different V1 Brain Model Variants to Improve Robustness to Image Corruptions in CNNs. (arXiv:2110.10645v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10645","description":"<p>While some convolutional neural networks (CNNs) have surpassed human visual\nabilities in object classification, they often struggle to recognize objects in\nimages corrupted with different types of common noise patterns, highlighting a\nmajor limitation of this family of models. Recently, it has been shown that\nsimulating a primary visual cortex (V1) at the front of CNNs leads to small\nimprovements in robustness to these image perturbations. In this study, we\nstart with the observation that different variants of the V1 model show gains\nfor specific corruption types. We then build a new model using an ensembling\ntechnique, which combines multiple individual models with different V1\nfront-end variants. The model ensemble leverages the strengths of each\nindividual model, leading to significant improvements in robustness across all\ncorruption categories and outperforming the base model by 38% on average.\nFinally, we show that using distillation, it is possible to partially compress\nthe knowledge in the ensemble model into a single model with a V1 front-end.\nWhile the ensembling and distillation techniques used here are hardly\nbiologically-plausible, the results presented here demonstrate that by\ncombining the specific strengths of different neuronal circuits in V1 it is\npossible to improve the robustness of CNNs for a wide range of perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baidya_A/0/1/0/all/0/1\">Avinash Baidya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dapello_J/0/1/0/all/0/1\">Joel Dapello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+DiCarlo_J/0/1/0/all/0/1\">James J. DiCarlo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marques_T/0/1/0/all/0/1\">Tiago Marques</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision and Spatial-Sequential Attention Based Loss for Multi-Person Pose Estimation. (arXiv:2110.10734v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10734","description":"<p>Bottom-up based multi-person pose estimation approaches use heatmaps with\nauxiliary predictions to estimate joint positions and belonging at one time.\nRecently, various combinations between auxiliary predictions and heatmaps have\nbeen proposed for higher performance, these predictions are supervised by the\ncorresponding L2 loss function directly. However, the lack of more explicit\nsupervision results in low features utilization and contradictions between\npredictions in one model. To solve these problems, this paper proposes (i) a\nnew loss organization method which uses self-supervised heatmaps to reduce\nprediction contradictions and spatial-sequential attention to enhance networks'\nfeatures extraction; (ii) a new combination of predictions composed by\nheatmaps, Part Affinity Fields (PAFs) and our block-inside offsets to fix\npixel-level joints positions and further demonstrates the effectiveness of\nproposed loss function. Experiments are conducted on the MS COCO keypoint\ndataset and adopting OpenPose as the baseline model. Our method outperforms the\nbaseline overall. On the COCO verification dataset, the mAP of OpenPose trained\nwith our proposals outperforms the OpenPose baseline by over 5.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dingli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Songlin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikenaga_T/0/1/0/all/0/1\">Takeshi Ikenaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Incremental Online Streaming Learning. (arXiv:2110.10741v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10741","description":"<p>A wide variety of methods have been developed to enable lifelong learning in\nconventional deep neural networks. However, to succeed, these methods require a\n`batch' of samples to be available and visited multiple times during training.\nWhile this works well in a static setting, these methods continue to suffer in\na more realistic situation where data arrives in \\emph{online streaming\nmanner}. We empirically demonstrate that the performance of current approaches\ndegrades if the input is obtained as a stream of data with the following\nrestrictions: $(i)$ each instance comes one at a time and can be seen only\nonce, and $(ii)$ the input data violates the i.i.d assumption, i.e., there can\nbe a class-based correlation. We propose a novel approach (CIOSL) for the\nclass-incremental learning in an \\emph{online streaming setting} to address\nthese challenges. The proposed approach leverages implicit and explicit dual\nweight regularization and experience replay. The implicit regularization is\nleveraged via the knowledge distillation, while the explicit regularization\nincorporates a novel approach for parameter regularization by learning the\njoint distribution of the buffer replay and the current sample. Also, we\npropose an efficient online memory replay and replacement buffer strategy that\nsignificantly boosts the model's performance. Extensive experiments and\nablation on challenging datasets show the efficacy of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Soumya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parag_T/0/1/0/all/0/1\">Toufiq Parag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Real-world Image Super-resolution via Hardware-based Adaptive Degradation Models. (arXiv:2110.10755v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10755","description":"<p>Most single image super-resolution (SR) methods are developed on synthetic\nlow-resolution (LR) and high-resolution (HR) image pairs, which are simulated\nby a predetermined degradation operation, e.g., bicubic downsampling. However,\nthese methods only learn the inverse process of the predetermined operation, so\nthey fail to super resolve the real-world LR images; the true formulation\ndeviates from the predetermined operation. To address this problem, we propose\na novel supervised method to simulate an unknown degradation process with the\ninclusion of the prior hardware knowledge of the imaging system. We design an\nadaptive blurring layer (ABL) in the supervised learning framework to estimate\nthe target LR images. The hyperparameters of the ABL can be adjusted for\ndifferent imaging hardware. The experiments on the real-world datasets validate\nthat our degradation model can estimate LR images more accurately than the\npredetermined degradation operation, as well as facilitate existing SR methods\nto perform reconstructions on real-world LR images more accurately than the\nconventional approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Czernik_J/0/1/0/all/0/1\">Johnathan Czernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_X/0/1/0/all/0/1\">Xian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closed-loop Feedback Registration for Consecutive Images of Moving Flexible Targets. (arXiv:2110.10772v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10772","description":"<p>Advancement of imaging techniques enables consecutive image sequences to be\nacquired for quality monitoring of manufacturing production lines. Registration\nfor these image sequences is essential for in-line pattern inspection and\nmetrology, e.g., in the printing process of flexible electronics. However,\nconventional image registration algorithms cannot produce accurate results when\nthe images contain many similar and deformable patterns in the manufacturing\nprocess. Such a failure originates from a fact that the conventional algorithms\nonly use the spatial and pixel intensity information for registration.\nConsidering the nature of temporal continuity and consecution of the product\nimages, in this paper, we propose a closed-loop feedback registration algorithm\nfor matching and stitching the deformable printed patterns on a moving flexible\nsubstrate. The algorithm leverages the temporal and spatial relationships of\nthe consecutive images and the continuity of the image sequence for fast,\naccurate, and robust point matching. Our experimental results show that our\nalgorithm can find more matching point pairs with a lower root mean squared\nerror (RMSE) compared to other state-of-the-art algorithms while offering\nsignificant improvements to running time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Agnostic 3D Reconstruction via Adversarial Style Transfer. (arXiv:2110.10784v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10784","description":"<p>Reconstructing the 3D geometry of an object from an image is a major\nchallenge in computer vision. Recently introduced differentiable renderers can\nbe leveraged to learn the 3D geometry of objects from 2D images, but those\napproaches require additional supervision to enable the renderer to produce an\noutput that can be compared to the input image. This can be scene information\nor constraints such as object silhouettes, uniform backgrounds, material,\ntexture, and lighting. In this paper, we propose an approach that enables a\ndifferentiable rendering-based learning of 3D objects from images with\nbackgrounds without the need for silhouette supervision. Instead of trying to\nrender an image close to the input, we propose an adversarial style-transfer\nand domain adaptation pipeline that allows to translate the input image domain\nto the rendered image domain. This allows us to directly compare between a\ntranslated image and the differentiable rendering of a 3D object reconstruction\nin order to train the 3D object reconstruction network. We show that the\napproach learns 3D geometry from images with backgrounds and provides a better\nperformance than constrained methods for single-view 3D object reconstruction\non this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldluecke_B/0/1/0/all/0/1\">Bastian Goldluecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DVIO: Depth aided visual inertial odometry for RGBD sensors. (arXiv:2110.10805v1 [cs.RO])","link":"http://arxiv.org/abs/2110.10805","description":"<p>In past few years we have observed an increase in the usage of RGBD sensors\nin mobile devices. These sensors provide a good estimate of the depth map for\nthe camera frame, which can be used in numerous augmented reality applications.\nThis paper presents a new visual inertial odometry (VIO) system, which uses\nmeasurements from a RGBD sensor and an inertial measurement unit (IMU) sensor\nfor estimating the motion state of the mobile device. The resulting system is\ncalled the depth-aided VIO (DVIO) system. In this system we add the depth\nmeasurement as part of the nonlinear optimization process. Specifically, we\npropose methods to use the depth measurement using one-dimensional (1D) feature\nparameterization as well as three-dimensional (3D) feature parameterization. In\naddition, we propose to utilize the depth measurement for estimating time\noffset between the unsynchronized IMU and the RGBD sensors. Last but not least,\nwe propose a novel block-based marginalization approach to speed up the\nmarginalization processes and maintain the real-time performance of the overall\nsystem. Experimental results validate that the proposed DVIO system outperforms\nthe other state-of-the-art VIO systems in terms of trajectory accuracy as well\nas processing time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Abhishek Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yangwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuangquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_D/0/1/0/all/0/1\">Dongwoon Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Based Person Search with Limited Data. (arXiv:2110.10807v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10807","description":"<p>Text-based person search (TBPS) aims at retrieving a target person from an\nimage gallery with a descriptive text query. Solving such a fine-grained\ncross-modal retrieval task is challenging, which is further hampered by the\nlack of large-scale datasets. In this paper, we present a framework with two\nnovel components to handle the problems brought by limited data. Firstly, to\nfully utilize the existing small-scale benchmarking datasets for more\ndiscriminative feature learning, we introduce a cross-modal momentum\ncontrastive learning framework to enrich the training data for a given\nmini-batch. Secondly, we propose to transfer knowledge learned from existing\ncoarse-grained large-scale datasets containing image-text pairs from\ndrastically different problem domains to compensate for the lack of TBPS\ntraining data. A transfer learning method is designed so that useful\ninformation can be transferred despite the large domain gap. Armed with these\ncomponents, our method achieves new state of the art on the CUHK-PEDES dataset\nwith significant improvements over the prior art in terms of Rank-1 and mAP.\nOur code is available at https://github.com/BrandonHanx/TextReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HALP: Hardware-Aware Latency Pruning. (arXiv:2110.10811v1 [cs.CV])","link":"http://arxiv.org/abs/2110.10811","description":"<p>Structural pruning can simplify network architecture and improve inference\nspeed. We propose Hardware-Aware Latency Pruning (HALP) that formulates\nstructural pruning as a global resource allocation optimization problem, aiming\nat maximizing the accuracy while constraining latency under a predefined\nbudget. For filter importance ranking, HALP leverages latency lookup table to\ntrack latency reduction potential and global saliency score to gauge accuracy\ndrop. Both metrics can be evaluated very efficiently during pruning, allowing\nus to reformulate global structural pruning under a reward maximization problem\ngiven target constraint. This makes the problem solvable via our augmented\nknapsack solver, enabling HALP to surpass prior work in pruning efficacy and\naccuracy-efficiency trade-off. We examine HALP on both classification and\ndetection tasks, over varying networks, on ImageNet and VOC datasets. In\nparticular, for ResNet-50/-101 pruning on ImageNet, HALP improves network\nthroughput by $1.60\\times$/$1.90\\times$ with $+0.3\\%$/$-0.2\\%$ top-1 accuracy\nchanges, respectively. For SSD pruning on VOC, HALP improves throughput by\n$1.94\\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior\nart, sometimes by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Maying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1\">Lei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianna Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CXR-Net: An Encoder-Decoder-Encoder Multitask Deep Neural Network for Explainable and Accurate Diagnosis of COVID-19 pneumonia with Chest X-ray Images. (arXiv:2110.10813v1 [eess.IV])","link":"http://arxiv.org/abs/2110.10813","description":"<p>Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal\npatient treatment. Chest X-Ray (CXR) is the first line imaging test for\nCOVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible.\nInspired by the success of deep learning (DL) in computer vision, many\nDL-models have been proposed to detect COVID-19 pneumonia using CXR images.\nUnfortunately, these deep classifiers lack the transparency in interpreting\nfindings, which may limit their applications in clinical practice. The existing\ncommonly used visual explanation methods are either too noisy or imprecise,\nwith low resolution, and hence are unsuitable for diagnostic purposes. In this\nwork, we propose a novel explainable deep learning framework (CXRNet) for\naccurate COVID-19 pneumonia detection with an enhanced pixel-level visual\nexplanation from CXR images. The proposed framework is based on a new\nEncoder-Decoder-Encoder multitask architecture, allowing for both disease\nclassification and visual explanation. The method has been evaluated on real\nworld CXR datasets from both public and private data sources, including:\nhealthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases The\nexperimental results demonstrate that the proposed method can achieve a\nsatisfactory level of accuracy and provide fine-resolution classification\nactivation maps for visual explanation in lung disease detection. The Average\nAccuracy, the Precision, Recall and F1-score of COVID-19 pneumonia reached\n0.879, 0.985, 0.992 and 0.989, respectively. We have also found that using lung\nsegmented (CXR) images can help improve the performance of the model. The\nproposed method can provide more detailed high resolution visual explanation\nfor the classification decision, compared to current state-of-the-art visual\nexplanation methods and has a great potential to be used in clinical practice\nfor COVID-19 pneumonia diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sobeih_T/0/1/0/all/0/1\">Tam Sobeih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Lianghao Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dempsey_N/0/1/0/all/0/1\">Nina Dempsey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lechareas_S/0/1/0/all/0/1\">Symeon Lechareas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tridente_A/0/1/0/all/0/1\">Ascanio Tridente</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+White_S/0/1/0/all/0/1\">Stephen White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10832","description":"<p>In Domain Generalization (DG) settings, models trained on a given set of\ntraining domains have notoriously chaotic performance on distribution shifted\ntest domains, and stochasticity in optimization (e.g. seed) plays a big role.\nThis makes deep learning models unreliable in real world settings. We first\nshow that a simple protocol for averaging model parameters along the\noptimization path, starting early during training, both significantly boosts\ndomain generalization and diminishes the impact of stochasticity by improving\nthe rank correlation between the in-domain validation accuracy and out-domain\ntest accuracy, which is crucial for reliable model selection. Next, we show\nthat an ensemble of independently trained models also has a chaotic behavior in\nthe DG setting. Taking advantage of our observation, we show that instead of\nensembling unaveraged models, ensembling moving average models (EoA) from\ndifferent runs does increase stability and further boosts performance. On the\nDomainBed benchmark, when using a ResNet-50 pre-trained on ImageNet, this\nensemble of averages achieves $88.6\\%$ on PACS, $79.1\\%$ on VLCS, $72.5\\%$ on\nOfficeHome, $52.3\\%$ on TerraIncognita, and $47.4\\%$ on DomainNet, an average\nof $68.0\\%$, beating ERM (w/o model averaging) by $\\sim 4\\%$. We also evaluate\na model that is pre-trained on a larger dataset, where we show EoA achieves an\naverage accuracy of $72.7\\%$, beating its corresponding ERM baseline by $5\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1\">Devansh Arpit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution rainfall-runoff modeling using graph neural network. (arXiv:2110.10833v1 [cs.LG])","link":"http://arxiv.org/abs/2110.10833","description":"<p>Time-series modeling has shown great promise in recent studies using the\nlatest deep learning algorithms such as LSTM (Long Short-Term Memory). These\nstudies primarily focused on watershed-scale rainfall-runoff modeling or\nstreamflow forecasting, but the majority of them only considered a single\nwatershed as a unit. Although this simplification is very effective, it does\nnot take into account spatial information, which could result in significant\nerrors in large watersheds. Several studies investigated the use of GNN (Graph\nNeural Networks) for data integration by decomposing a large watershed into\nmultiple sub-watersheds, but each sub-watershed is still treated as a whole,\nand the geoinformation contained within the watershed is not fully utilized. In\nthis paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel\ndeep learning model that makes full use of spatial information from\nhigh-resolution precipitation data, including flow direction and geographic\ninformation. When compared to baseline models, GNRRM has less over-fitting and\nsignificantly improves model performance. Our findings support the importance\nof hydrological data in deep learning-based rainfall-runoff modeling, and we\nencourage researchers to include more domain knowledge in their models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhongrun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization. (arXiv:2110.10834v1 [cs.CL])","link":"http://arxiv.org/abs/2110.10834","description":"<p>While much research has been done in text-to-image synthesis, little work has\nbeen done to explore the usage of linguistic structure of the input text. Such\ninformation is even more important for story visualization since its inputs\nhave an explicit narrative structure that needs to be translated into an image\nsequence (or visual story). Prior work in this domain has shown that there is\nample room for improvement in the generated image sequence in terms of visual\nquality, consistency and relevance. In this paper, we first explore the use of\nconstituency parse trees using a Transformer-based recurrent architecture for\nencoding structured input. Second, we augment the structured input with\ncommonsense information and study the impact of this external knowledge on the\ngeneration of visual story. Third, we also incorporate visual structure via\nbounding boxes and dense captioning to provide feedback about the\ncharacters/objects in generated images within a dual learning setup. We show\nthat off-the-shelf dense-captioning models trained on Visual Genome can improve\nthe spatial structure of images from a different target domain without needing\nfine-tuning. We train the model end-to-end using intra-story contrastive loss\n(between words and image sub-regions) and show significant improvements in\nseveral metrics (and human evaluation) for multiple datasets. Finally, we\nprovide an analysis of the linguistic and visuo-spatial information. Code and\ndata: https://github.com/adymaharana/VLCStoryGan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1\">Adyasha Maharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving the L1 regularized least square problem via a box-constrained smooth minimization. (arXiv:1704.03443v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/1704.03443","description":"<p>In this paper, an equivalent smooth minimization for the L1 regularized least\nsquare problem is proposed. The proposed problem is a convex box-constrained\nsmooth minimization which allows applying fast optimization methods to find its\nsolution. Further, it is investigated that the property \"the dual of dual is\nprimal\" holds for the L1 regularized least square problem. A solver for the\nsmooth problem is proposed, and its affinity to the proximal gradient is shown.\nFinally, the experiments on L1 and total variation regularized problems are\nperformed, and the corresponding results are reported.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Mohammadi_M/0/1/0/all/0/1\">Majid Mohammadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hofman_W/0/1/0/all/0/1\">Wout Hofman</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tan_Y/0/1/0/all/0/1\">Yaohua Tan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mousavi_S/0/1/0/all/0/1\">S. Hamid Mousavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Neural Network for Photo-realistic Image Super-Resolution. (arXiv:1903.02240v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.02240","description":"<p>Recent progress in deep learning-based models has improved photo-realistic\n(or perceptual) single-image super-resolution significantly. However, despite\ntheir powerful performance, many methods are difficult to apply to real-world\napplications because of the heavy computational requirements. To facilitate the\nuse of a deep model under such demands, we focus on keeping the network\nefficient while maintaining its performance. In detail, we design an\narchitecture that implements a cascading mechanism on a residual network to\nboost the performance with limited resources via multi-level feature fusion. In\naddition, our proposed model adopts group convolution and recursive schemes in\norder to achieve extreme efficiency. We further improve the perceptual quality\nof the output by employing the adversarial learning paradigm and a multi-scale\ndiscriminator approach. The performance of our method is investigated through\nextensive internal experiments and benchmarks using various datasets. Our\nresults show that our models outperform the recent methods with similar\ncomplexity, for both traditional pixel-based and perception-based tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1\">Namhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kyung-Ah Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09147","description":"<p>Convolutional neural network (CNN) has surpassed traditional methods for\nmedical image classification. However, CNN is vulnerable to adversarial attacks\nwhich may lead to disastrous consequences in medical applications. Although\nadversarial noises are usually generated by attack algorithms,\nwhite-noise-induced adversarial samples can exist, and therefore the threats\nare real. In this study, we propose a novel training method, named IMA, to\nimprove the robust-ness of CNN against adversarial noises. During training, the\nIMA method increases the margins of training samples in the input space, i.e.,\nmoving CNN decision boundaries far away from the training samples to improve\nrobustness. The IMA method is evaluated on publicly available datasets under\nstrong 100-PGD white-box adversarial attacks, and the results show that the\nproposed method significantly improved CNN classification and segmentation\naccuracy on noisy data while keeping a high accuracy on clean data. We hope our\napproach may facilitate the development of robust applications in medical\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Evaluating Weakly Supervised Action Segmentation Methods. (arXiv:2005.09743v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09743","description":"<p>Action segmentation is the task of temporally segmenting every frame of an\nuntrimmed video. Weakly supervised approaches to action segmentation,\nespecially from transcripts have been of considerable interest to the computer\nvision community. In this work, we focus on two aspects of the use and\nevaluation of weakly supervised action segmentation approaches that are often\noverlooked: the performance variance over multiple training runs and the impact\nof selecting feature extractors for this task. To tackle the first problem, we\ntrain each method on the Breakfast dataset 5 times and provide average and\nstandard deviation of the results. Our experiments show that the standard\ndeviation over these repetitions is between 1 and 2.5% and significantly\naffects the comparison between different approaches. Furthermore, our\ninvestigation on feature extraction shows that, for the studied\nweakly-supervised action segmentation methods, higher-level I3D features\nperform worse than classical IDT features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1\">Yaser Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Alexander Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1\">Luca Minciullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.12245","description":"<p>We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state of the art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have\nbeen made publicly available at github.com/plai-group/simple-cnaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Nonnegative Tensor Factorization and Completion with Noisy Observations. (arXiv:2007.10626v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2007.10626","description":"<p>In this paper, we study the sparse nonnegative tensor factorization and\ncompletion problem from partial and noisy observations for third-order tensors.\nBecause of sparsity and nonnegativity, the underlying tensor is decomposed into\nthe tensor-tensor product of one sparse nonnegative tensor and one nonnegative\ntensor. We propose to minimize the sum of the maximum likelihood estimation for\nthe observations with nonnegativity constraints and the tensor $\\ell_0$ norm\nfor the sparse factor. We show that the error bounds of the estimator of the\nproposed model can be established under general noise observations. The\ndetailed error bounds under specific noise distributions including additive\nGaussian noise, additive Laplace noise, and Poisson observations can be\nderived. Moreover, the minimax lower bounds are shown to be matched with the\nestablished upper bounds up to a logarithmic factor of the sizes of the\nunderlying tensor. These theoretical results for tensors are better than those\nobtained for matrices, and this illustrates the advantage of the use of\nnonnegative sparse tensor models for completion and denoising. Numerical\nexperiments are provided to validate the superiority of the proposed\ntensor-based method compared with the matrix-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongjun Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WAN: Watermarking Attack Network. (arXiv:2008.06255v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2008.06255","description":"<p>Multi-bit watermarking (MW) has been developed to improve robustness against\nsignal processing operations and geometric distortions. To this end, benchmark\ntools that test robustness by applying simulated attacks on watermarked images\nare available. However, limitations in these general attacks exist since they\ncannot exploit specific characteristics of the targeted MW. In addition, these\nattacks are usually devised without consideration of visual quality, which\nrarely occurs in the real world. To address these limitations, we propose a\nwatermarking attack network (WAN), a fully trainable watermarking benchmark\ntool that utilizes the weak points of the target MW and induces an inversion of\nthe watermark bit, thereby considerably reducing the watermark extractability.\nTo hinder the extraction of hidden information while ensuring high visual\nquality, we utilize a residual dense blocks-based architecture specialized in\nlocal and global feature learning. A novel watermarking attack loss is\nintroduced to break the MW systems. We empirically demonstrate that the WAN can\nsuccessfully fool various block-based MW systems. Moreover, we show that\nexisting MW methods can be improved with the help of the WAN as an add-on\nmodule.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seung-Hun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">In-Jae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_S/0/1/0/all/0/1\">Seung-Min Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daesik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualSem: A High-quality Knowledge Graph for Vision and Language. (arXiv:2008.09150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09150","description":"<p>An exciting frontier in natural language understanding (NLU) and generation\n(NLG) calls for (vision-and-) language models that can efficiently access\nexternal structured knowledge repositories. However, many existing knowledge\nbases only cover limited domains, or suffer from noisy data, and most of all\nare typically hard to integrate into neural language pipelines. To fill this\ngap, we release VisualSem: a high-quality knowledge graph (KG) which includes\nnodes with multilingual glosses, multiple illustrative images, and visually\nrelevant relations. We also release a neural multi-modal retrieval model that\ncan use images or sentences as inputs and retrieves entities in the KG. This\nmulti-modal retrieval model can be integrated into any (neural network) model\npipeline. We encourage the research community to use VisualSem for data\naugmentation and/or as a source of grounding, among other possible uses.\nVisualSem as well as the multi-modal retrieval models are publicly available\nand can be downloaded in this URL: https://github.com/iacercalixto/visualsem\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_H/0/1/0/all/0/1\">Houda Alberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Teresa Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_Y/0/1/0/all/0/1\">Yash Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1\">Clara Vania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval. (arXiv:2009.01485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.01485","description":"<p>The ability to efficiently search for images is essential for improving the\nuser experiences across various products. Incorporating user feedback, via\nmulti-modal inputs, to navigate visual search can help tailor retrieved results\nto specific user queries. We focus on the task of text-conditioned image\nretrieval that utilizes support text feedback alongside a reference image to\nretrieve images that concurrently satisfy constraints imposed by both inputs.\nThe task is challenging since it requires learning composite image-text\nfeatures by incorporating multiple cross-granular semantic edits from text\nfeedback and then applying the same to visual features. To address this, we\npropose a novel framework SAC which resolves the above in two major steps:\n\"where to see\" (Semantic Feature Attention) and \"how to change\" (Semantic\nFeature Modification). We systematically show how our architecture streamlines\nthe generation of text-aware image features by removing the need for various\nmodules required by other state-of-art techniques. We present extensive\nquantitative, qualitative analysis, and ablation studies, to show that our\narchitecture SAC outperforms existing techniques by achieving state-of-the-art\nperformance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words,\nwhile supporting natural language feedback of varying lengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jandial_S/0/1/0/all/0/1\">Surgan Jandial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badjatiya_P/0/1/0/all/0/1\">Pinkesh Badjatiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_P/0/1/0/all/0/1\">Pranit Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mausoom Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PERF-Net: Pose Empowered RGB-Flow Net. (arXiv:2009.13087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13087","description":"<p>In recent years, many works in the video action recognition literature have\nshown that two stream models (combining spatial and temporal input streams) are\nnecessary for achieving state of the art performance. In this paper we show the\nbenefits of including yet another stream based on human pose estimated from\neach frame -- specifically by rendering pose on input RGB frames. At first\nblush, this additional stream may seem redundant given that human pose is fully\ndetermined by RGB pixel values -- however we show (perhaps surprisingly) that\nthis simple and flexible addition can provide complementary gains. Using this\ninsight, we then propose a new model, which we dub PERF-Net (short for Pose\nEmpowered RGB-Flow Net), which combines this new pose stream with the standard\nRGB and flow based input streams via distillation techniques and show that our\nmodel outperforms the state-of-the-art by a large margin in a number of human\naction recognition datasets while not requiring flow or pose to be explicitly\ncomputed at inference time. The proposed pose stream is also part of the winner\nsolution of the ActivityNet Kinetics Challenge 2020.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jonathan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESAD: End-to-end Deep Semi-supervised Anomaly Detection. (arXiv:2012.04905v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.04905","description":"<p>This paper explores semi-supervised anomaly detection, a more practical\nsetting for anomaly detection where a small additional set of labeled samples\nare provided. We propose a new KL-divergence based objective function for\nsemi-supervised anomaly detection, and show that two factors: the mutual\ninformation between the data and latent representations, and the entropy of\nlatent representations, constitute an integral objective function for anomaly\ndetection. To resolve the contradiction in simultaneously optimizing the two\nfactors, we propose a novel encoder-decoder-encoder structure, with the first\nencoder focusing on optimizing the mutual information and the second encoder\nfocusing on optimizing the entropy. The two encoders are enforced to share\nsimilar encoding with a consistent constraint on their latent representations.\nExtensive experiments have revealed that the proposed method significantly\noutperforms several state-of-the-arts on multiple benchmark datasets, including\nmedical diagnosis and several classic anomaly detection benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peisen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Inter- and Intraframe Representations for Non-Lambertian Photometric Stereo. (arXiv:2012.13720v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13720","description":"<p>Photometric stereo provides an important method for high-fidelity 3D\nreconstruction based on multiple intensity images captured under different\nillumination directions. In this paper, we present a complete framework,\nincluding a multilight source illumination and acquisition hardware system and\na two-stage convolutional neural network (CNN) architecture, to construct\ninter- and intraframe representations for accurate normal estimation of\nnon-Lambertian objects. We experimentally investigate numerous network design\nalternatives for identifying the optimal scheme to deploy inter- and intraframe\nfeature extraction modules for the photometric stereo problem. Moreover, we\npropose utilizing the easily obtained object mask to eliminate adverse\ninterference from invalid background regions in intraframe spatial\nconvolutions, thus effectively improving the accuracy of normal estimation for\nsurfaces made of dark materials or with cast shadows. Experimental results\ndemonstrate that the proposed masked two-stage photometric stereo CNN model\n(MT-PS-CNN) performs favourably against state-of-the-art photometric stereo\ntechniques in terms of both accuracy and efficiency. In addition, the proposed\nmethod is capable of predicting accurate and rich surface normal details for\nnon-Lambertian objects of complex geometry and performs stably given inputs\ncaptured in both sparse and dense lighting distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanlong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Binjie Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zewei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiangxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanpeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Guided Articulated Hand Pose Tracking in Surgical Videos. (arXiv:2101.04281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04281","description":"<p>Articulated hand pose tracking is an under-explored problem that carries the\npotential for use in an extensive number of applications, especially in the\nmedical domain. With a robust and accurate tracking system on in-vivo surgical\nvideos, the motion dynamics and movement patterns of the hands can be captured\nand analyzed for many rich tasks. In this work, we propose a novel hand pose\nestimation model, Res152-CondPose, which improves detection and tracking\naccuracy by incorporating a hand pose prior into its pose prediction. We show\nimprovements over state-of-the-art methods which provide frame-wise independent\npredictions, by following a temporally guided approach that effectively\nleverages past predictions. Additionally, we collect the first dataset,\nSurgical Hands, that provides multi-instance articulated hand pose annotations\nfor in-vivo videos. Our dataset contains 76 video clips from 28 publicly\navailable surgical videos and over 8.1k annotated hand pose instances. We\nprovide bounding boxes, articulated hand pose annotations, and tracking IDs to\nenable multi-instance area-based and articulated tracking. When evaluated on\nSurgical Hands, we show our method outperforms the state-of-the-art method\nusing mean Average Precision (mAP), to measure pose estimation accuracy, and\nMultiple Object Tracking Accuracy (MOTA), to assess pose tracking performance.\nBoth the code and dataset are available at\nhttps://github.com/MichiganCOG/Surgical_ Hands_RELEASE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_N/0/1/0/all/0/1\">Nathan Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yule_S/0/1/0/all/0/1\">Steven J. Yule</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_R/0/1/0/all/0/1\">Roger D. Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manojlovich_M/0/1/0/all/0/1\">Milisa Manojlovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_F/0/1/0/all/0/1\">Francis D. Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likosky_D/0/1/0/all/0/1\">Donald S. Likosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs. (arXiv:2101.06894v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2101.06894","description":"<p>Humans are able to form a complex mental model of the environment they move\nin. This mental model captures geometric and semantic aspects of the scene,\ndescribes the environment at multiple levels of abstractions (e.g., objects,\nrooms, buildings), includes static and dynamic entities and their relations\n(e.g., a person is in a room at a given time). In contrast, current robots'\ninternal representations still provide a partial and fragmented understanding\nof the environment, either in the form of a sparse or dense set of geometric\nprimitives (e.g., points, lines, planes, voxels) or as a collection of objects.\nThis paper attempts to reduce the gap between robot and human perception by\nintroducing a novel representation, a 3D Dynamic Scene Graph(DSG), that\nseamlessly captures metric and semantic aspects of a dynamic environment. A DSG\nis a layered graph where nodes represent spatial concepts at different levels\nof abstraction, and edges represent spatio-temporal relations among nodes. Our\nsecond contribution is Kimera, the first fully automatic method to build a DSG\nfrom visual-inertial data. Kimera includes state-of-the-art techniques for\nvisual-inertial SLAM, metric-semantic 3D reconstruction, object localization,\nhuman pose and shape estimation, and scene parsing. Our third contribution is a\ncomprehensive evaluation of Kimera in real-life datasets and photo-realistic\nsimulations, including a newly released dataset, uHumans2, which simulates a\ncollection of crowded indoor and outdoor scenes. Our evaluation shows that\nKimera achieves state-of-the-art performance in visual-inertial SLAM, estimates\nan accurate 3D metric-semantic mesh model in real-time, and builds a DSG of a\ncomplex indoor environment with tens of objects and humans in minutes. Our\nfinal contribution shows how to use a DSG for real-time hierarchical semantic\npath-planning. The core modules in Kimera are open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1\">Antoni Rosinol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Violette_A/0/1/0/all/0/1\">Andrew Violette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abate_M/0/1/0/all/0/1\">Marcus Abate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_N/0/1/0/all/0/1\">Nathan Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingnan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arjun Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Too-Good-to-be-True Prior to Reduce Shortcut Reliance. (arXiv:2102.06406v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06406","description":"<p>Despite their impressive performance in object recognition and other tasks\nunder standard testing conditions, deep networks often fail to generalize to\nout-of-distribution (o.o.d.) samples. One cause for this shortcoming is that\nmodern architectures tend to rely on \"shortcuts\" - superficial features that\ncorrelate with categories without capturing deeper invariants that hold across\ncontexts. Real-world concepts often possess a complex structure that can vary\nsuperficially across contexts, which can make the most intuitive and promising\nsolutions in one context not generalize to others. One potential way to improve\no.o.d. generalization is to assume simple solutions are unlikely to be valid\nacross contexts and avoid them, which we refer to as the too-good-to-be-true\nprior. A low-capacity network (LCN) with a shallow architecture should only be\nable to learn surface relationships, including shortcuts. We find that LCNs can\nserve as shortcut detectors. Furthermore, an LCN's predictions can be used in a\ntwo-stage approach to encourage a high-capacity network (HCN) to rely on deeper\ninvariant features that should generalize broadly. In particular, items that\nthe LCN can master are downweighted when training the HCN. Using a modified\nversion of the CIFAR-10 dataset in which we introduced shortcuts, we found that\nthe two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated\no.o.d. generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dagaev_N/0/1/0/all/0/1\">Nikolay Dagaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1\">Brett D. Roads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barry_D/0/1/0/all/0/1\">Daniel N. Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1\">Kaustubh R. Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C. Love</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EllipsoidNet: Ellipsoid Representation for Point Cloud Classification and Segmentation. (arXiv:2103.02517v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02517","description":"<p>Point cloud patterns are hard to learn because of the implicit local geometry\nfeatures among the orderless points. In recent years, point cloud\nrepresentation in 2D space has attracted increasing research interest since it\nexposes the local geometry features in a 2D space. By projecting those points\nto a 2D feature map, the relationship between points is inherited in the\ncontext between pixels, which are further extracted by a 2D convolutional\nneural network. However, existing 2D representing methods are either accuracy\nlimited or time-consuming. In this paper, we propose a novel 2D representation\nmethod that projects a point cloud onto an ellipsoid surface space, where local\npatterns are well exposed in ellipsoid-level and point-level. Additionally, a\nnovel convolutional neural network named EllipsoidNet is proposed to utilize\nthose features for point cloud classification and segmentation applications.\nThe proposed methods are evaluated in ModelNet40 and ShapeNet benchmarks, where\nthe advantages are clearly shown over existing 2D representation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yecheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Vehicle Trajectories with Model-based Planning. (arXiv:2103.04027v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04027","description":"<p>Predicting the future trajectories of on-road vehicles is critical for\nautonomous driving. In this paper, we introduce a novel prediction framework\ncalled PRIME, which stands for Prediction with Model-based Planning. Unlike\nrecent prediction works that utilize neural networks to model scene context and\nproduce unconstrained trajectories, PRIME is designed to generate accurate and\nfeasibility-guaranteed future trajectory predictions. PRIME guarantees the\ntrajectory feasibility by exploiting a model-based generator to produce future\ntrajectories under explicit constraints and enables accurate multimodal\nprediction by utilizing a learning-based evaluator to select future\ntrajectories. We conduct experiments on the large-scale Argoverse Motion\nForecasting Benchmark, where PRIME outperforms the state-of-the-art methods in\nprediction accuracy, feasibility, and robustness under imperfect tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haoran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_D/0/1/0/all/0/1\">Di Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenchao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels. (arXiv:2103.13646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13646","description":"<p>The success of learning with noisy labels (LNL) methods relies heavily on the\nsuccess of a warm-up stage where standard supervised training is performed\nusing the full (noisy) training set. In this paper, we identify a \"warm-up\nobstacle\": the inability of standard warm-up stages to train high quality\nfeature extractors and avert memorization of noisy labels. We propose \"Contrast\nto Divide\" (C2D), a simple framework that solves this problem by pre-training\nthe feature extractor in a self-supervised fashion. Using self-supervised\npre-training boosts the performance of existing LNL approaches by drastically\nreducing the warm-up stage's susceptibility to noise level, shortening its\nduration, and improving extracted feature quality. C2D works out of the box\nwith existing methods and demonstrates markedly improved performance,\nespecially in the high noise regime, where we get a boost of more than 27% for\nCIFAR-100 with 90% noise over the previous state of the art. In real-life noise\nsettings, C2D trained on mini-WebVision outperforms previous works both in\nWebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an\nin-depth analysis of the framework, including investigating the performance of\ndifferent pre-training approaches and estimating the effective upper bound of\nthe LNL performance with semi-supervised learning. Code for reproducing our\nexperiments is available at https://github.com/ContrastToDivide/C2D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1\">Evgenii Zheltonozhskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_A/0/1/0/all/0/1\">Avi Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex M. Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds. (arXiv:2104.03437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03437","description":"<p>In this work, we tackle the problem of category-level online pose tracking of\nobjects from point cloud sequences. For the first time, we propose a unified\nframework that can handle 9DoF pose tracking for novel rigid object instances\nas well as per-part pose tracking for articulated objects from known\ncategories. Here the 9DoF pose, comprising 6D pose and 3D size, is equivalent\nto a 3D amodal bounding box representation with free 6D pose. Given the depth\npoint cloud at the current frame and the estimated pose from the last frame,\nour novel end-to-end pipeline learns to accurately update the pose. Our\npipeline is composed of three modules: 1) a pose canonicalization module that\nnormalizes the pose of the input depth point cloud; 2) RotationNet, a module\nthat directly regresses small interframe delta rotations; and 3) CoordinateNet,\na module that predicts the normalized coordinates and segmentation, enabling\nanalytical computation of the 3D size and translation. Leveraging the small\npose regime in the pose-canonicalized point clouds, our method integrates the\nbest of both worlds by combining dense coordinate prediction and direct\nrotation regression, thus yielding an end-to-end differentiable pipeline\noptimized for 9DoF pose accuracy (without using non-differentiable RANSAC). Our\nextensive experiments demonstrate that our method achieves new state-of-the-art\nperformance on category-level rigid object pose (NOCS-REAL275) and articulated\nobject pose benchmarks (SAPIEN, BMVC) at the fastest FPS ~12.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yijia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07961","description":"<p>Mitochondria instance segmentation from electron microscopy (EM) images has\nseen notable progress since the introduction of deep learning methods. In this\npaper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,\nfor 3D mitochondria instance segmentation from Rat and Human samples.\nSpecifically, we design a simple yet effective anisotropic convolution block\nand deploy a multi-scale training strategy, which together boost the\nsegmentation performance. Moreover, we enhance the generalizability of the\ntrained models on the test set by adding a denoising operation as\npre-processing. In the Large-scale 3D Mitochondria Instance Segmentation\nChallenge at ISBI 2021, our method ranks the 1st place. Code is available at\nhttps://github.com/Limingxing00/MitoEM2021-Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08760","description":"<p>Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its huge potential in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we reveal two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\nclasses. Over-clustering implies that the model cannot efficiently learn\nfeatures from excessive negative sample pairs, forcing the model to\nover-cluster samples of the same actual classes into different clusters. To\nsimultaneously overcome these two problems, we propose a novel self-supervised\nlearning framework using a truncated triplet loss. Precisely, we employ a\ntriplet loss tending to maximize the relative distance between the positive\npair and negative pairs to address the under-clustering problem; and we\nconstruct the negative pair by selecting a negative sample deputy from all\nnegative samples to avoid the over-clustering problem, guaranteed by the\nBernoulli Distribution model. We extensively evaluate our framework in several\nlarge-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results\ndemonstrate our model's superiority (e.g., the learning efficiency) over the\nlatest state-of-the-art methods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarioNette: Self-Supervised Sprite Learning. (arXiv:2104.14553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14553","description":"<p>Artists and video game designers often construct 2D animations using\nlibraries of sprites -- textured patches of objects and characters. We propose\na deep learning approach that decomposes sprite-based video animations into a\ndisentangled representation of recurring graphic elements in a self-supervised\nmanner. By jointly learning a dictionary of possibly transparent patches and\ntraining a network that places them onto a canvas, we deconstruct sprite-based\ncontent into a sparse, consistent, and explicit representation that can be\neasily used in downstream tasks, like editing or analysis. Our framework offers\na promising approach for discovering recurring visual patterns in image\ncollections without supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_D/0/1/0/all/0/1\">Dmitriy Smirnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Michael Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Semantic Photo Geolocation. (arXiv:2104.14995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14995","description":"<p>Planet-scale photo geolocalization is the complex task of estimating the\nlocation depicted in an image solely based on its visual content. Due to the\nsuccess of convolutional neural networks (CNNs), current approaches achieve\nsuper-human performance. However, previous work has exclusively focused on\noptimizing geolocalization accuracy. Due to the black-box property of deep\nlearning systems, their predictions are difficult to validate for humans.\nState-of-the-art methods treat the task as a classification problem, where the\nchoice of the classes, that is the partitioning of the world map, is crucial\nfor the performance. In this paper, we present two contributions to improve the\ninterpretability of a geolocalization model: (1) We propose a novel semantic\npartitioning method which intuitively leads to an improved understanding of the\npredictions, while achieving state-of-the-art results for geolocational\naccuracy on benchmark test sets; (2) We introduce a metric to assess the\nimportance of semantic visual concepts for a certain prediction to provide\nadditional interpretable information, which allows for a large-scale analysis\nof already trained models. Source code and dataset are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theiner_J/0/1/0/all/0/1\">Jonas Theiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_Budack_E/0/1/0/all/0/1\">Eric Mueller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Perceptual Distortion Reduction Framework: Towards Generating Adversarial Examples with High Perceptual Quality and Attack Success Rate. (arXiv:2105.00278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00278","description":"<p>Most of the adversarial attack methods suffer from large perceptual\ndistortions such as visible artifacts, when the attack strength is relatively\nhigh. These perceptual distortions contain a certain portion which contributes\nless to the attack success rate. This portion of distortions, which is induced\nby unnecessary modifications and lack of proper perceptual distortion\nconstraint, is the target of the proposed framework. In this paper, we propose\na perceptual distortion reduction framework to tackle this problem from two\nperspectives. Firstly, we propose a perceptual distortion constraint and add it\ninto the objective function to jointly optimize the perceptual distortions and\nattack success rate. Secondly, we propose an adaptive penalty factor $\\lambda$\nto balance the discrepancies between different samples. Since SGD and\nMomentum-SGD cannot optimize our complex non-convex problem, we exploit Adam in\noptimization. Extensive experiments have verified the superiority of our\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruikui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfang Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robotic Approach towards Quantifying Epipelagic Bound Plastic Using Deep Visual Models. (arXiv:2105.01882v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01882","description":"<p>The quantification of positively buoyant marine plastic debris is critical to\nunderstanding how plastic litter accumulates across the world's oceans and is\nalso crucial to identifying hotspots for targeted cleanup efforts. Currently,\nthe most common method to quantify marine plastic is using manta trawls for\nmanual sampling. However, this method is cost-intensive and requires human\nlabor. This study removes the need for manual sampling by using an autonomous\nmethod using neural networks and computer vision models, which trained on\nimages captured from various layers of the ocean column to perform real-time\nplastic quantification. The best performing model has a Mean Average Precision\nof 85% and an F1-Score of 0.89 while maintaining near real-time processing\nspeeds ~2 ms/img.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tata_G/0/1/0/all/0/1\">Gautam Tata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royer_S/0/1/0/all/0/1\">Sarah-Jeanne Royer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poirion_O/0/1/0/all/0/1\">Olivier Poirion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_J/0/1/0/all/0/1\">Jay Lowe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RandCrowns: A Quantitative Metric for Imprecisely Labeled Tree Crown Delineation. (arXiv:2105.02186v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02186","description":"<p>Supervised methods for object delineation in remote sensing require labeled\nground-truth data. Gathering sufficient high quality ground-truth data is\ndifficult, especially when targets are of irregular shape or difficult to\ndistinguish from background or neighboring objects. Tree crown delineation\nprovides key information from remote sensing images for forestry, ecology, and\nmanagement. However, tree crowns in remote sensing imagery are often difficult\nto label and annotate due to irregular shape, overlapping canopies, shadowing,\nand indistinct edges. There are also multiple approaches to annotation in this\nfield (e.g., rectangular boxes vs. convex polygons) that further contribute to\nannotation imprecision. However, current evaluation methods do not account for\nthis uncertainty in annotations, and quantitative metrics for evaluation can\nvary across multiple annotators. In this paper, we address these limitations by\ndeveloping an adaptation of the Rand index for weakly-labeled crown delineation\nthat we call RandCrowns. Our new RandCrowns evaluation metric provides a method\nto appropriately evaluate delineated tree crowns while taking into account\nimprecision in the ground-truth delineations. The RandCrowns metric\nreformulates the Rand index by adjusting the areas over which each term of the\nindex is computed to account for uncertain and imprecise object delineation\nlabels. Quantitative comparisons to the commonly used intersection over union\nmethod shows a decrease in the variance generated by differences among multiple\nannotators. Combined with qualitative examples, our results suggest that the\nRandCrowns metric is more robust for scoring target delineations in the\npresence of uncertainty and imprecision in annotations that are inherent to\ntree crown delineation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_D/0/1/0/all/0/1\">Dylan Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_S/0/1/0/all/0/1\">Sergio Marconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_B/0/1/0/all/0/1\">Ben G. Weinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_E/0/1/0/all/0/1\">Ethan P. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graves_S/0/1/0/all/0/1\">Sarah J. Graves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlman_S/0/1/0/all/0/1\">Stephanie A. Bohlman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Aided Saliency Maps Improve Generalization of Deep Learning. (arXiv:2105.03492v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03492","description":"<p>Deep learning has driven remarkable accuracy increases in many computer\nvision problems. One ongoing challenge is how to achieve the greatest accuracy\nin cases where training data is limited. A second ongoing challenge is that\ntrained models oftentimes do not generalize well even to new data that is\nsubjectively similar to the training set. We address these challenges in a\nnovel way, with the first-ever (to our knowledge) exploration of encoding human\njudgement about salient regions of images into the training data. We compare\nthe accuracy and generalization of a state-of-the-art deep learning algorithm\nfor a difficult problem in biometric presentation attack detection when trained\non (a) original images with typical data augmentations, and (b) the same\noriginal images transformed to encode human judgement about salient image\nregions. The latter approach results in models that achieve higher accuracy and\nbetter generalization, decreasing the error of the LivDet-Iris 2020 winner from\n29.78% to 16.37%, and achieving impressive generalization in a\nleave-one-attack-type-out evaluation scenario. This work opens a new area of\nstudy for how to embed human intelligence into training strategies for deep\nlearning to achieve high accuracy and generalization in cases of limited\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1\">Aidan Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truly shift-equivariant convolutional neural networks with adaptive polyphase upsampling. (arXiv:2105.04040v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04040","description":"<p>Convolutional neural networks lack shift equivariance due to the presence of\ndownsampling layers. In image classification, adaptive polyphase downsampling\n(APS-D) was recently proposed to make CNNs perfectly shift invariant. However,\nin networks used for image reconstruction tasks, it can not by itself restore\nshift equivariance. We address this problem by proposing adaptive polyphase\nupsampling (APS-U), a non-linear extension of conventional upsampling, which\nallows CNNs to exhibit perfect shift equivariance. With MRI and CT\nreconstruction experiments, we show that networks containing APS-D/U layers\nexhibit state of the art equivariance performance without sacrificing on image\nreconstruction quality. In addition, unlike prior methods like data\naugmentation and anti-aliasing, the gains in equivariance obtained from APS-D/U\nalso extend to images outside the training distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10620","description":"<p>This paper introduces HPNet, a novel deep-learning approach for segmenting a\n3D shape represented as a point cloud into primitive patches. The key to deep\nprimitive segmentation is learning a feature representation that can separate\npoints of different primitives. Unlike utilizing a single feature\nrepresentation, HPNet leverages hybrid representations that combine one learned\nsemantic descriptor, two spectral descriptors derived from predicted geometric\nparameters, as well as an adjacency matrix that encodes sharp edges. Moreover,\ninstead of merely concatenating the descriptors, HPNet optimally combines\nhybrid representations by learning combination weights. This weighting module\nbuilds on the entropy of input features. The output primitive segmentation is\nobtained from a mean-shift clustering module. Experimental results on benchmark\ndatasets ANSI and ABCParts show that HPNet leads to significant performance\ngains from baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1\">Etienne Vouga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention. (arXiv:2105.13495v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13495","description":"<p>Functional connectivity (FC) between regions of the brain can be assessed by\nthe degree of temporal correlation measured with functional neuroimaging\nmodalities. Based on the fact that these connectivities build a network,\ngraph-based approaches for analyzing the brain connectome have provided\ninsights into the functions of the human brain. The development of graph neural\nnetworks (GNNs) capable of learning representation from graph structured data\nhas led to increased interest in learning the graph representation of the brain\nconnectome. Although recent attempts to apply GNN to the FC network have shown\npromising results, there is still a common limitation that they usually do not\nincorporate the dynamic characteristics of the FC network which fluctuates over\ntime. In addition, a few studies that have attempted to use dynamic FC as an\ninput for the GNN reported a reduction in performance compared to static FC\nmethods, and did not provide temporal explainability. Here, we propose STAGIN,\na method for learning dynamic graph representation of the brain connectome with\nspatio-temporal attention. Specifically, a temporal sequence of brain graphs is\ninput to the STAGIN to obtain the dynamic graph representation, while novel\nREADOUT functions and the Transformer encoder provide spatial and temporal\nexplainability with attention, respectively. Experiments on the HCP-Rest and\nthe HCP-Task datasets demonstrate exceptional performance of our proposed\nmethod. Analysis of the spatio-temporal attention also provide concurrent\ninterpretation with the neuroscientific knowledge, which further validates our\nmethod. Code is available at https://github.com/egyptdj/stagin\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae-Jin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02320","description":"<p>Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as the conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and target images to facilitate the\nfew-shot semantic segmentation task. We design a novel Cycle-Consistent\nTransformer (CyCTR) module to aggregate pixel-wise support features into query\nones. CyCTR performs cross-attention between features from different images,\ni.e. support and query images. We observe that there may exist unexpected\nirrelevant pixel-level support features. Directly performing cross-attention\nmay aggregate these features from support to query and bias the query features.\nThus, we propose using a novel cycle-consistent attention mechanism to filter\nout possible harmful support features and encourage query features to attend to\nthe most informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Guoliang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Domain Invariant Features for Depth Estimation. (arXiv:2106.02594v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02594","description":"<p>We tackle the problem of unsupervised synthetic-to-real domain adaptation for\nsingle image depth estimation. An essential building block of single image\ndepth estimation is an encoder-decoder task network that takes RGB images as\ninput and produces depth maps as output. In this paper, we propose a novel\ntraining strategy to force the task network to learn domain invariant\nrepresentations in a selfsupervised manner. Specifically, we extend\nself-supervised learning from traditional representation learning, which works\non images from a single domain, to domain invariant representation learning,\nwhich works on images from two different domains by utilizing an image-to-image\ntranslation network. Firstly, we use an image-to-image translation network to\ntransfer domain-specific styles between synthetic and real domains. This style\ntransfer operation allows us to obtain similar images from the different\ndomains. Secondly, we jointly train our task network and Siamese network with\nthe same images from the different domains to obtain domain invariance for the\ntask network. Finally, we fine-tune the task network using labeled synthetic\nand unlabeled realworld data. Our training strategy yields improved\ngeneralization capability in the real-world domain. We carry out an extensive\nevaluation on two popular datasets for depth estimation, KITTI and Make3D. The\nresults demonstrate that our proposed method outperforms the state-of-the-art\non all metrics, e.g. by 14.7% on Sq Rel on KITTI. The source code and model\nweights will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akada_H/0/1/0/all/0/1\">Hiroyasu Akada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Shariq Farooq Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashim_I/0/1/0/all/0/1\">Ibraheem Alhashim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations. (arXiv:2106.05967v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05967","description":"<p>Contrastive self-supervised learning has outperformed supervised pretraining\non many downstream tasks like segmentation and object detection. However,\ncurrent methods are still primarily applied to curated datasets like ImageNet.\nIn this paper, we first study how biases in the dataset affect existing\nmethods. Our results show that current contrastive approaches work surprisingly\nwell across: (i) object- versus scene-centric, (ii) uniform versus long-tailed\nand (iii) general versus domain-specific datasets. Second, given the generality\nof the approach, we try to realize further gains with minor modifications. We\nshow that learning additional invariances -- through the use of multi-scale\ncropping, stronger augmentations and nearest neighbors -- improves the\nrepresentations. Finally, we observe that MoCo learns spatially structured\nrepresentations when trained with a multi-crop strategy. The representations\ncan be used for semantic segment retrieval and video instance segmentation\nwithout finetuning. Moreover, the results are on par with specialized models.\nWe hope this work will serve as a useful study for other researchers. The code\nand models are available at\nhttps://github.com/wvangansbeke/Revisiting-Contrastive-SSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1\">Wouter Van Gansbeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1\">Simon Vandenhende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Real than Real: A Study on Human Visual Perception of Synthetic Faces. (arXiv:2106.07226v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07226","description":"<p>Deep fakes became extremely popular in the last years, also thanks to their\nincreasing realism. Therefore, there is the need to measures human's ability to\ndistinguish between real and synthetic face images when confronted with\ncutting-edge creation technologies. We describe the design and results of a\nperceptual experiment we have conducted, where a wide and diverse group of\nvolunteers has been exposed to synthetic face images produced by\nstate-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\nStyleGAN2). The experiment outcomes reveal how strongly we should call into\nquestion our human ability to discriminate real faces from synthetic ones\ngenerated through modern AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lago_F/0/1/0/all/0/1\">Federica Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1\">Cecilia Pasquini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohme_R/0/1/0/all/0/1\">Rainer B&#xf6;hme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumont_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Dumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goffaux_V/0/1/0/all/0/1\">Val&#xe9;rie Goffaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1\">Giulia Boato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08601","description":"<p>Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate catastrophic forgetting in\nthe discriminator by introducing stationary learning environments. However, the\nseparate self-supervised tasks in existing self-supervised GANs cause a goal\ninconsistent with generative modeling due to the fact that their\nself-supervised classifiers are agnostic to the generator distribution. To\naddress this problem, we propose a novel self-supervised GAN that unifies the\nGAN task with the self-supervised task by augmenting the GAN labels (real or\nfake) via self-supervision of data transformation. Specifically, the original\ndiscriminator and self-supervised classifier are unified into a label-augmented\ndiscriminator that predicts the augmented labels to be aware of the generator\ndistribution and the data distribution under every transformation, and then\nprovide the discrepancy between them to optimize the generator. Theoretically,\nwe prove that the optimal generator converges to replicate the real data\ndistribution under mild assumptions. Empirically, we show that the proposed\nmethod significantly outperforms previous self-supervised and data augmentation\nGANs on both generative modeling and representation learning across various\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09996","description":"<p>Training a Convolutional Neural Network (CNN) to be robust against rotation\nhas mostly been done with data augmentation. In this paper, another progressive\nvision of research direction is highlighted to encourage less dependence on\ndata augmentation by achieving structural rotational invariance of a network.\nThe deep equivariance-bridged SO(2) invariant network is proposed to echo such\nvision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network\n(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the\ngraph representation of an image to acquire rotationally equivariant\nrepresentation, as GCN is more suitable for constructing deeper network than\nspectral graph convolution-based approaches. Then, invariant representation is\neventually obtained with Global Average Pooling (GAP), a permutation-invariant\noperation suitable for aggregating high-dimensional representations, over the\nequivariant set of vertices retrieved from SWN-GCN. Our method achieves the\nstate-of-the-art image classification performance on rotated MNIST and CIFAR-10\nimages, where the models are trained with a non-augmented dataset only.\nQuantitative validations over invariance of the representations also\ndemonstrate strong invariance of deep representations of SWN-GCN over\nrotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sungwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10137","description":"<p>Instance-level contrastive learning techniques, which rely on data\naugmentation and a contrastive loss function, have found great success in the\ndomain of visual representation learning. They are not suitable for exploiting\nthe rich dynamical structure of video however, as operations are done on many\naugmented instances. In this paper we propose \"Video Cross-Stream Prototypical\nContrasting\", a novel method which predicts consistent prototype assignments\nfrom both RGB and optical flow views, operating on sets of samples.\nSpecifically, we alternate the optimization process; while optimizing one of\nthe streams, all views are mapped to one set of stream prototype vectors. Each\nof the assignments is predicted with all views except the one matching the\nprediction, pushing representations closer to their assigned prototypes. As a\nresult, more efficient video embeddings with ingrained motion information are\nlearned, without the explicit need for optical flow computation during\ninference. We obtain state-of-the-art results on nearest-neighbour video\nretrieval and action recognition, outperforming previous best by +3.2% on\nUCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and\n+15.1% on HMDB51 using the R(2+1)D backbone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toering_M/0/1/0/all/0/1\">Martine Toering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatopoulos_I/0/1/0/all/0/1\">Ioannis Gatopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stol_M/0/1/0/all/0/1\">Maarten Stol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1\">Vincent Tao Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.15893","description":"<p>Automatic outlining of different tissue types in digitized histological\nspecimen provides a basis for follow-up analyses and can potentially guide\nsubsequent medical decisions. The immense size of whole-slide-images (WSI),\nhowever, poses a challenge in terms of computation time. In this regard, the\nanalysis of non-overlapping patches outperforms pixelwise segmentation\napproaches, but still leaves room for optimization. Furthermore, the division\ninto patches, regardless of the biological structures they contain, is a\ndrawback due to the loss of local dependencies. We propose to subdivide the WSI\ninto coherent regions prior to classification by grouping visually similar\nadjacent pixels into superpixels. Afterwards, only a random subset of patches\nper superpixel is classified and patch labels are combined into a superpixel\nlabel. We propose a metric for identifying superpixels with an uncertain\nclassification and evaluate two medical applications, namely tumor area and\ninvasive margin estimation and tumor composition analysis. The algorithm has\nbeen developed on 159 hand-annotated WSIs of colon resections and its\nperformance is compared to an analysis without prior segmentation. The\nalgorithm shows an average speed-up of 41% and an increase in accuracy from\n93.8% to 95.7%. By assigning a rejection label to uncertain superpixels, we\nfurther increase the accuracy by 0.4%. Whilst tumor area estimation shows high\nconcordance to the annotated area, the analysis of tumor composition highlights\nlimitations of our approach. By combining superpixel segmentation and patch\nclassification, we designed a fast and accurate framework for whole-slide\ncartography that is AI-model agnostic and provides the basis for various\nmedical endpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1\">Serop Baghdadlian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1\">David Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1\">Martin Weidenfeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1\">Susanne Merkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1\">Markus Eckstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00436","description":"<p>Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1\">Erik Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacVittie_K/0/1/0/all/0/1\">Korey MacVittie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilly_J/0/1/0/all/0/1\">John Lilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02067","description":"<p>Vision systems trained in closed-world scenarios fail when presented with new\nenvironmental conditions, new data distributions, and novel classes at\ndeployment time. How to move towards open-world learning is a long-standing\nresearch question. The existing solutions mainly focus on specific aspects of\nthe problem (single domain Open-Set, multi-domain Closed-Set), or propose\ncomplex strategies which combine several losses and manually tuned\nhyperparameters. In this work, we tackle multi-source Open-Set domain\nadaptation by introducing HyMOS: a straightforward model that exploits the\npower of contrastive learning and the properties of its hyperspherical feature\nspace to correctly predict known labels on the target, while rejecting samples\nbelonging to any unknown class. HyMOS includes style transfer among the\ninstance transformations of contrastive learning to get domain invariance while\navoiding the risk of negative-transfer. A self-paced threshold is defined on\nthe basis of the observed data distribution and updates online during training,\nallowing to handle the known-unknown separation. We validate our method over\nthree challenging datasets. The obtained results show that HyMOS outperforms\nseveral competitors, defining the new state-of-the-art. Our code is available\nat https://github.com/silvia1993/HyMOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.03996","description":"<p>We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod that leverages both proprioceptive states and visual observations for\nlocomotion control. We evaluate our method in challenging simulated\nenvironments with different obstacles and uneven terrain. We transfer our\nlearned policy from simulation to a real robot by running it indoor and\nin-the-wild with unseen obstacles and terrain. Our method not only\nsignificantly improves over baselines, but also achieves far better\ngeneralization performance, especially when transferred to the real robot. Our\nproject page with videos is at https://rchalyang.github.io/LocoTransformer/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10060","description":"<p>Conditional generative models aim to learn the underlying joint distribution\nof data and labels, and thus realize conditional generation. Among them,\nauxiliary classifier generative adversarial networks (AC-GAN) have been widely\nused, but suffer from the problem of low intra-class diversity on generated\nsamples. In this paper, we point out that the fundamental reason is that the\nclassifier of AC-GAN is generator-agnostic, and therefore cannot provide\ninformative guidance to the generator to approximate the target distribution,\nresulting in minimization of conditional entropy that decreases the intra-class\ndiversity. Motivated by this observation, we propose a novel conditional GAN\nwith auxiliary \\textit{discriminative} classifier (ADC-GAN) to resolve the\nproblem of AC-GAN. Specifically, the proposed auxiliary \\textit{discriminative}\nclassifier becomes generator-aware by recognizing the labels of the real data\nand the generated data \\textit{discriminatively}. Our theoretical analysis\nreveals that the generator can faithfully replicate the target distribution\neven without the original discriminator, making the proposed ADC-GAN robust to\nthe hyper-parameter and stable on the training process. Extensive experimental\nresults on synthetic and real-world datasets demonstrate the superiority of\nADC-GAN on conditional generative modeling compared with competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos. (arXiv:2107.11629v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11629","description":"<p>Exploring to what humans pay attention in dynamic panoramic scenes is useful\nfor many fundamental applications, including augmented reality (AR) in retail,\nAR-powered recruitment, and visual language navigation. With this goal in mind,\nwe propose PV-SOD, a new task that aims to segment salient objects from\npanoramic videos. In contrast to existing fixation-/object-level saliency\ndetection tasks, we focus on audio-induced salient object detection (SOD),\nwhere the salient objects are labeled with the guidance of audio-induced eye\nmovements. To support this task, we collect the first large-scale dataset,\nnamed ASOD60K, which contains 4K-resolution video frames annotated with a\nsix-level hierarchy, thus distinguishing itself with richness, diversity and\nquality. Specifically, each sequence is marked with both its super-/sub-class,\nwith objects of each sub-class being further annotated with human eye\nfixations, bounding boxes, object-/instance-level masks, and associated\nattributes (e.g., geometrical distortion). These coarse-to-fine annotations\nenable detailed analysis for PV-SOD modelling, e.g., determining the major\nchallenges for existing SOD models, and predicting scanpaths to study the\nlong-term eye fixation behaviors of humans. We systematically benchmark 11\nrepresentative approaches on ASOD60K and derive several interesting findings.\nWe hope this study could serve as a good starting point for advancing SOD\nresearch towards panoramic videos. The dataset and benchmark will be made\npublicly available at https://github.com/PanoAsh/ASOD60K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fang-Yi Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12085","description":"<p>Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose optimization-based ABA (OP-ABA)\nby iteratively optimizing an adversarial objective function against the\ntracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose one-step ABA (OS-ABA) where we design and train\na joint adversarial motion and accumulation predictive network (JAMANet) with\nthe guidance of OP-ABA, which is able to efficiently estimate the adversarial\nmotion and accumulation parameters in a one-step way. The experiments on four\npopular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that\nour methods are able to cause significant accuracy drops on four\nstate-of-the-art trackers with high transferability. Please find the source\ncode at \\url{https://github.com/tsingqguo/ABA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Semantic Segmentation with Superpixel-Mix. (arXiv:2108.00968v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00968","description":"<p>Along with predictive performance and runtime speed, reliability is a key\nrequirement for real-world semantic segmentation. Reliability encompasses\nrobustness, predictive uncertainty and reduced bias. To improve reliability, we\nintroduce Superpixel-mix, a new superpixel-based data augmentation method with\nteacher-student consistency training. Unlike other mixing-based augmentation\ntechniques, mixing superpixels between images is aware of object boundaries,\nwhile yielding consistent gains in segmentation accuracy. Our proposed\ntechnique achieves state-of-the-art results in semi-supervised semantic\nsegmentation on the Cityscapes dataset. Moreover, Superpixel-mix improves the\nreliability of semantic segmentation by reducing network uncertainty and bias,\nas confirmed by competitive results under strong distributions shift (adverse\nweather, image corruptions) and when facing out-of-distribution data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1\">Nacim Belkhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Pose Estimation of Illustrated Characters. (arXiv:2108.01819v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01819","description":"<p>Human pose information is a critical component in many downstream image\nprocessing tasks, such as activity recognition and motion tracking. Likewise, a\npose estimator for the illustrated character domain would provide a valuable\nprior for assistive content creation tasks, such as reference pose retrieval\nand automatic character animation. But while modern data-driven techniques have\nsubstantially improved pose estimation performance on natural images, little\nwork has been done for illustrations. In our work, we bridge this domain gap by\nefficiently transfer-learning from both domain-specific and task-specific\nsource models. Additionally, we upgrade and expand an existing illustrated pose\nestimation dataset, and introduce two new datasets for classification and\nsegmentation subtasks. We then apply the resultant state-of-the-art character\npose estimator to solve the novel task of pose-guided illustration retrieval.\nAll data, models, and code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03541","description":"<p>Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White blood cell subtype detection and classification. (arXiv:2108.04614v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04614","description":"<p>Machine learning has endless applications in the health care industry. White\nblood cell classification is one of the interesting and promising area of\nresearch. The classification of the white blood cells plays an important part\nin the medical diagnosis. In practise white blood cell classification is\nperformed by the haematologist by taking a small smear of blood and careful\nexamination under the microscope. The current procedures to identify the white\nblood cell subtype is more time taking and error-prone. The computer aided\ndetection and diagnosis of the white blood cells tend to avoid the human error\nand reduce the time taken to classify the white blood cells. In the recent\nyears several deep learning approaches have been developed in the context of\nclassification of the white blood cells that are able to identify but are\nunable to localize the positions of white blood cells in the blood cell image.\nFollowing this, the present research proposes to utilize YOLOv3 object\ndetection technique to localize and classify the white blood cells with\nbounding boxes. With exhaustive experimental analysis, the proposed work is\nfound to detect the white blood cell with 99.2% accuracy and classify with 90%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1\">Nalla Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syafrullah_M/0/1/0/all/0/1\">M. Syafrullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adiyarta_K/0/1/0/all/0/1\">Krisna Adiyarta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset -- Addressing the Noise-Latent Trade-Off. (arXiv:2108.08922v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08922","description":"<p>The state-of-the-art StyleGAN2 network supports powerful methods to create\nand edit art, including generating random images, finding images \"like\" some\nquery, and modifying content or style. Further, recent advancements enable\ntraining with small datasets. We apply these methods to synthesize card art, by\ntraining on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are\nessential for good synthesis, we find that coarse-scale noise interferes with\nlatent variables on this dataset because both control long-scale image effects.\nWe observe over-aggressive variation in art with changes in noise and weak\ncontent control via latent variable edits. Here, we demonstrate that training a\nmodified StyleGAN2, where coarse-scale noise is suppressed, removes these\nunwanted effects. We obtain a superior FID; changes in noise result in local\nexploration of style; and identity control is markedly improved. These results\nand analysis lead towards a GAN-assisted art synthesis tool for digital artists\nof all skill levels, which can be used in film, games, or any creative industry\nfor artistic ideation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1\">Vaibhav Vavilala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA requires reading and\nreasoning about local, often previously unseen, scene-text content of an image\nto generate answers. In this work, we propose the generalized use of external\nknowledge to augment our understanding of the said scene-text. We design a\nframework to extract, validate, and reason with knowledge using a standard\nmultimodal transformer for vision language understanding tasks. Through\nempirical evidence and qualitative results, we demonstrate how external\nknowledge can highlight instance-only cues and thus help deal with training\ndata bias, improve answer entity type correctness, and detect multiword named\nentities. We generate results comparable to the state-of-the-art on two\npublicly available datasets, under the constraints of similar upstream OCR\nsystems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Object Detection by Label Assignment Distillation. (arXiv:2108.10520v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10520","description":"<p>Label assignment in object detection aims to assign targets, foreground or\nbackground, to sampled regions in an image. Unlike labeling for image\nclassification, this problem is not well defined due to the object's bounding\nbox. In this paper, we investigate the problem from a perspective of\ndistillation, hence we call Label Assignment Distillation (LAD). Our initial\nmotivation is very simple, we use a teacher network to generate labels for the\nstudent. This can be achieved in two ways: either using the teacher's\nprediction as the direct targets (soft label), or through the hard labels\ndynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD\nis more effective than soft-label, but they are complementary. (ii) Using LAD,\na smaller teacher can also improve a larger student significantly, while\nsoft-label can't. We then introduce Co-learning LAD, in which two networks\nsimultaneously learn from scratch and the role of teacher and student are\ndynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques\ncan improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \\rm AP$ and\n$47.5\\rm AP$ on the COCO test-dev set. With a stronger teacher PAA-SwinB, we\nimprove the students PAA-ResNet50 to $43.7\\rm AP$ by only 1x schedule training\nand standard setting, and PAA-ResNet101 to $47.9\\rm AP$, significantly\nsurpassing the current methods. Our source code and checkpoints are released at\nhttps://git.io/JrDZo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy C. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tuan N. Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">Nam L.H. Phan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-RAFT: Dense Optical Flow from Event Cameras. (arXiv:2108.10552v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10552","description":"<p>We propose to incorporate feature correlation and sequential processing into\ndense optical flow estimation from event cameras. Modern frame-based optical\nflow methods heavily rely on matching costs computed from feature correlation.\nIn contrast, there exists no optical flow method for event cameras that\nexplicitly computes matching costs. Instead, learning-based approaches using\nevents usually resort to the U-Net architecture to estimate optical flow\nsparsely. Our key finding is that the introduction of correlation features\nsignificantly improves results compared to previous methods that solely rely on\nconvolution layers. Compared to the state-of-the-art, our proposed approach\ncomputes dense optical flow and reduces the end-point error by 23% on MVSEC.\nFurthermore, we show that all existing optical flow methods developed so far\nfor event cameras have been evaluated on datasets with very small displacement\nfields with a maximum flow magnitude of 10 pixels. Based on this observation,\nwe introduce a new real-world dataset that exhibits displacement fields with\nmagnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed\napproach reduces the end-point error on this dataset by 66%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millhausler_M/0/1/0/all/0/1\">Mario Millh&#xe4;usler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11765","description":"<p>Deep neural networks (DNNs) have become essential for processing the vast\namounts of aerial imagery collected using earth-observing satellite platforms.\nHowever, DNNs are vulnerable towards adversarial examples, and it is expected\nthat this weakness also plagues DNNs for aerial imagery. In this work, we\ndemonstrate one of the first efforts on physical adversarial attacks on aerial\nimagery, whereby adversarial patches were optimised, fabricated and installed\non or near target objects (cars) to significantly reduce the efficacy of an\nobject detector applied on overhead images. Physical adversarial attacks on\naerial images, particularly those captured from satellite platforms, are\nchallenged by atmospheric factors (lighting, weather, seasons) and the distance\nbetween the observer and target. To investigate the effects of these\nchallenges, we devised novel experiments and metrics to evaluate the efficacy\nof physical adversarial attacks against object detectors in aerial scenes. Our\nresults indicate the palpable threat posed by physical adversarial attacks\ntowards DNNs for processing satellite imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_A/0/1/0/all/0/1\">Andrew Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_Y/0/1/0/all/0/1\">Yee Wei Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_R/0/1/0/all/0/1\">Ramesh Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dillon Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Compression for Resource-Constrained Edge Computing Systems. (arXiv:2108.11898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11898","description":"<p>There has been much interest in deploying deep learning algorithms on\nlow-powered devices, including smartphones, drones, and medical sensors.\nHowever, full-scale deep neural networks are often too resource-intensive in\nterms of energy and storage. As a result, the bulk part of the machine learning\noperation is therefore often carried out on an edge server, where the data is\ncompressed and transmitted. However, compressing data (such as images) leads to\ntransmitting information irrelevant to the supervised task. Another popular\napproach is to split the deep network between the device and the server while\ncompressing intermediate features. To date, however, such split computing\nstrategies have barely outperformed the aforementioned naive data compression\nbaselines due to their inefficient approaches to feature compression. This\npaper adopts ideas from knowledge distillation and neural image compression to\ncompress intermediate feature representations more efficiently. Our supervised\ncompression approach uses a teacher model and a student model with a stochastic\nbottleneck and learnable prior for entropy coding (Entropic Student). We\ncompare our approach to various neural image and feature compression baselines\nin three vision tasks and found that it achieves better supervised\nrate-distortion performance while maintaining smaller end-to-end latency. We\nfurthermore show that the learned feature representations can be tuned to serve\nmultiple downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02639","description":"<p>Out-of-distribution (OOD) detection and lossless compression constitute two\nproblems that can be solved by the training of probabilistic models on a first\ndataset with subsequent likelihood evaluation on a second dataset, where data\ndistributions differ. By defining the generalization of probabilistic models in\nterms of likelihood we show that, in the case of image models, the OOD\ngeneralization ability is dominated by local features. This motivates our\nproposal of a Local Autoregressive model that exclusively models local image\nfeatures towards improving OOD performance. We apply the proposed model to OOD\ndetection tasks and achieve state-of-the-art unsupervised OOD detection\nperformance without the introduction of additional data. Additionally, we\nemploy our model to build a new lossless image compressor: NeLLoC (Neural Local\nLossless Compressor) and report state-of-the-art compression rates and model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting. (arXiv:2109.06061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06061","description":"<p>In this work, we address the problem of jointly estimating albedo, normals,\ndepth and 3D spatially-varying lighting from a single image. Most existing\nmethods formulate the task as image-to-image translation, ignoring the 3D\nproperties of the scene. However, indoor scenes contain complex 3D light\ntransport where a 2D representation is insufficient. In this paper, we propose\na unified, learning-based inverse rendering framework that formulates 3D\nspatially-varying lighting. Inspired by classic volume rendering techniques, we\npropose a novel Volumetric Spherical Gaussian representation for lighting,\nwhich parameterizes the exitant radiance of the 3D scene surfaces on a voxel\ngrid. We design a physics based differentiable renderer that utilizes our 3D\nlighting representation, and formulates the energy-conserving image formation\nprocess that enables joint training of all intrinsic properties with the\nre-rendering constraint. Our model ensures physically correct predictions and\navoids the need for ground-truth HDR lighting which is not easily accessible.\nExperiments show that our method outperforms prior works both quantitatively\nand qualitatively, and is capable of producing photorealistic results for AR\napplications such as virtual object insertion even for highly specular objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06638","description":"<p>Pooling is a simple but essential layer in modern deep CNN architectures for\nfeature aggregation and extraction. Typical CNN design focuses on the conv\nlayers and activation functions, while leaving the pooling layers with fewer\noptions. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that\ncan be applied universally to replace standard pooling operations to better\nextract features with improved accuracy and efficiency. Motivated from the\nwavelet theory, we adopt the low-pass (L) and high-pass (H) filters\nhorizontally and vertically for pooling on a 2D feature map. Feature signals\nare decomposed into four (LL, LH, HL, HH) subbands to retain features better\nand avoid information dropping. The wavelet transform ensures features after\npooling can be fully preserved and recovered. We next adopt an energy-based\nattention learning to fine-select crucial and representative features.\nLDW-Pooling is effective and efficient when compared with other\nstate-of-the-art pooling techniques such as WaveletPooling and LiftPooling.\nExtensive experimental validation shows that LDW-Pooling can be applied to a\nwide range of standard CNN architectures and consistently outperform standard\n(max, mean, mixed, and stochastic) pooling operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bor-Shiun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F-CAM: Full Resolution Class Activation Maps via Guided Parametric Upscaling. (arXiv:2109.07069v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07069","description":"<p>Class Activation Mapping (CAM) methods have recently gained much attention\nfor weakly-supervised object localization (WSOL) tasks. They allow for CNN\nvisualization and interpretation without training on fully annotated image\ndatasets. CAM methods are typically integrated within off-the-shelf CNN\nbackbones, such as ResNet50. Due to convolution and pooling operations, these\nbackbones yield low resolution CAMs with a down-scaling factor of up to 32,\ncontributing to inaccurate localizations. Interpolation is required to restore\nfull size CAMs, yet it does not consider the statistical properties of objects,\nsuch as color and texture, leading to activations with inconsistent boundaries,\nand inaccurate localizations. As an alternative, we introduce a generic method\nfor parametric upscaling of CAMs that allows constructing accurate full\nresolution CAMs (F-CAMs). In particular, we propose a trainable decoding\narchitecture that can be connected to any CNN classifier to produce highly\naccurate CAM localizations. Given an original low resolution CAM, foreground\nand background pixels are randomly sampled to fine-tune the decoder. Additional\npriors such as image statistics and size constraints are also considered to\nexpand and refine object boundaries. Extensive experiments, over three CNN\nbackbones and six WSOL baselines on the CUB-200-2011 and OpenImages datasets,\nindicate that our F-CAM method yields a significant improvement in CAM\nlocalization accuracy. F-CAM performance is competitive with state-of-art WSOL\nmethods, yet it requires fewer computations during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarraf_A/0/1/0/all/0/1\">Aydin Sarraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03825","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation. (arXiv:2110.06400v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.06400","description":"<p>We propose a novel approach to translate unpaired contrast computed\ntomography (CT) scans to non-contrast CT scans and the other way around.\nSolving this task has two important applications: (i) to automatically generate\ncontrast CT scans for patients for whom injecting contrast substance is not an\noption, and (ii) to enhance alignment between contrast and non-contrast CT by\nreducing the differences induced by the contrast substance before registration.\nOur approach is based on cycle-consistent generative adversarial convolutional\ntransformers, for short, CyTran. Our neural model can be trained on unpaired\nimages, due to the integration of a cycle-consistency loss. To deal with\nhigh-resolution images, we design a hybrid architecture based on convolutional\nand multi-head attention layers. In addition, we introduce a novel data set,\nColtea-Lung-CT-100W, containing 3D triphasic lung CT scans (with a total of\n37,290 images) collected from 100 female patients. Each scan contains three\nphases (non-contrast, early portal venous, and late arterial), allowing us to\nperform experiments to compare our novel approach with state-of-the-art methods\nfor image style transfer. Our empirical results show that CyTran outperforms\nall competing methods. Moreover, we show that CyTran can be employed as a\npreliminary step to improve a state-of-the-art medical image alignment method.\nWe release our novel model and data set as open source at:\nhttps://github.com/ristea/cycle-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miron_A/0/1/0/all/0/1\">Andreea-Iuliana Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savencu_O/0/1/0/all/0/1\">Olivian Savencu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verga_N/0/1/0/all/0/1\">Nicolae Verga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive review of Binary Neural Network. (arXiv:2110.06804v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.06804","description":"<p>Binary Neural Network (BNN) method is an extreme application of convolutional\nneural network (CNN) parameter quantization. As opposed to the original CNN\nmethods which employed floating-point computation with full-precision weights\nand activations, BBN uses 1-bit activations and weights. With BBNs, a\nsignificant amount of storage, network complexity and energy consumption can be\nreduced, and neural networks can be implemented more efficiently in embedded\napplications. Unfortunately, binarization causes severe information loss. A gap\nstill exists between full-precision CNN models and their binarized\ncounterparts. The recent developments in BNN have led to a lot of algorithms\nand solutions that have helped address this issue. This article provides a full\noverview of recent developments in BNN. The present paper focuses exclusively\non 1-bit activations and weights networks, as opposed to previous surveys in\nwhich low-bit works are mixed in. In this paper, we conduct a complete\ninvestigation of BNN's development from their predecessors to the latest BNN\nalgorithms and techniques, presenting a broad design pipeline, and discussing\neach module's variants. Along the way, this paper examines BNN (a) purpose:\ntheir early successes and challenges; (b) BNN optimization: selected\nrepresentative works that contain key optimization techniques; (c) deployment:\nopen-source frameworks for BNN modeling and development; (d) terminal:\nefficient computing architectures and devices for BNN and (e) applications:\ndiverse applications with BNN. Moreover, this paper discusses potential\ndirections and future research opportunities for the latest BNN algorithms and\ntechniques, presents a broad design pipeline, and discusses each module's\nvariants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1\">Sos S. Agaian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media. (arXiv:2110.07235v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07235","description":"<p>We introduce HUMAN4D, a large and multimodal 4D dataset that contains a\nvariety of human activities simultaneously captured by a professional\nmarker-based MoCap, a volumetric capture and an audio recording system. By\ncapturing 2 female and $2$ male professional actors performing various\nfull-body movements and expressions, HUMAN4D provides a diverse set of motions\nand poses encountered as part of single- and multi-person daily, physical and\nsocial activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),\nvolumetric and audio data. Despite the existence of multi-view color datasets\ncaptured with the use of hardware (HW) synchronization, to the best of our\nknowledge, HUMAN4D is the first and only public resource that provides\nvolumetric depth maps with high synchronization precision due to the use of\nintra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned\nand rigged 3D character complements HUMAN4D to enable joint research on\ntime-varying and high-quality dynamic meshes. We provide evaluation baselines\nby benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D\ncompression methods. For the former, we apply 2D and 3D pose estimation\nalgorithms both on single- and multi-view data cues. For the latter, we\nbenchmark open-source 3D codecs on volumetric data respecting online volumetric\nvideo encoding and steady bit-rates. Furthermore, qualitative and quantitative\nvisual comparison between mesh-based volumetric data reconstructed in different\nqualities showcases the available options with respect to 4D representations.\nHUMAN4D is introduced to the computer vision and graphics research communities\nto enable joint research on spatio-temporally aligned pose, volumetric, mRGBD\nand audio data cues. The dataset and its code are available\nhttps://tofis.github.io/myurls/human4d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzitofis_A/0/1/0/all/0/1\">Anargyros Chatzitofis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saroglou_L/0/1/0/all/0/1\">Leonidas Saroglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutis_P/0/1/0/all/0/1\">Prodromos Boutis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drakoulis_P/0/1/0/all/0/1\">Petros Drakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanyam_S/0/1/0/all/0/1\">Shishir Subramanyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kevelham_B/0/1/0/all/0/1\">Bart Kevelham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charbonnier_C/0/1/0/all/0/1\">Caecilia Charbonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesar_P/0/1/0/all/0/1\">Pablo Cesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Aggregation Network for Fast MR Imaging. (arXiv:2110.08080v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08080","description":"<p>Magnetic resonance (MR) imaging is a commonly used scanning technique for\ndisease detection, diagnosis and treatment monitoring. Although it is able to\nproduce detailed images of organs and tissues with better contrast, it suffers\nfrom a long acquisition time, which makes the image quality vulnerable to say\nmotion artifacts. Recently, many approaches have been developed to reconstruct\nfull-sampled images from partially observed measurements in order to accelerate\nMR imaging. However, most of these efforts focus on reconstruction over a\nsingle modality or simple fusion of multiple modalities, neglecting the\ndiscovery of correlation knowledge at different feature level. In this work, we\npropose a novel Multi-modal Aggregation Network, named MANet, which is capable\nof discovering complementary representations from a fully sampled auxiliary\nmodality, with which to hierarchically guide the reconstruction of a given\ntarget modality. In our MANet, the representations from the fully sampled\nauxiliary and undersampled target modalities are learned independently through\na specific network. Then, a guided attention module is introduced in each\nconvolutional stage to selectively aggregate multi-modal features for better\nreconstruction, yielding comprehensive, multi-scale, multi-modal feature\nfusion. Moreover, our MANet follows a hybrid domain learning framework, which\nallows it to simultaneously recover the frequency signal in the $k$-space\ndomain as well as restore the image details from the image domain. Extensive\nexperiments demonstrate the superiority of the proposed approach over\nstate-of-the-art MR image reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based detection of intravenous contrast in computed tomography scans. (arXiv:2110.08424v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08424","description":"<p>Purpose: Identifying intravenous (IV) contrast use within CT scans is a key\ncomponent of data curation for model development and testing. Currently, IV\ncontrast is poorly documented in imaging metadata and necessitates manual\ncorrection and annotation by clinician experts, presenting a major barrier to\nimaging analyses and algorithm deployment. We sought to develop and validate a\nconvolutional neural network (CNN)-based deep learning (DL) platform to\nidentify IV contrast within CT scans. Methods: For model development and\nevaluation, we used independent datasets of CT scans of head, neck (HN) and\nlung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans\nmanually annotated for contrast presence by clinical experts. Five different DL\nmodels were adopted and trained in HN training datasets for slice-level\ncontrast detection. Model performances were evaluated on a hold-out set and on\nan independent validation set from another institution. DL models was then\nfine-tuned on chest CT data and externally validated on a separate chest CT\ndataset. Results: Initial DICOM metadata tags for IV contrast were missing or\nerroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the\nbest overall detection performance. For HN scans, AUC was 0.996 in the internal\nvalidation set (n = 216) and 1.0 in the external validation set (n = 595). The\nfine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation\nset (n = 53), and AUC: 0.980 for the external validation set (n = 402).\nConclusion: The DL model could accurately detect IV contrast in both HN and\nchest CT scans with near-perfect performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zezhong Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_J/0/1/0/all/0/1\">Jack M. Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosny_A/0/1/0/all/0/1\">Ahmed Hosny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeleznik_R/0/1/0/all/0/1\">Roman Zeleznik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plana_D/0/1/0/all/0/1\">Deborah Plana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likitlersuang_J/0/1/0/all/0/1\">Jirapat Likitlersuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mak_R/0/1/0/all/0/1\">Raymond H. Mak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo J. W. L. Aerts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kann_B/0/1/0/all/0/1\">Benjamin H. Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08733","description":"<p>Deep learning approaches have shown promising results in remote sensing high\nspatial resolution (HSR) land-cover mapping. However, urban and rural scenes\ncan show completely different geographical landscapes, and the inadequate\ngeneralizability of these algorithms hinders city-level or national-level\nmapping. Most of the existing HSR land-cover datasets mainly promote the\nresearch of learning semantic representation, thereby ignoring the model\ntransferability. In this paper, we introduce the Land-cOVEr Domain Adaptive\nsemantic segmentation (LoveDA) dataset to advance semantic and transferable\nlearning. The LoveDA dataset contains 5987 HSR images with 166768 annotated\nobjects from three different cities. Compared to the existing datasets, the\nLoveDA dataset encompasses two domains (urban and rural), which brings\nconsiderable challenges due to the: 1) multi-scale objects; 2) complex\nbackground samples; and 3) inconsistent class distributions. The LoveDA dataset\nis suitable for both land-cover semantic segmentation and unsupervised domain\nadaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on\neleven semantic segmentation methods and eight UDA methods. Some exploratory\nstudies including multi-scale architectures and strategies, additional\nbackground supervision, and pseudo-label analysis were also carried out to\naddress these challenges. The code and data are available at\nhttps://github.com/Junjue-Wang/LoveDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Selfie Beautification Filters on Face Detection and Recognition. (arXiv:2110.08934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08934","description":"<p>Beautification and augmented reality filters are very popular in applications\nthat use selfie images captured with smartphones or personal devices. However,\nthey can distort or modify biometric features, severely affecting the\ncapability of recognizing individuals' identity or even detecting the face.\nAccordingly, we address the effect of such filters on the accuracy of automated\nface detection and recognition. The social media image filters studied either\nmodify the image contrast or illumination or occlude parts of the face with for\nexample artificial glasses or animal noses. We observe that the effect of some\nof these filters is harmful both to face detection and identity recognition,\nspecially if they obfuscate the eye or (to a lesser extent) the nose. To\ncounteract such effect, we develop a method to reconstruct the applied\nmanipulation with a modified version of the U-NET segmentation network. This is\nobserved to contribute to a better face detection and recognition accuracy.\nFrom a recognition perspective, we employ distance measures and trained machine\nlearning algorithms applied to features extracted using a ResNet-34 network\ntrained to recognize faces. We also evaluate if incorporating filtered images\nto the training set of machine learning approaches are beneficial for identity\nrecognition. Our results show good recognition when filters do not occlude\nimportant landmarks, specially the eyes (identification accuracy &gt;99%, EER&lt;2%).\nThe combined effect of the proposed approaches also allow to mitigate the\neffect produced by filters that occlude parts of the face, achieving an\nidentification accuracy of &gt;92% with the majority of perturbations evaluated,\nand an EER &lt;8%. Although there is room for improvement, when neither U-NET\nreconstruction nor training with filtered images is applied, the accuracy with\nfilters that severely occlude the eye is &lt;72% (identification) and &gt;12% (EER)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Pontus Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skepetzis_V/0/1/0/all/0/1\">Vasilios Skepetzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Modality Translation For Face Presentation Attack Detection. (arXiv:2110.09108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09108","description":"<p>Face presentation attack detection (PAD) is an essential measure to protect\nface recognition systems from being spoofed by malicious users and has\nattracted great attention from both academia and industry. Although most of the\nexisting methods can achieve desired performance to some extent, the\ngeneralization issue of face presentation attack detection under cross-domain\nsettings (e.g., the setting of unseen attacks and varying illumination) remains\nto be solved. In this paper, we propose a novel framework based on asymmetric\nmodality translation for face presentation attack detection in bi-modality\nscenarios. Under the framework, we establish connections between two modality\nimages of genuine faces. Specifically, a novel modality fusion scheme is\npresented that the image of one modality is translated to the other one through\nan asymmetric modality translator, then fused with its corresponding paired\nimage. The fusion result is fed as the input to a discriminator for inference.\nThe training of the translator is supervised by an asymmetric modality\ntranslation loss. Besides, an illumination normalization module based on\nPattern of Local Gravitational Force (PLGF) representation is used to reduce\nthe impact of illumination variation. We conduct extensive experiments on three\npublic datasets, which validate that our method is effective in detecting\nvarious types of attacks and achieves state-of-the-art performance under\ndifferent evaluation protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongjian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kwok-Yan Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh Convolutional Autoencoder for Semi-Regular Meshes of Different Sizes. (arXiv:2110.09401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09401","description":"<p>The analysis of deforming 3D surface meshes is accelerated by autoencoders\nsince the low-dimensional embeddings can be used to visualize underlying\ndynamics. But, state-of-the-art mesh convolutional autoencoders require a fixed\nconnectivity of all input meshes handled by the autoencoder. This is due to\neither the use of spectral convolutional layers or mesh dependent pooling\noperations. Therefore, the types of datasets that one can study are limited and\nthe learned knowledge cannot be transferred to other datasets that exhibit\nsimilar behavior. To address this, we transform the discretization of the\nsurfaces to semi-regular meshes that have a locally regular connectivity and\nwhose meshing is hierarchical. This allows us to apply the same spatial\nconvolutional filters to the local neighborhoods and to define a pooling\noperator that can be applied to every semi-regular mesh. We apply the same mesh\nautoencoder to different datasets and our reconstruction error is more than 50%\nlower than the error from state-of-the-art models, which have to be trained for\nevery mesh separately. Additionally, we visualize the underlying dynamics of\nunseen mesh sequences with an autoencoder trained on different classes of\nmeshes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_S/0/1/0/all/0/1\">Sara Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1\">Jochen Garcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRFormer: High-Resolution Transformer for Dense Prediction. (arXiv:2110.09408v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09408","description":"<p>We present a High-Resolution Transformer (HRFormer) that learns\nhigh-resolution representations for dense prediction tasks, in contrast to the\noriginal Vision Transformer that produces low-resolution representations and\nhas high memory and computational cost. We take advantage of the\nmulti-resolution parallel design introduced in high-resolution convolutional\nnetworks (HRNet), along with local-window self-attention that performs\nself-attention over small non-overlapping image windows, for improving the\nmemory and computation efficiency. In addition, we introduce a convolution into\nthe FFN to exchange information across the disconnected image windows. We\ndemonstrate the effectiveness of the High-Resolution Transformer on both human\npose estimation and semantic segmentation tasks, e.g., HRFormer outperforms\nSwin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer\nparameters and $30\\%$ fewer FLOPs. Code is available at:\nhttps://github.com/HRNet/HRFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Rao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Monocular Depth Estimation with Internal Feature Fusion. (arXiv:2110.09482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09482","description":"<p>Self-supervised learning for depth estimation uses geometry in image\nsequences for supervision and shows promising results. Like many computer\nvision tasks, depth network performance is determined by the capability to\nlearn accurate spatial and semantic representations from images. Therefore, it\nis natural to exploit semantic segmentation networks for depth estimation. In\nthis work, based on a well-developed semantic segmentation network HRNet, we\npropose a novel depth estimation networkDIFFNet, which can make use of semantic\ninformation in down and upsampling procedures. By applying feature fusion and\nan attention mechanism, our proposed method outperforms the state-of-the-art\nmonocular depth estimation methods on the KITTI benchmark. Our method also\ndemonstrates greater potential on higher resolution training data. We propose\nan additional extended evaluation strategy by establishing a test set of\nchallenging cases, empirically derived from the standard benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwood_D/0/1/0/all/0/1\">David Greenwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_S/0/1/0/all/0/1\">Sarah Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images. (arXiv:2110.09744v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09744","description":"<p>Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral\nimages as the product of endmember and abundance, which has been widely used in\nhyperspectral imagery analysis. However, the influence of light, acquisition\nconditions and the inherent properties of materials, results in that the\nidentified endmembers can vary spectrally within a given image (construed as\nspectral variability). To address this issue, recent methods usually use a\npriori obtained spectral library to represent multiple characteristic spectra\nof the same object, but few of them extracted the spectral variability\nexplicitly. In this paper, a spectral variability augmented sparse unmixing\nmodel (SVASU) is proposed, in which the spectral variability is extracted for\nthe first time. The variable spectra are divided into two parts of intrinsic\nspectrum and spectral variability for spectral reconstruction, and modeled\nsynchronously in the SU model adding the regular terms restricting the sparsity\nof abundance and the generalization of the variability coefficient. It is noted\nthat the spectral variability library and the intrinsic spectral library are\nall constructed from the In-situ observed image. Experimental results over both\nsynthetic and real-world data sets demonstrate that the augmented decomposition\nby spectral variability significantly improves the unmixing performance than\nthe decomposition only by spectral library, as well as compared to\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_S/0/1/0/all/0/1\">Shaohui Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Mingyang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry. (arXiv:2110.09772v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09772","description":"<p>This work studies learning from a synergy process of 3D Morphable Models\n(3DMM) and 3D facial landmarks to predict complete 3D facial geometry,\nincluding 3D alignment, face orientation, and 3D face modeling. Our synergy\nprocess leverages a representation cycle for 3DMM parameters and 3D landmarks.\n3D landmarks can be extracted and refined from face meshes built by 3DMM\nparameters. We next reverse the representation direction and show that\npredicting 3DMM parameters from sparse 3D landmarks improves the information\nflow. Together we create a synergy process that utilizes the relation between\n3D landmarks and 3DMM parameters, and they collaboratively contribute to better\nperformance. We extensively validate our contribution on full tasks of facial\ngeometry prediction and show our superior and robust performance on these tasks\nfor various scenarios. Particularly, we adopt only simple and widely-used\nnetwork operations to attain fast and accurate facial geometry prediction.\nCodes and data: https://choyingw.github.io/works/SynergyNet/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cho-Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}