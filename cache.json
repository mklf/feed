{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How Can Graph Neural Networks Help Document Retrieval: A Case Study on CORD19 with Concept Map Generation. (arXiv:2201.04672v1 [cs.IR])","link":"http://arxiv.org/abs/2201.04672","description":"<p>Graph neural networks (GNNs), as a group of powerful tools for representation\nlearning on irregular data, have manifested superiority in various downstream\ntasks. With unstructured texts represented as concept maps, GNNs can be\nexploited for tasks like document retrieval. Intrigued by how can GNNs help\ndocument retrieval, we conduct an empirical study on a large-scale\nmulti-discipline dataset CORD-19. Results show that instead of the complex\nstructure-oriented GNNs such as GINs and GATs, our proposed semantics-oriented\ngraph functions achieve better and more stable performance based on the BM25\nretrieved candidates. Our insights in this case study can serve as a guideline\nfor future work to develop effective GNNs with appropriate semantics-oriented\ninductive biases for textual reasoning tasks like document retrieval and\nclassification. All code for this case study is available at\nhttps://github.com/HennyJie/GNN-DocRetrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents. (arXiv:2201.04723v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04723","description":"<p>At the heart of improving conversational AI is the open problem of how to\nevaluate conversations. Issues with automatic metrics are well known (Liu et\nal., 2016, <a href=\"/abs/1603.08023\">arXiv:1603.08023</a>), with human evaluations still considered the gold\nstandard. Unfortunately, how to perform human evaluations is also an open\nproblem: differing data collection methods have varying levels of human\nagreement and statistical sensitivity, resulting in differing amounts of human\nannotation hours and labor costs. In this work we compare five different\ncrowdworker-based human evaluation methods and find that different methods are\nbest depending on the types of models compared, with no clear winner across the\nboard. While this highlights the open problems in the area, our analysis leads\nto advice of when to use which one, and possible future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_O/0/1/0/all/0/1\">Orion Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rebecca Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing semantic relation in sentence pairs using Tree-RNNs and Typed dependencies. (arXiv:2201.04810v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04810","description":"<p>Recursive neural networks (Tree-RNNs) based on dependency trees are\nubiquitous in modeling sentence meanings as they effectively capture semantic\nrelationships between non-neighborhood words. However, recognizing semantically\ndissimilar sentences with the same words and syntax is still a challenge to\nTree-RNNs. This work proposes an improvement to Dependency Tree-RNN (DT-RNN)\nusing the grammatical relationship type identified in the dependency parse. Our\nexperiments on semantic relatedness scoring (SRS) and recognizing textual\nentailment (RTE) in sentence pairs using SICK (Sentence Involving Compositional\nKnowledge) dataset show encouraging results. The model achieved a 2%\nimprovement in classification accuracy for the RTE task over the DT-RNN model.\nThe results show that Pearson's and Spearman's correlation measures between the\nmodel's predicted similarity scores and human ratings are higher than those of\nstandard DT-RNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kleenankandy_J/0/1/0/all/0/1\">Jeena Kleenankandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazeer_K/0/1/0/all/0/1\">K A Abdul Nazeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Relation Extraction with Context Guided Mention Integration and Inter-pair Reasoning. (arXiv:2201.04826v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04826","description":"<p>Document-level Relation Extraction (DRE) aims to recognize the relations\nbetween two entities. The entity may correspond to multiple mentions that span\nbeyond sentence boundary. Few previous studies have investigated the mention\nintegration, which may be problematic because coreferential mentions do not\nequally contribute to a specific relation. Moreover, prior efforts mainly focus\non reasoning at entity-level rather than capturing the global interactions\nbetween entity pairs. In this paper, we propose two novel techniques, Context\nGuided Mention Integration and Inter-pair Reasoning (CGM2IR), to improve the\nDRE. Instead of simply applying average pooling, the contexts are utilized to\nguide the integration of coreferential mentions in a weighted sum manner.\nAdditionally, inter-pair reasoning executes an iterative algorithm on the\nentity pair graph, so as to model the interdependency of relations. We evaluate\nour CGM2IR model on three widely used benchmark datasets, namely DocRED, CDR,\nand GDA. Experimental results show that our model outperforms previous\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Daojian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jianhua Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis. (arXiv:2201.04831v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04831","description":"<p>Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment\nanalysis. To better comprehend long complicated sentences and obtain accurate\naspect-specific information, linguistic and commonsense knowledge are generally\nrequired in this task. However, most methods employ complicated and inefficient\napproaches to incorporate external knowledge, e.g., directly searching the\ngraph nodes. Additionally, the complementarity between external knowledge and\nlinguistic information has not been thoroughly studied. To this end, we propose\na knowledge graph augmented network (KGAN), which aims to effectively\nincorporate external knowledge with explicitly syntactic and contextual\ninformation. In particular, KGAN captures the sentiment feature representations\nfrom multiple different perspectives, i.e., context-, syntax- and\nknowledge-based. First, KGAN learns the contextual and syntactic\nrepresentations in parallel to fully extract the semantic features. Then, KGAN\nintegrates the knowledge graphs into the embedding space, based on which the\naspect-specific knowledge representations are further obtained via an attention\nmechanism. Last, we propose a hierarchical fusion module to complement these\nmultiview representations in a local-to-global manner. Extensive experiments on\nthree popular ABSA benchmarks demonstrate the effectiveness and robustness of\nour KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN\nachieves a new record of state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LP-BERT: Multi-task Pre-training Knowledge Graph BERT for Link Prediction. (arXiv:2201.04843v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04843","description":"<p>Link prediction plays an significant role in knowledge graph, which is an\nimportant resource for many artificial intelligence tasks, but it is often\nlimited by incompleteness. In this paper, we propose knowledge graph BERT for\nlink prediction, named LP-BERT, which contains two training stages: multi-task\npre-training and knowledge graph fine-tuning. The pre-training strategy not\nonly uses Mask Language Model (MLM) to learn the knowledge of context corpus,\nbut also introduces Mask Entity Model (MEM) and Mask Relation Model (MRM),\nwhich can learn the relationship information from triples by predicting\nsemantic based entity and relation elements. Structured triple relation\ninformation can be transformed into unstructured semantic information, which\ncan be integrated into the pre-training model together with context corpus\ninformation. In the fine-tuning phase, inspired by contrastive learning, we\ncarry out a triple-style negative sampling in sample batch, which greatly\nincreased the proportion of negative sampling while keeping the training time\nalmost unchanged. Furthermore, we propose a data augmentation method based on\nthe inverse relationship of triples to further increase the sample diversity.\nWe achieve state-of-the-art results on WN18RR and UMLS datasets, especially the\nHits@10 indicator improved by 5\\% from the previous state-of-the-art result on\nWN18RR dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Ming Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yukai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Data Analysis with Next-step Natural Language Query Recommendation. (arXiv:2201.04868v1 [cs.HC])","link":"http://arxiv.org/abs/2201.04868","description":"<p>Natural language interfaces (NLIs) provide users with a convenient way to\ninteractively analyze data through natural language queries. Nevertheless,\ninteractive data analysis is a demanding process, especially for novice data\nanalysts. When exploring large and complex datasets from different domains,\ndata analysts do not necessarily have sufficient knowledge about data and\napplication domains. It makes them unable to efficiently elicit a series of\nqueries and extensively derive desirable data insights. In this paper, we\ndevelop an NLI with a step-wise query recommendation module to assist users in\nchoosing appropriate next-step exploration actions. The system adopts a\ndata-driven approach to generate step-wise semantically relevant and\ncontext-aware query suggestions for application domains of users' interest\nbased on their query logs. Also, the system helps users organize query\nhistories and results into a dashboard to communicate the discovered data\ninsights. With a comparative user study, we show that our system can facilitate\na more effective and systematic data analysis process than a baseline without\nthe recommendation module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Furui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quadratic 0-1 Programming Approach for Word Sense Disambiguation. (arXiv:2201.04877v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04877","description":"<p>Word Sense Disambiguation (WSD) is the task to determine the sense of an\nambiguous word in a given context. Previous approaches for WSD have focused on\nsupervised and knowledge-based methods, but inter-sense interactions patterns\nor regularities for disambiguation remain to be found. We argue the following\ncause as one of the major difficulties behind finding the right patterns: for a\nparticular context, the intended senses of a sequence of ambiguous words are\ndependent on each other, i.e. the choice of one word's sense is associated with\nthe choice of another word's sense, making WSD a combinatorial optimization\nproblem.In this work, we approach the interactions between senses of different\ntarget words by a Quadratic 0-1 Integer Programming model (QIP) that maximizes\nthe objective function consisting of (1) the similarity between candidate\nsenses of a target word and the word in a context (the sense-word similarity),\nand (2) the semantic interactions (relatedness) between senses of all words in\nthe context (the sense-sense relatedness).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Boliang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Word Embeddings Using Syllables. (arXiv:2201.04913v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04913","description":"<p>This work examines the possibility of using syllable embeddings, instead of\nthe often used $n$-gram embeddings, as subword embeddings. We investigate this\nfor two languages: English and Dutch. To this end, we also translated two\nstandard English word embedding evaluation datasets, WordSim353 and\nSemEval-2017, to Dutch. Furthermore, we provide the research community with\ndata sets of syllabic decompositions for both languages. We compare our\napproach to full word and $n$-gram embeddings. Compared to full word\nembeddings, we obtain English models that are 20 to 30 times smaller while\nretaining 80% of the performance. For Dutch, models are 15 times smaller for\n70% performance retention. Although less accurate than the $n$-gram baseline we\nused, our models can be trained in a matter of minutes, as opposed to hours for\nthe $n$-gram approach. We identify a path toward upgrading performance in\nfuture work. All code is made publicly available, as well as our collected\nEnglish and Dutch syllabic decompositions and Dutch evaluation set\ntranslations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mertens_L/0/1/0/all/0/1\">Laurent Mertens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vennekens_J/0/1/0/all/0/1\">Joost Vennekens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automated Error Analysis: Learning to Characterize Errors. (arXiv:2201.05017v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05017","description":"<p>Characterizing the patterns of errors that a system makes helps researchers\nfocus future development on increasing its accuracy and robustness. We propose\na novel form of \"meta learning\" that automatically learns interpretable rules\nthat characterize the types of errors that a system makes, and demonstrate\nthese rules' ability to help understand and improve two NLP systems. Our\napproach works by collecting error cases on validation data, extracting\nmeta-features describing these samples, and finally learning rules that\ncharacterize errors using these features. We apply our approach to VilBERT, for\nVisual Question Answering, and RoBERTa, for Common Sense Question Answering.\nOur system learns interpretable rules that provide insights into systemic\nerrors these systems make on the given tasks. Using these insights, we are also\nable to \"close the loop\" and modestly improve performance of these systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shivang Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LARD: Large-scale Artificial Disfluency Generation. (arXiv:2201.05041v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05041","description":"<p>Disfluency detection is a critical task in real-time dialogue systems.\nHowever, despite its importance, it remains a relatively unexplored field,\nmainly due to the lack of appropriate datasets. At the same time, existing\ndatasets suffer from various issues, including class imbalance issues, which\ncan significantly affect the performance of the model on rare classes, as it is\ndemonstrated in this paper. To this end, we propose LARD, a method for\ngenerating complex and realistic artificial disfluencies with little effort.\nThe proposed method can handle three of the most common types of disfluencies:\nrepetitions, replacements and restarts. In addition, we release a new\nlarge-scale dataset with disfluencies that can be used on four different tasks:\ndisfluency detection, classification, extraction and correction. Experimental\nresults on the LARD dataset demonstrate that the data produced by the proposed\nmethod can be effectively used for detecting and removing disfluencies, while\nalso addressing limitations of existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">T. Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavropoulos_T/0/1/0/all/0/1\">T. Mavropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">G. Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meditskos_G/0/1/0/all/0/1\">G. Meditskos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrochidis_S/0/1/0/all/0/1\">S. Vrochidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Resources in the Tamasheq Language. (arXiv:2201.05051v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05051","description":"<p>In this paper we present two datasets for Tamasheq, a developing language\nmainly spoken in Mali and Niger. These two datasets were made available for the\nIWSLT 2022 low-resource speech translation track, and they consist of\ncollections of radio recordings from the Studio Kalangou (Niger) and Studio\nTamani (Mali) daily broadcast news. We share (i) a massive amount of unlabeled\naudio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,\nTamasheq and Zarma, and (ii) a smaller parallel corpus of audio recordings (17\nhours) in Tamasheq, with utterance-level translations in the French language.\nAll this data is shared under the Creative Commons BY-NC-ND 3.0 license. We\nhope these resources will inspire the speech community to develop and benchmark\nmodels using the Tamasheq language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougares_F/0/1/0/all/0/1\">Fethi Bougares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbier_F/0/1/0/all/0/1\">Florentin Barbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahbiche_S/0/1/0/all/0/1\">Souhir Gahbiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-rich multiplex lexical networks reveal mental strategies of early language learning. (arXiv:2201.05061v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05061","description":"<p>Knowledge in the human mind exhibits a dualistic vector/network nature.\nModelling words as vectors is key to natural language processing, whereas\nnetworks of word associations can map the nature of semantic memory. We\nreconcile these paradigms - fragmented across linguistics, psychology and\ncomputer science - by introducing FEature-Rich MUltiplex LEXical (FERMULEX)\nnetworks. This novel framework merges structural similarities in networks and\nvector features of words, which can be combined or explored independently.\nSimilarities model heterogenous word associations across\nsemantic/syntactic/phonological aspects of knowledge. Words are enriched with\nmulti-dimensional feature embeddings including frequency, age of acquisition,\nlength and polysemy. These aspects enable unprecedented explorations of\ncognitive knowledge. Through CHILDES data, we use FERMULEX networks to model\nnormative language acquisition by 1000 toddlers between 18 and 30 months.\nSimilarities and embeddings capture word homophily via conformity, which\nmeasures assortative mixing via distance and features. Conformity unearths a\nlanguage kernel of frequent/polysemous/short nouns and verbs key for basic\nsentence production, supporting recent evidence of children's syntactic\nconstructs emerging at 30 months. This kernel is invisible to network\ncore-detection and feature-only clustering: It emerges from the dual\nvector/network nature of words. Our quantitative analysis reveals two key\nstrategies in early word learning. Modelling word acquisition as random walks\non FERMULEX topology, we highlight non-uniform filling of communicative\ndevelopmental inventories (CDIs). Conformity-based walkers lead to accurate\n(75%), precise (55%) and partially well-recalled (34%) predictions of early\nword learning in CDIs, providing quantitative support to previous empirical\nfindings and developmental theories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Citraro_S/0/1/0/all/0/1\">Salvatore Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitevitch_M/0/1/0/all/0/1\">Michael S. Vitevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetti_G/0/1/0/all/0/1\">Giulio Rossetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation. (arXiv:2201.05088v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05088","description":"<p>Interpreting the predictions of existing Question Answering (QA) models is\ncritical to many real-world intelligent applications, such as QA systems for\nhealthcare, education, and finance. However, existing QA models lack\ninterpretability and provide no feedback or explanation for end-users to help\nthem understand why a specific prediction is the answer to a question.In this\nresearch, we argue that the evidences of an answer is critical to enhancing the\ninterpretability of QA models. Unlike previous research that simply extracts\nseveral sentence(s) in the context as evidence, we are the first to explicitly\ndefine the concept of evidence as the supporting facts in a context which are\ninformative, concise, and readable. Besides, we provide effective strategies to\nquantitatively measure the informativeness, conciseness and readability of\nevidence. Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED)\nalgorithm to extract evidences from the contexts by trade-off informativeness,\nconciseness, and readability. We conduct extensive experiments on the SQuAD and\nTriviaQA datasets with several baseline models to evaluate the effect of GCED\non interpreting answers to questions. Human evaluation are also carried out to\ncheck the quality of distilled evidences. Experimental results show that\nautomatic distilled evidences have human-like informativeness, conciseness and\nreadability, which can enhance the interpretability of the answers to\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NorDiaChange: Diachronic Semantic Change Dataset for Norwegian. (arXiv:2201.05123v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05123","description":"<p>We describe NorDiaChange: the first diachronic semantic change dataset for\nNorwegian. NorDiaChange comprises two novel subsets, covering about 80\nNorwegian nouns manually annotated with graded semantic change over time. Both\ndatasets follow the same annotation procedure and can be used interchangeably\nas train and test splits for each other. NorDiaChange covers the time periods\nrelated to pre- and post-war events, oil and gas discovery in Norway, and\ntechnological developments. The annotation was done using the DURel framework\nand two large historical Norwegian corpora. NorDiaChange is published in full\nunder a permissive license, complete with raw annotation data and inferred\ndiachronic word usage graphs (DWUGs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maehlum_P/0/1/0/all/0/1\">Petter M&#xe6;hlum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enstad_T/0/1/0/all/0/1\">Tita Ranveig Enstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittemann_A/0/1/0/all/0/1\">Alexandra Wittemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09691","description":"<p>In 2018, Mikolov et al. introduced the positional language model, which has\ncharacteristics of attention-based neural machine translation models and which\nachieved state-of-the-art performance on the intrinsic word analogy task.\nHowever, the positional model is not practically fast and it has never been\nevaluated on qualitative criteria or extrinsic tasks. We propose a constrained\npositional model, which adapts the sparse attention mechanism from neural\nmachine translation to improve the speed of the positional model. We evaluate\nthe positional and constrained positional models on three novel qualitative\ncriteria and on language modeling. We show that the positional and constrained\npositional models contain interpretable information about the grammatical\nproperties of words and outperform other shallow models on language modeling.\nWe also show that our constrained model outperforms the positional model on\nlanguage modeling and trains twice as fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reh%5Cr%7Bu%7Drek_R/0/1/0/all/0/1\">Radim &#x158;eh&#x16f;&#x159;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Ackermannian lower bound for the Petri nets reachability problem. (arXiv:2105.08551v4 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2105.08551","description":"<p>Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n</p>\n<p>Our primary contribution is an improvement of the former construction, making\nit conceptually simpler and more direct. On the way we improve the lower bound\nfor vector addition systems with states in fixed dimension (or, equivalently,\nPetri nets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski\nprove $F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in\ndimension $6k$, our simplified construction yields $F_k$-hardness already in\ndimension $3k+2$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05426","description":"<p>How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07402","description":"<p>We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statistical Model of Word Rank Evolution. (arXiv:2107.09948v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09948","description":"<p>The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. The Google Books corpus unigram frequency data set\nis used to investigate the word rank dynamics in eight languages. We observed\nthe rank changes of the unigrams from 1900 to 2008 and compared it to a\nWright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing and added words. This work explains the mathematical framework of\nthe model - written as a Markov Chain with multinomial transition probabilities\n- to show how frequencies of words change in time. From our observations in the\ndata and our model, word rank stability shows two types of characteristics: (1)\nthe increase/decrease in ranks are monotonic, or (2) the rank stays the same.\nBased on our model, high-ranked words tend to be more stable while low-ranked\nwords tend to be more volatile. Some words change in ranks in two ways: (a) by\nan accumulation of small increasing/decreasing rank changes in time and (b) by\nshocks of increase/decrease in ranks. Most words in all of the languages we\nhave looked at are rank stable, but not as stable as a neutral model would\npredict. The stopwords and Swadesh words are observed to be rank stable across\neight languages indicating linguistic conformity in established languages.\nThese signatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quijano_A/0/1/0/all/0/1\">Alex John Quijano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_R/0/1/0/all/0/1\">Rick Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindi_S/0/1/0/all/0/1\">Suzanne Sindi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05090","description":"<p>Neural language modelling has progressed the state-of-the-art in different\ndownstream Natural Language Processing (NLP) tasks. One such area is of\nopen-domain dialog modelling, neural dialog models based on GPT-2 such as\nDialoGPT have shown promising performance in single-turn conversation. However,\nsuch (neural) dialog models have been criticized for generating responses which\nalthough may have relevance to the previous human response, tend to quickly\ndissipate human interest and descend into trivial conversation. One reason for\nsuch performance is the lack of explicit conversation strategy being employed\nin human-machine conversation. Humans employ a range of conversation strategies\nwhile engaging in a conversation, one such key social strategies is\nSelf-disclosure(SD). A phenomenon of revealing information about one-self to\nothers. Social penetration theory (SPT) proposes that communication between two\npeople moves from shallow to deeper levels as the relationship progresses\nprimarily through self-disclosure. Disclosure helps in creating rapport among\nthe participants engaged in a conversation. In this paper, Self-disclosure\nenhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic\nModel (SDTM) during inference stage of a neural dialog model to re-rank\nresponse candidates to enhance self-disclosure in single-turn responses from\nfrom the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1\">Benjamin Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Simultaneous Speech-to-Speech Translation with Variational Monotonic Multihead Attention. (arXiv:2110.08250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08250","description":"<p>We present a direct simultaneous speech-to-speech translation (Simul-S2ST)\nmodel, Furthermore, the generation of translation is independent from\nintermediate text representations. Our approach leverages recent progress on\ndirect speech-to-speech translation with discrete units, in which a sequence of\ndiscrete representations, instead of continuous spectrogram features, learned\nin an unsupervised manner, are predicted from the model and passed directly to\na vocoder for speech synthesis on-the-fly. We also introduce the variational\nmonotonic multihead attention (V-MMA), to handle the challenge of inefficient\npolicy learning in speech simultaneous translation. The simultaneous policy\nthen operates on source speech features and target discrete units. We carry out\nempirical studies to compare cascaded and direct approach on the Fisher\nSpanish-English and MuST-C English-Spanish datasets. Direct simultaneous model\nis shown to outperform the cascaded model by achieving a better tradeoff\nbetween translation quality and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Phillip Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AequeVox: Automated Fairness Testing of Speech Recognition Systems. (arXiv:2110.09843v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09843","description":"<p>Automatic Speech Recognition (ASR) systems have become ubiquitous. They can\nbe found in a variety of form factors and are increasingly important in our\ndaily lives. As such, ensuring that these systems are equitable to different\nsubgroups of the population is crucial. In this paper, we introduce, AequeVox,\nan automated testing framework for evaluating the fairness of ASR systems.\nAequeVox simulates different environments to assess the effectiveness of ASR\nsystems for different populations. In addition, we investigate whether the\nchosen simulations are comprehensible to humans. We further propose a fault\nlocalization technique capable of identifying words that are not robust to\nthese varying environments. Both components of AequeVox are able to operate in\nthe absence of ground truth data.\n</p>\n<p>We evaluated AequeVox on speech from four different datasets using three\ndifferent commercial ASRs. Our experiments reveal that non-native English,\nfemale and Nigerian English speakers generate 109%, 528.5% and 156.9% more\nerrors, on average than native English, male and UK Midlands speakers,\nrespectively. Our user study also reveals that 82.9% of the simulations\n(employed through speech transformations) had a comprehensibility rating above\nseven (out of ten), with the lowest rating being 6.78. This further validates\nthe fairness violations discovered by AequeVox. Finally, we show that the\nnon-robust words, as predicted by the fault localization technique embodied in\nAequeVox, show 223.8% more errors than the predicted robust words across all\nASRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1\">Sai Sathiesh Rajan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1\">Sakshi Udeshi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a> (1) ((1) Singapore University of Technology and Design)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets. (arXiv:2111.06467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06467","description":"<p>NLP researchers need more, higher-quality text datasets. Human-labeled\ndatasets are expensive to collect, while datasets collected via automatic\nretrieval from the web such as WikiBio are noisy and can include undesired\nbiases. Moreover, data sourced from the web is often included in datasets used\nto pretrain models, leading to inadvertent cross-contamination of training and\ntest sets. In this work we introduce a novel method for efficient dataset\ncuration: we use a large language model to provide seed generations to human\nraters, thereby changing dataset authoring from a writing task to an editing\ntask. We use our method to curate SynthBio - a new evaluation set for WikiBio -\ncomposed of structured attribute lists describing fictional individuals, mapped\nto natural language biographies. We show that our dataset of fictional\nbiographies is less noisy than WikiBio, and also more balanced with respect to\ngender and nationality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emojich -- zero-shot emoji generation using Russian language: a technical report. (arXiv:2112.02448v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02448","description":"<p>This technical report presents a text-to-image neural network \"Emojich\" that\ngenerates emojis using captions in Russian language as a condition. We aim to\nkeep the generalization ability of a pretrained big model ruDALL-E Malevich\n(XL) 1.3B parameters at the fine-tuning stage, while giving special style to\nthe images generated. Here are presented some engineering methods, code\nrealization, all hyper-parameters for reproducing results and a Telegram bot\nwhere everyone can create their own customized sets of stickers. Also, some\nnewly generated emojis obtained by \"Emojich\" model are demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1\">Daria Bakshandaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolich_A/0/1/0/all/0/1\">Aleksandr Nikolich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15417","description":"<p>Due to the exponentially increasing reach of social media, it is essential to\nfocus on its negative aspects as it can potentially divide society and incite\npeople into violence. In this paper, we present our system description of work\non the shared task ComMA@ICON, where we have to classify how aggressive the\nsentence is and if the sentence is gender-biased or communal biased. These\nthree could be the primary reasons to cause significant problems in society. As\nteam Hypers we have proposed an approach that utilizes different pretrained\nmodels with Attention and mean pooling methods. We were able to get Rank 3 with\n0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on\nMulti-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5\nwith 0.336 Instance F1 score on Hindi. The source code and the pretrained\nmodels of this work can be found here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Roshan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navaneethakrishnan_S/0/1/0/all/0/1\">Subalalitha Chinnaudayar Navaneethakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning. (arXiv:2201.03110v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03110","description":"<p>Achieving universal translation between all human language pairs is the\nholy-grail of machine translation (MT) research. While recent progress in\nmassively multilingual MT is one step closer to reaching this goal, it is\nbecoming evident that extending a multilingual MT system simply by training on\nmore parallel data is unscalable, since the availability of labeled data for\nlow-resource and non-English-centric language pairs is forbiddingly limited. To\nthis end, we present a pragmatic approach towards building a multilingual MT\nmodel that covers hundreds of languages, using a mixture of supervised and\nself-supervised objectives, depending on the data availability for different\nlanguage pairs. We demonstrate that the synergy between these two training\nparadigms enables the model to produce high-quality translations in the\nzero-resource setting, even surpassing supervised translation quality for low-\nand mid-resource languages. We conduct a wide array of experiments to\nunderstand the effect of the degree of multilingual supervision, domain\nmismatches and amounts of parallel and monolingual data on the quality of our\nself-supervised multilingual models. To demonstrate the scalability of the\napproach, we train models with over 200 languages and demonstrate high\nperformance on zero-resource translation on several previously under-studied\nlanguages. We hope our findings will serve as a stepping stone towards enabling\ntranslation for the next thousand languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Early Diagnosis of Parkinsons Disease by Analyzing Magnetic Resonance Imaging Brain Scans and Patient Characteristics. (arXiv:2201.04631v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04631","description":"<p>Parkinsons disease, PD, is a chronic condition that affects motor skills and\nincludes symptoms like tremors and rigidity. The current diagnostic procedure\nuses patient assessments to evaluate symptoms and sometimes a magnetic\nresonance imaging or MRI scan. However, symptom variations cause inaccurate\nassessments, and the analysis of MRI scans requires experienced specialists.\nThis research proposes to accurately diagnose PD severity with deep learning by\ncombining symptoms data and MRI data from the Parkinsons Progression Markers\nInitiative database. A new hybrid model architecture was implemented to fully\nutilize both forms of clinical data, and models based on only symptoms and only\nMRI scans were also developed. The symptoms based model integrates a fully\nconnected deep learning neural network, and the MRI scans and hybrid models\nintegrate transfer learning based convolutional neural networks. Instead of\nperforming only binary classification, all models diagnose patients into five\nseverity categories, with stage zero representing healthy patients and stages\nfour and five representing patients with PD. The symptoms only, MRI scans only,\nand hybrid models achieved accuracies of 0.77, 0.68, and 0.94, respectively.\nThe hybrid model also had high precision and recall scores of 0.94 and 0.95.\nReal clinical cases confirm the strong performance of the hybrid, where\npatients were classified incorrectly with both other models but correctly by\nthe hybrid. It is also consistent across the five severity stages, indicating\naccurate early detection. This is the first report to combine symptoms data and\nMRI scans with a machine learning approach on such a large scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1\">Sabrina Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniformer: Unified Transformer for Efficient Spatiotemporal Representation Learning. (arXiv:2201.04676v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04676","description":"<p>It is a challenging task to learn rich and multi-scale spatiotemporal\nsemantics from high-dimensional videos, due to large local redundancy and\ncomplex global dependency between video frames. The recent advances in this\nresearch have been mainly driven by 3D convolutional neural networks and vision\ntransformers. Although 3D convolution can efficiently aggregate local context\nto suppress local redundancy from a small 3D neighborhood, it lacks the\ncapability to capture global dependency because of the limited receptive field.\nAlternatively, vision transformers can effectively capture long-range\ndependency by self-attention mechanism, while having the limitation on reducing\nlocal redundancy with blind similarity comparison among all the tokens in each\nlayer. Based on these observations, we propose a novel Unified transFormer\n(UniFormer) which seamlessly integrates merits of 3D convolution and\nspatiotemporal self-attention in a concise transformer format, and achieves a\npreferable balance between computation and accuracy. Different from traditional\ntransformers, our relation aggregator can tackle both spatiotemporal redundancy\nand dependency, by learning local and global token affinity respectively in\nshallow and deep layers. We conduct extensive experiments on the popular video\nbenchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&amp;V2.\nWith only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1\naccuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than\nother state-of-the-art methods. For Something-Something V1 and V2, our\nUniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1\naccuracy respectively. Code is available at\nhttps://github.com/Sense-X/UniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations. (arXiv:2201.04684v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04684","description":"<p>Annotating images with pixel-wise labels is a time-consuming and costly\nprocess. Recently, DatasetGAN showcased a promising alternative - to synthesize\na large labeled dataset via a generative adversarial network (GAN) by\nexploiting a small set of manually labeled, GAN-generated images. Here, we\nscale DatasetGAN to ImageNet scale of class diversity. We take image samples\nfrom the class-conditional generative model BigGAN trained on ImageNet, and\nmanually annotate 5 images per class, for all 1k classes. By training an\neffective feature segmentation architecture on top of BigGAN, we turn BigGAN\ninto a labeled dataset generator. We further show that VQGAN can similarly\nserve as a dataset generator, leveraging the already annotated data. We create\na new ImageNet benchmark by labeling an additional set of 8k real images and\nevaluate segmentation performance in a variety of settings. Through an\nextensive ablation study we show big gains in leveraging a large generated\ndataset to train different supervised and self-supervised backbone models on\npixel-wise tasks. Furthermore, we demonstrate that using our synthesized\ndatasets for pre-training leads to improvements over standard ImageNet\npre-training on several downstream datasets, such as PASCAL-VOC, MS-COCO,\nCityscapes and chest X-ray, as well as tasks (detection, segmentation). Our\nbenchmark will be made public and maintain a leaderboard for this challenging\ntask. Project Page: https://nv-tlabs.github.io/big-datasetgan/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Huan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barriuso_A/0/1/0/all/0/1\">Adela Barriuso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Labeling of Human Action For Visually Impaired And Blind People Scene Interaction. (arXiv:2201.04706v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04706","description":"<p>The aim of this work is to contribute to the development of a tactile device\nfor visually impaired and blind persons in order to let them to understand\nactions of the surrounding people and to interact with them. First, based on\nthe state-of-the-art methods of human action recognition from RGB-D sequences,\nwe use the skeleton information provided by Kinect, with the disentangled and\nunified multi-scale Graph Convolutional (MS-G3D) model to recognize the\nperformed actions. We tested this model on real scenes and found some of\nconstraints and limitations. Next, we apply a fusion between skeleton modality\nwith MS-G3D and depth modality with CNN in order to bypass the discussed\nlimitations. Third, the recognized actions are labeled semantically and will be\nmapped into an output device perceivable by the touch sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhamida_L/0/1/0/all/0/1\">Leyla Benhamida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_S/0/1/0/all/0/1\">Slimane Larabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial-Attribution Instance Segmentation for Astronomical Source Detection and Deblending. (arXiv:2201.04714v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2201.04714","description":"<p>Astronomical source deblending is the process of separating the contribution\nof individual stars or galaxies (sources) to an image comprised of multiple,\npossibly overlapping sources. Astronomical sources display a wide range of\nsizes and brightnesses and may show substantial overlap in images. Astronomical\nimaging data can further challenge off-the-shelf computer vision algorithms\nowing to its high dynamic range, low signal-to-noise ratio, and unconventional\nimage format. These challenges make source deblending an open area of\nastronomical research, and in this work, we introduce a new approach called\nPartial-Attribution Instance Segmentation that enables source detection and\ndeblending in a manner tractable for deep learning models. We provide a novel\nneural network implementation as a demonstration of the method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Hausen_R/0/1/0/all/0/1\">Ryan Hausen</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Robertson_B/0/1/0/all/0/1\">Brant Robertson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially Robust Classification by Conditional Generative Model Inversion. (arXiv:2201.04733v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04733","description":"<p>Most adversarial attack defense methods rely on obfuscating gradients. These\nmethods are successful in defending against gradient-based attacks; however,\nthey are easily circumvented by attacks which either do not use the gradient or\nby attacks which approximate and use the corrected gradient. Defenses that do\nnot obfuscate gradients such as adversarial training exist, but these\napproaches generally make assumptions about the attack such as its magnitude.\nWe propose a classification model that does not obfuscate gradients and is\nrobust by construction without assuming prior knowledge about the attack. Our\nmethod casts classification as an optimization problem where we \"invert\" a\nconditional generator trained on unperturbed, natural images to find the class\nthat generates the closest sample to the query image. We hypothesize that a\npotential source of brittleness against adversarial attacks is the\nhigh-to-low-dimensional nature of feed-forward classifiers which allows an\nadversary to find small perturbations in the input space that lead to large\nchanges in the output space. On the other hand, a generative model is typically\na low-to-high-dimensional mapping. While the method is related to Defense-GAN,\nthe use of a conditional generative model and inversion in our model instead of\nthe feed-forward classifier is a critical difference. Unlike Defense-GAN, which\nwas shown to generate obfuscated gradients that are easily circumvented, we\nshow that our method does not obfuscate gradients. We demonstrate that our\nmodel is extremely robust against black-box attacks and has improved robustness\nagainst white-box attacks compared to naturally trained, feed-forward\nclassifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alirezaei_M/0/1/0/all/0/1\">Mitra Alirezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Map Vehicle Trajectory Detection Using Dynamic Mode Decomposition and Res-UNet+ Neural Networks. (arXiv:2201.04755v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04755","description":"<p>This paper presents a machine-learning-enhanced longitudinal scanline method\nto extract vehicle trajectories from high-angle traffic cameras. The Dynamic\nMode Decomposition (DMD) method is applied to extract vehicle strands by\ndecomposing the Spatial-Temporal Map (STMap) into the sparse foreground and\nlow-rank background. A deep neural network named Res-UNet+ was designed for the\nsemantic segmentation task by adapting two prevalent deep learning\narchitectures. The Res-UNet+ neural networks significantly improve the\nperformance of the STMap-based vehicle detection, and the DMD model provides\nmany interesting insights for understanding the evolution of underlying\nspatial-temporal structures preserved by STMap. The model outputs were compared\nwith the previous image processing model and mainstream semantic segmentation\ndeep neural networks. After a thorough evaluation, the model is proved to be\naccurate and robust against many challenging factors. Last but not least, this\npaper fundamentally addressed many quality issues found in NGSIM trajectory\ndata. The cleaned high-quality trajectory data are published to support future\ntheoretical and modeling research on traffic flow and microscopic vehicle\ncontrol. This method is a reliable solution for video-based trajectory\nextraction and has wide applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya T. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04756","description":"<p>In this paper, we present the solution of roadside LiDAR object detection\nusing a combination of two unsupervised learning algorithms. The 3D point\nclouds data are firstly converted into spherical coordinates and filled into\nthe azimuth grid matrix using a hash function. After that, the raw LiDAR data\nwere rearranged into spatial-temporal data structures to store the information\nof range, azimuth, and intensity. Dynamic Mode Decomposition method is applied\nfor decomposing the point cloud data into low-rank backgrounds and sparse\nforegrounds based on intensity channel pattern recognition. The Triangle\nAlgorithm automatically finds the dividing value to separate the moving targets\nfrom static background according to range information. After intensity and\nrange background subtraction, the foreground moving objects will be detected\nusing a density-based detector and encoded into the state-space model for\ntracking. The output of the proposed model includes vehicle trajectories that\ncan enable many mobility and safety applications. The method was validated\nagainst a commercial traffic data collection platform and demonstrated to be an\nefficient and reliable solution for infrastructure LiDAR object detection. In\ncontrast to the previous methods that process directly on the scattered and\ndiscrete point clouds, the proposed method can establish the less sophisticated\nlinear relationship of the 3D measurement data, which captures the\nspatial-temporal structure that we often desire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collision Detection: An Improved Deep Learning Approach Using SENet and ResNext. (arXiv:2201.04766v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04766","description":"<p>In recent days, with increased population and traffic on roadways, vehicle\ncollision is one of the leading causes of death worldwide. The automotive\nindustry is motivated on developing techniques to use sensors and advancements\nin the field of computer vision to build collision detection and collision\nprevention systems to assist drivers. In this article, a deep-learning-based\nmodel comprising of ResNext architecture with SENet blocks is proposed. The\nperformance of the model is compared to popular deep learning models like\nVGG16, VGG19, Resnet50, and stand-alone ResNext. The proposed model outperforms\nthe existing baseline models achieving a ROC-AUC of 0.91 using a significantly\nless proportion of the GTACrash synthetic data for training, thus reducing the\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aditya_A/0/1/0/all/0/1\">Aloukik Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liudu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vachhani_H/0/1/0/all/0/1\">Hrishika Vachhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1\">Dhivya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAg: a simple learning-based patient-level aggregation method for detecting microsatellite instability from whole-slide images. (arXiv:2201.04769v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04769","description":"<p>The prediction of microsatellite instability (MSI) and microsatellite\nstability (MSS) is essential in predicting both the treatment response and\nprognosis of gastrointestinal cancer. In clinical practice, a universal MSI\ntesting is recommended, but the accessibility of such a test is limited. Thus,\na more cost-efficient and broadly accessible tool is desired to cover the\ntraditionally untested patients. In the past few years, deep-learning-based\nalgorithms have been proposed to predict MSI directly from haematoxylin and\neosin (H&amp;E)-stained whole-slide images (WSIs). Such algorithms can be\nsummarized as (1) patch-level MSI/MSS prediction, and (2) patient-level\naggregation. Compared with the advanced deep learning approaches that have been\nemployed for the first stage, only the na\\\"ive first-order statistics (e.g.,\naveraging and counting) were employed in the second stage. In this paper, we\npropose a simple yet broadly generalizable patient-level MSI aggregation (MAg)\nmethod to effectively integrate the precious patch-level information. Briefly,\nthe entire probabilistic distribution in the first stage is modeled as\nhistogram-based features to be fused as the final outcome with machine learning\n(e.g., SVM). The proposed MAg method can be easily used in a plug-and-play\nmanner, which has been evaluated upon five broadly used deep neural networks:\nResNet, MobileNetV2, EfficientNet, Dpn and ResNext. From the results, the\nproposed MAg method consistently improves the accuracy of patient-level\naggregation for two publicly available datasets. It is our hope that the\nproposed method could potentially leverage the low-cost H&amp;E based MSI detection\nmethod. The code of our work has been made publicly available at\nhttps://github.com/Calvin-Pang/MAg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pang_K/0/1/0/all/0/1\">Kaifeng Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shilin Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking large-scale crop field delineation in smallholder farming systems with transfer learning and weak supervision. (arXiv:2201.04771v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04771","description":"<p>Crop field boundaries aid in mapping crop types, predicting yields, and\ndelivering field-scale analytics to farmers. Recent years have seen the\nsuccessful application of deep learning to delineating field boundaries in\nindustrial agricultural systems, but field boundary datasets remain missing in\nsmallholder systems due to (1) small fields that require high resolution\nsatellite imagery to delineate and (2) a lack of ground labels for model\ntraining and validation. In this work, we combine transfer learning and weak\nsupervision to overcome these challenges, and we demonstrate the methods'\nsuccess in India where we efficiently generated 10,000 new field labels. Our\nbest model uses 1.5m resolution Airbus SPOT imagery as input, pre-trains a\nstate-of-the-art neural network on France field boundaries, and fine-tunes on\nIndia labels to achieve a median Intersection over Union (IoU) of 0.86 in\nIndia. If using 4.8m resolution PlanetScope imagery instead, the best model\nachieves a median IoU of 0.72. Experiments also show that pre-training in\nFrance reduces the number of India field labels needed to achieve a given\nperformance level by as much as $20\\times$ when datasets are small. These\nfindings suggest our method is a scalable approach for delineating crop fields\nin regions of the world that currently lack field boundary datasets. We\npublicly release the 10,000 labels and delineation model to facilitate the\ncreation of field boundary maps and new methods by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sherrie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldner_F/0/1/0/all/0/1\">Francois Waldner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Masked Facial Detection Methods and Datasets for Fighting Against COVID-19. (arXiv:2201.04777v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04777","description":"<p>Coronavirus disease 2019 (COVID-19) continues to pose a great challenge to\nthe world since its outbreak. To fight against the disease, a series of\nartificial intelligence (AI) techniques are developed and applied to real-world\nscenarios such as safety monitoring, disease diagnosis, infection risk\nassessment, lesion segmentation of COVID-19 CT scans,etc. The coronavirus\nepidemics have forced people wear masks to counteract the transmission of\nvirus, which also brings difficulties to monitor large groups of people wearing\nmasks. In this paper, we primarily focus on the AI techniques of masked facial\ndetection and related datasets. We survey the recent advances, beginning with\nthe descriptions of masked facial detection datasets. Thirteen available\ndatasets are described and discussed in details. Then, the methods are roughly\ncategorized into two classes: conventional methods and neural network-based\nmethods. Conventional methods are usually trained by boosting algorithms with\nhand-crafted features, which accounts for a small proportion. Neural\nnetwork-based methods are further classified as three parts according to the\nnumber of processing stages. Representative algorithms are described in detail,\ncoupled with some typical techniques that are described briefly. Finally, we\nsummarize the recent benchmarking results, give the discussions on the\nlimitations of datasets and methods, and expand future research directions. To\nour knowledge, this is the first survey about masked facial detection methods\nand datasets. Hopefully our survey could provide some help to fight against\nepidemics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingshu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiangbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">C.L. Philip Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Singapore Trusted Media Challenge Dataset. (arXiv:2201.04788v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04788","description":"<p>The development of powerful deep learning technologies has brought about some\nnegative effects to both society and individuals. One such issue is the\nemergence of fake media. To tackle the issue, we have organized the Trusted\nMedia Challenge (TMC) to explore how Artificial Intelligence (AI) technologies\ncould be leveraged to combat fake media.\n</p>\n<p>Together with the challenge, we have released a challenge dataset which\nconsists of 4,380 fake and 2,563 real videos. All these videos are accompanied\nwith audios and different video and/or audio manipulation methods are adopted\nto produce different types of fake media. The videos in the dataset have\nvarious durations, background, illumination, a minimum resolution of 360p and\nmay contain perturbations that mimic transmission errors and bad compression.\n</p>\n<p>We have also carried out a user study to demonstrate the quality of our\ncomposed dataset. The results show that our dataset has a promising quality and\ncan fool human participants in many cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_B/0/1/0/all/0/1\">Benjamin Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMT-NET: Efficient multitask network for computer-aided diagnosis of breast cancer. (arXiv:2201.04795v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04795","description":"<p>Deep learning-based computer-aided diagnosis has achieved unprecedented\nperformance in breast cancer detection. However, most approaches are\ncomputationally intensive, which impedes their broader dissemination in\nreal-world applications. In this work, we propose an efficient and\nlight-weighted multitask learning architecture to classify and segment breast\ntumors simultaneously. We incorporate a segmentation task into a tumor\nclassification network, which makes the backbone network learn representations\nfocused on tumor regions. Moreover, we propose a new numerically stable loss\nfunction that easily controls the balance between the sensitivity and\nspecificity of cancer detection. The proposed approach is evaluated using a\nbreast ultrasound dataset with 1,511 images. The accuracy, sensitivity, and\nspecificity of tumor classification is 88.6%, 94.1%, and 85.3%, respectively.\nWe validate the model using a virtual mobile device, and the average inference\ntime is 0.35 seconds per image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiaqiao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_J/0/1/0/all/0/1\">Jianrui Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_C/0/1/0/all/0/1\">Chunping Ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation. (arXiv:2201.04796v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04796","description":"<p>Recently, there is growing attention on one-stage panoptic segmentation\nmethods which aim to segment instances and stuff jointly within a fully\nconvolutional pipeline efficiently. However, most of the existing works\ndirectly feed the backbone features to various segmentation heads ignoring the\ndemands for semantic and instance segmentation are different: The former needs\nsemantic-level discriminative features, while the latter requires features to\nbe distinguishable across instances. To alleviate this, we propose to first\npredict semantic-level and instance-level correlations among different\nlocations that are utilized to enhance the backbone features, and then feed the\nimproved discriminative features into the corresponding segmentation heads,\nrespectively. Specifically, we organize the correlations between a given\nlocation and all locations as a continuous sequence and predict it as a whole.\nConsidering that such a sequence can be extremely complicated, we adopt\nDiscrete Fourier Transform (DFT), a tool that can approximate an arbitrary\nsequence parameterized by amplitudes and phrases. For different tasks, we\ngenerate these parameters from the backbone features in a fully convolutional\nway which is optimized implicitly by corresponding tasks. As a result, these\naccurate and consistent correlations contribute to producing plausible\ndiscriminative features which meet the requirements of the complicated panoptic\nsegmentation task. To verify the effectiveness of our methods, we conduct\nexperiments on several challenging panoptic segmentation datasets and achieve\nstate-of-the-art performance on MS COCO with $45.1$\\% PQ and ADE20k with\n$32.6$\\% PQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenqing Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangfang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhenye Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Liang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Cluster-Consistency Statistics for Robust Multi-Object Matching. (arXiv:2201.04797v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04797","description":"<p>We develop new statistics for robustly filtering corrupted keypoint matches\nin the structure from motion pipeline. The statistics are based on consistency\nconstraints that arise within the clustered structure of the graph of keypoint\nmatches. The statistics are designed to give smaller values to corrupted\nmatches and than uncorrupted matches. These new statistics are combined with an\niterative reweighting scheme to filter keypoints, which can then be fed into\nany standard structure from motion pipeline. This filtering method can be\nefficiently implemented and scaled to massive datasets as it only requires\nsparse matrix multiplication. We demonstrate the efficacy of this method on\nsynthetic and real structure from motion datasets and show that it achieves\nstate-of-the-art accuracy and speed in these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maunu_T/0/1/0/all/0/1\">Tyler Maunu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RealGait: Gait Recognition for Person Re-Identification. (arXiv:2201.04806v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04806","description":"<p>Human gait is considered a unique biometric identifier which can be acquired\nin a covert manner at a distance. However, models trained on existing public\ndomain gait datasets which are captured in controlled scenarios lead to drastic\nperformance decline when applied to real-world unconstrained gait data. On the\nother hand, video person re-identification techniques have achieved promising\nperformance on large-scale publicly available datasets. Given the diversity of\nclothing characteristics, clothing cue is not reliable for person recognition\nin general. So, it is actually not clear why the state-of-the-art person\nre-identification methods work as well as they do. In this paper, we construct\na new gait dataset by extracting silhouettes from an existing video person\nre-identification challenge which consists of 1,404 persons walking in an\nunconstrained manner. Based on this dataset, a consistent and comparative study\nbetween gait recognition and person re-identification can be carried out. Given\nthat our experimental results show that current gait recognition approaches\ndesigned under data collected in controlled scenarios are inappropriate for\nreal surveillance scenarios, we propose a novel gait recognition method, called\nRealGait. Our results suggest that recognizing people by their gait in real\nsurveillance scenarios is feasible and the underlying gait pattern is probably\nthe true reason why video person re-idenfification works in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoxiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1\">Tianrui Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Variational Autoencoder with Balanced Pre-training for Generative Adversarial Networks. (arXiv:2201.04809v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04809","description":"<p>Class imbalance occurs in many real-world applications, including image\nclassification, where the number of images in each class differs significantly.\nWith imbalanced data, the generative adversarial networks (GANs) leans to\nmajority class samples. The two recent methods, Balancing GAN (BAGAN) and\nimproved BAGAN (BAGAN-GP), are proposed as an augmentation tool to handle this\nproblem and restore the balance to the data. The former pre-trains the\nautoencoder weights in an unsupervised manner. However, it is unstable when the\nimages from different categories have similar features. The latter is improved\nbased on BAGAN by facilitating supervised autoencoder training, but the\npre-training is biased towards the majority classes. In this work, we propose a\nnovel Conditional Variational Autoencoder with Balanced Pre-training for\nGenerative Adversarial Networks (CAPGAN) as an augmentation tool to generate\nrealistic synthetic images. In particular, we utilize a conditional\nconvolutional variational autoencoder with supervised and balanced pre-training\nfor the GAN initialization and training with gradient penalty. Our proposed\nmethod presents a superior performance of other state-of-the-art methods on the\nhighly imbalanced version of MNIST, Fashion-MNIST, CIFAR-10, and two medical\nimaging datasets. Our method can synthesize high-quality minority samples in\nterms of Fr\\'echet inception distance, structural similarity index measure and\nperceptual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuchong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangr_X/0/1/0/all/0/1\">Xiaohui Wangr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuanbang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaissi_A/0/1/0/all/0/1\">Ali Anaissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braytee_A/0/1/0/all/0/1\">Ali Braytee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Cross-Modality Retinal Vessel Segmentation via Disentangling Representation Style Transfer and Collaborative Consistency Learning. (arXiv:2201.04812v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04812","description":"<p>Various deep learning models have been developed to segment anatomical\nstructures from medical images, but they typically have poor performance when\ntested on another target domain with different data distribution. Recently,\nunsupervised domain adaptation methods have been proposed to alleviate this\nso-called domain shift issue, but most of them are designed for scenarios with\nrelatively small domain shifts and are likely to fail when encountering a large\ndomain gap. In this paper, we propose DCDA, a novel cross-modality unsupervised\ndomain adaptation framework for tasks with large domain shifts, e.g.,\nsegmenting retinal vessels from OCTA and OCT images. DCDA mainly consists of a\ndisentangling representation style transfer (DRST) module and a collaborative\nconsistency learning (CCL) module. DRST decomposes images into content\ncomponents and style codes and performs style transfer and image\nreconstruction. CCL contains two segmentation models, one for source domain and\nthe other for target domain. The two models use labeled data (together with the\ncorresponding transferred images) for supervised learning and perform\ncollaborative consistency learning on unlabeled data. Each model focuses on the\ncorresponding single domain and aims to yield an expertized domain-specific\nsegmentation model. Through extensive experiments on retinal vessel\nsegmentation, our framework achieves Dice scores close to target-trained oracle\nboth from OCTA to OCT and from OCT to OCTA, significantly outperforming other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Least Squares for Training and Pruning Convolutional Neural Networks. (arXiv:2201.04813v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04813","description":"<p>Convolutional neural networks (CNNs) have succeeded in many practical\napplications. However, their high computation and storage requirements often\nmake them difficult to deploy on resource-constrained devices. In order to\ntackle this issue, many pruning algorithms have been proposed for CNNs, but\nmost of them can't prune CNNs to a reasonable level. In this paper, we propose\na novel algorithm for training and pruning CNNs based on the recursive least\nsquares (RLS) optimization. After training a CNN for some epochs, our algorithm\ncombines inverse input autocorrelation matrices and weight matrices to evaluate\nand prune unimportant input channels or nodes layer by layer. Then, our\nalgorithm will continue to train the pruned network, and won't do the next\npruning until the pruned network recovers the full performance of the old\nnetwork. Besides for CNNs, the proposed algorithm can be used for feedforward\nneural networks (FNNs). Three experiments on MNIST, CIFAR-10 and SVHN datasets\nshow that our algorithm can achieve the more reasonable pruning and have higher\nlearning efficiency than other four popular pruning algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianzong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S$^2$FPR: Crowd Counting via Self-Supervised Coarse to Fine Feature Pyramid Ranking. (arXiv:2201.04819v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04819","description":"<p>Most conventional crowd counting methods utilize a fully-supervised learning\nframework to learn a mapping between scene images and crowd density maps. Under\nthe circumstances of such fully-supervised training settings, a large quantity\nof expensive and time-consuming pixel-level annotations are required to\ngenerate density maps as the supervision. One way to reduce costly labeling is\nto exploit self-structural information and inner-relations among unlabeled\nimages. Unlike the previous methods utilizing these relations and structural\ninformation from the original image level, we explore such self-relations from\nthe latent feature spaces because it can extract more abundant relations and\nstructural information. Specifically, we propose S$^2$FPR which can extract\nstructural information and learn partial orders of coarse-to-fine pyramid\nfeatures in the latent space for better crowd counting with massive unlabeled\nimages. In addition, we collect a new unlabeled crowd counting dataset\n(FUDAN-UCC) with 4,000 images in total for training. One by-product is that our\nproposed S$^2$FPR method can leverage numerous partial orders in the latent\nspace among unlabeled images to strengthen the model representation capability\nand reduce the estimation errors for the crowd counting task. Extensive\nexperiments on four benchmark datasets, i.e. the UCF-QNRF, the ShanghaiTech\nPartA and PartB, and the UCF-CC-50, show the effectiveness of our method\ncompared with previous semi-supervised methods. The source code and dataset are\navailable at https://github.com/bridgeqiqi/S2FPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhizhong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yiming Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei-Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SnapshotNet: Self-supervised Feature Learning for Point Cloud Data Segmentation Using Minimal Labeled Data. (arXiv:2201.04833v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04833","description":"<p>Manually annotating complex scene point cloud datasets is both costly and\nerror-prone. To reduce the reliance on labeled data, a new model called\nSnapshotNet is proposed as a self-supervised feature learning approach, which\ndirectly works on the unlabeled point cloud data of a complex 3D scene. The\nSnapshotNet pipeline includes three stages. In the snapshot capturing stage,\nsnapshots, which are defined as local collections of points, are sampled from\nthe point cloud scene. A snapshot could be a view of a local 3D scan directly\ncaptured from the real scene, or a virtual view of such from a large 3D point\ncloud dataset. Snapshots could also be sampled at different sampling rates or\nfields of view (FOVs), thus multi-FOV snapshots, to capture scale information\nfrom the scene. In the feature learning stage, a new pre-text task called\nmulti-FOV contrasting is proposed to recognize whether two snapshots are from\nthe same object or not, within the same FOV or across different FOVs. Snapshots\ngo through two self-supervised learning steps: the contrastive learning step\nwith both part and scale contrasting, followed by a snapshot clustering step to\nextract higher level semantic features. Then a weakly-supervised segmentation\nstage is implemented by first training a standard SVM classifier on the learned\nfeatures with a small fraction of labeled snapshots. The trained SVM is used to\npredict labels for input snapshots and predicted labels are converted into\npoint-wise label assignments for semantic segmentation of the entire scene\nusing a voting procedure. The experiments are conducted on the Semantic3D\ndataset and the results have shown that the proposed method is capable of\nlearning effective features from snapshots of complex scene data without any\nlabels. Moreover, the proposed method has shown advantages when comparing to\nthe SOA method on weakly-supervised point cloud semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Ling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhigang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BridgeFormer: Bridging Video-text Retrieval with Multiple Choice Questions. (arXiv:2201.04850v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04850","description":"<p>Pre-training a model to learn transferable video-text representation for\nretrieval has attracted a lot of attention in recent years. Previous dominant\nworks mainly adopt two separate encoders for efficient retrieval, but ignore\nlocal associations between videos and texts. Another line of research uses a\njoint encoder to interact video with texts, but results in low efficiency since\neach text-video pair needs to be fed into the model. In this work, we enable\nfine-grained video-text interactions while maintaining high efficiency for\nretrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ),\nwhere a parametric module BridgeFormer is trained to answer the \"questions\"\nconstructed by the text features via resorting to the video features.\nSpecifically, we exploit the rich semantics of text (i.e., nouns and verbs) to\nbuild questions, with which the video encoder can be trained to capture more\nregional content and temporal dynamics. In the form of questions and answers,\nthe semantic associations between local video-text features can be properly\nestablished. BridgeFormer is able to be removed for downstream retrieval,\nrendering an efficient and flexible model with only two encoders. Our method\noutperforms state-of-the-art methods on the popular text-to-video retrieval\ntask in five datasets with different experimental setups (i.e., zero-shot and\nfine-tune), including HowTo100M (one million videos). We further conduct\nzero-shot action recognition, which can be cast as video-to-text retrieval, and\nour approach also significantly surpasses its counterparts. As an additional\nbenefit, our method achieves competitive results with much shorter pre-training\nvideos on single-modality downstream tasks, e.g., action recognition with\nlinear evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning. (arXiv:2201.04851v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04851","description":"<p>Dancing video retargeting aims to synthesize a video that transfers the dance\nmovements from a source video to a target person. Previous work need collect a\nseveral-minute-long video of a target person with thousands of frames to train\na personalized model. However, the trained model can only generate videos of\nthe same person. To address the limitations, recent work tackled few-shot\ndancing video retargeting, which learns to synthesize videos of unseen persons\nby leveraging a few frames of them. In practice, given a few frames of a\nperson, these work simply regarded them as a batch of individual images without\ntemporal correlations, thus generating temporally incoherent dancing videos of\nlow visual quality. In this work, we model a few frames of a person as a series\nof dancing moves, where each move contains two consecutive frames, to extract\nthe appearance patterns and the temporal dynamics of this person. We propose\nMetaDance, which utilizes temporal-aware meta-learning to optimize the\ninitialization of a model through the synthesis of dancing moves, such that the\nmeta-trained model can be efficiently tuned towards enhanced visual quality and\nstrengthened temporal stability for unseen persons with a few frames. Extensive\nevaluations show large superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Scene Text Detection using Deep Reinforcement Learning. (arXiv:2201.04866v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04866","description":"<p>The challenging field of scene text detection requires complex data\nannotation, which is time-consuming and expensive. Techniques, such as weak\nsupervision, can reduce the amount of data needed. In this paper we propose a\nweak supervision method for scene text detection, which makes use of\nreinforcement learning (RL). The reward received by the RL agent is estimated\nby a neural network, instead of being inferred from ground-truth labels. First,\nwe enhance an existing supervised RL approach to text detection with several\ntraining optimizations, allowing us to close the performance gap to\nregression-based algorithms. We then use our proposed system in a weakly- and\nsemi-supervised training on real-world data. Our results show that training in\na weakly supervised setting is feasible. However, we find that using our model\nin a semi-supervised setting , e.g. when combining labeled synthetic data with\nunannotated real-world data, produces the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzenthin_E/0/1/0/all/0/1\">Emanuel Metzenthin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1\">Christian Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting. (arXiv:2201.04873v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04873","description":"<p>We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces\nwith convincing relighting. Our main contribution is a volumetric HDRI\nrelighting method that can efficiently accumulate albedo, diffuse and specular\nlighting contributions along each 3D ray for any desired HDR environmental map.\nAdditionally, we show the importance of supervising the image decomposition\nprocess using multiple discriminators. In particular, we propose a data\naugmentation technique that leverages recent advances in single image portrait\nrelighting to enforce consistent geometry, albedo, diffuse and specular\ncomponents. Multiple experiments and comparisons with other generative\nframeworks show how our model is a step forward towards photorealistic\nrelightable 3D generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Feitong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1\">Sean Fanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orts_Escolano_S/0/1/0/all/0/1\">Sergio Orts-Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">Jonathan Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Style Image Super-Resolution using Conditional Objective. (arXiv:2201.04898v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04898","description":"<p>Recent studies have significantly enhanced the performance of single-image\nsuper-resolution (SR) using convolutional neural networks (CNNs). While there\ncan be many high-resolution (HR) solutions for a given input, most existing\nCNN-based methods do not explore alternative solutions during the inference. A\ntypical approach to obtaining alternative SR results is to train multiple SR\nmodels with different loss weightings and exploit the combination of these\nmodels. Instead of using multiple models, we present a more efficient method to\ntrain a single adjustable SR model on various combinations of losses by taking\nadvantage of multi-task learning. Specifically, we optimize an SR model with a\nconditional objective during training, where the objective is a weighted sum of\nmultiple perceptual losses at different feature levels. The weights vary\naccording to given conditions, and the set of weights is defined as a style\ncontroller. Also, we present an architecture appropriate for this training\nscheme, which is the Residual-in-Residual Dense Block equipped with spatial\nfeature transformation layers. At the inference phase, our trained model can\ngenerate locally different outputs conditioned on the style control map.\nExtensive experiments show that the proposed SR model produces various\ndesirable reconstructions without artifacts and yields comparable quantitative\nperformance to state-of-the-art SR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1\">Young Su Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Object Interaction Reasoning. (arXiv:2201.04906v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04906","description":"<p>This paper proposes an interaction reasoning network for modelling\nspatio-temporal relationships between hands and objects in video. The proposed\ninteraction unit utilises a Transformer module to reason about each acting\nhand, and its spatio-temporal relation to the other hand as well as objects\nbeing interacted with. We show that modelling two-handed interactions are\ncritical for action recognition in egocentric video, and demonstrate that by\nusing positionally-encoded trajectories, the network can better recognise\nobserved interactions. We evaluate our proposal on EPIC-KITCHENS and\nSomething-Else datasets, with an ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Endoscopic Image Generation Method Using Virtual-to-real Image-domain Translation. (arXiv:2201.04918v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04918","description":"<p>This paper proposes a realistic image generation method for visualization in\nendoscopic simulation systems. Endoscopic diagnosis and treatment are performed\nin many hospitals. To reduce complications related to endoscope insertions,\nendoscopic simulation systems are used for training or rehearsal of endoscope\ninsertions. However, current simulation systems generate non-realistic virtual\nendoscopic images. To improve the value of the simulation systems, improvement\nof reality of their generated images is necessary. We propose a realistic image\ngeneration method for endoscopic simulation systems. Virtual endoscopic images\nare generated by using a volume rendering method from a CT volume of a patient.\nWe improve the reality of the virtual endoscopic images using a virtual-to-real\nimage-domain translation technique. The image-domain translator is implemented\nas a fully convolutional network (FCN). We train the FCN by minimizing a cycle\nconsistency loss function. The FCN is trained using unpaired virtual and real\nendoscopic images. To obtain high quality image-domain translation results, we\nperform an image cleansing to the real endoscopic image set. We tested to use\nthe shallow U-Net, U-Net, deep U-Net, and U-Net having residual units as the\nimage-domain translator. The deep U-Net and U-Net having residual units\ngenerated quite realistic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1\">Kiyohito Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takabatake_H/0/1/0/all/0/1\">Hirotsugu Takabatake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_M/0/1/0/all/0/1\">Masaki Mori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natori_H/0/1/0/all/0/1\">Hiroshi Natori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report for ICCV 2021 Challenge SSLAD-Track3B: Transformers Are Better Continual Learners. (arXiv:2201.04924v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04924","description":"<p>In the SSLAD-Track 3B challenge on continual learning, we propose the method\nof COntinual Learning with Transformer (COLT). We find that transformers suffer\nless from catastrophic forgetting compared to convolutional neural network. The\nmajor principle of our method is to equip the transformer based feature\nextractor with old knowledge distillation and head expanding strategies to\ncompete catastrophic forgetting. In this report, we first introduce the overall\nframework of continual learning for object detection. Then, we analyse the key\nelements' effect on withstanding catastrophic forgetting in our solution. Our\nmethod achieves 70.78 mAP on the SSLAD-Track 3B challenge test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guimei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantic Abstraction of Shape via 3D Region of Interest. (arXiv:2201.04945v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04945","description":"<p>In this paper, we focus on the two tasks of 3D shape abstraction and semantic\nanalysis. This is in contrast to current methods, which focus solely on either\n3D shape abstraction or semantic analysis. In addition, previous methods have\nhad difficulty producing instance-level semantic results, which has limited\ntheir application. We present a novel method for the joint estimation of a 3D\nshape abstraction and semantic analysis. Our approach first generates a number\nof 3D semantic candidate regions for a 3D shape; we then employ these\ncandidates to directly predict the semantic categories and refine the\nparameters of the candidate regions simultaneously using a deep convolutional\nneural network. Finally, we design an algorithm to fuse the predicted results\nand obtain the final semantic abstraction, which is shown to be an improvement\nover a standard non maximum suppression. Experimental results demonstrate that\nour approach can produce state-of-the-art results. Moreover, we also find that\nour results can be easily applied to instance-level semantic part segmentation\nand shape matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haiyue Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zheyuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yahao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shilin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents. (arXiv:2201.04990v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04990","description":"<p>Critical periods are phases during which a toddler's brain develops in\nspurts. To promote children's cognitive development, proper guidance is\ncritical in this stage. However, it is not clear whether such a critical period\nalso exists for the training of AI agents. Similar to human toddlers,\nwell-timed guidance and multimodal interactions might significantly enhance the\ntraining efficiency of AI agents as well. To validate this hypothesis, we adapt\nthis notion of critical periods to learning in AI agents and investigate the\ncritical period in the virtual environment for AI agents. We formalize the\ncritical period and Toddler-guidance learning in the reinforcement learning\n(RL) framework. Then, we built up a toddler-like environment with VECA toolkit\nto mimic human toddlers' learning characteristics. We study three discrete\nlevels of mutual interaction: weak-mentor guidance (sparse reward), moderate\nmentor guidance (helper-reward), and mentor demonstration (behavioral cloning).\nWe also introduce the EAVE dataset consisting of 30,000 real-world images to\nfully reflect the toddler's viewpoint. We evaluate the impact of critical\nperiods on AI agents from two perspectives: how and when they are guided best\nin both uni- and multimodal learning. Our experimental results show that both\nuni- and multimodal agents with moderate mentor guidance and critical period on\n1 million and 2 million training steps show a noticeable improvement. We\nvalidate these results with transfer learning on the EAVE dataset and find the\nperformance advancement on the same critical period and the guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junseok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hyunseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Ganghun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngki Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granularity Association Learning Framework for on-the-fly Fine-Grained Sketch-based Image Retrieval. (arXiv:2201.05007v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05007","description":"<p>Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of\nretrieving a particular photo in a given query sketch. However, its widespread\napplicability is limited by the fact that it is difficult to draw a complete\nsketch for most people, and the drawing process often takes time. In this\nstudy, we aim to retrieve the target photo with the least number of strokes\npossible (incomplete sketch), named on-the-fly FG-SBIR (Bhunia et al. 2020),\nwhich starts retrieving at each stroke as soon as the drawing begins. We\nconsider that there is a significant correlation among these incomplete\nsketches in the sketch drawing episode of each photo. To learn more efficient\njoint embedding space shared between the photo and its incomplete sketches, we\npropose a multi-granularity association learning framework that further\noptimizes the embedding space of all incomplete sketches. Specifically, based\non the integrity of the sketch, we can divide a complete sketch episode into\nseveral stages, each of which corresponds to a simple linear mapping layer.\nMoreover, our framework guides the vector space representation of the current\nsketch to approximate that of its later sketches to realize the retrieval\nperformance of the sketch with fewer strokes to approach that of the sketch\nwith more strokes. In the experiments, we proposed more realistic challenges,\nand our method achieved superior early retrieval efficiency over the\nstate-of-the-art methods and alternative baselines on two publicly available\nfine-grained sketch retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dawei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shuyin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Sparse Connectivity Learning for Neural Networks. (arXiv:2201.05020v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05020","description":"<p>Since sparse neural networks usually contain many zero weights, these\nunnecessary network connections can potentially be eliminated without degrading\nnetwork performance. Therefore, well-designed sparse neural networks have the\npotential to significantly reduce FLOPs and computational resources. In this\nwork, we propose a new automatic pruning method - Sparse Connectivity Learning\n(SCL). Specifically, a weight is re-parameterized as an element-wise\nmultiplication of a trainable weight variable and a binary mask. Thus, network\nconnectivity is fully described by the binary mask, which is modulated by a\nunit step function. We theoretically prove the fundamental principle of using a\nstraight-through estimator (STE) for network pruning. This principle is that\nthe proxy gradients of STE should be positive, ensuring that mask variables\nconverge at their minima. After finding Leaky ReLU, Softplus, and Identity STEs\ncan satisfy this principle, we propose to adopt Identity STE in SCL for\ndiscrete mask relaxation. We find that mask gradients of different features are\nvery unbalanced, hence, we propose to normalize mask gradients of each feature\nto optimize mask variable training. In order to automatically train sparse\nmasks, we include the total number of network connections as a regularization\nterm in our objective function. As SCL does not require pruning criteria or\nhyper-parameters defined by designers for network layers, the network is\nexplored in a larger hypothesis space to achieve optimized sparse connectivity\nfor the best performance. SCL overcomes the limitations of existing automatic\npruning methods. Experimental results demonstrate that SCL can automatically\nlearn and select important network connections for various baseline network\nstructures. Deep learning models trained by SCL outperform the SOTA\nhuman-designed and automatic pruning methods in sparsity, accuracy, and FLOPs\nreduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhimin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linkai Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Bike Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rujie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1\">Lvqing Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-semantic contour adaptation for cross modality brain tumor segmentation. (arXiv:2201.05022v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05022","description":"<p>Unsupervised domain adaptation (UDA) between two significantly disparate\ndomains to learn high-level semantic alignment is a crucial yet challenging\ntask.~To this end, in this work, we propose exploiting low-level edge\ninformation to facilitate the adaptation as a precursor task, which has a small\ncross-domain gap, compared with semantic segmentation.~The precise contour then\nprovides spatial information to guide the semantic adaptation. More\nspecifically, we propose a multi-task framework to learn a contouring\nadaptation network along with a semantic segmentation adaptation network, which\ntakes both magnetic resonance imaging (MRI) slice and its initial edge map as\ninput.~These two networks are jointly trained with source domain labels, and\nthe feature and edge map level adversarial learning is carried out for\ncross-domain alignment. In addition, self-entropy minimization is incorporated\nto further enhance segmentation performance. We evaluated our framework on the\nBraTS2018 database for cross-modality segmentation of brain tumors, showing the\nvalidity and superiority of our approach, compared with competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Magnification with Multi-Layer Images. (arXiv:2201.05023v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05023","description":"<p>Representing scenes with multiple semi-transparent colored layers has been a\npopular and successful choice for real-time novel view synthesis. Existing\napproaches infer colors and transparency values over regularly-spaced layers of\nplanar or spherical shape. In this work, we introduce a new view synthesis\napproach based on multiple semi-transparent layers with scene-adapted geometry.\nOur approach infers such representations from stereo pairs in two stages. The\nfirst stage infers the geometry of a small number of data-adaptive layers from\na given pair of views. The second stage infers the color and the transparency\nvalues for these layers producing the final representation for novel view\nsynthesis. Importantly, both stages are connected through a differentiable\nrenderer and are trained in an end-to-end manner. In the experiments, we\ndemonstrate the advantage of the proposed approach over the use of\nregularly-spaced layers with no adaptation to scene geometry. Despite being\norders of magnitude faster during rendering, our approach also outperforms a\nrecently proposed IBRNet system based on implicit geometry representation. See\nresults at https://samsunglabs.github.io/StereoLayers .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khakhulin_T/0/1/0/all/0/1\">Taras Khakhulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korzhenkov_D/0/1/0/all/0/1\">Denis Korzhenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solovev_P/0/1/0/all/0/1\">Pavel Solovev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterkin_G/0/1/0/all/0/1\">Gleb Sterkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardelean_T/0/1/0/all/0/1\">Timotei Ardelean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Data and How to Query Them. (arXiv:2201.05026v1 [cs.AI])","link":"http://arxiv.org/abs/2201.05026","description":"<p>It is commonly acknowledged that the availability of the huge amount of\n(training) data is one of the most important factors for many recent advances\nin Artificial Intelligence (AI). However, datasets are often designed for\nspecific tasks in narrow AI sub areas and there is no unified way to manage and\naccess them. This not only creates unnecessary overheads when training or\ndeploying Machine Learning models but also limits the understanding of the\ndata, which is very important for data-centric AI. In this paper, we present\nour vision about a unified framework for different datasets so that they can be\nintegrated and queried easily, e.g., using standard query languages. We\ndemonstrate this in our ongoing work to create a framework for datasets in\nComputer Vision and show its advantages in different scenarios. Our\ndemonstration is available at https://vision.semkg.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Kien Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Tuan_A/0/1/0/all/0/1\">Anh Le-Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_M/0/1/0/all/0/1\">Manh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jicheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Phuoc_D/0/1/0/all/0/1\">Danh Le-Phuoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers. (arXiv:2201.05047v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05047","description":"<p>Detection Transformer (DETR) and Deformable DETR have been proposed to\neliminate the need for many hand-designed components in object detection while\ndemonstrating good performance as previous complex hand-crafted detectors.\nHowever, their performance on Video Object Detection (VOD) has not been well\nexplored. In this paper, we present TransVOD, the first end-to-end video object\ndetection system based on spatial-temporal Transformer architectures. The first\ngoal of this paper is to streamline the pipeline of VOD, effectively removing\nthe need for many hand-crafted components for feature aggregation, e.g.,\noptical flow model, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS. In particular, we present a temporal Transformer to aggregate\nboth the spatial object queries and the feature memories of each frame. Our\ntemporal transformer consists of two components: Temporal Query Encoder (TQE)\nto fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to\nobtain current frame detection results. These designs boost the strong baseline\ndeformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID\ndataset. Then, we present two improved versions of TransVOD including\nTransVOD++ and TransVOD Lite. The former fuses object-level information into\nobject query via dynamic convolution while the latter models the entire video\nclips as the output to speed up the inference time. We give detailed analysis\nof all three models in the experiment part. In particular, our proposed\nTransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet\nVID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and\naccuracy trade-off with 83.7% mAP while running at around 30 FPS on a single\nV100 GPU device. Code and models will be available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles. (arXiv:2201.05057v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05057","description":"<p>Trajectory prediction is a critical component for autonomous vehicles (AVs)\nto perform safe planning and navigation. However, few studies have analyzed the\nadversarial robustness of trajectory prediction or investigated whether the\nworst-case prediction can still lead to safe planning. To bridge this gap, we\nstudy the adversarial robustness of trajectory prediction models by proposing a\nnew adversarial attack that perturbs normal vehicle trajectories to maximize\nthe prediction error. Our experiments on three models and three datasets show\nthat the adversarial prediction increases the prediction error by more than\n150%. Our case studies show that if an adversary drives a vehicle close to the\ntarget AV following the adversarial trajectory, the AV may make an inaccurate\nprediction and even make unsafe driving decisions. We also explore possible\nmitigation techniques via data augmentation and trajectory smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengtuo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiachen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Neural Networks Defenses and Attacks using NDCG and Reciprocal Rank Metrics. (arXiv:2201.05071v1 [cs.CR])","link":"http://arxiv.org/abs/2201.05071","description":"<p>The problem of attacks on neural networks through input modification (i.e.,\nadversarial examples) has attracted much attention recently. Being relatively\neasy to generate and hard to detect, these attacks pose a security breach that\nmany suggested defenses try to mitigate. However, the evaluation of the effect\nof attacks and defenses commonly relies on traditional classification metrics,\nwithout adequate adaptation to adversarial scenarios. Most of these metrics are\naccuracy-based, and therefore may have a limited scope and low distinctive\npower. Other metrics do not consider the unique characteristics of neural\nnetworks functionality, or measure the effect of the attacks indirectly (e.g.,\nthrough the complexity of their generation). In this paper, we present two\nmetrics which are specifically designed to measure the effect of attacks, or\nthe recovery effect of defenses, on the output of neural networks in multiclass\nclassification tasks. Inspired by the normalized discounted cumulative gain and\nthe reciprocal rank metrics used in information retrieval literature, we treat\nthe neural network predictions as ranked lists of results. Using additional\ninformation about the probability of the rank enabled us to define novel\nmetrics that are suited to the task at hand. We evaluate our metrics using\nvarious attacks and defenses on a pretrained VGG19 model and the ImageNet\ndataset. Compared to the common classification metrics, our proposed metrics\ndemonstrate superior informativeness and distinctiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brama_H/0/1/0/all/0/1\">Haya Brama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lihi Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinshpoun_T/0/1/0/all/0/1\">Tal Grinshpoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Event: Connecting Text and Images with Event Structures. (arXiv:2201.05078v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05078","description":"<p>Vision-language (V+L) pretraining models have achieved great success in\nsupporting multimedia applications by understanding the alignments between\nimages and text. While existing vision-language pretraining models primarily\nfocus on understanding objects in images or entities in text, they often ignore\nthe alignment at the level of events and their argument structures. % In this\nwork, we propose a contrastive learning framework to enforce vision-language\npretraining models to comprehend events and associated argument (participant)\nroles. To achieve this, we take advantage of text information extraction\ntechnologies to obtain event structural knowledge, and utilize multiple prompt\nfunctions to contrast difficult negative descriptions by manipulating event\nstructures. We also design an event graph alignment loss based on optimal\ntransport to capture event argument structures. In addition, we collect a large\nevent-rich dataset (106,875 images) for pretraining, which provides a more\nchallenging image retrieval benchmark to assess the understanding of\ncomplicated lengthy sentences. Experiments show that our zero-shot CLIP-Event\noutperforms the state-of-the-art supervised model in argument extraction on\nMultimedia Event Extraction, achieving more than 5\\% absolute F-score gain in\nevent extraction, as well as significant improvements on a variety of\ndownstream tasks under zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?. (arXiv:2201.05119v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05119","description":"<p>Despite recent progress made by self-supervised methods in representation\nlearning with residual networks, they still underperform supervised learning on\nthe ImageNet classification benchmark, limiting their applicability in\nperformance-critical settings. Building on prior theoretical insights from\nMitrovic et al., 2021, we propose ReLICv2 which combines an explicit invariance\nloss with a contrastive objective over a varied set of appropriately\nconstructed data views. ReLICv2 achieves 77.1% top-1 classification accuracy on\nImageNet using linear evaluation with a ResNet50 architecture and 80.6% with\nlarger ResNet models, outperforming previous state-of-the-art self-supervised\napproaches by a wide margin. Most notably, ReLICv2 is the first representation\nlearning method to consistently outperform the supervised baseline in a\nlike-for-like comparison using a range of standard ResNet architectures.\nFinally we show that despite using ResNet encoders, ReLICv2 is comparable to\nstate-of-the-art self-supervised vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1\">Nenad Tomasev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1\">Ioana Bica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McWilliams_B/0/1/0/all/0/1\">Brian McWilliams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buesing_L/0/1/0/all/0/1\">Lars Buesing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jovana Mitrovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeamlessGAN: Self-Supervised Synthesis of Tileable Texture Maps. (arXiv:2201.05120v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05120","description":"<p>We present SeamlessGAN, a method capable of automatically generating tileable\ntexture maps from a single input exemplar. In contrast to most existing\nmethods, focused solely on solving the synthesis problem, our work tackles both\nproblems, synthesis and tileability, simultaneously. Our key idea is to realize\nthat tiling a latent space within a generative network trained using\nadversarial expansion techniques produces outputs with continuity at the seam\nintersection that can be then be turned into tileable images by cropping the\ncentral area. Since not every value of the latent space is valid to produce\nhigh-quality outputs, we leverage the discriminator as a perceptual error\nmetric capable of identifying artifact-free textures during a sampling process.\nFurther, in contrast to previous work on deep texture synthesis, our model is\ndesigned and optimized to work with multi-layered texture representations,\nenabling textures composed of multiple maps such as albedo, normals, etc. We\nextensively test our design choices for the network architecture, loss function\nand sampling parameters. We show qualitatively and quantitatively that our\napproach outperforms previous methods and works for textures of different\ntypes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Pardo_C/0/1/0/all/0/1\">Carlos Rodriguez-Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garces_E/0/1/0/all/0/1\">Elena Garces</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization. (arXiv:2201.05121v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05121","description":"<p>Learning-based edge detection has hereunto been strongly supervised with\npixel-wise annotations which are tedious to obtain manually. We study the\nproblem of self-training edge detection, leveraging the untapped wealth of\nlarge-scale unlabeled image datasets. We design a self-supervised framework\nwith multi-layer regularization and self-teaching. In particular, we impose a\nconsistency regularization which enforces the outputs from each of the multiple\nlayers to be consistent for the input image and its perturbed counterpart. We\nadopt L0-smoothing as the 'perturbation' to encourage edge prediction lying on\nsalient boundaries following the cluster assumption in self-supervised\nlearning. Meanwhile, the network is trained with multi-layer supervision by\npseudo labels which are initialized with Canny edges and then iteratively\nrefined by the network as the training proceeds. The regularization and\nself-teaching together attain a good balance of precision and recall, leading\nto a significant performance boost over supervised methods, with lightweight\nrefinement on the target dataset. Furthermore, our method demonstrates strong\ncross-dataset generality. For example, it attains 4.8% improvement for ODS and\n5.8% for OIS when tested on the unseen BIPED dataset, compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunfan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Renjiao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradMax: Growing Neural Networks using Gradient Information. (arXiv:2201.05125v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05125","description":"<p>The architecture and the parameters of neural networks are often optimized\nindependently, which requires costly retraining of the parameters whenever the\narchitecture is modified. In this work we instead focus on growing the\narchitecture without requiring costly retraining. We present a method that adds\nnew neurons during training without impacting what is already learned, while\nimproving the training dynamics. We achieve the latter by maximizing the\ngradients of the new weights and find the optimal initialization efficiently by\nmeans of the singular value decomposition (SVD). We call this technique\nGradient Maximizing Growth (GradMax) and demonstrate its effectiveness in\nvariety of vision tasks and architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrienboer_B/0/1/0/all/0/1\">Bart van Merri&#xeb;nboer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1\">Fabian Pedregosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation. (arXiv:2201.05131v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05131","description":"<p>Feature regression is a simple way to distill large neural network models to\nsmaller ones. We show that with simple changes to the network architecture,\nregression can outperform more complex state-of-the-art approaches for\nknowledge distillation from self-supervised models. Surprisingly, the addition\nof a multi-layer perceptron head to the CNN backbone is beneficial even if used\nonly during distillation and discarded in the downstream task. Deeper\nnon-linear projections can thus be used to accurately mimic the teacher without\nchanging inference architecture and time. Moreover, we utilize independent\nprojection heads to simultaneously distill multiple teacher networks. We also\nfind that using the same weakly augmented image as input for both teacher and\nstudent networks aids distillation. Experiments on ImageNet dataset demonstrate\nthe efficacy of the proposed changes in various self-supervised distillation\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navaneet_K/0/1/0/all/0/1\">K L Navaneet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Adaptive Bayesian Algorithm for Data Analysis, FABADA. (arXiv:2201.05145v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2201.05145","description":"<p>The aim of this paper is to describe a novel non-parametric noise reduction\ntechnique from the point of view of Bayesian inference that may automatically\nimprove the signal-to-noise ratio of one- and two-dimensional data, such as\ne.g. astronomical images and spectra. The algorithm iteratively evaluates\npossible smoothed versions of the data, the smooth models, obtaining an\nestimation of the underlying signal that is statistically compatible with the\nnoisy measurements. Iterations stop based on the evidence and the $\\chi^2$\nstatistic of the last smooth model, and we compute the expected value of the\nsignal as a weighted average of the whole set of smooth models. In this paper,\nwe explain the mathematical formalism and numerical implementation of the\nalgorithm, and we evaluate its performance in terms of the peak signal to noise\nratio, the structural similarity index, and the time payload, using a battery\nof real astronomical observations. Our Fully Adaptive Bayesian Algorithm for\nData Analysis (FABADA) yields results that, without any parameter tuning, are\ncomparable to standard image processing algorithms whose parameters have been\noptimized based on the true signal to be recovered, something that is\nimpossible in a real application. State-of-the-art non-parametric methods, such\nas BM3D, offer slightly better performance at high signal-to-noise ratio, while\nour algorithm is significantly more accurate for extremely noisy data (higher\nthan $20-40\\%$ relative errors, a situation of particular interest in the field\nof astronomy). In this range, the standard deviation of the residuals obtained\nby our reconstruction may become more than an order of magnitude lower than\nthat of the original measurements. The source code needed to reproduce all the\nresults presented in this report, including the implementation of the method,\nis publicly available at https://github.com/PabloMSanAla/fabada\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Sanchez_Alarcon_P/0/1/0/all/0/1\">Pablo M Sanchez-Alarcon</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sequeiros_Y/0/1/0/all/0/1\">Yago Ascasibar Sequeiros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning. (arXiv:2201.05151v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05151","description":"<p>Modern deep learning requires large-scale extensively labelled datasets for\ntraining. Few-shot learning aims to alleviate this issue by learning\neffectively from few labelled examples. In previously proposed few-shot visual\nclassifiers, it is assumed that the feature manifold, where classifier\ndecisions are made, has uncorrelated feature dimensions and uniform feature\nvariance. In this work, we focus on addressing the limitations arising from\nthis assumption by proposing a variance-sensitive class of models that operates\nin a low-label regime. The first method, Simple CNAPS, employs a hierarchically\nregularized Mahalanobis-distance based classifier combined with a state of the\nart neural adaptive feature extractor to achieve strong performance on\nMeta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend\nthis approach to a transductive learning setting, proposing Transductive CNAPS.\nThis transductive method combines a soft k-means parameter refinement procedure\nwith a two-step task encoder to achieve improved test-time classification\naccuracy using unlabelled data. Transductive CNAPS achieves state of the art\nperformance on Meta-Dataset. Finally, we explore the use of our methods (Simple\nand Transductive) for \"out of the box\" continual and active learning. Extensive\nexperiments on large scale benchmarks illustrate robustness and versatility of\nthis, relatively speaking, simple class of models. All trained model\ncheckpoints and corresponding source codes have been made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raghav Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masrani_V/0/1/0/all/0/1\">Vaden Masrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlabeled Data Improves Adversarial Robustness. (arXiv:1905.13736v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/1905.13736","description":"<p>We demonstrate, theoretically and empirically, that adversarial robustness\ncan significantly benefit from semisupervised learning. Theoretically, we\nrevisit the simple Gaussian model of Schmidt et al. that shows a sample\ncomplexity gap between standard and robust classification. We prove that\nunlabeled data bridges this gap: a simple semisupervised learning procedure\n(self-training) achieves high robust accuracy using the same number of labels\nrequired for achieving high standard accuracy. Empirically, we augment CIFAR-10\nwith 500K unlabeled images sourced from 80 Million Tiny Images and use robust\nself-training to outperform state-of-the-art robust accuracies by over 5 points\nin (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial\ntraining and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via\nrandomized smoothing. On SVHN, adding the dataset's own extra training set with\nthe labels removed provides gains of 4 to 10 points, within 1 point of the gain\nfrom using the extra labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duchi_J/0/1/0/all/0/1\">John C. Duchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Scene Understanding via Online Semantic Reconstruction. (arXiv:1906.07409v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/1906.07409","description":"<p>We propose a novel approach to robot-operated active understanding of unknown\nindoor scenes, based on online RGBD reconstruction with semantic segmentation.\nIn our method, the exploratory robot scanning is both driven by and targeting\nat the recognition and segmentation of semantic objects from the scene. Our\nalgorithm is built on top of the volumetric depth fusion framework (e.g.,\nKinectFusion) and performs real-time voxel-based semantic labeling over the\nonline reconstructed volume. The robot is guided by an online estimated\ndiscrete viewing score field (VSF) parameterized over the 3D space of 2D\nlocation and azimuth rotation. VSF stores for each grid the score of the\ncorresponding view, which measures how much it reduces the uncertainty\n(entropy) of both geometric reconstruction and semantic labeling. Based on VSF,\nwe select the next best views (NBV) as the target for each time step. We then\njointly optimize the traverse path and camera trajectory between two adjacent\nNBVs, through maximizing the integral viewing score (information gain) along\npath and trajectory. Through extensive evaluation, we show that our method\nachieves efficient and accurate online scene parsing during exploratory\nscanning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lintao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiazhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning spatio-temporal representations with temporal squeeze pooling. (arXiv:2002.04685v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.04685","description":"<p>In this paper, we propose a new video representation learning method, named\nTemporal Squeeze (TS) pooling, which can extract the essential movement\ninformation from a long sequence of video frames and map it into a set of few\nimages, named Squeezed Images. By embedding the Temporal Squeeze pooling as a\nlayer into off-the-shelf Convolution Neural Networks (CNN), we design a new\nvideo classification model, named Temporal Squeeze Network (TeSNet). The\nresulting Squeezed Images contain the essential movement information from the\nvideo frames, corresponding to the optimization of the video classification\ntask. We evaluate our architecture on two video classification benchmarks, and\nthe results achieved are compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation. (arXiv:2003.06233v4 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2003.06233","description":"<p>Online semantic 3D segmentation in company with real-time RGB-D\nreconstruction poses special challenges such as how to perform 3D convolution\ndirectly over the progressively fused 3D geometric data, and how to smartly\nfuse information from frame to frame. We propose a novel fusion-aware 3D point\nconvolution which operates directly on the geometric surface being\nreconstructed and exploits effectively the inter-frame correlation for high\nquality 3D feature learning. This is enabled by a dedicated dynamic data\nstructure which organizes the online acquired point cloud with global-local\ntrees. Globally, we compile the online reconstructed 3D points into an\nincrementally growing coordinate interval tree, enabling fast point insertion\nand neighborhood query. Locally, we maintain the neighborhood information for\neach point using an octree whose construction benefits from the fast query of\nthe global tree.Both levels of trees update dynamically and help the 3D\nconvolution effectively exploits the temporal coherence for effective\ninformation fusion across RGB-D frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiazhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lintao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization. (arXiv:2101.07512v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07512","description":"<p>Fooling deep neural networks (DNNs) with the black-box optimization has\nbecome a popular adversarial attack fashion, as the structural prior knowledge\nof DNNs is always unknown. Nevertheless, recent black-box adversarial attacks\nmay struggle to balance their attack ability and visual quality of the\ngenerated adversarial examples (AEs) in tackling high-resolution images. In\nthis paper, we propose an attention-guided black-box adversarial attack based\non the large-scale multiobjective evolutionary optimization, termed as LMOA. By\nconsidering the spatial semantic information of images, we firstly take\nadvantage of the attention map to determine the perturbed pixels. Instead of\nattacking the entire image, reducing the perturbed pixels with the attention\nmechanism can help to avoid the notorious curse of dimensionality and thereby\nimproves the performance of attacking. Secondly, a large-scale multiobjective\nevolutionary algorithm is employed to traverse the reduced pixels in the\nsalient region. Benefiting from its characteristics, the generated AEs have the\npotential to fool target DNNs while being imperceptible by the human vision.\nExtensive experimental results have verified the effectiveness of the proposed\nLMOA on the ImageNet dataset. More importantly, it is more competitive to\ngenerate high-resolution AEs with better visual quality compared with the\nexisting black-box adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yang Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fine-Grained Segmentation of 3D Shapes without Part Labels. (arXiv:2103.13030v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13030","description":"<p>Learning-based 3D shape segmentation is usually formulated as a semantic\nlabeling problem, assuming that all parts of training shapes are annotated with\na given set of tags. This assumption, however, is impractical for learning\nfine-grained segmentation. Although most off-the-shelf CAD models are, by\nconstruction, composed of fine-grained parts, they usually miss semantic tags\nand labeling those fine-grained parts is extremely tedious. We approach the\nproblem with deep clustering, where the key idea is to learn part priors from a\nshape dataset with fine-grained segmentation but no part labels. Given point\nsampled 3D shapes, we model the clustering priors of points with a similarity\nmatrix and achieve part segmentation through minimizing a novel low rank loss.\nTo handle highly densely sampled point sets, we adopt a divide-and-conquer\nstrategy. We partition the large point set into a number of blocks. Each block\nis segmented using a deep-clustering-based part prior network trained in a\ncategory-agnostic manner. We then train a graph convolution network to merge\nthe segments of all blocks to form the final segmentation result. Our method is\nevaluated with a challenging benchmark of fine-grained segmentation, showing\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xinyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Busy-Quiet Video Disentangling for Video Classification. (arXiv:2103.15584v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15584","description":"<p>In video data, busy motion details from moving regions are conveyed within a\nspecific frequency bandwidth in the frequency domain. Meanwhile, the rest of\nthe frequencies of video data are encoded with quiet information with\nsubstantial redundancy, which causes low processing efficiency in existing\nvideo models that take as input raw RGB frames. In this paper, we consider\nallocating intenser computation for the processing of the important busy\ninformation and less computation for that of the quiet information. We design a\ntrainable Motion Band-Pass Module (MBPM) for separating busy information from\nquiet information in raw video data. By embedding the MBPM into a two-pathway\nCNN architecture, we define a Busy-Quiet Net (BQN). The efficiency of BQN is\ndetermined by avoiding redundancy in the feature space processed by the two\npathways: one operating on Quiet features of low-resolution, while the other\nprocesses Busy features. The proposed BQN outperforms many recent video\nprocessing models on Something-Something V1, Kinetics400, UCF101 and HMDB51\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Recursive Embedding for High-Dimensional Data. (arXiv:2104.05171v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05171","description":"<p>t-distributed stochastic neighbor embedding (t-SNE) is a well-established\nvisualization method for complex high-dimensional data. However, the original\nt-SNE method is nonparametric, stochastic, and often cannot well prevserve the\nglobal structure of data as it emphasizes local neighborhood. With t-SNE as a\nreference, we propose to combine the deep neural network (DNN) with the\nmathematical-grounded embedding rules for high-dimensional data embedding. We\nfirst introduce a deep embedding network (DEN) framework, which can learn a\nparametric mapping from high-dimensional space to low-dimensional embedding.\nDEN has a flexible architecture that can accommodate different input data\n(vector, image, or tensor) and loss functions. To improve the embedding\nperformance, a recursive training strategy is proposed to make use of the\nlatent representations extracted by DEN. Finally, we propose a two-stage loss\nfunction combining the advantages of two popular embedding methods, namely,\nt-SNE and uniform manifold approximation and projection (UMAP), for optimal\nvisualization effect. We name the proposed method Deep Recursive Embedding\n(DRE), which optimizes DEN with a recursive training strategy and two-stage\nlosse. Our experiments demonstrated the excellent performance of the proposed\nDRE method on high-dimensional data embedding, across a variety of public\ndatabases. Remarkably, our comparative results suggested that our proposed DRE\ncould lead to improved global structure preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelieveldt_B/0/1/0/all/0/1\">Boudewijn P.F. Lelieveldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qian Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Balanced Distillation for Long-Tailed Visual Recognition. (arXiv:2104.05279v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05279","description":"<p>Real-world imagery is often characterized by a significant imbalance of the\nnumber of images per class, leading to long-tailed distributions. An effective\nand simple approach to long-tailed visual recognition is to learn feature\nrepresentations and a classifier separately, with instance and class-balanced\nsampling, respectively. In this work, we introduce a new framework, by making\nthe key observation that a feature representation learned with instance\nsampling is far from optimal in a long-tailed setting. Our main contribution is\na new training method, referred to as Class-Balanced Distillation (CBD), that\nleverages knowledge distillation to enhance feature representations. CBD allows\nthe feature representation to evolve in the second training stage, guided by\nthe teacher learned in the first stage. The second stage uses class-balanced\nsampling, in order to focus on under-represented classes. This framework can\nnaturally accommodate the usage of multiple teachers, unlocking the information\nfrom an ensemble of models to enhance recognition capabilities. Our experiments\nshow that the proposed technique consistently outperforms the state of the art\non long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and\niNaturalist18.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1\">Andr&#xe9; Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BADet: Boundary-Aware 3D Object Detection from Point Clouds. (arXiv:2104.10330v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10330","description":"<p>Currently, existing state-of-the-art 3D object detectors are in two-stage\nparadigm. These methods typically comprise two steps: 1) Utilize a region\nproposal network to propose a handful of high-quality proposals in a bottom-up\nfashion. 2) Resize and pool the semantic features from the proposed regions to\nsummarize RoI-wise representations for further refinement. Note that these\nRoI-wise representations in step 2) are considered individually as uncorrelated\nentries when fed to following detection headers. Nevertheless, we observe these\nproposals generated by step 1) offset from ground truth somehow, emerging in\nlocal neighborhood densely with an underlying probability. Challenges arise in\nthe case where a proposal largely forsakes its boundary information due to\ncoordinate offset while existing networks lack corresponding information\ncompensation mechanism. In this paper, we propose $BADet$ for 3D object\ndetection from point clouds. Specifically, instead of refining each proposal\nindependently as previous works do, we represent each proposal as a node for\ngraph construction within a given cut-off threshold, associating proposals in\nthe form of local neighborhood graph, with boundary correlations of an object\nbeing explicitly exploited. Besides, we devise a lightweight Region Feature\nAggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise\nfeatures with expanding receptive fields for more informative RoI-wise\nrepresentations. We validate BADet both on widely used KITTI Dataset and highly\nchallenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par\nperformance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$\ndifficulty of $Car$ category on KITTI BEV detection leaderboard. The source\ncode is available at https://github.com/rui-qian/BADet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NURBS-Diff: A Differentiable Programming Module for NURBS. (arXiv:2104.14547v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.14547","description":"<p>Boundary representations (B-reps) using Non-Uniform Rational B-splines\n(NURBS) are the de facto standard used in CAD, but their utility in deep\nlearning-based approaches is not well researched. We propose a differentiable\nNURBS module to integrate NURBS representations of CAD models with deep\nlearning methods. We mathematically define the derivatives of the NURBS curves\nor surfaces with respect to the input parameters (control points, weights, and\nthe knot vector). These derivatives are used to define an approximate Jacobian\nused for performing the \"backward\" evaluation to train the deep learning\nmodels. We have implemented our NURBS module using GPU-accelerated algorithms\nand integrated it with PyTorch, a popular deep learning framework. We\ndemonstrate the efficacy of our NURBS module in performing CAD operations such\nas curve or surface fitting and surface offsetting. Further, we show its\nutility in deep learning for unsupervised point cloud reconstruction and\nenforce analysis constraints. These examples show that our module performs\nbetter for certain deep learning frameworks and can be directly integrated with\nany deep-learning framework requiring NURBS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Anjana Deva Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Adarsh Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An improved LogNNet classifier for IoT application. (arXiv:2105.14412v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14412","description":"<p>In the age of neural networks and Internet of Things (IoT), the search for\nnew neural network architectures capable of operating on devices with limited\ncomputing power and small memory size is becoming an urgent agenda. Designing\nsuitable algorithms for IoT applications is an important task. The paper\nproposes a feed forward LogNNet neural network, which uses a semi-linear Henon\ntype discrete chaotic map to classify MNIST-10 dataset. The model is composed\nof reservoir part and trainable classifier. The aim of the reservoir part is\ntransforming the inputs to maximize the classification accuracy using a special\nmatrix filing method and a time series generated by the chaotic map. The\nparameters of the chaotic map are optimized using particle swarm optimization\nwith random immigrants. As a result, the proposed LogNNet/Henon classifier has\nhigher accuracy and the same RAM usage, compared to the original version of\nLogNNet, and offers promising opportunities for implementation in IoT devices.\nIn addition, a direct relation between the value of entropy and accuracy of the\nclassification is demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hanif Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.12954","description":"<p>End-to-end optimized neural image compression (NIC) has obtained superior\nlossy compression performance recently. In this paper, we consider the problem\nof rate-distortion (R-D) characteristic analysis and modeling for NIC. We make\nefforts to formulate the essential mathematical functions to describe the R-D\nbehavior of NIC using deep networks. Thus arbitrary bit-rate points could be\nelegantly realized by leveraging such model via a single trained network. We\npropose a plugin-in module to learn the relationship between the target\nbit-rate and the binary representation for the latent variable of auto-encoder.\nThe proposed scheme resolves the problem of training distinct models to reach\ndifferent points in the R-D space. Furthermore, we model the rate and\ndistortion characteristic of NIC as a function of the coding parameter\n$\\lambda$ respectively. Our experiments show our proposed method is easy to\nadopt and realizes state-of-the-art continuous bit-rate coding performance,\nwhich implies that our approach would benefit the practical deployment of NIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Ziqing Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Depth Contribution for Camouflaged Object Detection. (arXiv:2106.13217v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13217","description":"<p>Camouflaged object detection (COD) aims to segment camouflaged objects hiding\nin the environment, which is challenging due to the similar appearance of\ncamouflaged objects and their surroundings. Research in biology suggests depth\ncan provide useful object localization cues for camouflaged object discovery.\nIn this paper, we study the depth contribution for camouflaged object\ndetection, where the depth maps are generated with existing monocular depth\nestimation (MDE) methods. Due to the domain gap between the MDE dataset and our\nCOD dataset, the generated depth maps are not accurate enough to be directly\nused. We then introduce two solutions to avoid the noisy depth maps from\ndominating the training process. Firstly, we present an auxiliary depth\nestimation branch (\"ADE\"), aiming to regress the depth maps. We find that \"ADE\"\nis especially necessary for our \"generated depth\" scenario. Secondly, we\nintroduce a multi-modal confidence-aware loss function via a generative\nadversarial network to weigh the contribution of depth for camouflaged object\ndetection. Our extensive experiments on various camouflaged object detection\ndatasets explain that the existing \"sensor depth\" based RGB-D segmentation\ntechniques work poorly with \"generated depth\", and our proposed two solutions\nwork cooperatively, achieving effective depth contribution exploration for\ncamouflaged object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1\">Mochu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Smooth Pose Sequences for Diverse Human Motion Prediction. (arXiv:2108.08422v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08422","description":"<p>Recent progress in stochastic motion prediction, i.e., predicting multiple\npossible future human motions given a single past pose sequence, has led to\nproducing truly diverse future motions and even providing control over the\nmotion of some body parts. However, to achieve this, the state-of-the-art\nmethod requires learning several mappings for diversity and a dedicated model\nfor controllable motion prediction. In this paper, we introduce a unified deep\ngenerative network for both diverse and controllable motion prediction. To this\nend, we leverage the intuition that realistic human motions consist of smooth\nsequences of valid poses, and that, given limited data, learning a pose prior\nis much more tractable than a motion one. We therefore design a generator that\npredicts the motion of different body parts sequentially, and introduce a\nnormalizing flow based pose prior, together with a joint angle loss, to achieve\nmotion realism.Our experiments on two standard benchmark datasets, Human3.6M\nand HumanEva-I, demonstrate that our approach outperforms the state-of-the-art\nbaselines in terms of both sample diversity and accuracy. The code is available\nat https://github.com/wei-mao-2019/gsps\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization Challenge. (arXiv:2108.11269v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11269","description":"<p>Assessing the Mitotic Count has a known high degree of intra- and inter-rater\nvariability. Computer-aided systems have proven to decrease this variability\nand reduce labeling time. These systems, however, are generally highly\ndependent on their training domain and show poor applicability to unseen\ndomains. In histopathology, these domain shifts can result from various\nsources, including different slide scanning systems used to digitize histologic\nsamples. The MItosis DOmain Generalization challenge focused on this specific\ndomain shift for the task of mitotic figure detection. This work presents a\nmitotic figure detection algorithm developed as a baseline for the challenge,\nbased on domain adversarial training. On the challenge's test set, the\nalgorithm scored an F$_1$ score of 0.7183. The corresponding network weights\nand code for implementing the network are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Automated Diagnosis of Retinal Disease Using Deep OCT Analysis. (arXiv:2109.02436v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.02436","description":"<p>30 million Optical Coherence Tomography (OCT) imaging tests are issued every\nyear to diagnose various retinal diseases, but accurate diagnosis of OCT scans\nrequires trained ophthalmologists who are still prone to making errors. With\nbetter systems for diagnosis, many cases of vision loss caused by retinal\ndisease could be entirely avoided. In this work, we develop a novel deep\nlearning architecture for explainable, accurate classification of retinal\ndisease which achieves state-of-the-art accuracy. Furthermore, we place an\nemphasis on producing both qualitative and quantitative explanations of the\nmodel's decisions. Our algorithm produces heatmaps indicating the exact regions\nin the OCT scan the model focused on when making its decision. In combination\nwith an OCT segmentation model, this allows us to produce quantitative\nbreakdowns of the specific retinal layers the model focused on for later review\nby an expert. Our work is the first to produce detailed quantitative\nexplanations of the model's decisions in this way. Our combination of accuracy\nand interpretability can be clinically applied for better patient care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wen_E/0/1/0/all/0/1\">Evan Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrlich_M/0/1/0/all/0/1\">Max Ehrlich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based person re-identification methods: A survey and outlook of recent works. (arXiv:2110.04764v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04764","description":"<p>In recent years, with the increasing demand for public safety and the rapid\ndevelopment of intelligent surveillance networks, person re-identification\n(Re-ID) has become one of the hot research topics in the computer vision field.\nThe main research goal of person Re-ID is to retrieve persons with the same\nidentity from different cameras. However, traditional person Re-ID methods\nrequire manual marking of person targets, which consumes a lot of labor cost.\nWith the widespread application of deep neural networks, many deep\nlearning-based person Re-ID methods have emerged. Therefore, this paper is to\nfacilitate researchers to understand the latest research results and the future\ntrends in the field. Firstly, we summarize the studies of several recently\npublished person Re-ID surveys and complement the latest research methods to\nsystematically classify deep learning-based person Re-ID methods. Secondly, we\npropose a multi-dimensional taxonomy that classifies current deep\nlearning-based person Re-ID methods into four categories according to metric\nand representation learning, including methods for deep metric learning, local\nfeature learning, generative adversarial learning and sequence feature\nlearning. Furthermore, we subdivide the above four categories according to\ntheir methodologies and motivations, discussing the advantages and limitations\nof part subcategories. Finally, we discuss some challenges and possible\nresearch directions for person Re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhangqiang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Min Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiamin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chengrui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12509","description":"<p>Estimating the lung depth on x-ray images could provide both an accurate\nopportunistic lung volume estimation during clinical routine and improve image\ncontrast in modern structural chest imaging techniques like x-ray dark-field\nimaging. We present a method based on a convolutional neural network that\nallows a per-pixel lung thickness estimation and subsequent total lung capacity\nestimation. The network was trained and validated using 5250 simulated\nradiographs generated from 525 real CT scans. Furthermore, we are able to infer\nthe model trained with simulation data on real radiographs.\n</p>\n<p>For 45 patients, quantitative and qualitative evaluation was performed on\nstandard clinical radiographs. The ground-truth for each patient's total lung\nvolume was defined based on the patients' corresponding CT scan. The\nmean-absolute error between the estimated lung volume on the 45 real\nradiographs and groundtruth volume was 0.83 liter. When accounting for the\npatient diameter, the error decreases to 0.66 liter. Additionaly, we predicted\nthe lung thicknesses on a synthetic dataset of 131 radiographs, where the\nmean-absolute error was 0.21 liter. The results show, that it is possible to\ntransfer the knowledge obtained in a simulation model to real x-ray images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schultheiss_M/0/1/0/all/0/1\">Manuel Schultheiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmette_P/0/1/0/all/0/1\">Philipp Schmette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellerer_T/0/1/0/all/0/1\">Thorsten Sellerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_R/0/1/0/all/0/1\">Rafael Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taphorn_K/0/1/0/all/0/1\">Kirsten Taphorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mechlem_K/0/1/0/all/0/1\">Korbinian Mechlem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birnbacher_L/0/1/0/all/0/1\">Lorenz Birnbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renger_B/0/1/0/all/0/1\">Bernhard Renger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_F/0/1/0/all/0/1\">Franz Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_D/0/1/0/all/0/1\">Daniela Pfeiffer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Deep Visual Tracking. (arXiv:2110.13259v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13259","description":"<p>Convolutional neural networks (CNNs) have been successfully applied to the\nsingle target tracking task in recent years. Generally, training a deep CNN\nmodel requires numerous labeled training samples, and the number and quality of\nthese samples directly affect the representational capability of the trained\nmodel. However, this approach is restrictive in practice, because manually\nlabeling such a large number of training samples is time-consuming and\nprohibitively expensive. In this paper, we propose an active learning method\nfor deep visual tracking, which selects and annotates the unlabeled samples to\ntrain the deep CNNs model. Under the guidance of active learning, the tracker\nbased on the trained deep CNNs model can achieve competitive tracking\nperformance while reducing the labeling cost. More specifically, to ensure the\ndiversity of selected samples, we propose an active learning method based on\nmulti-frame collaboration to select those training samples that should be and\nneed to be annotated. Meanwhile, considering the representativeness of these\nselected samples, we adopt a nearest neighbor discrimination method based on\nthe average nearest neighbor distance to screen isolated samples and\nlow-quality samples. Therefore, the training samples subset selected based on\nour method requires only a given budget to maintain the diversity and\nrepresentativeness of the entire sample set. Furthermore, we adopt a Tversky\nloss to improve the bounding box estimation of our tracker, which can ensure\nthat the tracker achieves more accurate target states. Extensive experimental\nresults confirm that our active learning-based tracker (ALT) achieves\ncompetitive tracking accuracy and speed compared with state-of-the-art trackers\non the seven most challenging evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Di Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dehua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash. (arXiv:2111.06628v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.06628","description":"<p>Apple recently revealed its deep perceptual hashing system NeuralHash to\ndetect child sexual abuse material (CSAM) on user devices before files are\nuploaded to its iCloud service. Public criticism quickly arose regarding the\nprotection of user privacy and the system's reliability. In this paper, we\npresent the first comprehensive empirical analysis of deep perceptual hashing\nbased on NeuralHash. Specifically, we show that current deep perceptual hashing\nmay not be robust. An adversary can manipulate the hash values by applying\nslight changes in images, either induced by gradient-based approaches or simply\nby performing standard image transformations, forcing or preventing hash\ncollisions. Such attacks permit malicious actors easily to exploit the\ndetection system: from hiding abusive material to framing innocent users,\neverything is possible. Moreover, using the hash values, inferences can still\nbe made about the data stored on user devices. In our view, based on our\nresults, deep perceptual hashing in its current form is generally not ready for\nrobust client-side scanning and should not be used from a privacy perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dyadic Human Motion Prediction. (arXiv:2112.00396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00396","description":"<p>Prior work on human motion forecasting has mostly focused on predicting the\nfuture motion of single subjects in isolation from their past pose sequence. In\nthe presence of closely interacting people, however, this strategy fails to\naccount for the dependencies between the different subject's motions. In this\npaper, we therefore introduce a motion prediction framework that explicitly\nreasons about the interactions of two observed subjects. Specifically, we\nachieve this by introducing a pairwise attention mechanism that models the\nmutual dependencies in the motion history of the two subjects. This allows us\nto preserve the long-term motion dynamics in a more realistic way and more\nrobustly predict unusual and fast-paced movements, such as the ones occurring\nin a dance scenario. To evaluate this, and because no existing motion\nprediction datasets depict two closely-interacting subjects, we introduce the\nLindyHop600K dance dataset. Our results evidence that our approach outperforms\nthe state-of-the-art single person motion prediction techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katircioglu_I/0/1/0/all/0/1\">Isinsu Katircioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgantas_C/0/1/0/all/0/1\">Costa Georgantas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Networks for Learning Physical Dynamics of Deformable Objects. (arXiv:2112.03728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03728","description":"<p>Learning the physical dynamics of deformable objects with particle-based\nrepresentation has been the objective of many computational models in machine\nlearning. While several state-of-the-art models have achieved this objective in\nsimulated environments, most existing models impose a precondition, such that\nthe input is a sequence of ordered point sets. That is, the order of the points\nin each point set must be the same across the entire input sequence. This\nprecondition restrains the model from generalizing to real-world data, which is\nconsidered to be a sequence of unordered point sets. In this paper, we propose\na model named time-wise PointNet (TP-Net) that solves this problem by directly\nconsuming a sequence of unordered point sets to infer the future state of a\ndeformable object with particle-based representation. Our model consists of a\nshared feature extractor that extracts global features from each input point\nset in parallel and a prediction network that aggregates and reasons on these\nfeatures for future prediction. The key concept of our approach is that we use\nglobal features rather than local features to achieve invariance to input\npermutations and ensure the stability and scalability of our model. Experiments\ndemonstrate that our model achieves state-of-the-art performance with real-time\nprediction speed in both synthetic dataset and real-world dataset. In addition,\nwe provide quantitative and qualitative analysis on why our approach is more\neffective and efficient than existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">DoHae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">In-Kwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15139","description":"<p>Quantized neural networks typically require smaller memory footprints and\nlower computation complexity, which is crucial for efficient deployment.\nHowever, quantization inevitably leads to a distribution divergence from the\noriginal network, which generally degrades the performance. To tackle this\nissue, massive efforts have been made, but most existing approaches lack\nstatistical considerations and depend on several manual configurations. In this\npaper, we present an adaptive-mapping quantization method to learn an optimal\nlatent sub-distribution that is inherent within models and smoothly\napproximated with a concrete Gaussian Mixture (GM). In particular, the network\nweights are projected in compliance with the GM-approximated sub-distribution.\nThis sub-distribution evolves along with the weight update in a co-tuning\nschema guided by the direct task-objective optimization. Sufficient experiments\non image classification and object detection over various modern architectures\ndemonstrate the effectiveness, generalization property, and transferability of\nthe proposed method. Besides, an efficient deployment flow for the mobile CPU\nis developed, achieving up to 7.46$\\times$ inference acceleration on an\nocta-core ARM CPU. Codes have been publicly released on Github\n(https://github.com/RunpeiDong/DGMS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhanhong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengdi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending One-Stage Detection with Open-World Proposals. (arXiv:2201.02302v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02302","description":"<p>In many applications, such as autonomous driving, hand manipulation, or robot\nnavigation, object detection methods must be able to detect objects unseen in\nthe training set. Open World Detection(OWD) seeks to tackle this problem by\ngeneralizing detection performance to seen and unseen class categories. Recent\nworks have seen success in the generation of class-agnostic proposals, which we\ncall Open-World Proposals(OWP), but this comes at the cost of a big drop on the\nclassification task when both tasks are considered in the detection model.\nThese works have investigated two-stage Region Proposal Networks (RPN) by\ntaking advantage of objectness scoring cues; however, for its simplicity,\nrun-time, and decoupling of localization and classification, we investigate OWP\nthrough the lens of fully convolutional one-stage detection network, such as\nFCOS. We show that our architectural and sampling optimizations on FCOS can\nincrease OWP performance by as much as 6% in recall on novel classes, marking\nthe first proposal-free one-stage detection network to achieve comparable\nperformance to RPN-based two-stage networks. Furthermore, we show that the\ninherent, decoupled architecture of FCOS has benefits to retaining\nclassification performance. While two-stage methods worsen by 6% in recall on\nnovel classes, we show that FCOS only drops 2% when jointly optimizing for OWP\nand classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konan_S/0/1/0/all/0/1\">Sachin Konan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kevin J Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Li Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03080","description":"<p>The rapid emergence of airborne platforms and imaging sensors are enabling\nnew forms of aerial surveillance due to their unprecedented advantages in\nscale, mobility, deployment and covert observation capabilities. This paper\nprovides a comprehensive overview of human-centric aerial surveillance tasks\nfrom a computer vision and pattern recognition perspective. It aims to provide\nreaders with an in-depth systematic review and technical analysis of the\ncurrent state of aerial surveillance tasks using drones, UAVs and other\nairborne platforms. The main object of interest is humans, where single or\nmultiple subjects are to be detected, identified, tracked, re-identified and\nhave their behavior analyzed. More specifically, for each of these four tasks,\nwe first discuss unique challenges in performing these tasks in an aerial\nsetting compared to a ground-based setting. We then review and analyze the\naerial datasets publicly available for each task, and delve deep into the\napproaches in the aerial literature and investigate how they presently address\nthe aerial challenges. We conclude the paper with discussion on the missing\ngaps and open research questions to inform future research avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Captcha Attack: Turning Captchas Against Humanity. (arXiv:2201.04014v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2201.04014","description":"<p>Nowadays, people generate and share massive content on online platforms\n(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook\nusers posted around 150 thousand photos every minute. Content moderators\nconstantly monitor these online platforms to prevent the spreading of\ninappropriate content (e.g., hate speech, nudity images). Based on deep\nlearning (DL) advances, Automatic Content Moderators (ACM) help human\nmoderators handle high data volume. Despite their advantages, attackers can\nexploit weaknesses of DL components (e.g., preprocessing, model) to affect\ntheir performance. Therefore, an attacker can leverage such techniques to\nspread inappropriate content by evading ACM.\n</p>\n<p>In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that\nallows users to spread inappropriate text online by evading ACM controls. CAPA,\nby generating custom textual CAPTCHAs, exploits ACM's careless design\nimplementations and internal procedures vulnerabilities. We test our attack on\nreal-world ACM, and the results confirm the ferocity of our simple yet\neffective attack, reaching up to a 100% evasion success in most cases. At the\nsame time, we demonstrate the difficulties in designing CAPA mitigations,\nopening new challenges in CAPTCHAs research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1\">Luca Pajola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tricomi_P/0/1/0/all/0/1\">Pier Paolo Tricomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Denoise Raw Mobile UI Layouts for Improving Datasets at Scale. (arXiv:2201.04100v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2201.04100","description":"<p>The layout of a mobile screen is a critical data source for UI design\nresearch and semantic understanding of the screen. However, UI layouts in\nexisting datasets are often noisy, have mismatches with their visual\nrepresentation, or consists of generic or app-specific types that are difficult\nto analyze and model. In this paper, we propose the CLAY pipeline that uses a\ndeep learning approach for denoising UI layouts, allowing us to automatically\nimprove existing mobile UI layout datasets at scale. Our pipeline takes both\nthe screenshot and the raw UI layout, and annotates the raw layout by removing\nincorrect nodes and assigning a semantically meaningful type to each node. To\nexperiment with our data-cleaning pipeline, we create the CLAY dataset of\n59,555 human-annotated screen layouts, based on screenshots and raw layouts\nfrom Rico, a public mobile UI corpus. Our deep models achieve high accuracy\nwith F1 scores of 82.7% for detecting layout objects that do not have a valid\nvisual representation and 85.9% for recognizing object types, which\nsignificantly outperforms a heuristic baseline. Our work lays a foundation for\ncreating large-scale high quality UI layout datasets for data-driven mobile UI\nresearch and reduces the need of manual labeling efforts that are prohibitively\nexpensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baechler_G/0/1/0/all/0/1\">Gilles Baechler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tragut_M/0/1/0/all/0/1\">Manuel Tragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adversarially Robust Deep Image Denoising. (arXiv:2201.04397v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04397","description":"<p>This work systematically investigates the adversarial robustness of deep\nimage denoisers (DIDs), i.e, how well DIDs can recover the ground truth from\nnoisy observations degraded by adversarial perturbations. Firstly, to evaluate\nDIDs' robustness, we propose a novel adversarial attack, namely\nObservation-based Zero-mean Attack ({\\sc ObsAtk}), to craft adversarial\nzero-mean perturbations on given noisy images. We find that existing DIDs are\nvulnerable to the adversarial noise generated by {\\sc ObsAtk}. Secondly, to\nrobustify DIDs, we propose an adversarial training strategy, hybrid adversarial\ntraining ({\\sc HAT}), that jointly trains DIDs with adversarial and\nnon-adversarial noisy data to ensure that the reconstruction quality is high\nand the denoisers around non-adversarial data are locally smooth. The resultant\nDIDs can effectively remove various types of synthetic and adversarial noise.\nWe also uncover that the robustness of DIDs benefits their generalization\ncapability on unseen real-world noise. Indeed, {\\sc HAT}-trained DIDs can\nrecover high-quality clean images from real-world noise even without training\non real noisy data. Extensive experiments on benchmark datasets, including\nSet68, PolyU, and SIDD, corroborate the effectiveness of {\\sc ObsAtk} and {\\sc\nHAT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}