{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Counterfactual Explanations for Natural Language Interfaces. (arXiv:2204.13192v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13192","description":"<p>A key challenge facing natural language interfaces is enabling users to\nunderstand the capabilities of the underlying system. We propose a novel\napproach for generating explanations of a natural language interface based on\nsemantic parsing. We focus on counterfactual explanations, which are post-hoc\nexplanations that describe to the user how they could have minimally modified\ntheir utterance to achieve their desired goal. In particular, the user provides\nan utterance along with a demonstration of their desired goal; then, our\nalgorithm synthesizes a paraphrase of their utterance that is guaranteed to\nachieve their goal. In two user studies, we demonstrate that our approach\nsubstantially improves user performance, and that it generates explanations\nthat more closely match the user's intent compared to two ablations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolkachev_G/0/1/0/all/0/1\">George Tolkachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mell_S/0/1/0/all/0/1\">Stephen Mell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zdancewic_S/0/1/0/all/0/1\">Steve Zdancewic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data. (arXiv:2204.13243v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13243","description":"<p>A pressing challenge in current dialogue systems is to successfully converse\nwith users on topics with information distributed across different modalities.\nPrevious work in multiturn dialogue systems has primarily focused on either\ntext or table information. In more realistic scenarios, having a joint\nunderstanding of both is critical as knowledge is typically distributed over\nboth unstructured and structured forms. We present a new dialogue dataset,\nHybriDialogue, which consists of crowdsourced natural conversations grounded on\nboth Wikipedia text and tables. The conversations are created through the\ndecomposition of complex multihop questions into simple, realistic multiturn\ndialogue interactions. We propose retrieval, system state tracking, and\ndialogue response generation tasks for our dataset and conduct baseline\nexperiments for each. Our results show that there is still ample opportunity\nfor improvement, demonstrating the importance of building stronger dialogue\nsystems that can reason over the complex setting of information-seeking\ndialogue grounded on tables and text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Kai Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Memory Networks for Radiology Report Generation. (arXiv:2204.13258v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13258","description":"<p>Medical imaging plays a significant role in clinical practice of medical\ndiagnosis, where the text reports of the images are essential in understanding\nthem and facilitating later treatments. By generating the reports\nautomatically, it is beneficial to help lighten the burden of radiologists and\nsignificantly promote clinical automation, which already attracts much\nattention in applying artificial intelligence to medical domain. Previous\nstudies mainly follow the encoder-decoder paradigm and focus on the aspect of\ntext generation, with few studies considering the importance of cross-modal\nmappings and explicitly exploit such mappings to facilitate radiology report\ngeneration. In this paper, we propose a cross-modal memory networks (CMN) to\nenhance the encoder-decoder framework for radiology report generation, where a\nshared memory is designed to record the alignment between images and texts so\nas to facilitate the interaction and generation across modalities. Experimental\nresults illustrate the effectiveness of our proposed model, where\nstate-of-the-art performance is achieved on two widely used benchmark datasets,\ni.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is\nable to better align information from radiology images and texts so as to help\ngenerating more accurate reports in terms of clinical indicators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaling Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving robustness of language models from a geometry-aware perspective. (arXiv:2204.13309v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13309","description":"<p>Recent studies have found that removing the norm-bounded projection and\nincreasing search steps in adversarial training can significantly improve\nrobustness. However, we observe that a too large number of search steps can\nhurt accuracy. We aim to obtain strong robustness efficiently using fewer\nsteps. Through a toy experiment, we find that perturbing the clean data to the\ndecision boundary but not crossing it does not degrade the test accuracy.\nInspired by this, we propose friendly adversarial data augmentation (FADA) to\ngenerate friendly adversarial data. On top of FADA, we propose geometry-aware\nadversarial training (GAT) to perform adversarial training on friendly\nadversarial data so that we can save a large number of search steps.\nComprehensive experiments across two widely used datasets and three pre-trained\nlanguage models demonstrate that GAT can obtain stronger robustness via fewer\nsteps. In addition, we provide extensive empirical results and in-depth\nanalyses on robustness to facilitate future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhaoquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Copenhagen Corpus of Eye Tracking Recordings from Natural Reading of Danish Texts. (arXiv:2204.13311v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13311","description":"<p>Eye movement recordings from reading are one of the richest signals of human\nlanguage processing. Corpora of eye movements during reading of contextualized\nrunning text is a way of making such records available for natural language\nprocessing purposes. Such corpora already exist in some languages. We present\nCopCo, the Copenhagen Corpus of eye tracking recordings from natural reading of\nDanish texts. It is the first eye tracking corpus of its kind for the Danish\nlanguage. CopCo includes 1,832 sentences with 34,897 tokens of Danish text\nextracted from a collection of speech manuscripts. This first release of the\ncorpus contains eye tracking data from 22 participants. It will be extended\ncontinuously with more participants and texts from other genres. We assess the\ndata quality of the recorded eye movements and find that the extracted features\nare in line with related research. The dataset available here:\nhttps://osf.io/ud8s5/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_M/0/1/0/all/0/1\">Maria Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjornsdottir_M/0/1/0/all/0/1\">Marina Bj&#xf6;rnsd&#xf3;ttir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniTE: Unified Translation Evaluation. (arXiv:2204.13346v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13346","description":"<p>Translation quality evaluation plays a crucial role in machine translation.\nAccording to the input format, it is mainly separated into three tasks, i.e.,\nreference-only, source-only and source-reference-combined. Recent methods,\ndespite their promising results, are specifically designed and optimized on one\nof them. This limits the convenience of these methods, and overlooks the\ncommonalities among tasks. In this paper, we propose UniTE, which is the first\nunified framework engaged with abilities to handle all three evaluation tasks.\nConcretely, we propose monotonic regional attention to control the interaction\namong input segments, and unified pretraining to better adapt multi-task\nlearning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality\nEstimation benchmarks. Extensive analyses show that our \\textit{single model}\ncan universally surpass various state-of-the-art or winner methods across\ntasks. Both source code and associated models are available at\nhttps://github.com/NLP2CT/UniTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoBLEURT Submission for the WMT2021 Metrics Task. (arXiv:2204.13352v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13352","description":"<p>In this paper, we present our submission to Shared Metrics Task: RoBLEURT\n(Robustly Optimizing the training of BLEURT). After investigating the recent\nadvances of trainable metrics, we conclude several aspects of vital importance\nto obtain a well-performed metric model by: 1) jointly leveraging the\nadvantages of source-included model and reference-only model, 2) continuously\npre-training the model with massive synthetic data pairs, and 3) fine-tuning\nthe model with data denoising strategy. Experimental results show that our\nmodel reaching state-of-the-art correlations with the WMT2020 human annotations\nupon 8 out of 10 to-English language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_T/0/1/0/all/0/1\">Tianchi Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Mechanism with Energy-Friendly Operations. (arXiv:2204.13353v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13353","description":"<p>Attention mechanism has become the dominant module in natural language\nprocessing models. It is computationally intensive and depends on massive\npower-hungry multiplications. In this paper, we rethink variants of attention\nmechanism from the energy consumption aspects. After reaching the conclusion\nthat the energy costs of several energy-friendly operations are far less than\ntheir multiplication counterparts, we build a novel attention model by\nreplacing multiplications with either selective operations or additions.\nEmpirical results on three machine translation tasks demonstrate that the\nproposed model, against the vanilla one, achieves competitable accuracy while\nsaving 99\\% and 66\\% energy during alignment calculation and the whole\nattention procedure. Code is available at: https://github.com/NLP2CT/E-Att.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Rong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints. (arXiv:2204.13355v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13355","description":"<p>However, current autoregressive approaches suffer from high latency. In this\npaper, we focus on non-autoregressive translation (NAT) for this problem for\nits efficiency advantage. We identify that current constrained NAT models,\nwhich are based on iterative editing, do not handle low-frequency constraints\nwell. To this end, we propose a plug-in algorithm for this line of work, i.e.,\nAligned Constrained Training (ACT), which alleviates this problem by\nfamiliarizing the model with the source-side context of the constraints.\nExperiments on the general and domain datasets show that our model improves\nover the backbone constrained NAT model in constraint preservation and\ntranslation quality, especially for rare constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tianyi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Ying Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation. (arXiv:2204.13362v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13362","description":"<p>Attribute-based Controlled Text Generation (CTG) refers to generating\nsentences that satisfy desirable attributes (e.g., emotions and topics).\nExisting works often utilize fine-tuning or resort to extra attribute\nclassifiers, yet suffer from storage and inference time increases. To address\nthese concerns, we explore attribute-based CTG in a prompt-based manner. In\nshort, the proposed Tailor represents each attribute as a pre-trained\ncontinuous vector (i.e., single-attribute prompt) and guides the generation of\na fixed PLM switch to a pre-specified attribute. We experimentally find that\nthese prompts can be simply concatenated as a whole to multi-attribute CTG\nwithout any re-training, yet raises problems of fluency decrease and position\nsensitivity. To this end, Tailor provides a multi-attribute prompt mask and a\nre-indexing position-ids sequence to bridge the gap between the training (one\nprompt for each task) and testing stage (concatenating more than one prompt).\nTo further enhance such single-attribute prompt combinations, Tailor also\nintroduces a trainable prompt connector, which can be concatenated with any two\nsingle-attribute prompts to multi-attribute text generation. Experiments on 11\nattribute-specific generation tasks demonstrate strong performances of Tailor\non both single-attribute and multi-attribute CTG, with 0.08\\% training\nparameters of a GPT-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kexin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v1 [cs.DL])","link":"http://arxiv.org/abs/2204.13384","description":"<p>DBLP is the largest open-access repository of scientific articles on computer\nscience and provides metadata associated with publications, authors, and\nvenues. We retrieved more than 6 million publications from DBLP and extracted\npertinent metadata (e.g., abstracts, author affiliations, citations) from the\npublication texts to create the DBLP Discovery Dataset (D3). D3 can be used to\nidentify trends in research activity, productivity, focus, bias, accessibility,\nand impact of computer science research. We present an initial analysis focused\non the volume of computer science research (e.g., number of papers, authors,\nresearch activity), trends in topics of interest, and citation patterns. Our\nfindings show that computer science is a growing research field (approx. 15%\nannually), with an active and collaborative researcher community. While papers\nin recent years present more bibliographical entries in comparison to previous\ndecades, the average number of citations has been declining. Investigating\npapers' abstracts reveals that recent topic trends are clearly reflected in D3.\nFinally, we list further applications of D3 and pose supplemental research\nquestions. The D3 dataset, our findings, and source code are publicly available\nfor research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online. (arXiv:2204.13400v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13400","description":"<p>Even though hate speech (HS) online has been an important object of research\nin the last decade, most HS-related corpora over-simplify the phenomenon of\nhate by attempting to label user comments as \"hate\" or \"neutral\". This ignores\nthe complex and subjective nature of HS, which limits the real-life\napplicability of classifiers trained on these corpora. In this study, we\npresent the M-Phasis corpus, a corpus of ~9k German and French user comments\ncollected from migration-related news articles. It goes beyond the\n\"hate\"-\"neutral\" dichotomy and is instead annotated with 23 features, which in\ncombination become descriptors of various types of speech, ranging from\ncritical comments to implicit and explicit expressions of hate. The annotations\nare performed by 4 native speakers per language and achieve high (0.77 &lt;= k &lt;=\n1) inter-annotator agreements. Besides describing the corpus creation and\npresenting insights from a content, error and domain analysis, we explore its\ndata characteristics by training several classification baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiners_L/0/1/0/all/0/1\">Liane Reiners</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSa_A/0/1/0/all/0/1\">Ashwin Geet D&#x27;Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fohr_D/0/1/0/all/0/1\">Dominique Fohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Illina_I/0/1/0/all/0/1\">Irina Illina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schemer_C/0/1/0/all/0/1\">Christian Schemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monnier_A/0/1/0/all/0/1\">Angeliki Monnier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeaNF: Weak Supervision with Normalizing Flows. (arXiv:2204.13409v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13409","description":"<p>A popular approach to decrease the need for costly manual annotation of large\ndata sets is weak supervision, which introduces problems of noisy labels,\ncoverage and bias. Methods for overcoming these problems have either relied on\ndiscriminative models, trained with cost functions specific to weak\nsupervision, and more recently, generative models, trying to model the output\nof the automatic annotation process. In this work, we explore a novel direction\nof generative modeling for weak supervision: Instead of modeling the output of\nthe annotation process (the labeling function matches), we generatively model\nthe input-side data distributions (the feature space) covered by labeling\nfunctions. Specifically, we estimate a density for each weak labeling source,\nor labeling function, by using normalizing flows. An integral part of our\nmethod is the flow-based modeling of multiple simultaneously matching labeling\nfunctions, and therefore phenomena such as labeling function overlap and\ncorrelations are captured. We analyze the effectiveness and modeling\ncapabilities on various commonly used weak supervision data sets, and show that\nweakly supervised normalizing flows compare favorably to standard weak\nsupervision baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification. (arXiv:2204.13413v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13413","description":"<p>Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex label hierarchy. Recently, the\npretrained language models (PLM) have been widely adopted in HTC through a\nfine-tuning paradigm. However, in this paradigm, there exists a huge gap\nbetween the classification tasks with sophisticated label hierarchy and the\nmasked language model (MLM) pretraining tasks of PLMs and thus the potentials\nof PLMs can not be fully tapped. To bridge the gap, in this paper, we propose\nHPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label\nMLM perspective. Specifically, we construct dynamic virtual template and label\nwords which take the form of soft prompts to fuse the label hierarchy knowledge\nand introduce a zero-bounded multi-label cross entropy loss to harmonize the\nobjectives of HTC and MLM. Extensive experiments show HPT achieves the\nstate-of-the-art performances on 3 popular HTC datasets and is adept at\nhandling the imbalance and low resource situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying Multilingual News Clustering Through Projection From a Shared Space. (arXiv:2204.13418v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13418","description":"<p>The task of organizing and clustering multilingual news articles for media\nmonitoring is essential to follow news stories in real time. Most approaches to\nthis task focus on high-resource languages (mostly English), with low-resource\nlanguages being disregarded. With that in mind, we present a much simpler\nonline system that is able to cluster an incoming stream of documents without\ndepending on language-specific features. We empirically demonstrate that the\nuse of multilingual contextual embeddings as the document representation\nsignificantly improves clustering quality. We challenge previous crosslingual\napproaches by removing the precondition of building monolingual clusters. We\nmodel the clustering process as a set of linear classifiers to aggregate\nsimilar documents, and correct closely-related multilingual clusters through\nmerging in an online fashion. Our system achieves state-of-the-art results on a\nmultilingual news stream clustering dataset, and we introduce a new evaluation\nfor zero-shot news clustering in multiple languages. We make our code available\nas open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jo&#xe3;o Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_A/0/1/0/all/0/1\">Afonso Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_S/0/1/0/all/0/1\">Sebasti&#xe3;o Miranda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification. (arXiv:2204.13496v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13496","description":"<p>Knowledge-based authentication is crucial for task-oriented spoken dialogue\nsystems that offer personalised and privacy-focused services. Such systems\nshould be able to enrol (E), verify (V), and identify (I) new and recurring\nusers based on their personal information, e.g. postcode, name, and date of\nbirth. In this work, we formalise the three authentication tasks and their\nevaluation protocols, and we present EVI, a challenging spoken multilingual\ndataset with 5,506 dialogues in English, Polish, and French. Our proposed\nmodels set the first competitive benchmarks, explore the challenges of\nmultilingual natural language processing of spoken dialogue, and set directions\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spithourakis_G/0/1/0/all/0/1\">Georgios P. Spithourakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lis_M/0/1/0/all/0/1\">Micha&#x142; Lis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-Training Dialogue Summarization using Pseudo-Paraphrasing. (arXiv:2204.13498v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13498","description":"<p>Previous dialogue summarization techniques adapt large language models\npretrained on the narrative text by injecting dialogue-specific features into\nthe models. These features either require additional knowledge to recognize or\nmake the resulting models harder to tune. To bridge the format gap between\ndialogues and narrative summaries in dialogue summarization tasks, we propose\nto post-train pretrained language models (PLMs) to rephrase from dialogue to\nnarratives. After that, the model is fine-tuned for dialogue summarization as\nusual. Comprehensive experiments show that our approach significantly improves\nvanilla PLMs on dialogue summarization and outperforms other SOTA models by the\nsummary quality and implementation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haifeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model. (arXiv:2204.13509v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13509","description":"<p>Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seongjin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Hwijeen Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyoungSeok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gichang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobBERTje: a Distilled Dutch BERT Model. (arXiv:2204.13511v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13511","description":"<p>Pre-trained large-scale language models such as BERT have gained a lot of\nattention thanks to their outstanding performance on a wide range of natural\nlanguage tasks. However, due to their large number of parameters, they are\nresource-intensive both to deploy and to fine-tune. Researchers have created\nseveral methods for distilling language models into smaller ones to increase\nefficiency, with a small performance trade-off. In this paper, we create\nseveral different distilled versions of the state-of-the-art Dutch RobBERT\nmodel and call them RobBERTje. The distillations differ in their distillation\ncorpus, namely whether or not they are shuffled and whether they are merged\nwith subsequent sentences. We found that the performance of the models using\nthe shuffled versus non-shuffled datasets is similar for most tasks and that\nrandomly merging subsequent sentences in a corpus creates models that train\nfaster and perform better on tasks with long sequences. Upon comparing\ndistillation architectures, we found that the larger DistilBERT architecture\nworked significantly better than the Bort hyperparametrization. Interestingly,\nwe also found that the distilled models exhibit less gender-stereotypical bias\nthan its teacher model. Since smaller architectures decrease the time to\nfine-tune, these models allow for more efficient training and more lightweight\ndeployment of many Dutch downstream language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delobelle_P/0/1/0/all/0/1\">Pieter Delobelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winters_T/0/1/0/all/0/1\">Thomas Winters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1\">Bettina Berendt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization. (arXiv:2204.13512v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13512","description":"<p>In zero-shot multilingual extractive text summarization, a model is typically\ntrained on English summarization dataset and then applied on summarization\ndatasets of other languages. Given English gold summaries and documents,\nsentence-level labels for extractive summarization are usually generated using\nheuristics. However, these monolingual labels created on English datasets may\nnot be optimal on datasets of other languages, for that there is the syntactic\nor semantic discrepancy between different languages. In this way, it is\npossible to translate the English dataset to other languages and obtain\ndifferent sets of labels again using heuristics. To fully leverage the\ninformation of these different sets of labels, we propose NLSSum (Neural Label\nSearch for Summarization), which jointly learns hierarchical weights for these\ndifferent sets of labels together with our summarization model. We conduct\nmultilingual zero-shot summarization experiments on MLSUM and WikiLingua\ndatasets, and we achieve state-of-the-art results using both human and\nautomatic evaluations across these two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruipeng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer. (arXiv:2204.13515v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13515","description":"<p>Building real-world complex Named Entity Recognition (NER) systems is a\nchallenging task. This is due to the complexity and ambiguity of named entities\nthat appear in various contexts such as short input sentences, emerging\nentities, and complex entities. Besides, real-world queries are mostly\nmalformed, as they can be code-mixed or multilingual, among other scenarios. In\nthis paper, we introduce our submitted system to the Multilingual Complex Named\nEntity Recognition (MultiCoNER) shared task. We approach the complex NER for\nmultilingual and code-mixed queries, by relying on the contextualized\nrepresentation provided by the multilingual Transformer XLM-RoBERTa. In\naddition to the CRF-based token classification layer, we incorporate a span\nclassification loss to recognize named entities spans. Furthermore, we use a\nself-training mechanism to generate weakly-annotated data from a large\nunlabeled dataset. Our proposed system is ranked 6th and 8th in the\nmultilingual and code-mixed MultiCoNER's tracks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akallouch_M/0/1/0/all/0/1\">Mohammed Akallouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1\">Ahmed Khoumsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification. (arXiv:2204.13516v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13516","description":"<p>Over the last five years, research on Relation Extraction (RE) witnessed\nextensive progress with many new dataset releases. At the same time, setup\nclarity has decreased, contributing to increased difficulty of reliable\nempirical evaluation (Taill\\'e et al., 2020). In this paper, we provide a\ncomprehensive survey of RE datasets, and revisit the task definition and its\nadoption by the community. We find that cross-dataset and cross-domain setups\nare particularly lacking. We present an empirical study on scientific Relation\nClassification across two datasets. Despite large data overlap, our analysis\nreveals substantial discrepancies in annotation. Annotation discrepancies\nstrongly impact Relation Classification performance, explaining large drops in\ncross-dataset evaluations. Variation within further sub-domains exists but\nimpacts Relation Classification only to limited degrees. Overall, our study\ncalls for more rigour in reporting setups in RE and evaluation across multiple\ntest sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Violence Risk Assessment Using Dutch Clinical Notes. (arXiv:2204.13535v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13535","description":"<p>Violence risk assessment in psychiatric institutions enables interventions to\navoid violence incidents. Clinical notes written by practitioners and available\nin electronic health records are valuable resources capturing unique\ninformation, but are seldom used to their full potential. We explore\nconventional and deep machine learning methods to assess violence risk in\npsychiatric patients using practitioner notes. The performance of our best\nmodels is comparable to the currently used questionnaire-based method, with an\narea under the Receiver Operating Characteristic curve of approximately 0.8. We\nfind that the deep-learning model BERTje performs worse than conventional\nmachine learning methods. We also evaluate our data and our classifiers to\nunderstand the performance of our models better. This is particularly important\nfor the applicability of evaluated classifiers to new data, and is also of\ngreat interest to practitioners, due to the increased availability of new data\nin electronic format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosteiro_P/0/1/0/all/0/1\">Pablo Mosteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijcken_E/0/1/0/all/0/1\">Emil Rijcken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zervanou_K/0/1/0/all/0/1\">Kalliopi Zervanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaymak_U/0/1/0/all/0/1\">Uzay Kaymak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheepers_F/0/1/0/all/0/1\">Floortje Scheepers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1\">Marco Spruit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life is not Always Depressing: Exploring the Happy Moments of People Diagnosed with Depression. (arXiv:2204.13569v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13569","description":"<p>In this work, we explore the relationship between depression and\nmanifestations of happiness in social media. While the majority of works\nsurrounding depression focus on symptoms, psychological research shows that\nthere is a strong link between seeking happiness and being diagnosed with\ndepression. We make use of Positive-Unlabeled learning paradigm to\nautomatically extract happy moments from social media posts of both controls\nand users diagnosed with depression, and qualitatively analyze them with\nlinguistic tools such as LIWC and keyness information. We show that the life of\ndepressed individuals is not always bleak, with positive events related to\nfriends and family being more noteworthy to their lives compared to the more\nmundane happy events reported by control users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeSHup: A Corpus for Full Text Biomedical Document Indexing. (arXiv:2204.13604v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13604","description":"<p>Medical Subject Heading (MeSH) indexing refers to the problem of assigning a\ngiven biomedical document with the most relevant labels from an extremely large\nset of MeSH terms. Currently, the vast number of biomedical articles in the\nPubMed database are manually annotated by human curators, which is time\nconsuming and costly; therefore, a computational system that can assist the\nindexing is highly valuable. When developing supervised MeSH indexing systems,\nthe availability of a large-scale annotated text corpus is desirable. A\npublicly available, large corpus that permits robust evaluation and comparison\nof various systems is important to the research community. We release a large\nscale annotated MeSH indexing corpus, MeSHup, which contains 1,342,667 full\ntext articles in English, together with the associated MeSH labels and\nmetadata, authors, and publication venues that are collected from the MEDLINE\ndatabase. We train an end-to-end model that combines features from documents\nand their associated labels on our corpus and report the new baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xindi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_R/0/1/0/all/0/1\">Robert E. Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Russian Texts Detoxification with Levenshtein Editing. (arXiv:2204.13638v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13638","description":"<p>Text detoxification is a style transfer task of creating neutral versions of\ntoxic texts. In this paper, we use the concept of text editing to build a\ntwo-step tagging-based detoxification model using a parallel corpus of Russian\ntexts. With this model, we achieved the best style transfer accuracy among all\nmodels in the RUSSE Detox shared task, surpassing larger sequence-to-sequence\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures. (arXiv:2204.13692v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13692","description":"<p>Being able to rank the similarity of short text segments is an interesting\nbonus feature of neural machine translation. Translation-based similarity\nmeasures include direct and pivot translation probability, as well as\ntranslation cross-likelihood, which has not been studied so far. We analyze\nthese measures in the common framework of multilingual NMT, releasing the\nNMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared\nto baselines such as sentence embeddings, translation-based measures prove\ncompetitive in paraphrase identification and are more robust against\nadversarial or multilingual input, especially if proper normalization is\napplied. When used for reference-based evaluation of data-to-text generation in\n2 tasks and 17 languages, translation-based measures show a relatively high\ncorrelation to human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Radiology Reports via Memory-driven Transformer. (arXiv:2010.16056v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16056","description":"<p>Medical imaging is frequently used in clinical practice and trials for\ndiagnosis and treatment. Writing imaging reports is time-consuming and can be\nerror-prone for inexperienced radiologists. Therefore, automatically generating\nradiology reports is highly desired to lighten the workload of radiologists and\naccordingly promote clinical automation, which is an essential task to apply\nartificial intelligence to the medical domain. In this paper, we propose to\ngenerate radiology reports with memory-driven Transformer, where a relational\nmemory is designed to record key information of the generation process and a\nmemory-driven conditional layer normalization is applied to incorporating the\nmemory into the decoder of Transformer. Experimental results on two prevailing\nradiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed\napproach outperforms previous models with respect to both language generation\nmetrics and clinical evaluations. Particularly, this is the first work\nreporting the generation results on MIMIC-CXR to the best of our knowledge.\nFurther analyses also demonstrate that our approach is able to generate long\nreports with necessary medical terms as well as meaningful image-text attention\nmappings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleKQC: A Style-Variant Paraphrase Corpus for Korean Questions and Commands. (arXiv:2103.13439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.13439","description":"<p>Paraphrasing is often performed with less concern for controlled style\nconversion. Especially for questions and commands, style-variant paraphrasing\ncan be crucial in tone and manner, which also matters with industrial\napplications such as dialog systems. In this paper, we attack this issue with a\ncorpus construction scheme that simultaneously considers the core content and\nstyle of directives, namely intent and formality, for the Korean language.\nUtilizing manually generated natural language queries on six daily topics, we\nexpand the corpus to formal and informal sentences by human rewriting and\ntransferring. We verify the validity and industrial applicability of our\napproach by checking the adequate classification and inference performance that\nfit with conventional fine-tuning approaches, at the same time proposing a\nsupervised formality transfer task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sangwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seok Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency Training with Virtual Adversarial Discrete Perturbation. (arXiv:2104.07284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07284","description":"<p>Consistency training regularizes a model by enforcing predictions of original\nand perturbed inputs to be similar. Previous studies have proposed various\naugmentation methods for the perturbation but are limited in that they are\nagnostic to the training model. Thus, the perturbed samples may not aid in\nregularization due to their ease of classification from the model. In this\ncontext, we propose an augmentation method of adding a discrete noise that\nwould incur the highest divergence between predictions. This virtual\nadversarial discrete noise obtained by replacing a small portion of tokens\nwhile keeping original semantics as much as possible efficiently pushes a\ntraining model's decision boundary. Experimental results show that our proposed\nmethod outperforms other consistency training baselines with text editing,\nparaphrasing, or a continuous noise on semi-supervised text classification\ntasks and a robustness benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Dialogue Summarization: Recent Advances and New Frontiers. (arXiv:2107.03175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03175","description":"<p>Dialogue summarization aims to condense the original dialogue into a shorter\nversion covering salient information, which is a crucial way to reduce dialogue\ndata overload. Recently, the promising achievements in both dialogue systems\nand natural language generation techniques drastically lead this task to a new\nlandscape, which results in significant research attentions. However, there\nstill remains a lack of a comprehensive survey for this task. To this end, we\ntake the first step and present a thorough review of this research field\ncarefully and widely. In detail, we systematically organize the current works\naccording to the characteristics of each domain, covering meeting, chat, email\nthread, customer service and medical dialogue. Additionally, we provide an\noverview of publicly available research datasets as well as organize two\nleaderboards under unified metrics. Furthermore, we discuss some future\ndirections, including faithfulness, multi-modal, multi-domain and multi-lingual\ndialogue summarization, and give our thoughts respectively. We hope that this\nfirst survey of dialogue summarization can provide the community with a quick\naccess and a general picture to this task and motivate future researches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03587","description":"<p>Sarcasm employs ambivalence, where one says something positive but actually\nmeans negative, and vice versa. The essence of sarcasm, which is also a\nsufficient and necessary condition, is the conflict between literal and implied\nsentiments expressed in one sentence. However, it is difficult to recognize\nsuch sentiment conflict because the sentiments are mixed or even implicit. As a\nresult, the recognition of sophisticated and obscure sentiment brings in a\ngreat challenge to sarcasm detection. In this paper, we propose a Dual-Channel\nFramework by modeling both literal and implied sentiments separately. Based on\nthis dual-channel framework, we design the Dual-Channel Network~(DC-Net) to\nrecognize sentiment conflict. Experiments on political debates (i.e. IAC-V1 and\nIAC-V2) and Twitter datasets show that our proposed DC-Net achieves\nstate-of-the-art performance on sarcasm recognition. Our code is released to\nsupport research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04825","description":"<p>The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Alexander Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning. (arXiv:2111.04198v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04198","description":"<p>Masked language models (MLMs) such as BERT and RoBERTa have revolutionized\nthe field of Natural Language Understanding in the past few years. However,\nexisting pre-trained MLMs often output an anisotropic distribution of token\nrepresentations that occupies a narrow subset of the entire representation\nspace. Such token representations are not ideal, especially for tasks that\ndemand discriminative semantic meanings of distinct tokens. In this work, we\npropose TaCL (Token-aware Contrastive Learning), a novel continual pre-training\napproach that encourages BERT to learn an isotropic and discriminative\ndistribution of token representations. TaCL is fully unsupervised and requires\nno additional data. We extensively test our approach on a wide range of English\nand Chinese benchmarks. The results show that TaCL brings consistent and\nnotable improvements over the original BERT model. Furthermore, we conduct\ndetailed analysis to reveal the merits and inner-workings of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Learning for Unsupervised Knowledge Grounded Dialogs. (arXiv:2112.00653v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.00653","description":"<p>Recent methods for knowledge grounded dialogs generate responses by\nincorporating information from an external textual document. These methods do\nnot require the exact document to be known during training and rely on the use\nof a retrieval system to fetch relevant documents from a large index. The\ndocuments used to generate the responses are modeled as latent variables whose\nprior probabilities need to be estimated. Models such as RAG and REALM,\nmarginalize the document probabilities over the documents retrieved from the\nindex to define the log likelihood loss function which is optimized end-to-end.\n</p>\n<p>In this paper, we develop a variational approach to the above technique\nwherein, we instead maximize the Evidence Lower bound (ELBO). Using a\ncollection of three publicly available open-conversation datasets, we\ndemonstrate how the posterior distribution, that has information from the\nground-truth response, allows for a better approximation of the objective\nfunction during training. To overcome the challenges associated with sampling\nover a large knowledge collection, we develop an efficient approach to\napproximate the ELBO. To the best of our knowledge we are the first to apply\nvariational training for open-scale unsupervised knowledge grounded dialog\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_D/0/1/0/all/0/1\">Dhiraj Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06223","description":"<p>Code-switching is a speech phenomenon occurring when a speaker switches\nlanguage during a conversation. Despite the spontaneous nature of\ncode-switching in conversational spoken language, most existing works collect\ncode-switching data from read speech instead of spontaneous speech. ASCEND (A\nSpontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English\ncode-switching corpus built on spontaneous multi-turn conversational dialogue\nsources collected in Hong Kong. We report ASCEND's design and procedure for\ncollecting the speech data, including annotations. ASCEND consists of 10.62\nhours of clean speech, collected from 23 bilingual speakers of Chinese and\nEnglish. Furthermore, we conduct baseline experiments using pre-trained wav2vec\n2.0 models, achieving a best performance of 22.69\\% character error rate and\n27.05% mixed error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Human Evaluation for Relative Model Comparisons. (arXiv:2112.08048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08048","description":"<p>Collecting human judgements is currently the most reliable evaluation method\nfor natural language generation systems. Automatic metrics have reported flaws\nwhen applied to measure quality aspects of generated text and have been shown\nto correlate poorly with human judgements. However, human evaluation is time\nand cost-intensive, and we lack consensus on designing and conducting human\nevaluation experiments. Thus there is a need for streamlined approaches for\nefficient collection of human judgements when evaluating natural language\ngeneration systems. Therefore, we present a dynamic approach to measure the\nrequired number of human annotations when evaluating generated outputs in\nrelative comparison settings. We propose an agent-based framework of human\nevaluation to assess multiple labelling strategies and methods to decide the\nbetter model in a simulation and a crowdsourcing case study. The main results\nindicate that a decision about the superior model can be made with high\nprobability across different labelling strategies, where assigning a single\nrandom worker per task requires the least overall labelling effort and thus the\nleast cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thorleiksdottir_T/0/1/0/all/0/1\">Th&#xf3;rhildur Thorleiksd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1\">Cedric Renggli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DREAM: Improving Situational QA by First Elaborating the Situation. (arXiv:2112.08656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08656","description":"<p>When people answer questions about a specific situation, e.g., \"I cheated on\nmy mid-term exam last week. Was that wrong?\", cognitive science suggests that\nthey form a mental picture of that situation before answering. While we do not\nknow how language models (LMs) answer such questions, we conjecture that they\nmay answer more accurately if they are also provided with additional details\nabout the question situation, elaborating the \"scene\". To test this conjecture,\nwe train a new model, DREAM, to answer questions that elaborate the scenes that\nsituated questions are about, and then provide those elaborations as additional\ncontext to a question-answering (QA) model. We find that DREAM is able to\ncreate better scene elaborations (more accurate, useful, and consistent) than a\nrepresentative state-of-the-art, zero-shot model (Macaw). We also find that\nusing the scene elaborations as additional context improves the answer accuracy\nof a downstream QA system, including beyond that obtainable by simply further\nfinetuning the QA system on DREAM's training data. These results suggest that\nadding focused elaborations about a situation can improve a system's reasoning\nabout it, and may serve as an effective way of injecting new scenario based\nknowledge into QA models. Finally, our approach is dataset-neutral; we observe\nimproved QA performance across different models, with even bigger gains on\nmodels with fewer parameters. We make our dataset and model publicly available\nat https://github.com/allenai/dream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Contextual Embeddings and their Extraction Layers for Depression Assessment. (arXiv:2112.13795v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13795","description":"<p>Recent works have demonstrated ability to assess aspects of mental health\nfrom personal discourse. At the same time, pre-trained contextual word\nembedding models have grown to dominate much of NLP but little is known\nempirically on how to best apply them for mental health assessment. Using\ndegree of depression as a case study, we do an empirical analysis on which\noff-the-shelf language model, individual layers, and combinations of layers\nseem most promising when applied to human-level NLP tasks. Notably, we find\nRoBERTa most effective and, despite the standard in past work suggesting the\nsecond-to-last or concatenation of the last 4 layers, we find layer 19\n(sixth-to last) is at least as good as layer 23 when using 1 layer. Further,\nwhen using multiple layers, distributing them across the second half (i.e.\nLayers 12+), rather than last 4, of the 24 layers yielded the most accurate\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matero_M/0/1/0/all/0/1\">Matthew Matero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_A/0/1/0/all/0/1\">Albert Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Pre-training Language Model for Semantic Network Completion. (arXiv:2201.04843v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04843","description":"<p>Semantic networks, such as the knowledge graph, can represent the knowledge\nleveraging the graph structure. Although the knowledge graph shows promising\nvalues in natural language processing, it suffers from incompleteness. This\npaper focuses on knowledge graph completion by predicting linkage between\nentities, which is a fundamental yet critical task. Semantic matching is a\npotential solution as it can deal with unseen entities, which the translational\ndistance based methods struggle with. However, to achieve competitive\nperformance as translational distance based methods, semantic matching based\nmethods require large-scale datasets for the training purpose, which are\ntypically unavailable in practical settings. Therefore, we employ the language\nmodel and introduce a novel knowledge graph architecture named LP-BERT, which\ncontains two main stages: multi-task pre-training and knowledge graph\nfine-tuning. In the pre-training phase, three tasks are taken to drive the\nmodel to learn the relationship from triples by predicting either entities or\nrelations. While in the fine-tuning phase, inspired by contrastive learning, we\ndesign a triple-style negative sampling in a batch, which greatly increases the\nproportion of negative sampling while keeping the training time almost\nunchanged. Furthermore, we propose a new data augmentation method utilizing the\ninverse relationship of triples to improve the performance and robustness of\nthe model. To demonstrate the effectiveness of our method, we conduct extensive\nexperiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The\nexperimental results demonstrate the superiority of our methods, and our\napproach achieves state-of-the-art results on WN18RR and FB15k-237 datasets.\nSignificantly, Hits@10 indicator is improved by 5% from previous\nstate-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Ming Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yukai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NorDiaChange: Diachronic Semantic Change Dataset for Norwegian. (arXiv:2201.05123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05123","description":"<p>We describe NorDiaChange: the first diachronic semantic change dataset for\nNorwegian. NorDiaChange comprises two novel subsets, covering about 80\nNorwegian nouns manually annotated with graded semantic change over time. Both\ndatasets follow the same annotation procedure and can be used interchangeably\nas train and test splits for each other. NorDiaChange covers the time periods\nrelated to pre- and post-war events, oil and gas discovery in Norway, and\ntechnological developments. The annotation was done using the DURel framework\nand two large historical Norwegian corpora. NorDiaChange is published in full\nunder a permissive licence, complete with raw annotation data and inferred\ndiachronic word usage graphs (DWUGs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maehlum_P/0/1/0/all/0/1\">Petter M&#xe6;hlum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enstad_T/0/1/0/all/0/1\">Tita Ranveig Enstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittemann_A/0/1/0/all/0/1\">Alexandra Wittemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00408","description":"<p>The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10496","description":"<p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10939","description":"<p>Document intelligence automates the extraction of information from documents\nand supports many business applications. Recent self-supervised learning\nmethods on large-scale unlabeled document datasets have opened up promising\ndirections towards reducing annotation efforts by training models with\nself-supervised objectives. However, most of the existing document pretraining\nmethods are still language-dominated. We present UDoc, a new unified\npretraining framework for document understanding. UDoc is designed to support\nmost document understanding tasks, extending the Transformer to take multimodal\nembeddings as input. Each input element is composed of words and visual\nfeatures from a semantic region of the input document image. An important\nfeature of UDoc is that it learns a generic representation by making use of\nthree self-supervised losses, encouraging the representation to model\nsentences, learn similarities, and align modalities. Extensive empirical\nanalysis demonstrates that the pretraining procedure learns better joint\nrepresentations and leads to improvements in downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad I. Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Dialogue Summarization System for Sales Calls. (arXiv:2204.12951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12951","description":"<p>Summarizing sales calls is a routine task performed manually by salespeople.\nWe present a production system which combines generative models fine-tuned for\ncustomer-agent setting, with a human-in-the-loop user experience for an\ninteractive summary curation process. We address challenging aspects of\ndialogue summarization task in a real-world setting including long input\ndialogues, content validation, lack of labeled data and quality evaluation. We\nshow how GPT-3 can be leveraged as an offline data labeler to handle training\ndata scarcity and accommodate privacy constraints in an industrial setting.\nExperiments show significant improvements by our models in tackling the\nsummarization and content validation tasks on public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asi_A/0/1/0/all/0/1\">Abedelkadir Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstadt_R/0/1/0/all/0/1\">Roy Eisenstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geckt_D/0/1/0/all/0/1\">Dean Geckt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuper_Y/0/1/0/all/0/1\">Yarin Kuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_R/0/1/0/all/0/1\">Royi Ronen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13021","description":"<p>We present NLU++, a novel dataset for natural language understanding (NLU) in\ntask-oriented dialogue (ToD) systems, with the aim to provide a much more\nchallenging evaluation environment for dialogue NLU models, up to date with the\ncurrent application and industry requirements. NLU++ is divided into two\ndomains (BANKING and HOTELS) and brings several crucial improvements over\ncurrent commonly used NLU datasets. 1) NLU++ provides fine-grained domain\nontologies with a large set of challenging multi-intent sentences, introducing\nand validating the idea of intent modules that can be combined into complex\nintents that convey complex user goals, combined with finer-grained and thus\nmore challenging slot sets. 2) The ontology is divided into domain-specific and\ngeneric (i.e., domain-universal) intent modules that overlap across domains,\npromoting cross-domain reusability of annotated examples. 3) The dataset design\nhas been inspired by the problems observed in industrial ToD systems, and 4) it\nhas been collected, filtered and carefully annotated by dialogue NLU experts,\nyielding high-quality annotated data. Finally, we benchmark a series of current\nstate-of-the-art NLU models on NLU++; the results demonstrate the challenging\nnature of the dataset, especially in low-data regimes, the validity of `intent\nmodularisation', and call for further research on ToD NLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spithourakis_G/0/1/0/all/0/1\">Georgios Spithourakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Borrow -- Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion. (arXiv:2204.13097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13097","description":"<p>Prior work on integrating text corpora with knowledge graphs (KGs) to improve\nKnowledge Graph Embedding (KGE) have obtained good performance for entities\nthat co-occur in sentences in text corpora. Such sentences (textual mentions of\nentity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between\ntwo entities. However, it is not possible to represent relations between\nentities that do not co-occur in a single sentence using LDPs. In this paper,\nwe propose and evaluate several methods to address this problem, where we\nborrow LDPs from the entity pairs that co-occur in sentences in the corpus\n(i.e. with mention entity pairs) to represent entity pairs that do not co-occur\nin any sentence in the corpus (i.e. without mention entity pairs). We propose a\nsupervised borrowing method, SuperBorrow, that learns to score the suitability\nof an LDP to represent a without-mention entity pair using pre-trained entity\nembeddings and contextualised LDP representations. Experimental results show\nthat SuperBorrow improves the link prediction performance of multiple\nwidely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakami_H/0/1/0/all/0/1\">Huda Hakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakami_M/0/1/0/all/0/1\">Mona Hakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandya_A/0/1/0/all/0/1\">Angrosh Mandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. (arXiv:2204.13132v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13132","description":"<p>Unsupervised domain adaptation (UDA) aims to adapt a model trained on the\nsource domain (e.g. synthetic data) to the target domain (e.g. real-world data)\nwithout requiring further annotations on the target domain. This work focuses\non UDA for semantic segmentation as real-world pixel-wise annotations are\nparticularly expensive to acquire. As UDA methods for semantic segmentation are\nusually GPU memory intensive, most previous methods operate only on downscaled\nimages. We question this design as low-resolution predictions often fail to\npreserve fine details. The alternative of training with random crops of\nhigh-resolution images alleviates this problem but falls short in capturing\nlong-range, domain-robust context information. Therefore, we propose HRDA, a\nmulti-resolution training approach for UDA, that combines the strengths of\nsmall high-resolution crops to preserve fine segmentation details and large\nlow-resolution crops to capture long-range context dependencies with a learned\nscale attention, while maintaining a manageable GPU memory footprint. HRDA\nenables adapting small objects and preserving fine segmentation details. It\nsignificantly improves the state-of-the-art performance by 5.5 mIoU for\nGTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in\nunprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available\nat https://github.com/lhoyer/HRDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1\">Lukas Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Nearest Neighbour Classifier. (arXiv:2204.13141v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13141","description":"<p>A windowed version of the Nearest Neighbour (WNN) classifier for images is\ndescribed. While its construction is inspired by the architecture of Artificial\nNeural Networks, the underlying theoretical framework is based on approximation\ntheory. We illustrate WNN on the datasets MNIST and EMNIST of images of\nhandwritten digits. In order to calibrate the parameters of WNN, we first study\nit on the classical MNIST dataset. We then apply WNN with these parameters to\nthe challenging EMNIST dataset. It is demonstrated that WNN misclassifies 0.42%\nof the images of EMNIST and therefore significantly outperforms predictions by\nhumans and shallow ANNs that both have more than 1.3% of errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setterqvist_E/0/1/0/all/0/1\">Eric Setterqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruglyak_N/0/1/0/all/0/1\">Natan Kruglyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forchheimer_R/0/1/0/all/0/1\">Robert Forchheimer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks. (arXiv:2204.13153v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13153","description":"<p>This paper follows cognitive studies to investigate a graph representation\nfor sketches, where the information of strokes, i.e., parts of a sketch, are\nencoded on vertices and information of inter-stroke on edges. The resultant\ngraph representation facilitates the training of a Graph Neural Networks for\nclassification tasks, and achieves accuracy and robustness comparable to the\nstate-of-the-art against translation and rotation attacks, as well as stronger\nattacks on graph vertices and topologies, i.e., modifications and addition of\nstrokes, all without resorting to adversarial training. Prior studies on\nsketches, e.g., graph transformers, encode control points of stroke on\nvertices, which are not invariant to spatial transformations. In contrary, we\nencode vertices and edges using pairwise distances among control points to\nachieve invariance. Compared with existing generative sketch model for one-shot\nclassification, our method does not rely on run-time statistical inference.\nLastly, the proposed representation enables generation of novel sketches that\nare structurally similar to while separable from the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Person Re-Identification. (arXiv:2204.13158v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13158","description":"<p>Person Re-Identification (Re-ID) is an important problem in computer\nvision-based surveillance applications, in which one aims to identify a person\nacross different surveillance photographs taken from different cameras having\nvarying orientations and field of views. Due to the increasing demand for\nintelligent video surveillance, Re-ID has gained significant interest in the\ncomputer vision community. In this work, we experiment on some existing Re-ID\nmethods that obtain state of the art performance in some open benchmarks. We\nqualitatively and quantitaively analyse their performance on a provided\ndataset, and then propose methods to improve the results. This work was the\nreport submitted for COL780 final project at IIT Delhi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Ebrahim Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tamajit Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13170","description":"<p>In Federated Learning a number of clients collaborate to train a model\nwithout sharing their data. Client models are optimized locally and are\ncommunicated through a central hub called server. A major challenge is to deal\nwith heterogeneity among clients' data which causes the local optimization to\ndrift away with respect to the global objective. In order to estimate and\ntherefore remove this drift, variance reduction techniques have been\nincorporated into Federated Learning optimization recently. However, the\nexisting solutions propagate the error of their estimations, throughout the\noptimization trajectory which leads to inaccurate approximations of the\nclients' drift and ultimately failure to remove them properly. In this paper,\nwe address this issue by introducing an adaptive algorithm that efficiently\nreduces clients' drift. Compared to the previous works on adapting variance\nreduction to Federated Learning, our approach uses less or the same level of\ncommunication bandwidth, computation or memory. Additionally, it addresses the\ninstability problem--prevalent in prior work, caused by increasing norm of the\nestimates which makes our approach a much more practical solution for large\nscale Federated Learning settings. Our experimental results demonstrate that\nthe proposed algorithm converges significantly faster and achieves higher\naccuracy compared to the baselines in an extensive set of Federated Learning\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varno_F/0/1/0/all/0/1\">Farshid Varno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saghayi_M/0/1/0/all/0/1\">Marzie Saghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiee_L/0/1/0/all/0/1\">Laya Rafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1\">Stan Matwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaei_M/0/1/0/all/0/1\">Mohammad Havaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Graph Convolutional Network of Multi-Modality Brain Imaging for Alzheimer's Disease Diagnosis. (arXiv:2204.13188v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13188","description":"<p>Identification of brain regions related to the specific neurological\ndisorders are of great importance for biomarker and diagnostic studies. In this\npaper, we propose an interpretable Graph Convolutional Network (GCN) framework\nfor the identification and classification of Alzheimer's disease (AD) using\nmulti-modality brain imaging data. Specifically, we extended the Gradient Class\nActivation Mapping (Grad-CAM) technique to quantify the most discriminative\nfeatures identified by GCN from brain connectivity patterns. We then utilized\nthem to find signature regions of interest (ROIs) by detecting the difference\nof features between regions in healthy control (HC), mild cognitive impairment\n(MCI), and AD groups. We conducted the experiments on the ADNI database with\nimaging data from three modalities, including VBM-MRI, FDG-PET, and AV45-PET,\nand showed that the ROI features learned by our method were effective for\nenhancing the performances of both clinical score prediction and disease status\nidentification. It also successfully identified biomarkers associated with AD\nand MCI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Houliang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework. (arXiv:2204.13207v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13207","description":"<p>Current contrastive learning frameworks focus on leveraging a single\nsupervisory signal to learn representations, which limits the efficacy on\nunseen data and downstream tasks. In this paper, we present a hierarchical\nmulti-label representation learning framework that can leverage all available\nlabels and preserve the hierarchical relationship between classes. We introduce\nnovel hierarchy preserving losses, which jointly apply a hierarchical penalty\nto the contrastive loss, and enforce the hierarchy constraint. The loss\nfunction is data driven and automatically adapts to arbitrary multi-label\nstructures. Experiments on several datasets show that our\nrelationship-preserving embedding performs well on a variety of tasks and\noutperform the baseline supervised and self-supervised approaches. Code is\navailable at https://github.com/salesforce/hierarchicalContrastiveLearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaiah_C/0/1/0/all/0/1\">Chetan Ramaiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offline Visual Representation Learning for Embodied Navigation. (arXiv:2204.13226v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13226","description":"<p>How should we learn visual representations for embodied agents that must see\nand move? The status quo is tabula rasa in vivo, i.e. learning visual\nrepresentations from scratch while also learning to move, potentially augmented\nwith auxiliary tasks (e.g. predicting the action taken between two successive\nobservations). In this paper, we show that an alternative 2-stage strategy is\nfar more effective: (1) offline pretraining of visual representations with\nself-supervised learning (SSL) using large-scale pre-rendered images of indoor\nenvironments (Omnidata), and (2) online finetuning of visuomotor\nrepresentations on specific tasks with image augmentations under long learning\nschedules. We call this method Offline Visual Representation Learning (OVRL).\nWe conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D,\nMP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL)\n- and find that the OVRL representations lead to significant across-the-board\nimprovements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute,\n86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28%\nrelative). Importantly, both results were achieved by the same visual encoder\ngeneralizing to datasets that were not seen during pretraining. While the\nbenefits of pretraining sometimes diminish (or entirely disappear) with long\nfinetuning schedules, we find that OVRL's performance gains continue to\nincrease (not decrease) as the agent is trained for 2 billion frames of\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1\">Karmesh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramrakhya_R/0/1/0/all/0/1\">Ram Ramrakhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Arjun Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berges_V/0/1/0/all/0/1\">Vincent-Pierre Berges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhar_S/0/1/0/all/0/1\">Sachit Kuhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Fine-tune with Dynamically Regulated Adversary. (arXiv:2204.13232v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13232","description":"<p>Adversarial training is an effective method to boost model robustness to\nmalicious, adversarial attacks. However, such improvement in model robustness\noften leads to a significant sacrifice of standard performance on clean images.\nIn many real-world applications such as health diagnosis and autonomous\nsurgical robotics, the standard performance is more valued over model\nrobustness against such extremely malicious attacks. This leads to the\nquestion: To what extent we can boost model robustness without sacrificing\nstandard performance? This work tackles this problem and proposes a simple yet\neffective transfer learning-based adversarial training strategy that\ndisentangles the negative effects of adversarial samples on model's standard\nperformance. In addition, we introduce a training-friendly adversarial attack\nalgorithm, which facilitates the boost of adversarial robustness without\nintroducing significant training complexity. Extensive experimentation\nindicates that the proposed method outperforms previous adversarial training\nalgorithms towards the target: to improve model robustness while preserving\nmodel's standard performance on clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengyue Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jie Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musilek_P/0/1/0/all/0/1\">Petr Musilek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection and Classification of Symbols in Engineering Drawings. (arXiv:2204.13277v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13277","description":"<p>A method of finding and classifying various components and objects in a\ndesign diagram, drawing, or planning layout is proposed. The method\nautomatically finds the objects present in a legend table and finds their\nposition, count and related information with the help of multiple deep neural\nnetworks. The method is pre-trained on several drawings or design templates to\nlearn the feature set that may help in representing the new templates. For a\ntemplate not seen before, it does not require any training with template\ndataset. The proposed method may be useful in multiple industry applications\nsuch as design validation, object count, connectivity of components, etc. The\nmethod is generic and domain independent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sourish Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Pranav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Sibsambhu Kar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resource-efficient domain adaptive pre-training for medical images. (arXiv:2204.13280v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13280","description":"<p>The deep learning-based analysis of medical images suffers from data scarcity\nbecause of high annotation costs and privacy concerns. Researchers in this\ndomain have used transfer learning to avoid overfitting when using complex\narchitectures. However, the domain differences between pre-training and\ndownstream data hamper the performance of the downstream task. Some recent\nstudies have successfully used domain-adaptive pre-training (DAPT) to address\nthis issue. In DAPT, models are initialized with the generic dataset\npre-trained weights, and further pre-training is performed using a moderately\nsized in-domain dataset (medical images). Although this technique achieved good\nresults for the downstream tasks in terms of accuracy and robustness, it is\ncomputationally expensive even when the datasets for DAPT are moderately sized.\nThese compute-intensive techniques and models impact the environment negatively\nand create an uneven playing field for researchers with limited resources. This\nstudy proposed computationally efficient DAPT without compromising the\ndownstream accuracy and robustness. This study proposes three techniques for\nthis purpose, where the first (partial DAPT) performs DAPT on a subset of\nlayers. The second one adopts a hybrid strategy (hybrid DAPT) by performing\npartial DAPT for a few epochs and then full DAPT for the remaining epochs. The\nthird technique performs DAPT on simplified variants of the base architecture.\nThe results showed that compared to the standard DAPT (full DAPT), the hybrid\nDAPT technique achieved better performance on the development and external\ndatasets. In contrast, simplified architectures (after DAPT) achieved the best\nrobustness while achieving modest performance on the development dataset .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehmood_Y/0/1/0/all/0/1\">Yasar Mehmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajwa_U/0/1/0/all/0/1\">Usama Ijaz Bajwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xianfang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer. (arXiv:2204.13286v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13286","description":"<p>Single-image super-resolution (SISR) has achieved significant breakthroughs\nwith the development of deep learning. However, these methods are difficult to\nbe applied in real-world scenarios since they are inevitably accompanied by the\nproblems of computational and memory costs caused by the complex operations. To\nsolve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR.\nSpecifically, an effective Symmetric CNN is designed for local feature\nextraction and coarse image reconstruction. Meanwhile, we propose a Recursive\nTransformer to fully learn the long-term dependence of images thus the global\ninformation can be fully used to further refine texture details. Studies show\nthat the hybrid of CNN and Transformer can build a more efficient model.\nExtensive experiments have proved that our LBNet achieves more prominent\nperformance than other state-of-the-art methods with a relatively low\ncomputational cost and memory consumption. The code is available at\nhttps://github.com/IVIPLab/LBNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-level Contrastive and Consistency Learning for Semi-Supervised Semantic Segmentation. (arXiv:2204.13314v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13314","description":"<p>Current semi-supervised semantic segmentation methods mainly focus on\ndesigning pixel-level consistency and contrastive regularization. However,\npixel-level regularization is sensitive to noise from pixels with incorrect\npredictions, and pixel-level contrastive regularization has memory and\ncomputational cost with O(pixel_num^2). To address the issues, we propose a\nnovel region-level contrastive and consistency learning framework (RC^2L) for\nsemi-supervised semantic segmentation. Specifically, we first propose a Region\nMask Contrastive (RMC) loss and a Region Feature Contrastive (RFC) loss to\naccomplish region-level contrastive property. Furthermore, Region Class\nConsistency (RCC) loss and Semantic Mask Consistency (SMC) loss are proposed\nfor achieving region-level consistency. Based on the proposed region-level\ncontrastive and consistency regularization, we develop a region-level\ncontrastive and consistency learning framework (RC^2L) for semi-supervised\nsemantic segmentation, and evaluate our RC$^2$L on two challenging benchmarks\n(PASCAL VOC 2012 and Cityscapes), outperforming the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chuanghao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13317","description":"<p>We present an open-source toolbox, named MMRotate, which provides a coherent\nalgorithm framework of training, inferring, and evaluation for the popular\nrotated object detection algorithm based on deep learning. MMRotate implements\n18 state-of-the-art algorithms and supports the three most frequently used\nangle definition methods. To facilitate future research and industrial\napplications of rotated object detection-related problems, we also provide a\nlarge number of trained models and detailed benchmarks to give insights into\nthe performance of rotated object detection. MMRotate is publicly released at\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Decades of Colorization and Decolorization for Images and Videos. (arXiv:2204.13322v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13322","description":"<p>Colorization is a computer-aided process, which aims to give color to a gray\nimage or video. It can be used to enhance black-and-white images, including\nblack-and-white photos, old-fashioned films, and scientific imaging results. On\nthe contrary, decolorization is to convert a color image or video into a\ngrayscale one. A grayscale image or video refers to an image or video with only\nbrightness information without color information. It is the basis of some\ndownstream image processing applications such as pattern recognition, image\nsegmentation, and image enhancement. Different from image decolorization, video\ndecolorization should not only consider the image contrast preservation in each\nvideo frame, but also respect the temporal and spatial consistency between\nvideo frames. Researchers were devoted to develop decolorization methods by\nbalancing spatial-temporal consistency and algorithm efficiency. With the\nprevalance of the digital cameras and mobile phones, image and video\ncolorization and decolorization have been paid more and more attention by\nresearchers. This paper gives an overview of the progress of image and video\ncolorization and decolorization methods in the last two decades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative-Region Attention and Orthogonal-View Generation Model for Vehicle Re-Identification. (arXiv:2204.13323v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13323","description":"<p>Vehicle re-identification (Re-ID) is urgently demanded to alleviate\nthepressure caused by the increasingly onerous task of urban traffic\nmanagement. Multiple challenges hamper the applications of vision-based vehicle\nRe-ID methods: (1) The appearances of different vehicles of the same\nbrand/model are often similar; However, (2) the appearances of the same vehicle\ndiffer significantly from different viewpoints. Previous methods mainly use\nmanually annotated multi-attribute datasets to assist the network in getting\ndetailed cues and in inferencing multi-view to improve the vehicle Re-ID\nperformance. However, finely labeled vehicle datasets are usually unattainable\nin real application scenarios. Hence, we propose a Discriminative-Region\nAttention and Orthogonal-View Generation (DRA-OVG) model, which only requires\nidentity (ID) labels to conquer the multiple challenges of vehicle Re-ID.The\nproposed DRA model can automatically extract the discriminative region\nfeatures, which can distinguish similar vehicles. And the OVG model can\ngenerate multi-view features based on the input view features to reduce the\nimpact of viewpoint mismatches. Finally, the distance between vehicle\nappearances is presented by the discriminative region features and multi-view\nfeatures together. Therefore, the significance of pairwise distance measure\nbetween vehicles is enhanced in acomplete feature space. Extensive experiments\nsubstantiate the effectiveness of each proposed ingredient, and experimental\nresults indicate that our approach achieves remarkable improvements over the\nstate- of-the-art vehicle Re-ID methods on VehicleID and VeRi-776 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huadong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Li Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Image Captioning. (arXiv:2204.13324v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13324","description":"<p>State-of-the-art image captioners can generate accurate sentences to describe\nimages in a sequence to sequence manner without considering the controllability\nand interpretability. This, however, is far from making image captioning widely\nused as an image can be interpreted in infinite ways depending on the target\nand the context at hand. Achieving controllability is important especially when\nthe image captioner is used by different people with different way of\ninterpreting the images. In this paper, we introduce a novel framework for\nimage captioning which can generate diverse descriptions by capturing the\nco-dependence between Part-Of-Speech tags and semantics. Our model decouples\ndirect dependence between successive variables. In this way, it allows the\ndecoder to exhaustively search through the latent Part-Of-Speech choices, while\nkeeping decoding speed proportional to the size of the POS vocabulary. Given a\ncontrol signal in the form of a sequence of Part-Of-Speech tags, we propose a\nmethod to generate captions through a Transformer network, which predicts words\nbased on the input Part-Of-Speech tag sequences. Experiments on publicly\navailable datasets show that our model significantly outperforms\nstate-of-the-art methods on generating diverse image captions with high\nqualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_L/0/1/0/all/0/1\">Luka Maxwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13339","description":"<p>Image or video appearance features (e.g., color, texture, tone, illumination,\nand so on) reflect one's visual perception and direct impression of an image or\nvideo. Given a source image (video) and a target image (video), the image\n(video) color transfer technique aims to process the color of the source image\nor video (note that the source image or video is also referred to the reference\nimage or video in some literature) to make it look like that of the target\nimage or video, i.e., transferring the appearance of the target image or video\nto that of the source image or video, which can thereby change one's perception\nof the source image or video. As an extension of color transfer, style transfer\nrefers to rendering the content of a target image or video in the style of an\nartist with either a style sample or a set of images through a style transfer\nmodel. As an emerging field, the study of style transfer has attracted the\nattention of a large number of researchers. After decades of development, it\nhas become a highly interdisciplinary research with a variety of artistic\nexpression styles can be achieved. This paper provides an overview of color\ntransfer and style transfer methods over the past years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Progressive Attention for Early Action Prediction. (arXiv:2204.13340v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13340","description":"<p>Early action prediction deals with inferring the ongoing action from\npartially-observed videos, typically at the outset of the video. We propose a\nbottleneck-based attention model that captures the evolution of the action,\nthrough progressive sampling over fine-to-coarse scales. Our proposed Temporal\nProgressive (TemPr) model is composed of multiple attention towers, one for\neach scale. The predicted action label is based on the collective agreement\nconsidering confidences of these attention towers. Extensive experiments over\nthree video datasets showcase state-of-the-art performance on the task of Early\nAction Prediction across a range of backbone architectures. We demonstrate the\neffectiveness and consistency of TemPr through detailed ablations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stergiou_A/0/1/0/all/0/1\">Alexandros Stergiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAGNet: Bidirectional Aware Guidance Network for Malignant Breast lesions Segmentation. (arXiv:2204.13342v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13342","description":"<p>Breast lesions segmentation is an important step of computer-aided diagnosis\nsystem, and it has attracted much attention. However, accurate segmentation of\nmalignant breast lesions is a challenging task due to the effects of\nheterogeneous structure and similar intensity distributions. In this paper, a\nnovel bidirectional aware guidance network (BAGNet) is proposed to segment the\nmalignant lesion from breast ultrasound images. Specifically, the bidirectional\naware guidance network is used to capture the context between global\n(low-level) and local (high-level) features from the input coarse saliency map.\nThe introduction of the global feature map can reduce the interference of\nsurrounding tissue (background) on the lesion regions. To evaluate the\nsegmentation performance of the network, we compared with several\nstate-of-the-art medical image segmentation methods on the public breast\nultrasound dataset using six commonly used evaluation metrics. Extensive\nexperimental results indicate that our method achieves the most competitive\nsegmentation results on malignant breast ultrasound images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Gongping Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yu Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianxun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_L/0/1/0/all/0/1\">Liang Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_X/0/1/0/all/0/1\">Xiaotao Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Branch Classifiers of Multi-exit Architectures. (arXiv:2204.13347v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13347","description":"<p>Multi-exit architectures consist of a backbone and branch classifiers that\noffer shortened inference pathways to reduce the run-time of deep neural\nnetworks. In this paper, we analyze different branching patterns that vary in\ntheir allocation of computational complexity for the branch classifiers.\nConstant-complexity branching keeps all branches the same, while\ncomplexity-increasing and complexity-decreasing branching place more complex\nbranches later or earlier in the backbone respectively. Through extensive\nexperimentation on multiple backbones and datasets, we find that\ncomplexity-decreasing branches are more effective than constant-complexity or\ncomplexity-increasing branches, which achieve the best accuracy-cost trade-off.\nWe investigate a cause by using knowledge consistency to probe the effect of\nadding branches onto a backbone. Our findings show that complexity-decreasing\nbranching yields the least disruption to the feature abstraction hierarchy of\nthe backbone, which explains the effectiveness of the branching patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generalized Unfolding Networks for Image Restoration. (arXiv:2204.13348v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13348","description":"<p>Deep neural networks (DNN) have achieved great success in image restoration.\nHowever, most DNN methods are designed as a black box, lacking transparency and\ninterpretability. Although some methods are proposed to combine traditional\noptimization algorithms with DNN, they usually demand pre-defined degradation\nprocesses or handcrafted assumptions, making it difficult to deal with complex\nand real-world applications. In this paper, we propose a Deep Generalized\nUnfolding Network (DGUNet) for image restoration. Concretely, without loss of\ninterpretability, we integrate a gradient estimation strategy into the gradient\ndescent step of the Proximal Gradient Descent (PGD) algorithm, driving it to\ndeal with complex and real-world image degradation. In addition, we design\ninter-stage information pathways across proximal mapping in different PGD\niterations to rectify the intrinsic information loss in most deep unfolding\nnetworks (DUN) through a multi-scale and spatial-adaptive way. By integrating\nthe flexible gradient descent and informative proximal mapping, we unfold the\niterative PGD algorithm into a trainable DNN. Extensive experiments on various\nimage restoration tasks demonstrate the superiority of our method in terms of\nstate-of-the-art performance, interpretability, and generalizability. The\nsource code is available at\nhttps://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chong Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning with Bayesian Model based on a Fixed Pre-trained Feature Extractor. (arXiv:2204.13349v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13349","description":"<p>Deep learning has shown its human-level performance in various applications.\nHowever, current deep learning models are characterised by catastrophic\nforgetting of old knowledge when learning new classes. This poses a challenge\nparticularly in intelligent diagnosis systems where initially only training\ndata of a limited number of diseases are available. In this case, updating the\nintelligent system with data of new diseases would inevitably downgrade its\nperformance on previously learned diseases. Inspired by the process of learning\nnew knowledge in human brains, we propose a Bayesian generative model for\ncontinual learning built on a fixed pre-trained feature extractor. In this\nmodel, knowledge of each old class can be compactly represented by a collection\nof statistical distributions, e.g. with Gaussian mixture models, and naturally\nkept from forgetting in continual learning over time. Unlike existing\nclass-incremental learning methods, the proposed approach is not sensitive to\nthe continual learning process and can be additionally well applied to the\ndata-incremental learning scenario. Experiments on multiple medical and natural\nimage classification tasks showed that the proposed approach outperforms\nstate-of-the-art approaches which even keep some images of old classes during\ncontinual learning of new classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Changhong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poly-CAM: High resolution class activation map for convolutional neural networks. (arXiv:2204.13359v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13359","description":"<p>The need for Explainable AI is increasing with the development of deep\nlearning. The saliency maps derived from convolutional neural networks\ngenerally fail in localizing with accuracy the image features justifying the\nnetwork prediction. This is because those maps are either low-resolution as for\nCAM [Zhou et al., 2016], or smooth as for perturbation-based methods [Zeiler\nand Fergus, 2014], or do correspond to a large number of widespread peaky spots\nas for gradient-based approaches [Sundararajan et al., 2017, Smilkov et al.,\n2017]. In contrast, our work proposes to combine the information from earlier\nnetwork layers with the one from later layers to produce a high resolution\nClass Activation Map that is competitive with the previous art in term of\ninsertion-deletion faithfulness metrics, while outperforming it in term of\nprecision of class-specific features localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Englebert_A/0/1/0/all/0/1\">Alexandre Englebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornu_O/0/1/0/all/0/1\">Olivier Cornu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning. (arXiv:2204.13371v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13371","description":"<p>Occlusion caused by vegetation is an essential problem for remote sensing\napplications in areas, such as search and rescue, wildfire detection, wildlife\nobservation, surveillance, border control, and others. Airborne Optical\nSectioning (AOS) is an optical, wavelength-independent synthetic aperture\nimaging technique that supports computational occlusion removal in real-time.\nIt can be applied with manned or unmanned aircrafts, such as drones. In this\narticle, we demonstrate a relationship between forest density and field of view\n(FOV) of applied imaging systems. This finding was made with the help of a\nsimulated procedural forest model which offers the consideration of more\nrealistic occlusion properties than our previous statistical model. While AOS\nhas been explored with automatic and autonomous research prototypes in the\npast, we present a free AOS integration for DJI systems. It enables bluelight\norganizations and others to use and explore AOS with compatible, manually\noperated, off-the-shelf drones. The (digitally cropped) default FOV for this\nimplementation was chosen based on our new finding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seits_F/0/1/0/all/0/1\">Francis Seits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1\">Indrajit Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathan_R/0/1/0/all/0/1\">Rakesh John Amala Arokia Nathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortner_R/0/1/0/all/0/1\">Rudolf Ortner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1\">Oliver Bimber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphing Attack Potential. (arXiv:2204.13374v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13374","description":"<p>In security systems the risk assessment in the sense of common criteria\ntesting is a very relevant topic; this requires quantifying the attack\npotential in terms of the expertise of the attacker, his knowledge about the\ntarget and access to equipment. Contrary to those attacks, the recently\nrevealed morphing attacks against Face Recognition Systems (FRSs) can not be\nassessed by any of the above criteria. But not all morphing techniques pose the\nsame risk for an operational face recognition system. This paper introduces\nwith the Morphing Attack Potential (MAP) a consistent methodology, that can\nquantify the risk, which a certain morphing attack creates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_M/0/1/0/all/0/1\">Matteo Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_A/0/1/0/all/0/1\">Annalisa Franco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1\">Davide Maltoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keep the Caption Information: Preventing Shortcut Learning in Contrastive Image-Caption Retrieval. (arXiv:2204.13382v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13382","description":"<p>To train image-caption retrieval (ICR) methods, contrastive loss functions\nare a common choice for optimization functions. Unfortunately, contrastive ICR\nmethods are vulnerable to learning shortcuts: decision rules that perform well\non the training data but fail to transfer to other testing conditions. We\nintroduce an approach to reduce shortcut feature representations for the ICR\ntask: latent target decoding (LTD). We add an additional decoder to the\nlearning framework to reconstruct the input caption, which prevents the image\nand caption encoder from learning shortcut features. Instead of reconstructing\ninput captions in the input space, we decode the semantics of the caption in a\nlatent space. We implement the LTD objective as an optimization constraint, to\nensure that the reconstruction loss is below a threshold value while primarily\noptimizing for the contrastive loss. Importantly, LTD does not depend on\nadditional training data or expensive (hard) negative mining strategies. Our\nexperiments show that, unlike reconstructing the input caption, LTD reduces\nshortcut learning and improves generalizability by obtaining higher recall@k\nand r-precision scores. Additionally, we show that the evaluation scores\nbenefit from implementing LTD as an optimization constraint instead of a dual\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Contrastive Learning for Self-supervised Action Recognition. (arXiv:2204.13386v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13386","description":"<p>The underlying correlation between audio and visual modalities within videos\ncan be utilized to learn supervised information for unlabeled videos. In this\npaper, we present an end-to-end self-supervised framework named Audio-Visual\nContrastive Learning (AVCL), to learn discriminative audio-visual\nrepresentations for action recognition. Specifically, we design an attention\nbased multi-modal fusion module (AMFM) to fuse audio and visual modalities. To\nalign heterogeneous audio-visual modalities, we construct a novel\nco-correlation guided representation alignment module (CGRA). To learn\nsupervised information from unlabeled videos, we propose a novel\nself-supervised contrastive learning module (SelfCL). Furthermore, to expand\nthe existing audio-visual action recognition datasets and better evaluate our\nframework AVCL, we build a new audio-visual action recognition dataset named\nKinetics-Sounds100. Experimental results on Kinetics-Sounds32 and\nKinetics-Sounds100 datasets demonstrate the superiority of our AVCL over the\nstate-of-the-art methods on large-scale action recognition benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Haoyuan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"List-Mode PET Image Reconstruction Using Deep Image Prior. (arXiv:2204.13404v1 [physics.med-ph])","link":"http://arxiv.org/abs/2204.13404","description":"<p>List-mode positron emission tomography (PET) image reconstruction is an\nimportant tool for PET scanners with many lines-of-response (LORs) and\nadditional information such as time-of-flight and depth-of-interaction. Deep\nlearning is one possible solution to enhance the quality of PET image\nreconstruction. However, the application of deep learning techniques to\nlist-mode PET image reconstruction have not been progressed because list data\nis a sequence of bit codes and unsuitable for processing by convolutional\nneural networks (CNN). In this study, we propose a novel list-mode PET image\nreconstruction method using an unsupervised CNN called deep image prior (DIP)\nand a framework of alternating direction method of multipliers. The proposed\nlist-mode DIP reconstruction (LM-DIPRecon) method alternatively iterates\nregularized list-mode dynamic row action maximum likelihood algorithm\n(LM-DRAMA) and magnetic resonance imaging conditioned DIP (MR-DIP). We\nevaluated LM-DIPRecon using both simulation and clinical data, and it achieved\nsharper images and better tradeoff curves between contrast and noise than the\nLM-DRAMA and MR-DIP. These results indicated that the LM-DIPRecon is useful for\nquantitative PET imaging with limited events. In addition, as list data has\nfiner temporal information than dynamic sinograms, list-mode deep image prior\nreconstruction is expected to be useful for 4D PET imaging and motion\ncorrection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Onishi_Y/0/1/0/all/0/1\">Yuya Onishi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ouchi_Y/0/1/0/all/0/1\">Yasuomi Ouchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-MoreGAN: A New Semi-supervised Generative Adversarial Network for Mixture of Rain Removal. (arXiv:2204.13420v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13420","description":"<p>Rain is one of the most common weather which can completely degrade the image\nquality and interfere with the performance of many computer vision tasks,\nespecially under heavy rain conditions. We observe that: (i) rain is a mixture\nof rain streaks and rainy haze; (ii) the scene depth determines the intensity\nof rain streaks and the transformation into the rainy haze; (iii) most existing\nderaining methods are only trained on synthetic rainy images, and hence\ngeneralize poorly to the real-world scenes. Motivated by these observations, we\npropose a new SEMI-supervised Mixture Of rain REmoval Generative Adversarial\nNetwork (Semi-MoreGAN), which consists of four key modules: (I) a novel\nattentional depth prediction network to provide precise depth estimation; (ii)\na context feature prediction network composed of several well-designed detailed\nresidual blocks to produce detailed image context features; (iii) a pyramid\ndepth-guided non-local network to effectively integrate the image context with\nthe depth information, and produce the final rain-free images; and (iv) a\ncomprehensive semi-supervised loss function to make the model not limited to\nsynthetic datasets but generalize smoothly to real-world heavy rainy scenes.\nExtensive experiments show clear improvements of our approach over twenty\nrepresentative state-of-the-arts on both synthetic and real-world rainy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gary Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu Lee Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Relation Guided Set Matching for Few-shot Action Recognition. (arXiv:2204.13423v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13423","description":"<p>Current few-shot action recognition methods reach impressive performance by\nlearning discriminative features for each video via episodic training and\ndesigning various temporal alignment strategies. Nevertheless, they are limited\nin that (a) learning individual features without considering the entire task\nmay lose the most relevant information in the current episode, and (b) these\nalignment strategies may fail in misaligned instances. To overcome the two\nlimitations, we propose a novel Hybrid Relation guided Set Matching (HyRSM)\napproach that incorporates two key components: hybrid relation module and set\nmatching metric. The purpose of the hybrid relation module is to learn\ntask-specific embeddings by fully exploiting associated relations within and\ncross videos in an episode. Built upon the task-specific features, we\nreformulate distance measure between query and support videos as a set matching\nproblem and further design a bidirectional Mean Hausdorff Metric to improve the\nresilience to misaligned instances. By this means, the proposed HyRSM can be\nhighly informative and flexible to predict query categories under the few-shot\nsettings. We evaluate HyRSM on six challenging benchmarks, and the experimental\nresults show its superiority over the state-of-the-art methods by a convincing\nmargin. Project page: https://hyrsm-cvpr2022.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1\">Zhengrong Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object Manipulation. (arXiv:2204.13426v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13426","description":"<p>We propose a novel framework for 3D-aware object manipulation, called\nAuto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated\nin an auto-encoder architecture, extracts disentangled 3D attributes such as 3D\nshape, appearance, and camera pose from an image, and a high-quality image is\nrendered from the attributes through disentangled generative Neural Radiance\nFields (NeRF). To improve the disentanglement ability, we present two losses,\nglobal-local attribute consistency loss defined between input and output, and\nswapped-attribute classification loss. Since training such auto-encoding\nnetworks from scratch without ground-truth shape and appearance information is\nnon-trivial, we present a stage-wise training scheme, which dramatically helps\nto boost the performance. We conduct experiments to demonstrate the\neffectiveness of the proposed model over the latest methods and provide\nextensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mira Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jaehoon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyusun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junmyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Daewon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching. (arXiv:2204.13453v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13453","description":"<p>State-of-the-art fully intrinsic networks for non-rigid shape matching often\nstruggle to disambiguate the symmetries of the shapes leading to unstable\ncorrespondence predictions. Meanwhile, recent advances in the functional map\nframework allow to enforce orientation preservation using a functional\nrepresentation for tangent vector field transfer, through so-called complex\nfunctional maps. Using this representation, we propose a new deep learning\napproach to learn orientation-aware features in a fully unsupervised setting.\nOur architecture is built on top of DiffusionNet, making it robust to\ndiscretization changes. Additionally, we introduce a vector field-based loss,\nwhich promotes orientation preservation without using (often unstable)\nextrinsic descriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Donati_N/0/1/0/all/0/1\">Nicolas Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corman_E/0/1/0/all/0/1\">Etienne Corman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Pixel-Level Noisy Label : A New Perspective for Light Field Saliency Detection. (arXiv:2204.13456v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13456","description":"<p>Saliency detection with light field images is becoming attractive given the\nabundant cues available, however, this comes at the expense of large-scale\npixel level annotated data which is expensive to generate. In this paper, we\npropose to learn light field saliency from pixel-level noisy labels obtained\nfrom unsupervised hand crafted featured based saliency methods. Given this\ngoal, a natural question is: can we efficiently incorporate the relationships\namong light field cues while identifying clean labels in a unified framework?\nWe address this question by formulating the learning as a joint optimization of\nintra light field features fusion stream and inter scenes correlation stream to\ngenerate the predictions. Specially, we first introduce a pixel forgetting\nguided fusion module to mutually enhance the light field features and exploit\npixel consistency across iterations to identify noisy pixels. Next, we\nintroduce a cross scene noise penalty loss for better reflecting latent\nstructures of training data and enabling the learning to be invariant to noise.\nExtensive experiments on multiple benchmark datasets demonstrate the\nsuperiority of our framework showing that it learns saliency prediction\ncomparable to state-of-the-art fully supervised light field saliency methods.\nOur code is available at https://github.com/OLobbCode/NoiseLF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kendong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13483","description":"<p>The new generation of 4D high-resolution imaging radar provides not only a\nhuge amount of point cloud but also additional elevation measurement, which has\na great potential of 3D sensing in autonomous driving. In this paper, we\nintroduce an autonomous driving dataset named TJ4DRadSet, including multi-modal\nsensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in\ntotal. 7757 frames within 44 consecutive sequences in various driving scenarios\nare well annotated with 3D bounding boxes and track id. We provide a 4D\nradar-based 3D object detection baseline for our dataset to demonstrate the\neffectiveness of deep learning methods for 4D radar point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhixiong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xichan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_K/0/1/0/all/0/1\">Kai Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengyue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Libo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jie Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multiscale Deep Equilibrium Models. (arXiv:2204.13492v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13492","description":"<p>We present StreamDEQ, a method that infers frame-wise representations on\nvideos with minimal per-frame computation. In contrast to conventional methods\nwhere compute time grows at least linearly with the network depth, we aim to\nupdate the representations in a continuous manner. For this purpose, we\nleverage the recently emerging implicit layer model which infers the\nrepresentation of an image by solving a fixed-point problem. Our main insight\nis to leverage the slowly changing nature of videos and use the previous frame\nrepresentation as an initial condition on each frame. This scheme effectively\nrecycles the recent inference computations and greatly reduces the needed\nprocessing time. Through extensive experimental analysis, we show that\nStreamDEQ is able to recover near-optimal representations in a few frames time,\nand maintain an up-to-date representation throughout the video duration. Our\nexperiments on video semantic segmentation and video object detection show that\nStreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while\nbeing more than $3\\times$ faster. The project page is available at:\nhttps://ufukertenli.github.io/streamdeq/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ertenli_C/0/1/0/all/0/1\">Can Ufuk Ertenli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Spatial-spectral Hyperspectral Image Reconstruction and Clustering with Diffusion Geometry. (arXiv:2204.13497v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13497","description":"<p>Hyperspectral images, which store a hundred or more spectral bands of\nreflectance, have become an important data source in natural and social\nsciences. Hyperspectral images are often generated in large quantities at a\nrelatively coarse spatial resolution. As such, unsupervised machine learning\nalgorithms incorporating known structure in hyperspectral imagery are needed to\nanalyze these images automatically. This work introduces the Spatial-Spectral\nImage Reconstruction and Clustering with Diffusion Geometry (DSIRC) algorithm\nfor partitioning highly mixed hyperspectral images. DSIRC reduces measurement\nnoise through a shape-adaptive reconstruction procedure. In particular, for\neach pixel, DSIRC locates spectrally correlated pixels within a data-adaptive\nspatial neighborhood and reconstructs that pixel's spectral signature using\nthose of its neighbors. DSIRC then locates high-density, high-purity pixels far\nin diffusion distance (a data-dependent distance metric) from other\nhigh-density, high-purity pixels and treats these as cluster exemplars, giving\neach a unique label. Non-modal pixels are assigned the label of their diffusion\ndistance-nearest neighbor of higher density and purity that is already labeled.\nStrong numerical results indicate that incorporating spatial information\nthrough image reconstruction substantially improves the performance of\npixel-wise clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Raymond H. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse-Designed Meta-Optics with Spectral-Spatial Engineered Response to Mimic Color Perception. (arXiv:2204.13520v1 [physics.optics])","link":"http://arxiv.org/abs/2204.13520","description":"<p>Meta-optics have rapidly become a major research field within the optics and\nphotonics community, strongly driven by the seemingly limitless opportunities\nmade possible by controlling optical wavefronts through interaction with arrays\nof sub-wavelength scatterers. As more and more modalities are explored, the\ndesign strategies to achieve desired functionalities become increasingly\ndemanding, necessitating more advanced design techniques. Herein, the\ninverse-design approach is utilized to create a set of single-layer meta-optics\nthat simultaneously focus light and shape the spectra of focused light without\nusing any filters. Thus, both spatial and spectral properties of the\nmeta-optics are optimized, resulting in spectra that mimic the color matching\nfunctions of the CIE 1931 XYZ color space, which links the distributions of\nwavelengths in light and the color perception of a human eye. Experimental\ndemonstrations of these meta-optics show qualitative agreement with the\ntheoretical predictions and help elucidate the focusing mechanism of these\ndevices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Munley_C/0/1/0/all/0/1\">Chris Munley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ma_W/0/1/0/all/0/1\">Wenchao Ma</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Froch_J/0/1/0/all/0/1\">Johannes E. Fr&#xf6;ch</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tanguy_Q/0/1/0/all/0/1\">Quentin A. A. Tanguy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bayati_E/0/1/0/all/0/1\">Elyas Bayati</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bohringer_K/0/1/0/all/0/1\">Karl F. B&#xf6;hringer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_Z/0/1/0/all/0/1\">Zin Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pestourie_R/0/1/0/all/0/1\">Rapha&#xeb;l Pestourie</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Johnson_S/0/1/0/all/0/1\">Steven G. Johnson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Majumdar_A/0/1/0/all/0/1\">Arka Majumdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos. (arXiv:2204.13548v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13548","description":"<p>In videos that contain actions performed unintentionally, agents do not\nachieve their desired goals. In such videos, it is challenging for computer\nvision systems to understand high-level concepts such as goal-directed\nbehavior, an ability present in humans from a very early age. Inculcating this\nability in artificially intelligent agents would make them better social\nlearners by allowing them to evaluate human action under a teleological lens.\nTo validate the ability of deep learning models to perform this task, we curate\nthe W-Oops dataset, built upon the Oops dataset [15]. W-Oops consists of 2,100\nunintentional human action videos, with 44 goal-directed and 30 unintentional\nvideo-level activity labels collected through human annotations. Due to the\nexpensive segment annotation procedure, we propose a weakly supervised\nalgorithm for localizing the goal-directed as well as unintentional temporal\nregions in the video leveraging solely video-level labels. In particular, we\nemploy an attention mechanism-based strategy that predicts the temporal regions\nwhich contribute the most to a classification task. Meanwhile, our designed\noverlap regularization allows the model to focus on distinct portions of the\nvideo for inferring the goal-directed and unintentional activity while\nguaranteeing their temporal ordering. Extensive quantitative experiments verify\nthe validity of our localization method. We further conduct a video captioning\nexperiment which demonstrates that the proposed localization module does indeed\nassist teleological action understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthy_A/0/1/0/all/0/1\">Arnav Chakravarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixup-based Deep Metric Learning Approaches for Incomplete Supervision. (arXiv:2204.13572v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13572","description":"<p>Deep learning architectures have achieved promising results in different\nareas (e.g., medicine, agriculture, and security). However, using those\npowerful techniques in many real applications becomes challenging due to the\nlarge labeled collections required during training. Several works have pursued\nsolutions to overcome it by proposing strategies that can learn more for less,\ne.g., weakly and semi-supervised learning approaches. As these approaches do\nnot usually address memorization and sensitivity to adversarial examples, this\npaper presents three deep metric learning approaches combined with Mixup for\nincomplete-supervision scenarios. We show that some state-of-the-art approaches\nin metric learning might not work well in such scenarios. Moreover, the\nproposed approaches outperform most of them in different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buris_L/0/1/0/all/0/1\">Luiz H. Buris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedronette_D/0/1/0/all/0/1\">Daniel C. G. Pedronette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Joao P. Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1\">Jurandy Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_F/0/1/0/all/0/1\">Fabio A. Faria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetric Transformer-based Network for Unsupervised Image Registration. (arXiv:2204.13575v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13575","description":"<p>Medical image registration is a fundamental and critical task in medical\nimage analysis. With the rapid development of deep learning, convolutional\nneural networks (CNN) have dominated the medical image registration field. Due\nto the disadvantage of the local receptive field of CNN, some recent\nregistration methods have focused on using transformers for non-local\nregistration. However, the standard Transformer has a vast number of parameters\nand high computational complexity, which causes Transformer can only be applied\nat the bottom of the registration models. As a result, only coarse information\nis available at the lowest resolution, limiting the contribution of Transformer\nin their models. To address these challenges, we propose a convolution-based\nefficient multi-head self-attention (CEMSA) block, which reduces the parameters\nof the traditional Transformer and captures local spatial context information\nfor reducing semantic ambiguity in the attention mechanism. Based on the\nproposed CEMSA, we present a novel Symmetric Transformer-based model\n(SymTrans). SymTrans employs the Transformer blocks in the encoder and the\ndecoder respectively to model the long-range spatial cross-image relevance. We\napply SymTrans to the displacement field and diffeomorphic registration.\nExperimental results show that our proposed method achieves state-of-the-art\nperformance in image registration. Our code is publicly available at\n\\url{https://github.com/MingR-Ma/SymTrans}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guixia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Sleeping Quality using Convolutional Neural Networks. (arXiv:2204.13584v1 [eess.SP])","link":"http://arxiv.org/abs/2204.13584","description":"<p>Identifying sleep stages and patterns is an essential part of diagnosing and\ntreating sleep disorders. With the advancement of smart technologies, sensor\ndata related to sleeping patterns can be captured easily. In this paper, we\npropose a Convolution Neural Network (CNN) architecture that improves the\nclassification performance. In particular, we benchmark the classification\nperformance from different methods, including traditional machine learning\nmethods such as Logistic Regression (LR), Decision Trees (DT), k-Nearest\nNeighbour (k-NN), Naive Bayes (NB) and Support Vector Machine (SVM), on 3\npublicly available sleep datasets. The accuracy, sensitivity, specificity,\nprecision, recall, and F-score are reported and will serve as a baseline to\nsimulate the research in this direction in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sathish_V/0/1/0/all/0/1\">Vidya Rohini Konanur Sathish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_W/0/1/0/all/0/1\">Wai Lok Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_E/0/1/0/all/0/1\">Edmond S. L. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Close Look into Human Activity Recognition Models using Deep Learning. (arXiv:2204.13589v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13589","description":"<p>Human activity recognition using deep learning techniques has become\nincreasing popular because of its high effectivity with recognizing complex\ntasks, as well as being relatively low in costs compared to more traditional\nmachine learning techniques. This paper surveys some state-of-the-art human\nactivity recognition models that are based on deep learning architecture and\nhas layers containing Convolution Neural Networks (CNN), Long Short-Term Memory\n(LSTM), or a mix of more than one type for a hybrid system. The analysis\noutlines how the models are implemented to maximize its effectivity and some of\nthe potential limitations it faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tee_W/0/1/0/all/0/1\">Wei Zhong Tee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seliya_N/0/1/0/all/0/1\">Naeem Seliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanamala_M/0/1/0/all/0/1\">Mounika Vanamala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision for Road Imaging and Pothole Detection: A State-of-the-Art Review of Systems and Algorithms. (arXiv:2204.13590v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13590","description":"<p>Computer vision algorithms have been prevalently utilized for 3-D road\nimaging and pothole detection for over two decades. Nonetheless, there is a\nlack of systematic survey articles on state-of-the-art (SoTA) computer vision\ntechniques, especially deep learning models, developed to tackle these\nproblems. This article first introduces the sensing systems employed for 2-D\nand 3-D road data acquisition, including camera(s), laser scanners, and\nMicrosoft Kinect. Afterward, it thoroughly and comprehensively reviews the SoTA\ncomputer vision algorithms, including (1) classical 2-D image processing, (2)\n3-D point cloud modeling and segmentation, and (3) machine/deep learning,\ndeveloped for road pothole detection. This article also discusses the existing\nchallenges and future development trends of computer vision-based road pothole\ndetection approaches: classical 2-D image processing-based and 3-D point cloud\nmodeling and segmentation-based approaches have already become history; and\nConvolutional neural networks (CNNs) have demonstrated compelling road pothole\ndetection results and are promising to break the bottleneck with the future\nadvances in self/un-supervised learning for multi-modal semantic segmentation.\nWe believe that this survey can serve as practical guidance for developing the\nnext-generation road condition assessment systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nachuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiahe Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenshuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lihua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks for Image Super-Resolution: A Survey. (arXiv:2204.13620v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13620","description":"<p>Single image super-resolution (SISR) has played an important role in the\nfield of image processing. Recent generative adversarial networks (GANs) can\nachieve excellent results on low-resolution images with small samples. However,\nthere are little literatures summarizing different GANs in SISR. In this paper,\nwe conduct a comparative study of GANs from different perspectives. We first\ntake a look at developments of GANs. Second, we present popular architectures\nfor GANs in big and small samples for image applications. Then, we analyze\nmotivations, implementations and differences of GANs based optimization methods\nand discriminative learning for image super-resolution in terms of supervised,\nsemi-supervised and unsupervised manners. Next, we compare performance of these\npopular GANs on public datasets via quantitative and qualitative analysis in\nSISR. Finally, we highlight challenges of GANs and potential research points\nfor SISR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_C/0/1/0/all/0/1\">Chunwei Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jerry Chun-Wen Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotationally Equivariant 3D Object Detection. (arXiv:2204.13630v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13630","description":"<p>Rotation equivariance has recently become a strongly desired property in the\n3D deep learning community. Yet most existing methods focus on equivariance\nregarding a global input rotation while ignoring the fact that rotation\nsymmetry has its own spatial support. Specifically, we consider the object\ndetection problem in 3D scenes, where an object bounding box should be\nequivariant regarding the object pose, independent of the scene motion. This\nsuggests a new desired property we call object-level rotation equivariance. To\nincorporate object-level rotation equivariance into 3D object detectors, we\nneed a mechanism to extract equivariant features with local object-level\nspatial support while being able to model cross-object context information. To\nthis end, we propose Equivariant Object detection Network (EON) with a rotation\nequivariance suspension design to achieve object-level equivariance. EON can be\napplied to modern point cloud object detectors, such as VoteNet and PointRCNN,\nenabling them to exploit object rotation symmetry in scene-scale inputs. Our\nexperiments on both indoor scene and autonomous driving datasets show that\nsignificant improvements are obtained by plugging our EON design into existing\nstate-of-the-art 3D object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly. (arXiv:2204.13631v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13631","description":"<p>Machine learning has advanced dramatically, narrowing the accuracy gap to\nhumans in multimodal tasks like visual question answering (VQA). However, while\nhumans can say \"I don't know\" when they are uncertain (i.e., abstain from\nanswering a question), such ability has been largely neglected in multimodal\nresearch, despite the importance of this problem to the usage of VQA in real\nsettings. In this work, we promote a problem formulation for reliable VQA,\nwhere we prefer abstention over providing an incorrect answer. We first enable\nabstention capabilities for several VQA models, and analyze both their\ncoverage, the portion of questions answered, and risk, the error on that\nportion. For that we explore several abstention approaches. We find that\nalthough the best performing models achieve over 71% accuracy on the VQA v2\ndataset, introducing the option to abstain by directly using a model's softmax\nscores limits them to answering less than 8% of the questions to achieve a low\nrisk of error (i.e., 1%). This motivates us to utilize a multimodal selection\nfunction to directly estimate the correctness of the predicted answers, which\nwe show can triple the coverage from, for example, 5.0% to 16.7% at 1% risk.\nWhile it is important to analyze both coverage and risk, these metrics have a\ntrade-off which makes comparing VQA models challenging. To address this, we\nalso propose an Effective Reliability metric for VQA that places a larger cost\non incorrect answers compared to abstentions. This new problem formulation,\nmetric, and analysis for VQA provide the groundwork for building effective and\nreliable VQA models that have the self-awareness to abstain if and only if they\ndon't know the answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petryk_S/0/1/0/all/0/1\">Suzanne Petryk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakib_V/0/1/0/all/0/1\">Vedaad Shakib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion. (arXiv:2204.13635v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13635","description":"<p>Depth completion involves recovering a dense depth map from a sparse map and\nan RGB image. Recent approaches focus on utilizing color images as guidance\nimages to recover depth at invalid pixels. However, color images alone are not\nenough to provide the necessary semantic understanding of the scene.\nConsequently, the depth completion task suffers from sudden illumination\nchanges in RGB images (e.g., shadows). In this paper, we propose a novel\nthree-branch backbone comprising color-guided, semantic-guided, and\ndepth-guided branches. Specifically, the color-guided branch takes a sparse\ndepth map and RGB image as an input and generates color depth which includes\ncolor cues (e.g., object boundaries) of the scene. The predicted dense depth\nmap of color-guided branch along-with semantic image and sparse depth map is\npassed as input to semantic-guided branch for estimating semantic depth. The\ndepth-guided branch takes sparse, color, and semantic depths to generate the\ndense depth map. The color depth, semantic depth, and guided depth are\nadaptively fused to produce the output of our proposed three-branch backbone.\nIn addition, we also propose to apply semantic-aware multi-modal\nattention-based fusion block (SAMMAFB) to fuse features between all three\nbranches. We further use CSPN++ with Atrous convolutions to refine the dense\ndepth map produced by our three-branch backbone. Extensive experiments show\nthat our model achieves state-of-the-art performance in the KITTI depth\ncompletion benchmark at the time of submission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nazir_D/0/1/0/all/0/1\">Danish Nazir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_M/0/1/0/all/0/1\">Muhammad Zeshan Afzal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Extract Building Footprints from Off-Nadir Aerial Images. (arXiv:2204.13637v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13637","description":"<p>Extracting building footprints from aerial images is essential for precise\nurban mapping with photogrammetric computer vision technologies. Existing\napproaches mainly assume that the roof and footprint of a building are well\noverlapped, which may not hold in off-nadir aerial images as there is often a\nbig offset between them. In this paper, we propose an offset vector learning\nscheme, which turns the building footprint extraction problem in off-nadir\nimages into an instance-level joint prediction problem of the building roof and\nits corresponding \"roof to footprint\" offset vector. Thus the footprint can be\nestimated by translating the predicted roof mask according to the predicted\noffset vector. We further propose a simple but effective feature-level offset\naugmentation module, which can significantly refine the offset vector\nprediction by introducing little extra cost. Moreover, a new dataset, Buildings\nin Off-Nadir Aerial Images (BONAI), is created and released in this paper. It\ncontains 268,958 building instances across 3,300 aerial images with fully\nannotated instance-level roof, footprint, and corresponding offset vector for\neach building. Experiments on the BONAI dataset demonstrate that our method\nachieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39\npoints in F1-score. The codes, datasets, and trained models are available at\nhttps://github.com/jwwangchn/BONAI.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingxuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking High-Accuracy Differentially Private Image Classification through Scale. (arXiv:2204.13650v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13650","description":"<p>Differential Privacy (DP) provides a formal privacy guarantee preventing\nadversaries with access to a machine learning model from extracting information\nabout individual training points. Differentially Private Stochastic Gradient\nDescent (DP-SGD), the most popular DP training method, realizes this protection\nby injecting noise during training. However previous works have found that\nDP-SGD often leads to a significant degradation in performance on standard\nimage classification benchmarks. Furthermore, some authors have postulated that\nDP-SGD inherently performs poorly on large models, since the norm of the noise\nrequired to preserve privacy is proportional to the model dimension. In\ncontrast, we demonstrate that DP-SGD on over-parameterized models can perform\nsignificantly better than previously thought. Combining careful hyper-parameter\ntuning with simple techniques to ensure signal propagation and improve the\nconvergence rate, we obtain a new SOTA on CIFAR-10 of 81.4% under (8,\n10^{-5})-DP using a 40-layer Wide-ResNet, improving over the previous SOTA of\n71.7%. When fine-tuning a pre-trained 200-layer Normalizer-Free ResNet, we\nachieve a remarkable 77.1% top-1 accuracy on ImageNet under (1, 8*10^{-7})-DP,\nand achieve 81.1% under (8, 8*10^{-7})-DP. This markedly exceeds the previous\nSOTA of 47.9% under a larger privacy budget of (10, 10^{-6})-DP. We believe our\nresults are a significant step towards closing the accuracy gap between private\nand non-private image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Soham De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1\">Leonard Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1\">Jamie Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_B/0/1/0/all/0/1\">Borja Balle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRIT: General Robust Image Task Benchmark. (arXiv:2204.13653v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13653","description":"<p>Computer vision models excel at making predictions when the test distribution\nclosely resembles the training distribution. Such models have yet to match the\nability of biological vision to learn from multiple sources and generalize to\nnew data sources and tasks. To facilitate the development and evaluation of\nmore general vision systems, we introduce the General Robust Image Task (GRIT)\nbenchmark. GRIT evaluates the performance, robustness, and calibration of a\nvision system across a variety of image prediction tasks, concepts, and data\nsources. The seven tasks in GRIT are selected to cover a range of visual\nskills: object categorization, object localization, referring expression\ngrounding, visual question answering, segmentation, human keypoint detection,\nand surface normal estimation. GRIT is carefully designed to enable the\nevaluation of robustness under image perturbations, image source distribution\nshift, and concept distribution shift. By providing a unified platform for\nthorough assessment of skills and concepts learned by a vision model, we hope\nGRIT catalyzes the development of performant and robust general purpose vision\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1\">Ryan Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation. (arXiv:2204.13656v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13656","description":"<p>In clinical practice, well-aligned multi-modal images, such as Magnetic\nResonance (MR) and Computed Tomography (CT), together can provide complementary\ninformation for image-guided therapies. Multi-modal image registration is\nessential for the accurate alignment of these multi-modal images. However, it\nremains a very challenging task due to complicated and unknown spatial\ncorrespondence between different modalities. In this paper, we propose a novel\ntranslation-based unsupervised deformable image registration approach to\nconvert the multi-modal registration problem to a mono-modal one. Specifically,\nour approach incorporates a discriminator-free translation network to\nfacilitate the training of the registration network and a patchwise contrastive\nloss to encourage the translation network to preserve object shapes.\nFurthermore, we propose to replace an adversarial loss, that is widely used in\nprevious multi-modal image registration methods, with a pixel loss in order to\nintegrate the output of translation into the target modality. This leads to an\nunsupervised method requiring no ground-truth deformation or pairs of aligned\nimages for training. We evaluate four variants of our approach on the public\nLearn2Reg 2021 datasets \\cite{hering2021learn2reg}. The experimental results\ndemonstrate that the proposed architecture achieves state-of-the-art\nperformance. Our code is available at https://github.com/heyblackC/DFMIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jia Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Articulated Objects in Free-form Hand Interaction. (arXiv:2204.13662v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13662","description":"<p>We use our hands to interact with and to manipulate objects. Articulated\nobjects are especially interesting since they often require the full dexterity\nof human hands to manipulate them. To understand, model, and synthesize such\ninteractions, automatic and robust methods that reconstruct hands and\narticulated objects in 3D from a color image are needed. Existing methods for\nestimating 3D hand and object pose from images focus on rigid objects. In part,\nbecause such methods rely on training data and no dataset of articulated object\nmanipulation exists. Consequently, we introduce ARCTIC - the first dataset of\nfree-form interactions of hands and articulated objects. ARCTIC has 1.2M images\npaired with accurate 3D meshes for both hands and for objects that move and\ndeform over time. The dataset also provides hand-object contact information. To\nshow the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D\nreconstruction of two hands and an articulated object in interaction; (2) an\nestimation of dense hand-object relative distances, which we call interaction\nfield estimation. For the first task, we present ArcticNet, a baseline method\nfor the task of jointly reconstructing two hands and an articulated object from\nan RGB image. For interaction field estimation, we predict the relative\ndistances from each hand vertex to the object surface, and vice versa. We\nintroduce InterField, the first method that estimates such distances from a\nsingle RGB image. We provide qualitative and quantitative experiments for both\ntasks, and provide detailed analysis on the data. Code and data will be\navailable at https://arctic.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_O/0/1/0/all/0/1\">Omid Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1\">Manuel Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Simulation, Perception, and Generation of Human Behavior. (arXiv:2204.13678v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13678","description":"<p>Understanding and modeling human behavior is fundamental to almost any\ncomputer vision and robotics applications that involve humans. In this thesis,\nwe take a holistic approach to human behavior modeling and tackle its three\nessential aspects -- simulation, perception, and generation. Throughout the\nthesis, we show how the three aspects are deeply connected and how utilizing\nand improving one aspect can greatly benefit the other aspects. We also discuss\nthe lessons learned and our vision for what is next for human behavior\nmodeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients. (arXiv:2204.13683v1 [cs.RO])","link":"http://arxiv.org/abs/2204.13683","description":"<p>Simulators offer the possibility of safe, low-cost development of\nself-driving systems. However, current driving simulators exhibit na\\\"ive\nbehavior models for background traffic. Hand-tuned scenarios are typically\nadded during simulation to induce safety-critical situations. An alternative\napproach is to adversarially perturb the background traffic trajectories. In\nthis paper, we study this approach to safety-critical driving scenario\ngeneration using the CARLA simulator. We use a kinematic bicycle model as a\nproxy to the simulator's true dynamics and observe that gradients through this\nproxy model are sufficient for optimizing the background traffic trajectories.\nBased on this finding, we propose KING, which generates safety-critical driving\nscenarios with a 20% higher success rate than black-box optimization. By\nsolving the scenarios generated by KING using a privileged rule-based expert\nalgorithm, we obtain training data for an imitation learning policy. After\nfine-tuning on this new data, we show that the policy becomes better at\navoiding collisions. Importantly, our generated data leads to reduced\ncollisions on both held-out scenarios generated via KING as well as traditional\nhand-crafted scenarios, demonstrating improved robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanselmann_N/0/1/0/all/0/1\">Niklas Hanselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_K/0/1/0/all/0/1\">Katrin Renz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Apratim Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling. (arXiv:2204.13686v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13686","description":"<p>4D human sensing and modeling are fundamental tasks in vision and graphics\nwith numerous applications. With the advances of new sensors and algorithms,\nthere is an increasing demand for more versatile datasets. In this work, we\ncontribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human\nsubjects, 400k sequences and 60M frames. HuMMan has several appealing\nproperties: 1) multi-modal data and annotations including color images, point\nclouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile\ndevice is included in the sensor suite; 3) a set of 500 actions, designed to\ncover fundamental movements; 4) multiple tasks such as action recognition, pose\nestimation, parametric human recovery, and textured mesh reconstruction are\nsupported and evaluated. Extensive experiments on HuMMan voice the need for\nfurther study on challenges such as fine-grained action recognition, dynamic\nhuman mesh reconstruction, point cloud-based parametric human recovery, and\ncross-device domain gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Daxuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhengyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiangyu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fangzhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeurMiPs: Neural Mixture of Planar Experts for View Synthesis. (arXiv:2204.13696v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13696","description":"<p>We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based\nscene representation for modeling geometry and appearance. NeurMiPs leverages a\ncollection of local planar experts in 3D space as the scene representation.\nEach planar expert consists of the parameters of the local rectangular shape\nrepresenting geometry and a neural radiance field modeling the color and\nopacity. We render novel views by calculating ray-plane intersections and\ncomposite output colors and densities at intersected points to the image.\nNeurMiPs blends the efficiency of explicit mesh rendering and flexibility of\nthe neural radiance field. Experiments demonstrate superior performance and\nspeed of our proposed method, compared to other 3D representations in novel\nview synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhi-Hao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hao-Yu Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenlong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing. (arXiv:2004.13316v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.13316","description":"<p>Small and cluttered objects are common in real-world which are challenging\nfor detection. The difficulty is further pronounced when the objects are\nrotated, as traditional detectors often routinely locate the objects in\nhorizontal bounding box such that the region of interest is contaminated with\nbackground or nearby interleaved objects. In this paper, we first innovatively\nintroduce the idea of denoising to object detection. Instance-level denoising\non the feature map is performed to enhance the detection to small and cluttered\nobjects. To handle the rotation variation, we also add a novel IoU constant\nfactor to the smooth L1 loss to address the long standing boundary problem,\nwhich to our analysis, is mainly caused by the periodicity of angular (PoA) and\nexchangeability of edges (EoE). By combing these two features, our proposed\ndetector is termed as SCRDet++. Extensive experiments are performed on large\naerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image\ndataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD\nand our released S$^2$TLD by this paper. The results show the effectiveness of\nour approach. The released dataset S2TLD is made public available, which\ncontains 5,786 images with 14,130 traffic light instances across five\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenlong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Machine Learning for Particle Track Identification in the CLAS12 Detector. (arXiv:2008.12860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12860","description":"<p>Particle track reconstruction is the most computationally intensive process\nin nuclear physics experiments. Traditional algorithms use a combinatorial\napproach that exhaustively tests track measurements (\"hits\") to identify those\nthat form an actual particle trajectory. In this article, we describe the\ndevelopment of four machine learning (ML) models that assist the tracking\nalgorithm by identifying valid track candidates from the measurements in drift\nchambers. Several types of machine learning models were tested, including:\nConvolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Extremely\nRandomized Trees (ERT) and Recurrent Neural Networks (RNN). As a result of this\nwork, an MLP network classifier was implemented as part of the CLAS12\nreconstruction software to provide the tracking code with recommended track\ncandidates. The resulting software achieved accuracy of greater than 99\\% and\nresulted in an end-to-end speedup of 35\\% compared to existing algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomadakis_P/0/1/0/all/0/1\">Polykarpos Thomadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Angelos Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavalian_G/0/1/0/all/0/1\">Gagik Gavalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisochoides_N/0/1/0/all/0/1\">Nikos Chrisochoides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Forensics via An Adversarial Game. (arXiv:2103.13567v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13567","description":"<p>With the progress in AI-based facial forgery (i.e., deepfake), people are\nincreasingly concerned about its abuse. Albeit effort has been made for\ntraining classification (also known as deepfake detection) models to recognize\nsuch forgeries, existing models suffer from poor generalization to unseen\nforgery technologies and high sensitivity to changes in image/video quality. In\nthis paper, we advocate adversarial training for improving the generalization\nability to both unseen facial forgeries and unseen image/video qualities. We\nbelieve training with samples that are adversarially crafted to attack the\nclassification models improves the generalization ability considerably.\nConsidering that AI-based face manipulation often leads to high-frequency\nartifacts that can be easily spotted by models yet difficult to generalize, we\nfurther propose a new adversarial training method that attempts to blur out\nthese specific artifacts, by introducing pixel-wise Gaussian blurring models.\nWith adversarial training, the classification models are forced to learn more\ndiscriminative and generalizable features, and the effectiveness of our method\ncan be verified by plenty of empirical evidence. Our code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransCenter: Transformers with Dense Representations for Multiple-Object Tracking. (arXiv:2103.15145v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15145","description":"<p>Transformers have proven superior performance for a wide variety of tasks\nsince they were introduced, which has drawn in recent years the attention of\nthe vision community where efforts were made such as image classification and\nobject detection. Despite this wave, building an accurate and efficient\nmultiple-object tracking (MOT) method with transformers is not a trivial task.\nWe argue that the direct application of a transformer architecture with\nquadratic complexity and insufficient noise-initialized sparse queries -- is\nnot optimal for MOT. Inspired by recent research, we propose TransCenter, a\ntransformer-based MOT architecture with dense representations for accurately\ntracking all the objects while keeping a reasonable runtime. Methodologically,\nwe propose the use of dense image-related multi-scale detection queries\nproduced by an efficient transformer architecture. The queries allow inferring\ntargets' locations globally and robustly from dense heatmap outputs. In\nparallel, a set of efficient sparse tracking queries interacting with image\nfeatures in the TransCenter Decoder to associate object positions through time.\nTransCenter exhibits remarkable performance improvements and outperforms by a\nlarge margin the current state-of-the-art in two standard MOT benchmarks with\ntwo tracking (public/private) settings. The proposed efficient and accurate\ntransformer architecture for MOT is proven with an extensive ablation study,\ndemonstrating its advantage compared to more naive alternatives and concurrent\nworks. The code will be made publicly available at\nhttps://github.com/yihongxu/transcenter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yutong Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delorme_G/0/1/0/all/0/1\">Guillaume Delorme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00394","description":"<p>Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Unsupervised Denoising Learning. (arXiv:2108.02574v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02574","description":"<p>Recently, much progress has been made in unsupervised denoising learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised denoising learning without any\nprior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\ndenoising learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we prove that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images. Code is available at\nhttps://github.com/wangweiSJTU/OTUR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach. (arXiv:2108.06771v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06771","description":"<p>Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r&gt;0.95$ vs. $r&lt;0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khawaled_S/0/1/0/all/0/1\">Samah Khawaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Image Segmentation with 3D Convolutional Neural Networks: A Survey. (arXiv:2108.08467v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08467","description":"<p>Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01052","description":"<p>We introduce a framework for calibrating machine learning models so that\ntheir predictions satisfy explicit, finite-sample statistical guarantees. Our\ncalibration algorithm works with any underlying model and (unknown)\ndata-generating distribution and does not require model refitting. The\nframework addresses, among other examples, false discovery rate control in\nmulti-label classification, intersection-over-union control in instance\nsegmentation, and the simultaneous control of the type-1 error of outlier\ndetection and confidence set coverage in classification or regression. Our main\ninsight is to reframe the risk-control problem as multiple hypothesis testing,\nenabling techniques and mathematical arguments different from those in the\nprevious literature. We use our framework to provide new calibration methods\nfor several core machine learning tasks with detailed worked examples in\ncomputer vision and tabular medical data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candes_E/0/1/0/all/0/1\">Emmanuel J. Cand&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05922","description":"<p>\"The power of a generalization system follows directly from its biases\"\n(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --\nbut to what degree have we understood how their inductive bias influences model\ndecisions? We here attempt to disentangle the various aspects that determine\nhow a model decides. In particular, we ask: what makes one model decide\ndifferently from another? In a meticulously controlled setting, we find that\n(1.) irrespective of the network architecture or objective (e.g.\nself-supervised, semi-supervised, vision transformers, recurrent models) all\nmodels end up with a similar decision boundary. (2.) To understand these\nfindings, we analysed model decisions on the ImageNet validation set from epoch\nto epoch and image by image. We find that the ImageNet validation set, among\nothers, suffers from dichotomous data difficulty (DDD): For the range of\ninvestigated models and their accuracies, it is dominated by 46.0% \"trivial\"\nand 11.5% \"impossible\" images (beyond label errors). Only 42.5% of the images\ncould possibly be responsible for the differences between two models' decision\nboundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us\nto see pronounced differences between models. (4.) Humans are highly accurate\nat predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%).\nThis implies that in future comparisons of brains, machines and behaviour, much\nmay be gained from investigating the decisive role of images and the\ndistribution of their difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meding_K/0/1/0/all/0/1\">Kristof Meding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschoff_L/0/1/0/all/0/1\">Luca M. Schulze Buschoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1\">Felix A. Wichmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06081","description":"<p>Inspired by the diversity of biological neurons, quadratic artificial neurons\ncan play an important role in deep learning models. The type of quadratic\nneurons of our interest replaces the inner-product operation in the\nconventional neuron with a quadratic function. Despite promising results so far\nachieved by networks of quadratic neurons, there are important issues not well\naddressed. Theoretically, the superior expressivity of a quadratic network over\neither a conventional network or a conventional network via quadratic\nactivation is not fully elucidated, which makes the use of quadratic networks\nnot well grounded. Practically, although a quadratic network can be trained via\ngeneric backpropagation, it can be subject to a higher risk of collapse than\nthe conventional counterpart. To address these issues, we first apply the\nspline theory and a measure from algebraic geometry to give two theorems that\ndemonstrate better model expressivity of a quadratic network than the\nconventional counterpart with or without quadratic activation. Then, we propose\nan effective and efficient training strategy referred to as ReLinear to\nstabilize the training process of a quadratic network, thereby unleashing the\nfull potential in its associated machine learning tasks. Comprehensive\nexperiments on popular datasets are performed to support our findings and\nevaluate the performance of quadratic deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Feng-Lei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Rongjie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPTS: Single-Point Text Spotting. (arXiv:2112.07917v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07917","description":"<p>Existing scene text spotting (i.e., end-to-end text detection and\nrecognition) methods rely on costly bounding box annotations (e.g., text-line,\nword-level, or character-level bounding boxes). For the first time, we\ndemonstrate that training scene text spotting models can be achieved with an\nextremely low-cost annotation of a single-point for each instance. We propose\nan end-to-end scene text spotting method that tackles scene text spotting as a\nsequence prediction task. Given an image as input, we formulate the desired\ndetection and recognition results as a sequence of discrete tokens and use an\nauto-regressive Transformer to predict the sequence. The proposed method is\nsimple yet effective, which can achieve state-of-the-art results on widely used\nbenchmarks. Most significantly, we show that the performance is not very\nsensitive to the positions of the point annotation, meaning that it can be much\neasier to be annotated or even be automatically generated than the bounding box\nthat requires precise positions. We believe that such a pioneer attempt\nindicates a significant opportunity for scene text spotting applications of a\nmuch larger scale than previously possible. The code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Geometry-aware 3D Generative Adversarial Networks. (arXiv:2112.07945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07945","description":"<p>Unsupervised generation of high-quality multi-view-consistent images and 3D\nshapes using only collections of single-view 2D photographs has been a\nlong-standing challenge. Existing 3D GANs are either compute-intensive or make\napproximations that are not 3D-consistent; the former limits quality and\nresolution of the generated images and the latter adversely affects multi-view\nconsistency and shape quality. In this work, we improve the computational\nefficiency and image quality of 3D GANs without overly relying on these\napproximations. We introduce an expressive hybrid explicit-implicit network\narchitecture that, together with other design choices, synthesizes not only\nhigh-resolution multi-view-consistent images in real time but also produces\nhigh-quality 3D geometry. By decoupling feature generation and neural\nrendering, our framework is able to leverage state-of-the-art 2D CNN\ngenerators, such as StyleGAN2, and inherit their efficiency and expressiveness.\nWe demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats,\namong other experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1\">Eric R. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Connor Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1\">Matthew A. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1\">Koki Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Boxiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallo_O/0/1/0/all/0/1\">Orazio Gallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1\">Sameh Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2112.14025v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14025","description":"<p>Clustering-based unsupervised domain adaptive (UDA) person re-identification\n(ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature\nembedding and imperfect clustering, pseudo labels for target domain data\ninherently contain an unknown proportion of wrong ones, which would mislead\nfeature learning. In this paper, we propose an approach named probabilistic\nuncertainty guided progressive label refinery (P$^2$LR) for domain adaptive\nperson re-identification. First, we propose to model the labeling uncertainty\nwith the probabilistic distance along with ideal single-peak distributions. A\nquantitative criterion is established to measure the uncertainty of pseudo\nlabels and facilitate the network training. Second, we explore a progressive\nstrategy for refining pseudo labels. With the uncertainty-guided alternative\noptimization, we balance between the exploration of target domain data and the\nnegative effects of noisy labeling. On top of a strong baseline, we obtain\nsignificant improvements and achieve the state-of-the-art performance on four\nUDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5%\nmAP on the Duke2Market task, while surpassing the state-of-the-art method by\n2.5% mAP on the Market2MSMT task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1\">Ya-Li li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDS-Net: A Multi-scale Depth Stratification Based Monocular 3D Object Detection Algorithm. (arXiv:2201.04341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04341","description":"<p>Monocular 3D object detection is very challenging in autonomous driving due\nto the lack of depth information. This paper proposes a one-stage monocular 3D\nobject detection algorithm based on multi-scale depth stratification, which\nuses the anchor-free method to detect 3D objects in a per-pixel prediction. In\nthe proposed MDS-Net, a novel depth-based stratification structure is developed\nto improve the network's ability of depth prediction by establishing\nmathematical models between depth and image size of objects. A new angle loss\nfunction is then developed to further improve the accuracy of the angle\nprediction and increase the convergence speed of training. An optimized\nsoft-NMS is finally applied in the post-processing stage to adjust the\nconfidence of candidate boxes. Experiments on the KITTI benchmark show that the\nMDS-Net outperforms the existing monocular 3D detection methods in 3D detection\nand BEV detection tasks while fulfilling real-time requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhouzhen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zecheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03937","description":"<p>Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by each query\nattending to all keys/values, various methods have constrained the range of\nattention within local regions, where each query only attends to keys/values\nwithin a hand-crafted window. However, these hand-crafted window partition\nmechanisms are data-agnostic and ignore their input content, so it is likely\nthat one query maybe attends to irrelevant keys/values. To address this issue,\nwe propose a Dynamic Group Attention (DG-Attention), which dynamically divides\nall queries into multiple groups and selects the most relevant keys/values for\neach group. Our DG-Attention can flexibly model more relevant dependencies\nwithout any spatial constraint that is used in hand-crafted window based\nattention. Built on the DG-Attention, we develop a general vision transformer\nbackbone named Dynamic Group Transformer (DGT). Extensive experiments show that\nour models can outperform the state-of-the-art methods on multiple common\nvision tasks, including image classification, semantic segmentation, object\ndetection, and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary Neural Cascade Search across Supernetworks. (arXiv:2203.04011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04011","description":"<p>To achieve excellent performance with modern neural networks, having the\nright network architecture is important. Neural Architecture Search (NAS)\nconcerns the automatic discovery of task-specific network architectures. Modern\nNAS approaches leverage supernetworks whose subnetworks encode candidate neural\nnetwork architectures. These subnetworks can be trained simultaneously,\nremoving the need to train each network from scratch, thereby increasing the\nefficiency of NAS. A recent method called Neural Architecture Transfer (NAT)\nfurther improves the efficiency of NAS for computer vision tasks by using a\nmulti-objective evolutionary algorithm to find high-quality subnetworks of a\nsupernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS -\nEvolutionary Neural Cascade Search. ENCAS can be used to search over multiple\npretrained supernetworks to achieve a trade-off front of cascades of different\nneural network architectures, maximizing accuracy while minimizing FLOPs count.\nWe test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100,\nImageNet) and achieve Pareto dominance over previous state-of-the-art NAS\nmodels up to 1.5 GFLOPs. Additionally, applying ENCAS to a pool of 518 publicly\navailable ImageNet classifiers leads to Pareto dominance in all computation\nregimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied\nby an 18\\% decrease in computation effort from 362 to 296 GFLOPs. Our code is\navailable at https://github.com/AwesomeLemon/ENCAS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebykin_A/0/1/0/all/0/1\">Alexander Chebykin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP). (arXiv:2203.04180v3 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2203.04180","description":"<p>Magnetic Resonance Imaging (MRI) has excellent soft tissue contrast but is\nhindered by an inherently slow data acquisition process. Compressed sensing,\nwhich reconstructs sparse signals from incoherently sampled data, has been\nwidely applied to accelerate MRI acquisitions. Compressed sensing MRI requires\none or more model parameters to be tuned, which is usually done by hand, giving\nsub-optimal tuning in general. To address this issue, we build on previous work\nby the authors on the single-coil Variable Density Approximate Message Passing\n(VDAMP) algorithm, extending the framework to multiple receiver coils to\npropose the Parallel VDAMP (P-VDAMP) algorithm. For Bernoulli random variable\ndensity sampling, P-VDAMP obeys a \"state evolution\", where the intermediate\nper-iteration image estimate is distributed according to the ground truth\ncorrupted by a zero-mean Gaussian vector with approximately known covariance.\nTo our knowledge, P-VDAMP is the first algorithm for multi-coil MRI data that\nobeys a state evolution with accurately tracked parameters. We leverage state\nevolution to automatically tune sparse parameters on-the-fly with Stein's\nUnbiased Risk Estimate (SURE). P-VDAMP is evaluated on brain, knee and\nangiogram datasets and compared with four variants of the Fast Iterative\nShrinkage-Thresholding algorithm (FISTA), including two tuning-free variants\nfrom the literature. The proposed method is found to have a similar\nreconstruction quality and time to convergence as FISTA with an optimally tuned\nsparse weighting and offers substantial robustness and reconstruction quality\nimprovements over competing tuning-free methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Millard_C/0/1/0/all/0/1\">Charles Millard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chiew_M/0/1/0/all/0/1\">Mark Chiew</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hess_A/0/1/0/all/0/1\">Aaron T. Hess</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mailhe_B/0/1/0/all/0/1\">Boris Mailhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.06622","description":"<p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low\ndynamic range (LDR) images captured at different exposure times. While these\nmethods work well in static scenes, dynamic scenes remain a challenge since the\nLDR images still suffer from saturation and noise. In such scenarios, event\ncameras would be a valid complement, thanks to their higher temporal resolution\nand dynamic range. In this paper, we propose the first multi-bracket HDR\npipeline combining a standard camera with an event camera. Our results show\nbetter overall robustness when using events, with improvements in PSNR by up to\n5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a\nnew dataset containing bracketed LDR images with aligned events and HDR ground\ntruth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulyakov_S/0/1/0/all/0/1\">Stepan Tulyakov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erbach_J/0/1/0/all/0/1\">Julius Erbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bochicchio_A/0/1/0/all/0/1\">Alfredo Bochicchio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyou Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10454","description":"<p>In contrastive learning in the image domain, the anchor and positive samples\nare forced to have as close representations as possible. However, forcing the\ntwo samples to have the same representation could be misleading because the\ndata augmentation techniques make the two samples different. In this paper, we\nintroduce a new representation, partitioned representation, which can learn\nboth common and unique features of the anchor and positive samples in\ncontrastive learning. The partitioned representation consists of two parts: the\ncontent part and the style part. The content part represents common features of\nthe class, and the style part represents the own features of each sample, which\ncan lead to the representation of the data augmentation method. We can achieve\nthe partitioned representation simply by decomposing a loss function of\ncontrastive learning into two terms on the two separate representations,\nrespectively. To evaluate our representation with two parts, we take two\nframework models: Variational AutoEncoder (VAE) and BootstrapYour Own\nLatent(BYOL) to show the separability of content and style, and to confirm the\ngeneralization ability in classification, respectively. Based on the\nexperiments, we show that our approach can separate two types of information in\nthe VAE framework and outperforms the conventional BYOL in linear separability\nand a few-shot learning task as downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunsub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient-VDVAE: Less is more. (arXiv:2203.13751v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13751","description":"<p>Hierarchical VAEs have emerged in recent years as a reliable option for\nmaximum likelihood estimation. However, instability issues and demanding\ncomputational requirements have hindered research progress in the area. We\npresent simple modifications to the Very Deep VAE to make it converge up to\n$2.6\\times$ faster, save up to $20\\times$ in memory load and improve stability\nduring training. Despite these changes, our models achieve comparable or better\nnegative log-likelihood performance than current state-of-the-art models on all\n$7$ commonly used image datasets we evaluated on. We also make an argument\nagainst using 5-bit benchmarks as a way to measure hierarchical VAE's\nperformance due to undesirable biases caused by the 5-bit quantization.\nAdditionally, we empirically demonstrate that roughly $3\\%$ of the hierarchical\nVAE's latent space dimensions is sufficient to encode most of the image\ninformation, without loss of performance, opening up the doors to efficiently\nleverage the hierarchical VAEs' latent space in downstream tasks. We release\nour source code and models at https://github.com/Rayhane-mamah/Efficient-VDVAE .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazami_L/0/1/0/all/0/1\">Louay Hazami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mama_R/0/1/0/all/0/1\">Rayhane Mama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurairatnam_R/0/1/0/all/0/1\">Ragavan Thurairatnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning. (arXiv:2204.00951v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00951","description":"<p>In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi\ncountry benchmark dataset, tailored for agricultural monitoring applications\nwith Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer\ndeclarations collected via the Land Parcel Identification System (LPIS) for\nharmonizing country wide labels. These declarations have only recently been\nmade available as open data, allowing for the first time the labeling of\nsatellite imagery from ground truth data. We proceed to propose and standardise\na new crop type taxonomy across Europe that address Common Agriculture Policy\n(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative\nCrop Classification scheme. Sen4AgriNet is the only multi-country, multi-year\ndataset that includes all spectral information. It is constructed to cover the\nperiod 2016-2020 for Catalonia and France, while it can be extended to include\nadditional countries. Currently, it contains 42.5 million parcels, which makes\nit significantly larger than other available archives. We extract two\nsub-datasets to highlight its value for diverse Deep Learning applications; the\nObject Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD\ncapitalizes zonal statistics of each parcel, thus creating a powerful\nlabel-to-features instance for classification algorithms. On the other hand,\nPAD structure generalizes the classification problem to parcel extraction and\nsemantic segmentation and labeling. The PAD and OAD are examined under three\ndifferent scenarios to showcase and model the effects of spatial and temporal\nvariability across different years and different countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sykas_D/0/1/0/all/0/1\">Dimitrios Sykas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sdraka_M/0/1/0/all/0/1\">Maria Sdraka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zografakis_D/0/1/0/all/0/1\">Dimitrios Zografakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining time-space traffic diagrams: A multiple linear regression model. (arXiv:2204.04457v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04457","description":"<p>A time-space traffic (TS) diagram, which presents traffic states in\ntime-space cells with color, is an important traffic analysis and visualization\ntool. Despite its importance for transportation research and engineering, most\nTS diagrams that have already existed or are being produced are too coarse to\nexhibit detailed traffic dynamics due to the limitations of existing\ninformation technology and traffic infrastructure investment. To increase the\nresolution of a TS diagram and enable it to present ample traffic details, this\npaper introduces the TS diagram refinement problem and proposes a multiple\nlinear regression-based model to solve the problem. Two tests, which attempt to\nincrease the resolution of a TS diagram 4 and 16 times, are carried out to\nevaluate the performance of the proposed model. Data collected at different\ntimes, in different locations and even in different countries are employed to\nthoroughly evaluate the accuracy and transferability of the proposed model.\nStrict tests with diverse data show that the proposed model, despite its\nsimplicity, is able to refine a TS diagram with promising accuracy and reliable\ntransferability. The proposed refinement model will \"save\" widely existing TS\ndiagrams from their blurry \"faces\" and enable TS diagrams to show more traffic\ndetails.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengbing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08058","description":"<p>Multimodal video-audio-text understanding and generation can benefit from\ndatasets that are narrow but rich. The narrowness allows bite-sized challenges\nthat the research community can make progress on. The richness ensures we are\nmaking progress along the core challenges. To this end, we present a\nlarge-scale video-audio-text dataset MUGEN, collected using the open-sourced\nplatform game CoinRun [11]. We made substantial modifications to make the game\nricher by introducing audio and enabling new interactions. We trained RL agents\nwith different objectives to navigate the game and interact with 13 objects and\ncharacters. This allows us to automatically extract a large collection of\ndiverse videos and associated audio. We sample 375K video clips (3.2s each) and\ncollect text descriptions from human annotators. Each video has additional\nannotations that are extracted automatically from the game engine, such as\naccurate semantic maps for each frame and templated textual descriptions.\nAltogether, MUGEN can help progress research in many tasks in multimodal\nunderstanding and generation. We benchmark representative approaches on tasks\ninvolving video-audio-text retrieval and generation. Our dataset and code are\nreleased at: https://mugen-org.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Thomas Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1\">Sasha Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09826","description":"<p>Recent advances in skeleton-based person re-identification (re-ID) obtain\nimpressive performance via either hand-crafted skeleton descriptors or skeleton\nrepresentation learning with deep learning paradigms. However, they typically\nrequire skeletal pre-modeling and label information for training, which leads\nto limited applicability of these methods. In this paper, we focus on\nunsupervised skeleton-based person re-ID, and present a generic Simple Masked\nContrastive learning (SimMC) framework to learn effective representations from\nunlabeled 3D skeletons for person re-ID. Specifically, to fully exploit\nskeleton features within each skeleton sequence, we first devise a masked\nprototype contrastive learning (MPC) scheme to cluster the most typical\nskeleton features (skeleton prototypes) from different subsequences randomly\nmasked from raw sequences, and contrast the inherent similarity between\nskeleton features and different prototypes to learn discriminative skeleton\nrepresentations without using any label. Then, considering that different\nsubsequences within the same sequence usually enjoy strong correlations due to\nthe nature of motion continuity, we propose the masked intra-sequence\ncontrastive learning (MIC) to capture intra-sequence pattern consistency\nbetween subsequences, so as to encourage learning more effective skeleton\nrepresentations for person re-ID. Extensive experiments validate that the\nproposed SimMC outperforms most state-of-the-art skeleton-based methods. We\nfurther show its scalability and efficiency in enhancing the performance of\nexisting models. Our codes are available at https://github.com/Kali-Hac/SimMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10496","description":"<p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10939","description":"<p>Document intelligence automates the extraction of information from documents\nand supports many business applications. Recent self-supervised learning\nmethods on large-scale unlabeled document datasets have opened up promising\ndirections towards reducing annotation efforts by training models with\nself-supervised objectives. However, most of the existing document pretraining\nmethods are still language-dominated. We present UDoc, a new unified\npretraining framework for document understanding. UDoc is designed to support\nmost document understanding tasks, extending the Transformer to take multimodal\nembeddings as input. Each input element is composed of words and visual\nfeatures from a semantic region of the input document image. An important\nfeature of UDoc is that it learns a generic representation by making use of\nthree self-supervised losses, encouraging the representation to model\nsentences, learn similarities, and align modalities. Extensive empirical\nanalysis demonstrates that the pretraining procedure learns better joint\nrepresentations and leads to improvements in downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad I. Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Unsupervised Industrial Anomaly Detection Algorithms. (arXiv:2204.11161v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11161","description":"<p>In line with the development of Industry 4.0, more and more attention is\nattracted to the field of surface defect detection. Improving efficiency as\nwell as saving labor costs has steadily become a matter of great concern in\nindustry field, where deep learning-based algorithms performs better than\ntraditional vision inspection methods in recent years. While existing deep\nlearning-based algorithms are biased towards supervised learning, which not\nonly necessitates a huge amount of labeled data and a significant amount of\nlabor, but it is also inefficient and has certain limitations. In contrast,\nrecent research shows that unsupervised learning has great potential in\ntackling above disadvantages for visual anomaly detection. In this survey, we\nsummarize current challenges and provide a thorough overview of recently\nproposed unsupervised algorithms for visual anomaly detection covering five\ncategories, whose innovation points and frameworks are described in detail.\nMeanwhile, information on publicly available datasets containing surface image\nsamples are provided. By comparing different classes of methods, the advantages\nand disadvantages of anomaly detection algorithms are summarized. It is\nexpected to assist both the research community and industry in developing a\nbroader and cross-domain perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yajie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shiguo Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.11479","description":"<p>While efficient architectures and a plethora of augmentations for end-to-end\nimage classification tasks have been suggested and heavily investigated,\nstate-of-the-art techniques for audio classifications still rely on numerous\nrepresentations of the audio signal together with large architectures,\nfine-tuned from large datasets. By utilizing the inherited lightweight nature\nof audio and novel audio augmentations, we were able to present an efficient\nend-to-end network with strong generalization ability. Experiments on a variety\nof sound classification sets demonstrate the effectiveness and robustness of\nour approach, by achieving state-of-the-art results in various settings. Public\ncode will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gazneli_A/0/1/0/all/0/1\">Avi Gazneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2204.11573v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11573","description":"<p>This paper focuses on the weakly-supervised audio-visual video parsing task,\nwhich aims to recognize all events belonging to each modality and localize\ntheir temporal boundaries. This task is challenging because only overall labels\nindicating the video events are provided for training. However, an event might\nbe labeled but not appear in one of the modalities, which results in a\nmodality-specific noisy label problem. Motivated by two observations that\nnetworks tend to learn clean samples first and that a labeled event would\nappear in at least one modality, we propose a training strategy to identify and\nremove modality-specific noisy labels dynamically. Specifically, we sort the\nlosses of all instances within a mini-batch individually in each modality, then\nselect noisy samples according to relationships between intra-modal and\ninter-modal losses. Besides, we also propose a simple but valid noise ratio\nestimation method by calculating the proportion of instances whose confidence\nis below a preset threshold. Our method makes large improvements over the\nprevious state of the arts (e.g., from 60.0% to 63.8% in segment-level visual\nmetric), which demonstrates the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Haoyue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Logit Adjustment. (arXiv:2204.11822v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11822","description":"<p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses\nchallenges in recognizing novel classes in the test phase. The development of\ngenerative models enables current GZSL techniques to probe further into the\nsemantic-visual link, culminating in a two-stage form that includes a generator\nand a classifier. However, existing generation-based methods focus on enhancing\nthe generator's effect while neglecting the improvement of the classifier. In\nthis paper, we first analyze of two properties of the generated pseudo unseen\nsamples: bias and homogeneity. Then, we perform variational Bayesian inference\nto back-derive the evaluation metrics, which reflects the balance of the seen\nand unseen classes. As a consequence of our derivation, the aforementioned two\nproperties are incorporated into the classifier training as seen-unseen priors\nvia logit adjustment. The Zero-Shot Logit Adjustment further puts\nsemantic-based classifiers into effect in generation-based GZSL. Our\nexperiments demonstrate that the proposed technique achieves state-of-the-art\nwhen combined with the basic generator, and it can improve various generative\nzero-shot learning frameworks. Our codes are available on\nhttps://github.com/cdb342/IJCAI-2022-ZLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dubing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. (arXiv:2204.12997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12997","description":"<p>Transformers are successfully applied to computer vision due to their\npowerful modeling capacity with self-attention. However, the excellent\nperformance of transformers heavily depends on enormous training images. Thus,\na data-efficient transformer solution is urgently needed. In this work, we\npropose an early knowledge distillation framework, which is termed as DearKD,\nto improve the data efficiency required by transformers. Our DearKD is a\ntwo-stage framework that first distills the inductive biases from the early\nintermediate layers of a CNN and then gives the transformer full play by\ntraining without distillation. Further, our DearKD can be readily applied to\nthe extreme data-free case where no real images are available. In this case, we\npropose a boundary-preserving intra-divergence loss based on DeepInversion to\nfurther close the performance gap against the full-data counterpart. Extensive\nexperiments on ImageNet, partial ImageNet, data-free setting and other\ndownstream tasks prove the superiority of DearKD over its baselines and\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}