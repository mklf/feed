{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2203.07376v1 [cs.DB])","link":"http://arxiv.org/abs/2203.07376","description":"<p>Recently, context-dependent text-to-SQL semantic parsing which translates\nnatural language into SQL in an interaction process has attracted a lot of\nattention. Previous works leverage context-dependence information either from\ninteraction history utterances or the previous predicted SQL queries but fail\nin taking advantage of both since of the mismatch between natural language and\nlogic-form SQL. In this work, we propose a History Information Enhanced\ntext-to-SQL model (HIE-SQL) to exploit context-dependence information from both\nhistory utterances and the last predicted SQL query. In view of the mismatch,\nwe treat natural language and SQL as two modalities and propose a bimodal\npre-trained model to bridge the gap between them. Besides, we design a\nschema-linking graph to enhance connections from utterances and the SQL query\nto the database schema. We show our history information enhanced methods\nimprove the performance of HIE-SQL by a significant margin, which achieves new\nstate-of-the-art results on the two context-dependent text-to-SQL benchmarks,\nthe SparC and CoSQL datasets, at the writing time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Baohua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changshan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models. (arXiv:2203.07402v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07402","description":"<p>Compositional generalization is a fundamental trait in humans, allowing us to\neffortlessly combine known phrases to form novel sentences. Recent works have\nclaimed that standard seq-to-seq models severely lack the ability to\ncompositionally generalize. In this paper, we focus on one-shot primitive\ngeneralization as introduced by the popular SCAN benchmark. We demonstrate that\nmodifying the training distribution in simple and intuitive ways enables\nstandard seq-to-seq models to achieve near-perfect generalization performance,\nthereby showing that their compositional generalization abilities were\npreviously underestimated. We perform detailed empirical analysis of this\nphenomenon. Our results indicate that the generalization performance of models\nis highly sensitive to the characteristics of the training data which should be\ncarefully considered while designing such benchmarks in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Arkil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattamishra_S/0/1/0/all/0/1\">Satwik Bhattamishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information. (arXiv:2203.07426v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07426","description":"<p>In linguistics, a sememe is defined as the minimum semantic unit of\nlanguages. Sememe knowledge bases (KBs), which are built by manually annotating\nwords with sememes, have been successfully applied to various NLP tasks.\nHowever, existing sememe KBs only cover a few languages, which hinders the wide\nutilization of sememes. To address this issue, the task of sememe prediction\nfor BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe\nKB based on BabelNet, a multilingual encyclopedia dictionary. By automatically\npredicting sememes for a BabelNet synset, the words in many languages in the\nsynset would obtain sememe annotations simultaneously. However, previous SPBS\nmethods have not taken full advantage of the abundant information in BabelNet.\nIn this paper, we utilize the multilingual synonyms, multilingual glosses and\nimages in BabelNet for SPBS. We design a multimodal information fusion model to\nencode and combine this information for sememe prediction. Experimental results\nshow the substantial outperformance of our model over previous methods (about\n10 MAP and F1 scores). All the code and data of this paper can be obtained at\nhttps://github.com/thunlp/MSGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chuancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Pairwise Ranking Model for Readability Assessment. (arXiv:2203.07450v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07450","description":"<p>Automatic Readability Assessment (ARA), the task of assigning a reading level\nto a text, is traditionally treated as a classification problem in NLP\nresearch. In this paper, we propose the first neural, pairwise ranking approach\nto ARA and compare it with existing classification, regression, and\n(non-neural) ranking methods. We establish the performance of our model by\nconducting experiments with three English, one French and one Spanish datasets.\nWe demonstrate that our approach performs well in monolingual single/cross\ncorpus testing scenarios and achieves a zero-shot cross-lingual ranking\naccuracy of over 80% for both French and Spanish when trained on English data.\nAdditionally, we also release a new parallel bilingual readability dataset in\nEnglish and French. To our knowledge, this paper proposes the first neural\npairwise ranking model for ARA, and shows the first results of cross-lingual,\nzero-shot evaluation of ARA with neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Justin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajjala_S/0/1/0/all/0/1\">Sowmya Vajjala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Estimation for Language Reward Models. (arXiv:2203.07472v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07472","description":"<p>Language models can learn a range of capabilities from unsupervised training\non text corpora. However, to solve a particular problem (such as text\nsummarization) it is typically necessary to fine-tune them on a task-specific\ndataset. It is often easier for humans to choose between options than to\nprovide labeled data, and prior work has achieved state-of-the-art performance\nby training a reward model from such preference comparisons. However,\ncollecting a large preference comparison dataset is still expensive -- and the\nlearned reward models are unreliable out-of-distribution. We seek to address\nthese problems via uncertainty estimation, which can improve sample efficiency\nand robustness using active learning and risk-averse reinforcement learning\n(RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble\nof reward models differing in the initialization of their final layer.\nEnsembles have proved successful in prior applications of active learning, but\nwe find that in our setting ensemble active learning does not outperform random\nsampling. Further experiments show that while the aggregate predictions are\nwell-calibrated, the ensemble's estimated epistemic uncertainty is only weakly\ncorrelated with model error. We suspect this is because the ensemble members\nare fine-tuned from a single model and so are similar to one another. This\nsuggests current pre-training methods will need to be modified to support\nuncertainty estimation, e.g. by training multiple language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gleave_A/0/1/0/all/0/1\">Adam Gleave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models. (arXiv:2203.07504v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07504","description":"<p>VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation\ntask for contextualized word embeddings (CWEs). VAST uses valence, the\nassociation of a word with pleasantness, to measure the correspondence of\nword-level LM semantics with widely used human judgments, and examines the\neffects of contextualization, tokenization, and LM-specific geometry. Because\nprior research has found that CWEs from GPT-2 perform poorly on other intrinsic\nevaluations, we select GPT-2 as our primary subject, and include results\nshowing that VAST is useful for 7 other LMs, and can be used in 7 languages.\nGPT-2 results show that the semantics of a word incorporate the semantics of\ncontext in layers closer to model output, such that VAST scores diverge between\nour contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.\nWe also show that multiply tokenized words are not semantically encoded until\nlayer 8, where they achieve Pearson's rho of .46, indicating the presence of an\nencoding process for multiply tokenized words which differs from that of singly\ntokenized words, for which rho is highest in layer 0. We find that a few\nneurons with values having greater magnitude than the rest mask word-level\nsemantics in GPT-2's top layer, but that word-level semantics can be recovered\nby nullifying non-semantic principal components: Pearson's rho in the top layer\nimproves from .32 to .76. After isolating semantics, we show the utility of\nVAST for understanding LM semantics via improvements over related work on four\nword similarity tasks, with a score of .50 on SimLex-999, better than the\nprevious best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,\nwhich compare differences in word embedding associations between groups of\nwords, exhibit more stereotype-congruent biases after isolating semantics,\nindicating that non-semantic structures in LMs also mask biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations. (arXiv:2203.07511v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07511","description":"<p>We examine the effects of contrastive visual semantic pretraining by\ncomparing the geometry and semantic properties of contextualized English\nlanguage representations formed by GPT-2 and CLIP, a zero-shot multimodal image\nclassifier which adapts the GPT-2 architecture to encode image captions. We\nfind that contrastive visual semantic pretraining significantly mitigates the\nanisotropy found in contextualized word embeddings from GPT-2, such that the\nintra-layer self-similarity (mean pairwise cosine similarity) of CLIP word\nembeddings is under .25 in all layers, compared to greater than .95 in the top\nlayer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic\nintrinsic evaluation tasks, and achieve a new corpus-based state of the art for\nthe RG65 evaluation, at .88. CLIP also forms fine-grained semantic\nrepresentations of sentences, and obtains Spearman's rho = .73 on the\nSemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning,\ncompared to no greater than rho = .45 in any layer of GPT-2. Finally,\nintra-layer self-similarity of CLIP sentence embeddings decreases as the layer\nindex increases, finishing at .25 in the top layer, while the self-similarity\nof GPT-2 sentence embeddings formed using the EOS token increases\nlayer-over-layer and never falls below .97. Our results indicate that high\nanisotropy is not an inevitable consequence of contextualization, and that\nvisual semantic pretraining is beneficial not only for ordering visual\nrepresentations, but also for encoding useful semantic representations of\nlanguage, both on the word level and the sentence level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer. (arXiv:2203.07519v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07519","description":"<p>Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering. (arXiv:2203.07522v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07522","description":"<p>While both extractive and generative readers have been successfully applied\nto the Question Answering (QA) task, little attention has been paid toward the\nsystematic comparison of them. Characterizing the strengths and weaknesses of\nthe two readers is crucial not only for making a more informed reader selection\nin practice but also for developing a deeper understanding to foster further\nresearch on improving readers in a principled manner. Motivated by this goal,\nwe make the first attempt to systematically study the comparison of extractive\nand generative readers for question answering. To be aligned with the\nstate-of-the-art, we explore nine transformer-based large pre-trained language\nmodels (PrLMs) as backbone architectures. Furthermore, we organize our findings\nunder two main categories: (1) keeping the architecture invariant, and (2)\nvarying the underlying PrLMs. Among several interesting findings, it is\nimportant to highlight that (1) the generative readers perform better in long\ncontext QA, (2) the extractive readers perform better in short context while\nalso showing better out-of-domain generalization, and (3) the encoder of\nencoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and\noutperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also\nstudy the effect of multi-task learning on the two types of readers varying the\nunderlying PrLMs and perform qualitative and quantitative diagnosis to provide\nfurther insights into future directions in modeling better readers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sense Embeddings are also Biased--Evaluating Social Biases in Static and Contextualised Sense Embeddings. (arXiv:2203.07523v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07523","description":"<p>Sense embedding learning methods learn different embeddings for the different\nsenses of an ambiguous word. One sense of an ambiguous word might be socially\nbiased while its other senses remain unbiased. In comparison to the numerous\nprior work evaluating the social biases in pretrained word embeddings, the\nbiases in sense embeddings have been relatively understudied. We create a\nbenchmark dataset for evaluating the social biases in sense embeddings and\npropose novel sense-specific bias evaluation measures. We conduct an extensive\nevaluation of multiple static and contextualised sense embeddings for various\ntypes of social biases using the proposed measures. Our experimental results\nshow that even in cases where no biases are found at word-level, there still\nexist worrying levels of social biases at sense-level, which are often ignored\nby the word-level bias evaluation measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScienceWorld: Is your Agent Smarter than a 5th Grader?. (arXiv:2203.07540v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07540","description":"<p>This paper presents a new benchmark, ScienceWorld, to test agents' scientific\nreasoning abilities in a new interactive text environment at the level of a\nstandard elementary school science curriculum. Despite the recent\ntransformer-based progress seen in adjacent fields such as question-answering,\nscientific text processing, and the wider area of natural language processing,\nwe find that current state-of-the-art models are unable to reason about or\nexplain learned science concepts in novel contexts. For instance, models can\neasily answer what the conductivity of a previously seen material is but\nstruggle when asked how they would conduct an experiment in a grounded,\ninteractive environment to find the conductivity of an unknown material. This\nbegs the question of whether current models are simply retrieving answers by\nway of seeing a large number of similar input examples or if they have learned\nto reason about concepts in a reusable manner. We hypothesize that agents need\nto be grounded in interactive environments to achieve such reasoning\ncapabilities. Our experiments provide empirical evidence supporting this\nhypothesis -- showing that a 1.5 million parameter agent trained interactively\nfor 100k steps outperforms a 11 billion parameter model statically trained for\nscientific question-answering and reasoning via millions of expert\ndemonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency. (arXiv:2203.07559v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07559","description":"<p>A well-calibrated neural model produces confidence (probability outputs)\nclosely approximated by the expected accuracy. While prior studies have shown\nthat mixup training as a data augmentation technique can improve model\ncalibration on image classification tasks, little is known about using mixup\nfor model calibration on natural language understanding (NLU) tasks. In this\npaper, we explore mixup for model calibration on several NLU tasks and propose\na novel mixup strategy for pre-trained language models that improves model\ncalibration further. Our proposed mixup is guided by both the Area Under the\nMargin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each\nsample (Simonyan et al.,2013). Moreover, we combine our mixup strategy with\nmodel miscalibration correction techniques (i.e., label smoothing and\ntemperature scaling) and provide detailed analyses of their impact on our\nproposed mixup. We focus on systematically designing experiments on three NLU\ntasks: natural language inference, paraphrase detection, and commonsense\nreasoning. Our method achieves the lowest expected calibration error compared\nto strong baselines on both in-domain and out-of-domain test samples while\nmaintaining competitive accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seo Yeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSM: Measuring the Enticement of Honeyfiles with Natural Language Processing. (arXiv:2203.07580v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07580","description":"<p>Honeyfile deployment is a useful breach detection method in cyber deception\nthat can also inform defenders about the intent and interests of intruders and\nmalicious insiders. A key property of a honeyfile, enticement, is the extent to\nwhich the file can attract an intruder to interact with it. We introduce a\nnovel metric, Topic Semantic Matching (TSM), which uses topic modelling to\nrepresent files in the repository and semantic matching in an embedding vector\nspace to compare honeyfile text and topic words robustly. We also present a\nhoneyfile corpus created with different Natural Language Processing (NLP)\nmethods. Experiments show that TSM is effective in inter-corpus comparisons and\nis a promising tool to measure the enticement of honeyfiles. TSM is the first\nmeasure to use NLP techniques to quantify the enticement of honeyfile content\nthat compares the essential topical content of local contexts to honeyfiles and\nis robust to paraphrasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timmer_R/0/1/0/all/0/1\">Roelien C. Timmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebowitz_D/0/1/0/all/0/1\">David Liebowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1\">Surya Nepal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1\">Salil Kanhere</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Document Summarization with Top-down and Bottom-up Inference. (arXiv:2203.07586v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07586","description":"<p>Text summarization aims to condense long documents and retain key\ninformation. Critical to the success of a summarization model is the faithful\ninference of latent representations of words or tokens in the source documents.\nMost recent models infer the latent representations with a transformer encoder,\nwhich is purely bottom-up. Also, self-attention-based inference models face the\nchallenge of quadratic complexity with respect to sequence length. We propose a\nprincipled inference framework to improve summarization models on these two\naspects. Our framework assumes a hierarchical latent structure of a document\nwhere the top-level captures the long range dependency at a coarser time scale\nand the bottom token level preserves the details. Critically, this hierarchical\nstructure enables token representations to be updated in both a bottom-up and\ntop-down manner. In the bottom-up pass, token representations are inferred with\nlocal self-attention to leverage its efficiency. Top-down correction is then\napplied to allow tokens to capture long-range dependency. We demonstrate the\neffectiveness of the proposed framework on a diverse set of summarization\ndatasets, including narrative, conversational, scientific documents and news.\nOur model achieves (1) competitive or better performance on short documents\nwith higher memory and compute efficiency, compared to full attention\ntransformers, and (2) state-of-the-art performance on a wide range of long\ndocument summarization benchmarks, compared to recent efficient transformers.\nWe also show that our model can summarize an entire book and achieve\ncompetitive performance using $0.27\\%$ parameters (464M vs. 175B) and much less\ntraining data, compared to a recent GPT-3-based model. These results indicate\nthe general applicability and benefits of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1\">Erik Nijkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedural Text Understanding via Scene-Wise Evolution. (arXiv:2203.07600v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07600","description":"<p>Procedural text understanding requires machines to reason about entity states\nwithin the dynamical narratives. Current procedural text understanding\napproaches are commonly \\textbf{entity-wise}, which separately track each\nentity and independently predict different states of each entity. Such an\nentity-wise paradigm does not consider the interaction between entities and\ntheir states. In this paper, we propose a new \\textbf{scene-wise} paradigm for\nprocedural text understanding, which jointly tracks states of all entities in a\nscene-by-scene manner. Based on this paradigm, we propose \\textbf{S}cene\n\\textbf{G}raph \\textbf{R}easoner (\\textbf{SGR}), which introduces a series of\ndynamically evolving scene graphs to jointly formulate the evolution of\nentities, states and their associations throughout the narrative. In this way,\nthe deep interactions between all entities and states can be jointly captured\nand simultaneously derived from scene graphs. Experiments show that SGR not\nonly achieves the new state-of-the-art performance but also significantly\naccelerates the speed of reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jialong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Meng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weijian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARETS: A Consistency And Robustness Evaluative Test Suite for VQA. (arXiv:2203.07613v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07613","description":"<p>We introduce CARETS, a systematic test suite to measure consistency and\nrobustness of modern VQA models through a series of six fine-grained capability\ntests. In contrast to existing VQA test sets, CARETS features balanced question\ngeneration to create pairs of instances to test models, with each pair focusing\non a specific capability such as rephrasing, logical symmetry or image\nobfuscation. We evaluate six modern VQA systems on CARETS and identify several\nactionable weaknesses in model comprehension, especially with concepts such as\nnegation, disjunction, or hypernym invariance. Interestingly, even the most\nsophisticated models are sensitive to aspects such as swapping the order of\nterms in a conjunction or varying the number of answer choices mentioned in the\nquestion. We release CARETS to be used as an extensible tool for evaluating\nmulti-modal model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_C/0/1/0/all/0/1\">Carlos E. Jimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Plagiarize?. (arXiv:2203.07618v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07618","description":"<p>Past literature has illustrated that language models do not fully understand\nthe context and sensitivity of text and can sometimes memorize phrases or\nsentences present in their training sets. In this paper, we investigate whether\nthey not only memorize but also plagiarize training samples when generating\nartificial texts. Our findings support that they, especially GPT-2, reuse\nparticular pieces of texts from the training corpus with or without\nobfuscation. We have four main results: 1) language models with more capacity\nplagiarize more; 2) fine-tuned language models demonstrate differing patterns\nof plagiarism based on characteristics of auxiliary data; 3) sampling from\ntruncated language modeling distributions tends to heighten the degree of\nplagiarism as opposed to temperature sampling, and 4) plagiarism in language\nmodels can have serious privacy consequences. Overall, our work implies that\nfuture research on neural language models should take precautions to avoid\nmodels plagiarizing their training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jooyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation. (arXiv:2203.07627v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07627","description":"<p>Multilingual neural machine translation models are trained to maximize the\nlikelihood of a mix of examples drawn from multiple language pairs. The\ndominant inductive bias applied to these models is a shared vocabulary and a\nshared set of parameters across languages; the inputs and labels corresponding\nto examples drawn from different language pairs might still reside in distinct\nsub-spaces. In this paper, we introduce multilingual crossover encoder-decoder\n(mXEncDec) to fuse language pairs at an instance level. Our approach\ninterpolates instances from different language pairs into joint `crossover\nexamples' in order to encourage sharing input and output spaces across\nlanguages. To ensure better fusion of examples in multilingual settings, we\npropose several techniques to improve example interpolation across dissimilar\nlanguages under heavy data imbalance. Experiments on a large-scale WMT\nmultilingual dataset demonstrate that our approach significantly improves\nquality on English-to-Many, Many-to-English and zero-shot translation tasks\n(from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets\ndemonstrate the capability of our approach to improve model generalization to\nout-of-distribution multilingual examples. We also conduct qualitative and\nquantitative representation comparisons to analyze the advantages of our\napproach at the representation level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering. (arXiv:2203.07633v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07633","description":"<p>Representations of events described in text are important for various tasks.\nIn this work, we present SWCC: a Simultaneous Weakly supervised Contrastive\nlearning and Clustering framework for event representation learning. SWCC\nlearns event representations by making better use of co-occurrence information\nof events. Specifically, we introduce a weakly supervised contrastive learning\nmethod that allows us to consider multiple positives and multiple negatives,\nand a prototype-based clustering method that avoids semantically related events\nbeing pulled apart. For model training, SWCC learns representations by\nsimultaneously performing weakly supervised contrastive learning and\nprototype-based clustering. Experimental results show that SWCC outperforms\nother baselines on Hard Similarity and Transitive Sentence Similarity tasks. In\naddition, a thorough analysis of the prototype-based clustering method\ndemonstrates that the learned prototype vectors are able to implicitly capture\nvarious relations between events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wilfred Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Keyphrase Extraction via Interpretable Neural Networks. (arXiv:2203.07640v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07640","description":"<p>Keyphrase extraction aims at automatically extracting a list of \"important\"\nphrases which represent the key concepts in a document. Prior approaches for\nunsupervised keyphrase extraction resort to heuristic notions of phrase\nimportance via embedding similarities or graph centrality, requiring extensive\ndomain expertise to develop them. Our work proposes an alternative operational\ndefinition: phrases that are most useful for predicting the topic of a text are\nimportant keyphrases. To this end, we propose INSPECT -- a self-explaining\nneural framework for identifying influential keyphrases by measuring the\npredictive impact of input phrases on the downstream task of topic\nclassification. We show that this novel approach not only alleviates the need\nfor ad-hoc heuristics but also achieves state-of-the-art results in\nunsupervised keyphrase extraction across four diverse datasets in two domains:\nscientific publications and news articles. Ultimately, our study suggests a new\nusage of interpretable neural networks as an intrinsic component in NLP\nsystems, and not only as a tool for explaining model predictions to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rishabh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saldanha_E/0/1/0/all/0/1\">Emily Saldanha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkova_S/0/1/0/all/0/1\">Svitlana Volkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Synthetic Translations Improve Bitext Quality?. (arXiv:2203.07643v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07643","description":"<p>Synthetic translations have been used for a wide range of NLP tasks primarily\nas a means of data augmentation. This work explores, instead, how synthetic\ntranslations can be used to revise potentially imperfect reference translations\nin mined bitext. We find that synthetic samples can improve bitext quality\nwithout any additional bilingual supervision when they replace the originals\nbased on a semantic equivalence classifier that helps mitigate NMT noise. The\nimproved quality of the revised bitext is confirmed intrinsically via human\nevaluation and extrinsically through bilingual induction and MT tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1\">Eleftheria Briakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Long Sequence Encoding via Synchronization. (arXiv:2203.07644v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07644","description":"<p>Pre-trained Transformer models have achieved successes in a wide range of NLP\ntasks, but are inefficient when dealing with long input sequences. Existing\nstudies try to overcome this challenge via segmenting the long sequence\nfollowed by hierarchical encoding or post-hoc aggregation. We propose a\nsynchronization mechanism for hierarchical encoding. Our approach first\nidentifies anchor tokens across segments and groups them by their roles in the\noriginal input sequence. Then inside Transformer layer, anchor embeddings are\nsynchronized within their group via a self-attention module. Our approach is a\ngeneral framework with sufficient flexibility -- when adapted to a new task, it\nis easy to be enhanced with the task-specific anchor definitions. Experiments\non two representative tasks with different types of long input texts,\nNarrativeQA summary setting and wild multi-hop reasoning from HotpotQA,\ndemonstrate that our approach is able to improve the global information\nexchange among segments while maintaining efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiangyang Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoDCL: A Distantly Supervised Contrastive Learning Framework for Social Meaning. (arXiv:2203.07648v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07648","description":"<p>Existing supervised contrastive learning frameworks suffer from two major\ndrawbacks: (i) they depend on labeled data, which is limited for the majority\nof tasks in real-world, and (ii) they incorporate inter-class relationships\nbased on instance-level information, while ignoring corpus-level information,\nfor weighting negative samples. To mitigate these challenges, we propose an\neffective distantly supervised contrastive learning framework (InfoDCL) that\nmakes use of naturally occurring surrogate labels in the context of contrastive\nlearning and employs pointwise mutual information to leverage corpus-level\ninformation. Our framework outperforms an extensive set of existing contrastive\nlearning methods (self-supervised, supervised, and weakly supervised) on a wide\nrange of social meaning tasks (in-domain and out-of-domain), in both the\ngeneral and few-shot settings. Our method is also language-agnostic, as we\ndemonstrate on three languages in addition to English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness. (arXiv:2203.07653v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07653","description":"<p>Data modification, either via additional training datasets, data\naugmentation, debiasing, and dataset filtering, has been proposed as an\neffective solution for generalizing to out-of-domain (OOD) inputs, in both\nnatural language processing and computer vision literature. However, the effect\nof data modification on adversarial robustness remains unclear. In this work,\nwe conduct a comprehensive study of common data modification strategies and\nevaluate not only their in-domain and OOD performance, but also their\nadversarial robustness (AR). We also present results on a two-dimensional\nsynthetic dataset to visualize the effect of each method on the training\ndistribution. This work serves as an empirical study towards understanding the\nrelationship between generalizing to unseen domains and defending against\nadversarial perturbations. Our findings suggest that more data (either via\nadditional datasets or data augmentation) benefits both OOD accuracy and AR.\nHowever, data filtering (previously shown to improve OOD accuracy on natural\nlanguage inference) hurts OOD accuracy on other tasks such as question\nanswering and image classification. We provide insights from our experiments to\ninform future work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1\">Bhavdeep Singh Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue. (arXiv:2203.07657v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07657","description":"<p>Effective human-chatbot conversations need to achieve both coherence and\nefficiency. Complex conversation settings such as persuasion involve\ncommunicating changes in attitude or behavior, so users' perspectives need to\nbe carefully considered and addressed, even when not directly related to the\ntopic. In this work, we contribute a novel modular dialogue system framework\nthat seamlessly integrates factual information and social content into\npersuasive dialogue. Our framework is generalizable to any dialogue tasks that\nhave mixed social and task contents. We conducted a study that compared user\nevaluations of our framework versus a baseline end-to-end generation model. We\nfound our model was evaluated to be more favorable in all dimensions including\ncompetence and friendliness compared to the baseline model which does not\nexplicitly handle social content or factual questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Feifan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Ryan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Agent To Rule Them All: Towards Multi-agent Conversational AI. (arXiv:2203.07665v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07665","description":"<p>The increasing volume of commercially available conversational agents (CAs)\non the market has resulted in users being burdened with learning and adopting\nmultiple agents to accomplish their tasks. Though prior work has explored\nsupporting a multitude of domains within the design of a single agent, the\ninteraction experience suffers due to the large action space of desired\ncapabilities. To address these problems, we introduce a new task BBAI:\nBlack-Box Agent Integration, focusing on combining the capabilities of multiple\nblack-box CAs at scale. We explore two techniques: question agent pairing and\nquestion response pairing aimed at resolving this task. Leveraging these\ntechniques, we design One For All (OFA), a scalable system that provides a\nunified interface to interact with multiple CAs. Additionally, we introduce\nMARS: Multi-Agent Response Selection, a new encoder model for question response\npairing that jointly encodes user question and agent response pairs. We\ndemonstrate that OFA is able to automatically and accurately integrate an\nensemble of commercially available CAs spanning disparate domains.\nSpecifically, using the MARS encoder we achieve the highest accuracy on our\nBBAI task, outperforming strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Christopher Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peper_J/0/1/0/all/0/1\">Joseph Joshua Peper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_K/0/1/0/all/0/1\">Karthik Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talamonti_W/0/1/0/all/0/1\">Walter Talamonti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasecki_W/0/1/0/all/0/1\">Walter Lasecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yiping Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lingjia Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1\">Jason Mars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation. (arXiv:2203.07687v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07687","description":"<p>How to learn highly compact yet effective sentence representation?\nPre-trained language models have been effective in many NLP tasks. However,\nthese models are often huge and produce large sentence embeddings. Moreover,\nthere is a big performance gap between large and small models. In this paper,\nwe propose Homomorphic Projective Distillation (HPD) to learn compressed\nsentence embeddings. Our method augments a small Transformer encoder model with\nlearnable projection layers to produce compact representations while mimicking\na large pre-trained language model to retain the sentence representation\nquality. We evaluate our method with different model sizes on both semantic\ntextual similarity (STS) and semantic retrieval (SR) tasks. Experiments show\nthat our method achieves 2.7-4.5 points performance gain on STS tasks compared\nwith previous best representations of the same size. In SR tasks, our method\nimproves retrieval speed (8.2$\\times$) and memory usage (8.0$\\times$) compared\nwith state-of-the-art large models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiguo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReACC: A Retrieval-Augmented Code Completion Framework. (arXiv:2203.07722v1 [cs.SE])","link":"http://arxiv.org/abs/2203.07722","description":"<p>Code completion, which aims to predict the following code token(s) according\nto the code context, can improve the productivity of software development.\nRecent work has proved that statistical language modeling with transformers can\ngreatly improve the performance in the code completion task via learning from\nlarge-scale source code datasets. However, current approaches focus only on\ncode context within the file or project, i.e. internal context. Our distinction\nis utilizing \"external\" context, inspired by human behaviors of copying from\nthe related code snippets when writing code. Specifically, we propose a\nretrieval-augmented code completion framework, leveraging both lexical copying\nand referring to code with similar semantics by retrieval. We adopt a\nstage-wise training approach that combines a source code retriever and an\nauto-regressive language model for programming language. We evaluate our\napproach in the code completion task in Python and Java programming languages,\nachieving a state-of-the-art performance on CodeXGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hojae Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating BERT-based Pre-training Language Models for Detecting Misinformation. (arXiv:2203.07731v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07731","description":"<p>It is challenging to control the quality of online information due to the\nlack of supervision over all the information posted online. Manual checking is\nalmost impossible given the vast number of posts made on online media and how\nquickly they spread. Therefore, there is a need for automated rumour detection\ntechniques to limit the adverse effects of spreading misinformation. Previous\nstudies mainly focused on finding and extracting the significant features of\ntext data. However, extracting features is time-consuming and not a highly\neffective process. This study proposes the BERT- based pre-trained language\nmodels to encode text data into vectors and utilise neural network models to\nclassify these vectors to detect misinformation. Furthermore, different\nlanguage models (LM) ' performance with different trainable parameters was\ncompared. The proposed technique is tested on different short and long text\ndatasets. The result of the proposed technique has been compared with the\nstate-of-the-art techniques on the same datasets. The results show that the\nproposed technique performs better than the state-of-the-art techniques. We\nalso tested the proposed technique by combining the datasets. The results\ndemonstrated that the large data training and testing size considerably\nimproves the technique's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anggrainingsih_R/0/1/0/all/0/1\">Rini Anggrainingsih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1\">Ghulam Mubashar Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Amitava Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language. (arXiv:2203.07742v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07742","description":"<p>Most of the current task-oriented dialogue systems (ToD), despite having\ninteresting results, are designed for a handful of languages like Chinese and\nEnglish. Therefore, their performance in low-resource languages is still a\nsignificant problem due to the absence of a standard dataset and evaluation\npolicy. To address this problem, we proposed ViWOZ, a fully-annotated\nVietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,\nmulti-domain tasked oriented dataset in Vietnamese, a low-resource language.\nThe dataset consists of a total of 5,000 dialogues, including 60,946 fully\nannotated utterances. Furthermore, we provide a comprehensive benchmark of both\nmodular and end-to-end models in low-resource language scenarios. With those\ncharacteristics, the ViWOZ dataset enables future studies on creating a\nmultilingual task-oriented dialogue system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1\">Phi Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tung Cao Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manh_D/0/1/0/all/0/1\">Dung Nguyen Manh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_Q/0/1/0/all/0/1\">Quan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1\">Long Tran Quoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL. (arXiv:2203.07781v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07781","description":"<p>Existing text-to-SQL semantic parsers are typically designed for particular\nsettings such as handling queries that span multiple tables, domains or turns\nwhich makes them ineffective when applied to different settings. We present\nUniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits\nfrom directly using an off-the-shelf language model architecture and\ndemonstrates consistently high performance under different settings.\nSpecifically, UniSAr extends existing autoregressive language models to\nincorporate three non-invasive extensions to make them structure-aware: (1)\nadding structure mark to encode database schema, conversation context, and\ntheir relationships; (2) constrained decoding to decode well structured SQL for\na given database schema; and (3) SQL completion to complete potential missing\nJOIN relationships in SQL based on database schema. On seven well-known\ntext-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr\ndemonstrates highly comparable or better performance to the most advanced\nspecifically-designed text-to-SQL models. Importantly, our UniSAr is\nnon-invasive, such that other core model advances in text-to-SQL can also adopt\nour extensions to further enhance performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">Dechen Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Ghost in the Machine has an American accent: value conflict in GPT-3. (arXiv:2203.07785v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07785","description":"<p>The alignment problem in the context of large language models must consider\nthe plurality of human values in our world. Whilst there are many resonant and\noverlapping values amongst the world's cultures, there are also many\nconflicting, yet equally valid, values. It is important to observe which\ncultural values a model exhibits, particularly when there is a value conflict\nbetween input prompts and generated outputs. We discuss how the co-creation of\nlanguage and cultural value impacts large language models (LLMs). We explore\nthe constitution of the training data for GPT-3 and compare that to the world's\nlanguage and internet access demographics, as well as to reported statistical\nprofiles of dominant values in some Nation-states. We stress tested GPT-3 with\na range of value-rich texts representing several languages and nations;\nincluding some with values orthogonal to dominant US public opinion as reported\nby the World Values Survey. We observed when values embedded in the input text\nwere mutated in the generated outputs and noted when these conflicting values\nwere more aligned with reported dominant US values. Our discussion of these\nresults uses a moral value pluralism (MVP) lens to better understand these\nvalue mutations. Finally, we provide recommendations for how our work may\ncontribute to other current work in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_R/0/1/0/all/0/1\">Rebecca L Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pistilli_G/0/1/0/all/0/1\">Giada Pistilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menedez_Gonzalez_N/0/1/0/all/0/1\">Natalia Men&#xe9;dez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duran_L/0/1/0/all/0/1\">Leslye Denisse Dias Duran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panai_E/0/1/0/all/0/1\">Enrico Panai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpokiene_J/0/1/0/all/0/1\">Julija Kalpokiene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertulfo_D/0/1/0/all/0/1\">Donald Jay Bertulfo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs. (arXiv:2203.07828v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07828","description":"<p>Pre-trained Transformers are good foundations for unified multi-task models\nowing to their task-agnostic representation. Pre-trained Transformers are often\ncombined with text-to-text framework to execute multiple tasks by a single\nmodel. Performing a task through a graphical user interface (GUI) is another\ncandidate to accommodate various tasks, including multi-step tasks with vision\nand language inputs. However, few papers combine pre-trained Transformers with\nperforming through GUI. To fill this gap, we explore a framework in which a\nmodel performs a task by manipulating the GUI implemented with web pages in\nmultiple steps. We develop task pages with and without page transitions and\npropose a BERT extension for the framework. We jointly trained our BERT\nextension with those task pages, and made the following observations. (1) The\nmodel learned to use both task pages with and without page transition. (2) In\nfour out of five tasks without page transitions, the model performs greater\nthan 75% of the performance of the original BERT, which does not use browsers.\n(3) The model did not generalize effectively on unseen tasks. These results\nsuggest that we can fine-tune BERTs to multi-step tasks through GUIs, and there\nis room for improvement in their generalizability. Code will be available\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iki_T/0/1/0/all/0/1\">Taichi Iki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07836","description":"<p>Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCD: Self-Contrastive Decorrelation for Sentence Embeddings. (arXiv:2203.07847v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07847","description":"<p>In this paper, we propose Self-Contrastive Decorrelation (SCD), a\nself-supervised approach. Given an input sentence, it optimizes a joint\nself-contrastive and decorrelation objective. Learning a representation is\nfacilitated by leveraging the contrast arising from the instantiation of\nstandard dropout at different rates. The proposed method is conceptually simple\nyet empirically powerful. It achieves comparable results with state-of-the-art\nmethods on multiple benchmarks without using contrastive pairs. This study\nopens up avenues for efficient self-supervised learning methods that are more\nrobust than current contrastive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting. (arXiv:2203.07856v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07856","description":"<p>In document classification for, e.g., legal and biomedical text, we often\ndeal with hundreds of classes, including very infrequent ones, as well as\ntemporal concept drift caused by the influence of real world events, e.g.,\npolicy changes, conflicts, or pandemics. Class imbalance and drift can\nsometimes be mitigated by resampling the training data to simulate (or\ncompensate for) a known target distribution, but what if the target\ndistribution is determined by unknown future events? Instead of simply\nresampling uniformly to hedge our bets, we focus on the underlying optimization\nalgorithms used to train such document classifiers and evaluate several\ngroup-robust optimization algorithms, initially proposed to mitigate\ngroup-level disparities. Reframing group-robust algorithms as adaptation\nalgorithms under concept drift, we find that Invariant Risk Minimization and\nSpectral Decoupling outperform sampling-based approaches to class imbalance and\nconcept drift, and lead to much better performance on minority classes. The\neffect is more pronounced the larger the label set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost. (arXiv:2203.07860v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07860","description":"<p>State-of-the-art NLP systems represent inputs with word embeddings, but these\nare brittle when faced with Out-of-Vocabulary (OOV) words. To address this\nissue, we follow the principle of mimick-like models to generate vectors for\nunseen words, by learning the behavior of pre-trained embeddings using only the\nsurface form of words. We present a simple contrastive learning framework,\nLOVE, which extends the word representation of an existing pre-trained language\nmodel (such as BERT), and makes it robust to OOV with few additional\nparameters. Extensive evaluations demonstrate that our lightweight model\nachieves similar or even better performances than prior competitors, both on\noriginal datasets and on corrupted variants. Moreover, it can be used in a\nplug-and-play fashion with FastText and BERT, where it significantly improves\ntheir robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian M. Suchanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition. (arXiv:2203.07890v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07890","description":"<p>Visual Question Generation (VQG) is a task to generate questions from images.\nWhen humans ask questions about an image, their goal is often to acquire some\nnew knowledge. However, existing studies on VQG have mainly addressed question\ngeneration from answers or question categories, overlooking the objectives of\nknowledge acquisition. To introduce a knowledge acquisition perspective into\nVQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is\nthe first large, humanly annotated dataset in which questions regarding images\nare tied to structured knowledge. We also developed a new VQG model that can\nencode and use knowledge as the target for a question. The experiment results\nshow that our model outperforms existing models on the K-VQG dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07893","description":"<p>We describe a simple and effective method (Spectral Attribute removaL; SAL)\nto remove guarded information from neural representations. Our method uses\nsingular value decomposition and eigenvalue decomposition to project the input\nrepresentations into directions with reduced covariance with the guarded\ninformation rather than maximal covariance as normally these factorization\nmethods are used. We begin with linear information removal and proceed to\ngeneralize our algorithm to the case of nonlinear information removal through\nthe use of kernels. Our experiments demonstrate that our algorithm retains\nbetter main task performance after removing the guarded information compared to\nprevious methods. In addition, our experiments demonstrate that we need a\nrelatively small amount of guarded attribute data to remove information about\nthese attributes, which lowers the exposure to such possibly sensitive data and\nfits better low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models. (arXiv:2203.07911v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07911","description":"<p>Natural language processing models learn word representations based on the\ndistributional hypothesis, which asserts that word context (e.g.,\nco-occurrence) correlates with meaning. We propose that $n$-grams composed of\nrandom character sequences, or $garble$, provide a novel context for studying\nword meaning both within and beyond extant language. In particular, randomly\ngenerated character $n$-grams lack meaning but contain primitive information\nbased on the distribution of characters they contain. By studying the\nembeddings of a large corpus of garble, extant language, and pseudowords using\nCharacterBERT, we identify an axis in the model's high-dimensional embedding\nspace that separates these classes of $n$-grams. Furthermore, we show that this\naxis relates to structure within extant language, including word\npart-of-speech, morphology, and concept concreteness. Thus, in contrast to\nstudies that are mainly limited to extant language, our work reveals that\nmeaning and primitive information are intrinsically linked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1\">Mark Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desikan_B/0/1/0/all/0/1\">Bhargav Srinivasa Desikan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_E/0/1/0/all/0/1\">Ethan O. Nadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardo_R/0/1/0/all/0/1\">Ruggerio L. Sardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darragh_Ford_E/0/1/0/all/0/1\">Elise Darragh-Ford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guilbeault_D/0/1/0/all/0/1\">Douglas Guilbeault</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Extractive Opinion Summarization Using Sparse Coding. (arXiv:2203.07921v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07921","description":"<p>Opinion summarization is the task of automatically generating summaries that\nencapsulate information from multiple user reviews. We present Semantic\nAutoencoder (SemAE) to perform extractive opinion summarization in an\nunsupervised manner. SemAE uses dictionary learning to implicitly capture\nsemantic information from the review and learns a latent representation of each\nsentence over semantic units. A semantic unit is supposed to capture an\nabstract semantic concept. Our extractive summarization algorithm leverages the\nrepresentations to identify representative opinions among hundreds of reviews.\nSemAE is also able to perform controllable summarization to generate\naspect-specific summaries. We report strong performance on SPACE and AMAZON\ndatasets, and perform experiments to investigate the functioning of our model.\nOur code is publicly available at https://github.com/brcsomnath/SemAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness of Neural-Statistical Features in Detection of Generative Transformers. (arXiv:2203.07983v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07983","description":"<p>The detection of computer-generated text is an area of rapidly increasing\nsignificance as nascent generative models allow for efficient creation of\ncompelling human-like text, which may be abused for the purposes of spam,\ndisinformation, phishing, or online influence campaigns. Past work has studied\ndetection of current state-of-the-art models, but despite a developing threat\nlandscape, there has been minimal analysis of the robustness of detection\nmethods to adversarial attacks. To this end, we evaluate neural and non-neural\napproaches on their ability to detect computer-generated text, their robustness\nagainst text adversarial attacks, and the impact that successful adversarial\nattacks have on human judgement of text quality. We find that while statistical\nfeatures underperform neural features, statistical features provide additional\nadversarial robustness that can be leveraged in ensemble detection models. In\nthe process, we find that previously effective complex phrasal features for\ndetection of computer-generated text hold little predictive power against\ncontemporary generative models, and identify promising statistical features to\nuse instead. Finally, we pioneer the usage of $\\Delta$MAUVE as a proxy measure\nfor human judgement of adversarial text quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crothers_E/0/1/0/all/0/1\">Evan Crothers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viktor_H/0/1/0/all/0/1\">Herna Viktor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_P/0/1/0/all/0/1\">Paula Branco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based Multi-Modal Fact-Checking. (arXiv:2203.07990v1 [cs.MM])","link":"http://arxiv.org/abs/2203.07990","description":"<p>Identifying fake news is a very difficult task, especially when considering\nthe multiple modes of conveying information through text, image, video and/or\naudio. We attempted to tackle the problem of automated\nmisinformation/disinformation detection in multi-modal news sources (including\ntext and images) through our simple, yet effective, approach in the FACTIFY\nshared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of\n74.807%, which was the fourth best out of all the submissions. In this paper we\nwill explain our approach to undertake the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhankar_A/0/1/0/all/0/1\">Abhishek Dhankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolduc_F/0/1/0/all/0/1\">Francois Bolduc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v1 [cs.SD])","link":"http://arxiv.org/abs/2203.07996","description":"<p>Training Transformer-based models demands a large amount of data, while\nobtaining parallel aligned and labelled data in multimodality is rather\ncost-demanding, especially for audio-visual speech recognition (AVSR). Thus it\nmakes a lot of sense to make use of unlabelled uni-modal data. On the other\nside, although the effectiveness of large-scale self-supervised learning is\nwell established in both audio and visual modalities, how to integrate those\npre-trained models into a multimodal scenario remains underexplored. In this\nwork, we successfully leverage uni-modal self-supervised learning to promote\nthe multimodal AVSR. In particular, we first train audio and visual encoders on\na large-scale uni-modal dataset, then we integrate components of both encoders\ninto a larger multimodal framework which learns to recognize paired\naudio-visual data into characters through a combination of CTC and seq2seq\ndecoding. We show that both components inherited from uni-modal self-supervised\nlearning cooperate well, resulting in that the multimodal framework yields\ncompetitive results through fine-tuning. Our model is experimentally validated\non both word-level and sentence-level AVSR tasks. Especially, even without an\nexternal language model, our proposed model raises the state-of-the-art\nperformances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a\nlarge margin, with a relative improvement of 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xichen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular and Parameter-Efficient Multimodal Fusion with Prompting. (arXiv:2203.08055v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08055","description":"<p>Recent research has made impressive progress in large-scale multimodal\npre-training. In the context of the rapid growth of model size, it is necessary\nto seek efficient and flexible methods other than finetuning. In this paper, we\npropose to use prompt vectors to align the modalities. Our method achieves\ncomparable performance to several other multimodal fusion methods in\nlow-resource settings. We further show that our method is modular and\nparameter-efficient for processing tasks involving two or more data modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08075","description":"<p>Spatial commonsense, the knowledge about spatial position and relationship\nbetween objects (like the relative size of a lion and a girl, and the position\nof a boy relative to a bicycle when cycling), is an important part of\ncommonsense knowledge. Although pretrained language models (PLMs) succeed in\nmany NLP tasks, they are shown to be ineffective in spatial commonsense\nreasoning. Starting from the observation that images are more likely to exhibit\nspatial commonsense than texts, we explore whether models with visual signals\nlearn more spatial commonsense than text-based PLMs. We propose a spatial\ncommonsense benchmark that focuses on the relative scales of objects, and the\npositional relationship between people and objects under different actions. We\nprobe PLMs and models with visual signals, including vision-language pretrained\nmodels and image synthesis models, on this benchmark, and find that image\nsynthesis models are more capable of learning accurate and consistent spatial\nknowledge than other models. The spatial knowledge from image synthesis models\nalso helps in natural language understanding tasks that require spatial\ncommonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns. (arXiv:2203.08085v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08085","description":"<p>There is a growing interest in the combined use of NLP and machine learning\nmethods to predict gaze patterns during naturalistic reading. While promising\nresults have been obtained through the use of transformer-based language\nmodels, little work has been undertaken to relate the performance of such\nmodels to general text characteristics. In this paper we report on experiments\nwith two eye-tracking corpora of naturalistic reading and two language models\n(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of\nfeatures for predicting human reading behavior that fall into five categories\n(syntactic complexity, lexical richness, register-based multiword combinations,\nreadability and psycholinguistic word properties). Our experiments show that\nboth the features included and the architecture of the transformer-based\nlanguage models play a role in predicting multiple eye-tracking measures during\nnaturalistic reading. We also report the results of experiments aimed at\ndetermining the relative importance of features from different groups using\nSP-LIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1\">Daniel Wiechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1\">Elma Kerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Corpus Quality Really Matter for Low-Resource Languages?. (arXiv:2203.08111v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08111","description":"<p>The vast majority of non-English corpora are derived from automatically\nfiltered versions of CommonCrawl. While prior work has identified major issues\non the quality of these datasets (Kreutzer et al., 2021), it is not clear how\nthis impacts downstream performance. Taking Basque as a case study, we explore\ntailored crawling (manually identifying and scraping websites with high-quality\ncontent) as an alternative to filtering CommonCrawl. Our new corpus, called\nEusCrawl, is similar in size to the Basque portion of popular multilingual\ncorpora like CC100 and mC4, yet it has a much higher quality according to\nnative annotators. For instance, 66% of documents are rated as high-quality for\nEusCrawl, in contrast with &lt;33% for both mC4 and CC100. Nevertheless, we obtain\nsimilar results on downstream tasks regardless of the corpus used for\npre-training. Our work suggests that NLU performance in low-resource languages\nis primarily constrained by the quantity rather than the quality of the data,\nprompting for methods to exploit more diverse data sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldabe_I/0/1/0/all/0/1\">Itziar Aldabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_de_Vinaspre_O/0/1/0/all/0/1\">Olatz Perez-de-Vi&#xf1;aspre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Resource-Constrained Keyphrase Generation. (arXiv:2203.08118v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08118","description":"<p>State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with constrained\nresources. To overcome this challenge, we investigate strategies to learn an\nintermediate representation suitable for the keyphrase generation task. We\nintroduce salient span recovery and salient span prediction as guided denoising\nlanguage modeling objectives that condense the domain-specific knowledge\nessential for keyphrase generation. Through experiments on multiple scientific\nkeyphrase generation benchmarks, we show the effectiveness of the proposed\napproach for facilitating low-resource and zero-shot keyphrase generation.\nFurthermore, we observe that our method especially benefits the generation of\nabsent keyphrases, approaching the performance of SOTA methods trained with\nlarge training sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Code Assignment with Gated Convolution and Note-Code Interaction. (arXiv:2010.06975v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06975","description":"<p>Medical code assignment from clinical text is a fundamental task in clinical\ninformation system management. As medical notes are typically lengthy and the\nmedical coding system's code space is large, this task is a long-standing\nchallenge. Recent work applies deep neural network models to encode the medical\nnotes and assign medical codes to clinical documents. However, these methods\nare still ineffective as they do not fully encode and capture the lengthy and\nrich semantic information of medical notes nor explicitly exploit the\ninteractions between the notes and codes. We propose a novel method, gated\nconvolutional neural networks, and a note-code interaction (GatedCNN-NCI), for\nautomatic medical code assignment to overcome these challenges. Our methods\ncapture the rich semantic information of the lengthy clinical text for better\nrepresentation by utilizing embedding injection and gated information\npropagation in the medical note encoding module. With a novel note-code\ninteraction design and a graph message passing mechanism, we explicitly capture\nthe underlying dependency between notes and codes, enabling effective code\nprediction. A weight sharing scheme is further designed to decrease the number\nof trainable parameters. Empirical experiments on real-world clinical datasets\nshow that our proposed model outperforms state-of-the-art models in most cases,\nand our model size is on par with light-weighted baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models. (arXiv:2103.17151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.17151","description":"<p>Multi-encoder models are a broad family of context-aware neural machine\ntranslation systems that aim to improve translation quality by encoding\ndocument-level contextual information alongside the current sentence. The\ncontext encoding is undertaken by contextual parameters, trained on\ndocument-level data. In this work, we discuss the difficulty of training these\nparameters effectively, due to the sparsity of the words in need of context\n(i.e., the training signal), and their relevant context. We propose to\npre-train the contextual parameters over split sentence pairs, which makes an\nefficient use of the available data for two reasons. Firstly, it increases the\ncontextual training signal by breaking intra-sentential syntactic relations,\nand thus pushing the model to search the context for disambiguating clues more\nfrequently. Secondly, it eases the retrieval of relevant context, since context\nsegments become shorter. We propose four different splitting methods, and\nevaluate our approach with BLEU and contrastive test sets. Results show that it\nconsistently improves learning of contextual parameters, both in low and high\nresource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupo_L/0/1/0/all/0/1\">Lorenzo Lupo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinarelli_M/0/1/0/all/0/1\">Marco Dinarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08678","description":"<p>Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorisation versus Generalisation in Pre-trained Language Models. (arXiv:2105.00828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00828","description":"<p>State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanzer_M/0/1/0/all/0/1\">Michael T&#xe4;nzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Parsing as MRC-based Span-Span Prediction. (arXiv:2105.07654v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07654","description":"<p>Higher-order methods for dependency parsing can partially but not fully\naddress the issue that edges in dependency trees should be constructed at the\ntext span/subtree level rather than word level. In this paper, we propose a new\nmethod for dependency parsing to address this issue. The proposed method\nconstructs dependency trees by directly modeling span-span (in other words,\nsubtree-subtree) relations. It consists of two modules: the {\\it text span\nproposal module} which proposes candidate text spans, each of which represents\na subtree in the dependency tree denoted by (root, start, end); and the {\\it\nspan linking module}, which constructs links between proposed spans. We use the\nmachine reading comprehension (MRC) framework as the backbone to formalize the\nspan linking module, where one span is used as a query to extract the text\nspan/subtree it should be linked to. The proposed method has the following\nmerits: (1) it addresses the fundamental problem that edges in a dependency\ntree should be constructed between subtrees; (2) the MRC framework allows the\nmethod to retrieve missing spans in the span proposal stage, which leads to\nhigher recall for eligible spans. Extensive experiments on the PTB, CTB and\nUniversal Dependencies (UD) benchmarks demonstrate the effectiveness of the\nproposed method. The code is available at\n\\url{https://github.com/ShannonAI/mrc-for-dependency-parsing}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Leilei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.02736","description":"<p>While recent work has shown that scores from models trained by the ubiquitous\nmasked language modeling (MLM) objective effectively discriminate probable from\nimprobable sequences, it is still an open question if these MLMs specify a\nprincipled probability distribution over the space of possible sequences. In\nthis paper, we interpret MLMs as energy-based sequence models and propose two\nenergy parametrizations derivable from the trained MLMs. In order to draw\nsamples correctly from these models, we develop a tractable sampling scheme\nbased on the Metropolis--Hastings Monte Carlo algorithm. In our approach,\nsamples are proposed from the same masked conditionals used for training the\nmasked language models, and they are accepted or rejected based on their energy\nvalues according to the target distribution. We validate the effectiveness of\nthe proposed parametrizations by exploring the quality of samples drawn from\nthese energy-based models for both open-ended unconditional generation and a\nconditional generation task of machine translation. We theoretically and\nempirically justify our sampling algorithm by showing that the masked\nconditionals on their own do not yield a Markov chain whose stationary\ndistribution is that of our target distribution, and our approach generates\nhigher quality samples than other recently proposed undirected generation\napproaches (Wang et al., 2019, Ghazvininejad et al., 2019).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1\">Kartik Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07967","description":"<p>We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14282","description":"<p>Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been many efforts to understand what information they contain,\nand why they seem to be universally successful. The most common approach to use\nthese representations involves fine-tuning them for an end task. Yet, how\nfine-tuning changes the underlying embedding space is less studied. In this\nwork, we study the English BERT family and use two probing techniques to\nanalyze how fine-tuning changes the space. We hypothesize that fine-tuning\naffects classification performance by increasing the distances between examples\nassociated with different labels. We confirm this hypothesis with carefully\ndesigned experiments on five different NLP tasks. Via these experiments, we\nalso discover an exception to the prevailing wisdom that \"fine-tuning always\nimproves performance\". Finally, by comparing the representations before and\nafter fine-tuning, we discover that fine-tuning does not introduce arbitrary\nchanges to representations; instead, it adjusts the representations to\ndownstream tasks while largely preserving the original spatial structure of the\ndata points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaVIQ: FAct Verification from Information-seeking Questions. (arXiv:2107.02153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02153","description":"<p>Despite significant interest in developing general purpose fact checking\nmodels, it is challenging to construct a large-scale fact verification dataset\nwith realistic real-world claims. Existing claims are either authored by\ncrowdworkers, thereby introducing subtle biases that are difficult to control\nfor, or manually verified by professional fact checkers, causing them to be\nexpensive and limited in scale. In this paper, we construct a large-scale\nchallenging fact verification dataset called FAVIQ, consisting of 188k claims\nderived from an existing corpus of ambiguous information-seeking questions. The\nambiguities in the questions enable automatically constructing true and false\nclaims that reflect user confusions (e.g., the year of the movie being filmed\nvs. being released). Claims in FAVIQ are verified to be natural, contain little\nlexical bias, and require a complete understanding of the evidence for\nverification. Our experiments show that the state-of-the-art models are far\nfrom solving our new task. Moreover, training on our data helps in professional\nfact-checking, outperforming models trained on the widely used dataset FEVER or\nin-domain data by up to 17% absolute. Altogether, our data will serve as a\nchallenging benchmark for natural language understanding and support future\nprogress in professional fact checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification. (arXiv:2108.04106v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04106","description":"<p>We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive methods (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning. (arXiv:2108.06332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06332","description":"<p>Most previous methods for text data augmentation are limited to simple tasks\nand weak baselines. We explore data augmentation on hard tasks (i.e., few-shot\nnatural language understanding) and strong baselines (i.e., pretrained models\nwith over one billion parameters). Under this setting, we reproduced a large\nnumber of previous augmentation methods and found that these methods bring\nmarginal gains at best and sometimes degrade the performance much. To address\nthis challenge, we propose a novel data augmentation method FlipDA that jointly\nuses a generative model and a classifier to generate label-flipped data.\nCentral to the idea of FlipDA is the discovery that generating label-flipped\ndata is more crucial to the performance than generating label-preserved data.\nExperiments show that FlipDA achieves a good tradeoff between effectiveness and\nrobustness -- it substantially improves many tasks while not negatively\naffecting the others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statutory Article Retrieval Dataset in French. (arXiv:2108.11792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11792","description":"<p>Statutory article retrieval is the task of automatically retrieving law\narticles relevant to a legal question. While recent advances in natural\nlanguage processing have sparked considerable interest in many legal tasks,\nstatutory article retrieval remains primarily untouched due to the scarcity of\nlarge-scale and high-quality annotated datasets. To address this bottleneck, we\nintroduce the Belgian Statutory Article Retrieval Dataset (BSARD), which\nconsists of 1,100+ French native legal questions labeled by experienced jurists\nwith relevant articles from a corpus of 22,600+ Belgian law articles. Using\nBSARD, we benchmark several state-of-the-art retrieval approaches, including\nlexical and dense architectures, both in zero-shot and supervised setups. We\nfind that fine-tuned dense retrieval models significantly outperform other\nsystems. Our best performing baseline achieves 74.8% R@100, which is promising\nfor the feasibility of the task and indicates there is still room for\nimprovement. By the specificity of the domain and addressed task, BSARD\npresents a unique challenge problem for future research on legal information\nretrieval. Our dataset and source code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Antoine Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03490","description":"<p>L1 French participants learned novel L2 English words over two days of\nlearning sessions, with half of the words presented with their orthographic\nforms (Audio-Ortho) and half without (Audio only). One group heard the words\npronounced by a single talker, while another group heard them pronounced by\nmultiple talkers. On the third day, they completed a variety of tasks to\nevaluate their learning. Our results show a robust influence of orthography,\nwith faster response times in both production (Picture naming) and recognition\n(Picture mapping) tasks for words learned in the Audio-Ortho condition.\nMoreover, formant analyses of the Picture naming responses show that\northographic input pulls pronunciations of English novel words towards a\nnon-native (French) phonological target. Words learned with their orthographic\nforms were pronounced more precisely (with smaller Dispersion Scores), but were\nmisplaced in the vowel space (as reflected by smaller Euclidian distances with\nrespect to French vowels). Finally, we found only limited evidence of an effect\nof talker-based acoustic variability: novel words learned with multiple talkers\nshowed faster responses times in the Picture naming task, but only in the\nAudio-only condition, which suggests that orthographic information may have\noverwhelmed any advantage of talker-based acoustic variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welby_P/0/1/0/all/0/1\">Pauline Welby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinelli_E/0/1/0/all/0/1\">Elsa Spinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burki_A/0/1/0/all/0/1\">Audrey B&#xfc;rki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraT5: Text-to-Text Transformers for Arabic Language Generation. (arXiv:2109.12068v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12068","description":"<p>Transfer learning with a unified Transformer framework (T5) that converts all\nlanguage problems into a text-to-text format was recently proposed as a simple\nand effective transfer learning approach. Although a multilingual version of\nthe T5 model (mT5) was also introduced, it is not clear how well it can fare on\nnon-English tasks involving diverse data. To investigate this question, we\napply mT5 on a language with a wide variety of dialects--Arabic. For\nevaluation, we introduce a novel benchmark for ARabic language GENeration\n(ARGEN), covering seven important tasks. For model comparison, we pre-train\nthree powerful Arabic T5-style models and evaluate them on ARGEN. Although\npre-trained with ~49 less data, our new models perform significantly better\nthan mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new\nSOTAs. Our models also establish new SOTA on the recently-proposed, large\nArabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al.,\n2021). Our new models are publicly available. We also link to ARGEN datasets\nthrough our repository: https://github.com/UBC-NLP/araT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. (arXiv:2109.12742v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12742","description":"<p>The few-shot natural language understanding (NLU) task has attracted much\nrecent attention. However, prior methods have been evaluated under a disparate\nset of protocols, which hinders fair comparison and measuring progress of the\nfield. To address this issue, we introduce an evaluation framework that\nimproves previous evaluation procedures in three key aspects, i.e., test\nperformance, dev-test correlation, and stability. Under this new evaluation\nframework, we re-evaluate several state-of-the-art few-shot methods for NLU\ntasks. Our framework reveals new insights: (1) both the absolute performance\nand relative gap of the methods were not accurately estimated in prior\nliterature; (2) no single method dominates most tasks with consistent\nperformance; (3) improvements of some methods diminish with a larger pretrained\nmodel; and (4) gains from different methods are often complementary and the\nbest combined model performs close to a strong fully-supervised baseline. We\nopen-source our toolkit, FewNLU, that implements our evaluation framework along\nwith a number of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chonghua Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07165","description":"<p>Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Abhishek Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07586","description":"<p>NLP practitioners often want to take existing trained models and apply them\nto data from new domains. While fine-tuning or few-shot learning can be used to\nadapt a base model, there is no single recipe for making these techniques work;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. We study how to improve a black box model's\nperformance on a new domain by leveraging explanations of the model's behavior.\nOur approach first extracts a set of features combining human intuition about\nthe task with model attributions generated by black box interpretation\ntechniques, then uses a simple calibrator, in the form of a classifier, to\npredict whether the base model was correct or not. We experiment with our\nmethod on two tasks, extractive question answering and natural language\ninference, covering adaptation from several pairs of domains with limited\ntarget-domain data. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models, boosting accuracy\nwhen predictions do not have to be returned on every example. We further show\nthat the calibration model transfers to some extent between tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Characterization for Dialogue Disentanglement. (arXiv:2110.08018v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08018","description":"<p>Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinbei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dubber: Dubbing for Videos According to Scripts. (arXiv:2110.08243v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.08243","description":"<p>Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given video\nfrom the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that\nutilizes the lip movement in the video to control the prosody of the generated\nspeech. Furthermore, an image-based speaker embedding (ISE) module is developed\nfor the multi-speaker setting, which enables Neural Dubber to generate speech\nwith a reasonable timbre according to the speaker's face. Experiments on the\nchemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show\nthat Neural Dubber can generate speech audios on par with state-of-the-art TTS\nmodels in terms of speech quality. Most importantly, both qualitative and\nquantitative evaluations show that Neural Dubber can control the prosody of\nsynthesized speech by the video, and generate high-fidelity speech temporally\nsynchronized with the video. Our project page is at\nhttps://tsinghua-mars-lab.github.io/NeuralDubber/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Chenxu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1\">Qiao Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tingle Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASPECTNEWS: Aspect-Oriented Summarization of News Documents. (arXiv:2110.08296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08296","description":"<p>Generic summaries try to cover an entire document and query-based summaries\ntry to answer document-specific questions. But real users' needs often fall in\nbetween these extremes and correspond to aspects, high-level topics discussed\namong similar types of documents. In this paper, we collect a dataset of\nrealistic aspect-oriented summaries, AspectNews, which covers different\nsubtopics about articles in news sub-domains. We annotate data across two\ndomains of articles, earthquakes and fraud investigations, where each article\nis annotated with two distinct summaries focusing on different aspects for each\ndomain. A system producing a single generic summary cannot concisely satisfy\nboth aspects. Our focus in evaluation is how well existing techniques can\ngeneralize to these domains without seeing in-domain training data, so we turn\nto techniques to construct synthetic training data that have been used in\nquery-focused summarization work. We compare several training schemes that\ndiffer in how strongly keywords are used and how oracle summaries are\nextracted. Our evaluation shows that our final approach yields (a) focused\nsummaries, better than those from a generic summarization system or from\nkeyword matching; (b) a system sensitive to the choice of keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_O/0/1/0/all/0/1\">Ojas Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horecka_K/0/1/0/all/0/1\">Kevin Horecka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamics for Text Summarization Models. (arXiv:2110.08370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08370","description":"<p>Pre-trained language models (e.g. BART) have shown impressive results when\nfine-tuned on large summarization datasets. However, little is understood about\nthis fine-tuning process, including what knowledge is retained from\npre-training time or how content selection and generation strategies are learnt\nacross iterations. In this work, we analyze the training dynamics for\ngeneration models, focusing on summarization. Across different datasets\n(CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and\nhallucination, we study what the model learns at different stages of its\nfine-tuning process. We find that a propensity to copy the input is learned\nearly in the training process consistently across all datasets studied. On the\nother hand, factual errors, such as hallucination of unsupported facts, are\nlearnt in the later stages, though this behavior is more varied across domains.\nBased on these observations, we explore complementary approaches for modifying\ntraining: first, disregarding high-loss tokens that are challenging to learn\nand second, disregarding low-loss tokens that are learnt very quickly in the\nlatter stages of the training process. We show that these simple training\nmodifications allow us to configure our model to achieve different goals, such\nas improving factuality or improving abstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages. (arXiv:2110.08415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08415","description":"<p>We show that unsupervised sequence-segmentation performance can be\ntransferred to extremely low-resource languages by pre-training a Masked\nSegmental Language Model (Downey et al., 2021) multilingually. Further, we show\nthat this transfer can be achieved by training over a collection of\nlow-resource languages that are typologically similar (but phylogenetically\nunrelated) to the target language. In our experiments, we transfer from a\ncollection of 10 Indigenous American languages (AmericasNLP, Mager et al.,\n2021) to K'iche', a Mayan language. We compare our multilingual model to a\nmonolingual (from-scratch) baseline, as well as a model pre-trained on Quechua\nonly. We show that the multilingual pre-trained approach yields consistent\nsegmentation quality across target dataset sizes, exceeding the monolingual\nbaseline in 6/10 experimental settings. Our model yields especially strong\nresults at small target sizes, including a zero-shot performance of 20.6 F1.\nThese results have promising implications for low-resource NLP pipelines\ninvolving human-like linguistic units, such as the sparse transcription\nframework proposed by Bird (2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drizin_S/0/1/0/all/0/1\">Shannon Drizin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroutunian_L/0/1/0/all/0/1\">Levon Haroutunian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thukral_S/0/1/0/all/0/1\">Shivin Thukral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Natural Language Inference Using PHL Triplet Generation. (arXiv:2110.08438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08438","description":"<p>Transformer-based models achieve impressive performance on numerous Natural\nLanguage Inference (NLI) benchmarks when trained on respective training\ndatasets. However, in certain cases, training samples may not be available or\ncollecting them could be time-consuming and resource-intensive. In this work,\nwe address the above challenge and present an explorative study on unsupervised\nNLI, a paradigm in which no human-annotated training samples are available. We\ninvestigate it under three settings: PH, P, and NPH that differ in the extent\nof unlabeled data available for learning. As a solution, we propose a\nprocedural data generation approach that leverages a set of sentence\ntransformations to collect PHL (Premise, Hypothesis, Label) triplets for\ntraining NLI models, bypassing the need for human-annotated training data.\nComprehensive experiments with several NLI datasets show that the proposed\napproach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH\nsettings respectively, outperforming all existing unsupervised baselines.\nFurthermore, fine-tuning our model with as little as ~0.1% of the\nhuman-annotated training dataset (500 instances) leads to 12.2% higher accuracy\nthan the model trained from scratch on the same 500 instances. Supported by\nthis superior performance, we conclude with a recommendation for collecting\nhigh-quality task-specific data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08484","description":"<p>Large pre-trained vision-language (VL) models can learn a new task with a\nhandful of examples and generalize to a new task without fine-tuning. However,\nthese VL models are hard to deploy for real-world applications due to their\nimpractically huge sizes and slow inference speed. To solve this limitation, we\nstudy prompt-based low-resource learning of VL tasks with our proposed method,\nFewVLM, relatively smaller than recent few-shot learners. For FewVLM, we\npre-train a sequence-to-sequence transformer model with prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we\nanalyze the effect of diverse prompts for few-shot tasks. Experimental results\non VQA show that FewVLM with prompt-based learning outperforms Frozen which is\n31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x\nlarger model, PICa. In our analysis, we observe that (1) prompts significantly\naffect zero-shot performance but marginally affect few-shot performance, (2)\nmodels with noisy prompts learn as quickly as hand-crafted prompts given larger\ntraining data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts\ncaptioning performance. Our code is publicly available at\n\\url{https://github.com/woojeongjin/FewVLM}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsClaims: A New Benchmark for Claim Detection from News with Background Knowledge. (arXiv:2112.08544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08544","description":"<p>Claim detection and verification are crucial for news understanding and have\nemerged as promising technologies for mitigating news misinformation. However,\nmost existing work has focused on {\\em claim sentence} analysis while\noverlooking crucial background attributes (e.g., claimer, claim objects). In\nthis work, we present NewsClaims, a new benchmark for knowledge-aware claim\ndetection in the news domain. We redefine the claim detection problem to\ninclude extraction of additional background attributes related to each claim\nand release 889 claims annotated over 143 news articles. NewsClaims aims to\nbenchmark claim detection systems in emerging scenarios, comprising unseen\ntopics with little or no training data. To this end, we provide a comprehensive\nevaluation of zero-shot and prompt-based baselines for NewsClaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinthakindi_S/0/1/0/all/0/1\">Sai Chinthakindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conger_K/0/1/0/all/0/1\">Kathryn S. Conger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1\">Ahmed S. Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incomplete Knowledge Graph Alignment. (arXiv:2112.09266v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09266","description":"<p>Knowledge graph (KG) alignment - the task of recognizing entities referring\nto the same thing in different KGs - is recognized as one of the most important\noperations in the field of KG construction and completion. However, existing\nalignment techniques often assume that the input KGs are complete and\nisomorphic, which is not true due to the real-world heterogeneity in the\ndomain, size, and sparsity. In this work, we address the problem of aligning\nincomplete KGs with representation learning. Our KG embedding framework\nexploits two feature channels: transitivity-based and proximity-based. The\nformer captures the consistency constraints between entities via translation\npaths, while the latter captures the neighbourhood structure of KGs via\nattention guided relation-aware graph neural network. The two feature channels\nare jointly learned to exchange important features between the input KGs while\nenforcing the output representations of the input KGs in the same embedding\nspace. Also, we develop a missing links detector that discovers and recovers\nthe missing links in the input KGs during the training process, which helps\nmitigate the incompleteness issue and thus improve the compatibility of the\nlearned representations. The embeddings then are fused to generate the\nalignment result, and the high-confidence matched node pairs are updated to the\npre-aligned supervision data to improve the embeddings gradually. Empirical\nresults show that our model is more accurate than the SOTA and is robust\nagainst different levels of incompleteness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Van Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Thanh Trung Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Viet Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_Q/0/1/0/all/0/1\">Quyet Thang Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03086","description":"<p>Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coster_M/0/1/0/all/0/1\">Mathieu De Coster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreweghe_M/0/1/0/all/0/1\">Mieke Van Herreweghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement. (arXiv:2202.06205v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2202.06205","description":"<p>Despite its benefits for children's skill development and parent-child\nbonding, many parents do not often engage in interactive storytelling by having\nstory-related dialogues with their child due to limited availability or\nchallenges in coming up with appropriate questions. While recent advances made\nAI generation of questions from stories possible, the fully-automated approach\nexcludes parent involvement, disregards educational goals, and underoptimizes\nfor child engagement. Informed by need-finding interviews and participatory\ndesign (PD) results, we developed StoryBuddy, an AI-enabled system for parents\nto create interactive storytelling experiences. StoryBuddy's design highlighted\nthe need for accommodating dynamic user needs between the desire for parent\ninvolvement and parent-child bonding and the goal of minimizing parent\nintervention when busy. The PD revealed varied assessment and educational goals\nof parents, which StoryBuddy addressed by supporting configuring question types\nand tracking child progress. A user study validated StoryBuddy's usability and\nsuggested design insights for future parent-AI collaboration systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims. (arXiv:2202.08901v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2202.08901","description":"<p>We conducted a lab-based eye-tracking study to investigate how the\ninteractivity of an AI-powered fact-checking system affects user interactions,\nsuch as dwell time, attention, and mental resources involved in using the\nsystem. A within-subject experiment was conducted, where participants used an\ninteractive and a non-interactive version of a mock AI fact-checking system and\nrated their perceived correctness of COVID-19 related claims. We collected\nweb-page interactions, eye-tracking data, and mental workload using NASA-TLX.\nWe found that the presence of the affordance of interactively manipulating the\nAI system's prediction parameters affected users' dwell times, and\neye-fixations on AOIs, but not mental workload. In the interactive system,\nparticipants spent the most time evaluating claims' correctness, followed by\nreading news. This promising result shows a positive role of interactivity in a\nmixed-initiative AI-powered system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Li Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_N/0/1/0/all/0/1\">Nilavra Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwidzka_J/0/1/0/all/0/1\">Jacek Gwidzka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.12163","description":"<p>In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, a\nsimple domain adaptation mechanism is introduced to allow adapting an existing\nlanguage identification model to a new domain where the prior language\ndistribution is different. We perform a comparative study of different model\ntopologies under different constraints of model size, and find that\nconformer-base models outperform LSTM and transformer based models. Our\nexperiments also show that attentive temporal pooling and domain adaptation\nsignificantly improve the model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13100","description":"<p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm\nfor training classifiers that generalize over output spaces. In contrast to\nstandard classification, which treats classes as discrete symbols, SemSup\nrepresents them as dense vector features obtained from descriptions of classes\n(e.g., \"The cat is a small carnivorous mammal\"). This allows the output space\nto be unbounded (in the space of descriptions) and enables models to generalize\nboth over unseen inputs and unseen outputs (e.g. \"The aardvark is a nocturnal\nburrowing mammal with long ears\"). Specifically, SemSup enables four types of\ngeneralization, to -- (1) unseen class descriptions, (2) unseen classes, (3)\nunseen super-classes, and (4) unseen tasks. Through experiments on four\nclassification datasets across two variants (multi-class and multi-label), two\ninput modalities (text and images), and two output description modalities (text\nand JSON), we show that our SemSup models significantly outperform standard\nsupervised models and existing models that leverage word embeddings over class\nnames. For instance, our model outperforms baselines by 40% and 15% precision\npoints on unseen descriptions and classes, respectively, on a news\ncategorization dataset (RCV1). SemSup can serve as a pathway for scaling neural\nmodels to large unbounded output spaces and enabling better generalization and\nmodel reuse for unseen tasks and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13363","description":"<p>In this paper, we propose a variational autoencoder with disentanglement\npriors, VAE-DPRIOR, for conditional natural language generation with none or a\nhandful of task-specific labeled examples. In order to improve compositional\ngeneralization, our model performs disentangled representation learning by\nintroducing a prior for the latent content space and another prior for the\nlatent label space. We show both empirically and theoretically that the\nconditional priors can already disentangle representations even without\nspecific regularizations as in the prior work. We can also sample diverse\ncontent representations from the content space without accessing data of the\nseen tasks, and fuse them with the representations of novel tasks for\ngenerating diverse texts in the low-resource settings. Our extensive\nexperiments demonstrate the superior performance of our model over competitive\nbaselines in terms of i) data augmentation in continuous zero/few-shot\nlearning, and ii) text style transfer in both zero/few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tianyang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13404","description":"<p>Entity linking (EL) is the task of linking entity mentions in a document to\nreferent entities in a knowledge base (KB). Many previous studies focus on\nWikipedia-derived KBs. There is little work on EL over Wikidata, even though it\nis the most extensive crowdsourced KB. The scale of Wikidata can open up many\nnew real-world applications, but its massive number of entities also makes EL\nchallenging. To effectively narrow down the search space, we propose a novel\ncandidate retrieval paradigm based on entity profiling. Wikidata entities and\ntheir textual fields are first indexed into a text search engine (e.g.,\nElasticsearch). During inference, given a mention and its context, we use a\nsequence-to-sequence (seq2seq) model to generate the profile of the target\nentity, which consists of its title and description. We use the profile to\nquery the indexed search engine to retrieve candidate entities. Our approach\ncomplements the traditional approach of using a Wikipedia anchor-text\ndictionary, enabling us to further design a highly effective hybrid method for\ncandidate retrieval. Combined with a simple cross-attention reranker, our\ncomplete EL framework achieves state-of-the-art results on three Wikidata-based\ndatasets and strong performance on TACKBP-2010.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformers use variable binding?. (arXiv:2203.00162v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.00162","description":"<p>Increasing the explainability of deep neural networks (DNNs) requires\nevaluating whether they implement symbolic computation. One central symbolic\ncapacity is variable binding: linking an input value to an abstract variable\nheld in system-internal memory. Prior work on the computational abilities of\nDNNs has not resolved the question of whether their internal processes involve\nvariable binding. We argue that the reason for this is fundamental, inherent in\nthe way experiments in prior work were designed. We provide the first\nsystematic evaluation of the variable binding capacities of the\nstate-of-the-art Transformer networks BERT and RoBERTa. Our experiments are\ndesigned such that the model must generalize a rule across disjoint subsets of\nthe input vocabulary, and cannot rely on associative pattern matching alone.\nThe results show a clear discrepancy between classification and\nsequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or\nreverse strings even when trained on task-specific vocabularies that are\nswitched in the test set; but both models completely fail to generalize across\nvocabularies in similar sequence classification tasks. These findings indicate\nthat the effectiveness of Transformers in sequence modelling may lie in their\nextensive use of the input itself as an external \"memory\" rather than\nnetwork-internal symbolic operations involving variable binding. Therefore, we\npropose a novel direction for future work: augmenting the inputs available to\ncircumvent the lack of network-internal variable binding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grondahl_T/0/1/0/all/0/1\">Tommi Gr&#xf6;ndahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speciesist Language and Nonhuman Animal Bias in English Masked Language Models. (arXiv:2203.05140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05140","description":"<p>Various existing studies have analyzed what social biases are inherited by\nNLP models. These biases may directly or indirectly harm people, therefore\nprevious studies have focused only on human attributes. If the social biases in\nNLP models can be indirectly harmful to humans involved, then the models can\nalso indirectly harm nonhuman animals. However, until recently no research on\nsocial biases in NLP regarding nonhumans existed. In this paper, we analyze\nbiases to nonhuman animals, i.e. speciesist bias, inherent in English Masked\nLanguage Models. We analyze this bias using template-based and corpus-extracted\nsentences which contain speciesist (or non-speciesist) language, to show that\nthese models tend to associate harmful words with nonhuman animals. Our code\nfor reproducing the experiments will be made available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takeshita_M/0/1/0/all/0/1\">Masashi Takeshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepka_R/0/1/0/all/0/1\">Rafal Rzepka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_K/0/1/0/all/0/1\">Kenji Araki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show the proposed VPTSL outperforms\nother state-of-the-art methods, which demonstrates the effectiveness of visual\nprompt and the text span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciNLI: A Corpus for Natural Language Inference on Scientific Text. (arXiv:2203.06728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06728","description":"<p>Existing Natural Language Inference (NLI) datasets, while being instrumental\nin the advancement of Natural Language Understanding (NLU) research, are not\nrelated to scientific text. In this paper, we introduce SciNLI, a large dataset\nfor NLI that captures the formality in scientific text and contains 107,412\nsentence pairs extracted from scholarly papers on NLP and computational\nlinguistics. Given that the text used in scientific literature differs vastly\nfrom the text used in everyday language both in terms of vocabulary and\nsentence structure, our dataset is well suited to serve as a benchmark for the\nevaluation of scientific NLU models. Our experiments show that SciNLI is harder\nto classify than the existing NLI datasets. Our best performing model with\nXLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%\nshowing that there is substantial room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadat_M/0/1/0/all/0/1\">Mobashir Sadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models. (arXiv:2203.06904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06904","description":"<p>Despite the success, the process of fine-tuning large-scale PLMs brings\nprohibitive adaptation costs. In fact, fine-tuning all the parameters of a\ncolossal model and retaining separate instances for different tasks are\npractically infeasible. This necessitates a new branch of research focusing on\nthe parameter-efficient adaptation of PLMs, dubbed as delta tuning in this\npaper. In contrast with the standard fine-tuning, delta tuning only fine-tunes\na small portion of the model parameters while keeping the rest untouched,\nlargely reducing both the computation and storage costs. Recent studies have\ndemonstrated that a series of delta tuning methods with distinct tuned\nparameter selection could achieve performance on a par with full-parameter\nfine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In\nthis paper, we first formally describe the problem of delta tuning and then\ncomprehensively review recent delta tuning approaches. We also propose a\nunified categorization criterion that divide existing delta tuning methods into\nthree groups: addition-based, specification-based, and reparameterization-based\nmethods. Though initially proposed as an efficient method to steer large\nmodels, we believe that some of the fascinating evidence discovered along with\ndelta tuning could help further reveal the mechanisms of PLMs and even deep\nneural networks. To this end, we discuss the theoretical principles underlying\nthe effectiveness of delta tuning and propose frameworks to interpret delta\ntuning from the perspective of optimization and optimal control, respectively.\nFurthermore, we provide a holistic empirical study of representative methods,\nwhere results on over 100 NLP tasks demonstrate a comprehensive performance\ncomparison of different approaches. The experimental results also cover the\nanalysis of combinatorial, scaling and transferable properties of delta tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fuchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chi-Min Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. (arXiv:2203.06947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06947","description":"<p>Recently, various multimodal networks for Visually-Rich Document\nUnderstanding(VRDU) have been proposed, showing the promotion of transformers\nby integrating visual and layout information with the text embeddings. However,\nmost existing approaches utilize the position embeddings to incorporate the\nsequence information, neglecting the noisy improper reading order obtained by\nOCR tools. In this paper, we propose a robust layout-aware multimodal network\nnamed XYLayoutLM to capture and leverage rich layout information from proper\nreading orders produced by our Augmented XY Cut. Moreover, a Dilated\nConditional Position Encoding module is proposed to deal with the input\nsequence of variable lengths, and it additionally extracts local layout\ninformation from both textual and visual modalities while generating position\nembeddings. Experiment results show that our XYLayoutLM achieves competitive\nresults on document understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhangxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Changhua Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1\">Jun Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Ming Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Enhancing crowd flow prediction in various spatial and temporal granularities. (arXiv:2203.07372v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07372","description":"<p>Thanks to the diffusion of the Internet of Things, nowadays it is possible to\nsense human mobility almost in real time using unconventional methods (e.g.,\nnumber of bikes in a bike station). Due to the diffusion of such technologies,\nthe last years have witnessed a significant growth of human mobility studies,\nmotivated by their importance in a wide range of applications, from traffic\nmanagement to public security and computational epidemiology. A mobility task\nthat is becoming prominent is crowd flow prediction, i.e., forecasting\naggregated incoming and outgoing flows in the locations of a geographic region.\nAlthough several deep learning approaches have been proposed to solve this\nproblem, their usage is limited to specific types of spatial tessellations and\ncannot provide sufficient explanations of their predictions. We propose\nCrowdNet, a solution to crowd flow prediction based on graph convolutional\nnetworks. Compared with state-of-the-art solutions, CrowdNet can be used with\nregions of irregular shapes and provide meaningful explanations of the\npredicted crowd flows. We conduct experiments on public data varying the\nspatio-temporal granularity of crowd flows to show the superiority of our model\nwith respect to existing methods, and we investigate CrowdNet's reliability to\nmissing or noisy input data. Our model is a step forward in the design of\nreliable deep learning models to predict and explain human displacements in\nurban environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardia_M/0/1/0/all/0/1\">Marco Cardia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SATr: Slice Attention with Transformer for Universal Lesion Detection. (arXiv:2203.07373v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07373","description":"<p>Universal Lesion Detection (ULD) in computed tomography plays an essential\nrole in computer-aided diagnosis. Promising ULD results have been reported by\nmulti-slice-input detection approaches which model 3D context from multiple\nadjacent CT slices, but such methods still experience difficulty in obtaining a\nglobal representation among different slices and within each individual slice\nsince they only use convolution-based fusion operations. In this paper, we\npropose a novel Slice Attention Transformer (SATr) block which can be easily\nplugged into convolution-based ULD backbones to form hybrid network structures.\nSuch newly formed hybrid backbones can better model long-distance feature\ndependency via the cascaded self-attention modules in the Transformer block\nwhile still holding a strong power of modeling local features with the\nconvolutional operations in the original backbone. Experiments with five\nstate-of-the-art methods show that the proposed SATr block can provide an\nalmost free boost to lesion detection accuracy without extra hyperparameters or\nspecial network designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There's no difference: Convolutional Neural Networks for transient detection without template subtraction. (arXiv:2203.07390v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07390","description":"<p>We present a Convolutional Neural Network (CNN) model for the separation of\nastrophysical transients from image artifacts, a task known as \"real-bogus\"\nclassification, that does not rely on Difference Image Analysis (DIA) which is\na computationally expensive process involving image matching on small spatial\nscales in large volumes of data. We explore the use of CNNs to (1) automate the\n\"real-bogus\" classification, (2) reduce the computational costs of transient\ndiscovery. We compare the efficiency of two CNNs with similar architectures,\none that uses \"image triplets\" (templates, search, and the corresponding\ndifference image) and one that adopts a similar architecture but takes as input\nthe template and search only. Without substantially changing the model\narchitecture or retuning the hyperparameters to the new input, we observe only\na small decrease in model efficiency (97% to 92% accuracy). We further\ninvestigate how the model that does not receive the difference image learns the\nrequired information from the template and search by exploring the saliency\nmaps. Our work demonstrates that (1) CNNs are excellent models for \"real-bogus\"\nclassification that rely exclusively on the imaging data and require no feature\nengineering task; (2) high-accuracy models can be built without the need to\nconstruct difference images. Since once trained, neural networks can generate\npredictions at minimal computational costs, we argue that future\nimplementations of this methodology could dramatically reduce the computational\ncosts in the detection of genuine transients in synoptic surveys like Rubin\nObservatory's Legacy Survey of Space and Time by bypassing the DIA step\nentirely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acero_Cuellar_T/0/1/0/all/0/1\">Tatiana Acero-Cuellar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianco_F/0/1/0/all/0/1\">Federica Bianco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobler_G/0/1/0/all/0/1\">Gregory Dobler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sako_M/0/1/0/all/0/1\">Masao Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Helen Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic animal pose estimators are zero-shot performers. (arXiv:2203.07436v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07436","description":"<p>Animal pose estimation is critical in applications ranging from life science\nresearch, agriculture, to veterinary medicine. Compared to human pose\nestimation, the performance of animal pose estimation is limited by the size of\navailable datasets and the generalization of a model across datasets. Typically\ndifferent keypoints are labeled regardless of whether the species are the same\nor not, leaving animal pose datasets to have disjoint or partially overlapping\nkeypoints. As a consequence, a model cannot be used as a plug-and-play solution\nacross datasets. This reality motivates us to develop panoptic animal pose\nestimation models that are able to predict keypoints defined in all datasets.\nIn this work we propose a simple yet effective way to merge differentially\nlabeled datasets to obtain the largest quadruped and lab mouse pose dataset.\nUsing a gradient masking technique, so called SuperAnimal-models are able to\npredict keypoints that are distributed across datasets and exhibit strong\nzero-shot performance. The models can be further improved by (pseudo) labeled\nfine-tuning. These models outperform ImageNet-initialized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathis_A/0/1/0/all/0/1\">Alexander Mathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathis_M/0/1/0/all/0/1\">Mackenzie Weygandt Mathis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Clustering of Roman Potsherds via Variational Autoencoders. (arXiv:2203.07437v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07437","description":"<p>In this paper we propose an artificial intelligence imaging solution to\nsupport archaeologists in the classification task of Roman commonware\npotsherds. Usually, each potsherd is represented by its sectional profile as a\ntwo dimensional black-white image and printed in archaeological books related\nto specific archaeological excavations. The partiality and handcrafted variance\nof the fragments make their matching a challenging problem: we propose to pair\nsimilar profiles via the unsupervised hierarchical clustering of non-linear\nfeatures learned in the latent space of a deep convolutional Variational\nAutoencoder (VAE) network. Our contribution also include the creation of a\nROman COmmonware POTtery (ROCOPOT) database, with more than 4000 potsherds\nprofiles extracted from 25 Roman pottery corpora, and a MATLAB GUI software for\nthe easy inspection of shape similarities. Results are commented both from a\nmathematical and archaeological perspective so as to unlock new research\ndirections in both communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_S/0/1/0/all/0/1\">Simone Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leone_N/0/1/0/all/0/1\">Ninetta Leone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launaro_A/0/1/0/all/0/1\">Alessandro Launaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep learning pipeline for breast cancer ki-67 proliferation index scoring. (arXiv:2203.07452v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07452","description":"<p>The Ki-67 proliferation index is an essential biomarker that helps\npathologists to diagnose and select appropriate treatments. However, automatic\nevaluation of Ki-67 is difficult due to nuclei overlapping and complex\nvariations in their properties. This paper proposes an integrated pipeline for\naccurate automatic counting of Ki-67, where the impact of nuclei separation\ntechniques is highlighted. First, semantic segmentation is performed by\ncombining the Squeez and Excitation Resnet and Unet algorithms to extract\nnuclei from the background. The extracted nuclei are then divided into\noverlapped and non-overlapped regions based on eight geometric and statistical\nfeatures. A marker-based Watershed algorithm is subsequently proposed and\napplied only to the overlapped regions to separate nuclei. Finally, deep\nfeatures are extracted from each nucleus patch using Resnet18 and classified\ninto positive or negative by a random forest classifier. The proposed\npipeline's performance is validated on a dataset from the Department of\nPathology at H\\^opital Nord Franche-Comt\\'e hospital.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Benaggoune_K/0/1/0/all/0/1\">Khaled Benaggoune</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masry_Z/0/1/0/all/0/1\">Zeina Al Masry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Devalland_C/0/1/0/all/0/1\">Christine Devalland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mouss_L/0/1/0/all/0/1\">L.H Mouss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zerhouni_N/0/1/0/all/0/1\">Noureddine Zerhouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skydiver: A Spiking Neural Network Accelerator Exploiting Spatio-Temporal Workload Balance. (arXiv:2203.07516v1 [cs.AR])","link":"http://arxiv.org/abs/2203.07516","description":"<p>Spiking Neural Networks (SNNs) are developed as a promising alternative to\nArtificial Neural networks (ANNs) due to their more realistic brain-inspired\ncomputing models. SNNs have sparse neuron firing over time, i.e.,\nspatio-temporal sparsity; thus, they are useful to enable energy-efficient\nhardware inference. However, exploiting spatio-temporal sparsity of SNNs in\nhardware leads to unpredictable and unbalanced workloads, degrading the energy\nefficiency. In this work, we propose an FPGA-based convolutional SNN\naccelerator called Skydiver that exploits spatio-temporal workload balance. We\npropose the Approximate Proportional Relation Construction (APRC) method that\ncan predict the relative workload channel-wisely and a Channel-Balanced\nWorkload Schedule (CBWS) method to increase the hardware workload balance ratio\nto over 90%. Skydiver was implemented on a Xilinx XC7Z045 FPGA and verified on\nimage segmentation and MNIST classification tasks. Results show improved\nthroughput by 1.4X and 1.2X for the two tasks. Skydiver achieved 22.6 KFPS\nthroughput, and 42.4 uJ/Image prediction energy on the classification task with\n98.5% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xinyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_H/0/1/0/all/0/1\">Haitao Luan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Active Monocular Distance Estimation from Time-to-Contact. (arXiv:2203.07530v1 [cs.RO])","link":"http://arxiv.org/abs/2203.07530","description":"<p>Distance estimation is fundamental for a variety of robotic applications\nincluding navigation, manipulation and planning. Inspired by the mammal's\nvisual system, which gazes at specific objects (active fixation), and estimates\nwhen the object will reach it (time-to-contact), we develop a novel constraint\nbetween time-to-contact, acceleration, and distance that we call the\n$\\tau$-constraint. It allows an active monocular camera to estimate depth using\ntime-to-contact and inertial measurements (linear accelerations and angular\nvelocities) within a window of time.\n</p>\n<p>Our work differs from other approaches by focusing on patches instead of\nfeature points. This is, because the change in the patch area determines the\ntime-to-contact directly. The result enables efficient estimation of distance\nwhile using only a small portion of the image, leading to a large speedup.\n</p>\n<p>We successfully validate the proposed $\\tau$-constraint in the application of\nestimating camera position with a monocular grayscale camera and an Inertial\nMeasurement Unit (IMU). Specifically, we test our method on different\nreal-world planar objects over trajectories 8-40 seconds in duration and 7-35\nmeters long. Our method achieves 8.5 cm Average Trajectory Error (ATE) while\nthe popular Visual-Inertial Odometry methods VINS-Mono and ROVIO achieve 12.2\nand 16.9 cm ATE respectively. Additionally, our implementation runs 27$\\times$\nfaster than VINS-Mono's and 6.8$\\times$ faster than ROVIO's. We believe these\nresults indicate the $\\tau$-constraints potential to be the basis of robust,\nsophisticated algorithms for a multitude of applications involving an active\ncamera and an IMU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burner_L/0/1/0/all/0/1\">Levi Burner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanket_N/0/1/0/all/0/1\">Nitin J. Sanket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPFusion: Joint 3D Volume and Pixel-Aligned Feature Fusion for Single and Multi-view 3D Reconstruction. (arXiv:2203.07553v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07553","description":"<p>We introduce a unified single and multi-view neural implicit 3D\nreconstruction framework VPFusion. VPFusion~attains high-quality reconstruction\nusing both - 3D feature volume to capture 3D-structure-aware context, and\npixel-aligned image features to capture fine local detail. Existing approaches\nuse RNN, feature pooling, or attention computed independently in each view for\nmulti-view fusion. RNNs suffer from long-term memory loss and permutation\nvariance, while feature pooling or independently computed attention leads to\nrepresentation in each view being unaware of other views before the final\npooling step. In contrast, we show improved multi-view feature fusion by\nestablishing transformer-based pairwise view association. In particular, we\npropose a novel interleaved 3D reasoning and pairwise view association\narchitecture for feature volume fusion across different views. Using this\nstructure-aware and multi-view-aware feature volume, we show improved 3D\nreconstruction performance compared to existing methods. VPFusion improves the\nreconstruction quality further by also incorporating pixel-aligned local image\nfeatures to capture fine detail. We verify the effectiveness of VPFusion~on the\nShapeNet and ModelNet datasets, where we outperform or perform on-par the\nstate-of-the-art single and multi-view 3D shape reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1\">Jisan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frahm_J/0/1/0/all/0/1\">Jan-Michael Frahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Agnostic Robust Representation Learning. (arXiv:2203.07596v1 [cs.LG])","link":"http://arxiv.org/abs/2203.07596","description":"<p>It has been reported that deep learning models are extremely vulnerable to\nsmall but intentionally chosen perturbations of its input. In particular, a\ndeep network, despite its near-optimal accuracy on the clean images, often\nmis-classifies an image with a worst-case but humanly imperceptible\nperturbation (so-called adversarial examples). To tackle this problem, a great\namount of research has been done to study the training procedure of a network\nto improve its robustness. However, most of the research so far has focused on\nthe case of supervised learning. With the increasing popularity of\nself-supervised learning methods, it is also important to study and improve the\nrobustness of their resulting representation on the downstream tasks. In this\npaper, we study the problem of robust representation learning with unlabeled\ndata in a task-agnostic manner. Specifically, we first derive an upper bound on\nthe adversarial loss of a prediction model (which is based on the learned\nrepresentation) on any downstream task, using its loss on the clean data and a\nrobustness regularizer. Moreover, the regularizer is task-independent, thus we\npropose to minimize it directly during the representation learning phase to\nmake the downstream prediction model more robust. Extensive experiments show\nthat our method achieves preferable adversarial performance compared to\nrelevant baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">A. Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARETS: A Consistency And Robustness Evaluative Test Suite for VQA. (arXiv:2203.07613v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07613","description":"<p>We introduce CARETS, a systematic test suite to measure consistency and\nrobustness of modern VQA models through a series of six fine-grained capability\ntests. In contrast to existing VQA test sets, CARETS features balanced question\ngeneration to create pairs of instances to test models, with each pair focusing\non a specific capability such as rephrasing, logical symmetry or image\nobfuscation. We evaluate six modern VQA systems on CARETS and identify several\nactionable weaknesses in model comprehension, especially with concepts such as\nnegation, disjunction, or hypernym invariance. Interestingly, even the most\nsophisticated models are sensitive to aspects such as swapping the order of\nterms in a conjunction or varying the number of answer choices mentioned in the\nquestion. We release CARETS to be used as an extensible tool for evaluating\nmulti-modal model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_C/0/1/0/all/0/1\">Carlos E. Jimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning What Not to Segment: A New Perspective on Few-Shot Segmentation. (arXiv:2203.07615v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07615","description":"<p>Recently few-shot segmentation (FSS) has been extensively developed. Most\nprevious works strive to achieve generalization through the meta-learning\nframework derived from classification tasks; however, the trained models are\nbiased towards the seen classes instead of being ideally class-agnostic, thus\nhindering the recognition of new concepts. This paper proposes a fresh and\nstraightforward insight to alleviate the problem. Specifically, we apply an\nadditional branch (base learner) to the conventional FSS model (meta learner)\nto explicitly identify the targets of base classes, i.e., the regions that do\nnot need to be segmented. Then, the coarse results output by these two learners\nin parallel are adaptively integrated to yield precise segmentation prediction.\nConsidering the sensitivity of meta learner, we further introduce an adjustment\nfactor to estimate the scene differences between the input image pairs for\nfacilitating the model ensemble forecasting. The substantial performance gains\non PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our\nversatile scheme sets a new state-of-the-art even with two plain learners.\nMoreover, in light of the unique nature of the proposed approach, we also\nextend it to a more realistic but challenging setting, i.e., generalized FSS,\nwhere the pixels of both base and novel classes are required to be determined.\nThe source code is available at github.com/chunbolang/BAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_B/0/1/0/all/0/1\">Binfei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation. (arXiv:2203.07628v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07628","description":"<p>This paper introduces a novel Pre-trained Spatial Temporal Many-to-One\n(P-STMO) model for 2D-to-3D human pose estimation task. To reduce the\ndifficulty of capturing spatial and temporal information, we divide this task\ninto two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I,\na self-supervised pre-training sub-task, termed masked pose modeling, is\nproposed. The human joints in the input sequence are randomly masked in both\nspatial and temporal domains. A general form of denoising auto-encoder is\nexploited to recover the original 2D poses and the encoder is capable of\ncapturing spatial and temporal dependencies in this way. In Stage II, the\npre-trained encoder is loaded to STMO model and fine-tuned. The encoder is\nfollowed by a many-to-one frame aggregator to predict the 3D pose in the\ncurrent frame. Especially, an MLP block is utilized as the spatial feature\nextractor in STMO, which yields better performance than other methods. In\naddition, a temporal downsampling strategy is proposed to diminish data\nredundancy. Extensive experiments on two benchmarks show that our method\noutperforms state-of-the-art methods with fewer parameters and less\ncomputational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on\nHuman3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings\na 1.5-7.1 times speedup to state-of-the-art methods. Code is available at\nhttps://github.com/paTRICK-swk/P-STMO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenkang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness. (arXiv:2203.07653v1 [cs.CL])","link":"http://arxiv.org/abs/2203.07653","description":"<p>Data modification, either via additional training datasets, data\naugmentation, debiasing, and dataset filtering, has been proposed as an\neffective solution for generalizing to out-of-domain (OOD) inputs, in both\nnatural language processing and computer vision literature. However, the effect\nof data modification on adversarial robustness remains unclear. In this work,\nwe conduct a comprehensive study of common data modification strategies and\nevaluate not only their in-domain and OOD performance, but also their\nadversarial robustness (AR). We also present results on a two-dimensional\nsynthetic dataset to visualize the effect of each method on the training\ndistribution. This work serves as an empirical study towards understanding the\nrelationship between generalizing to unseen domains and defending against\nadversarial perturbations. Our findings suggest that more data (either via\nadditional datasets or data augmentation) benefits both OOD accuracy and AR.\nHowever, data filtering (previously shown to improve OOD accuracy on natural\nlanguage inference) hurts OOD accuracy on other tasks such as question\nanswering and image classification. We provide insights from our experiments to\ninform future work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1\">Bhavdeep Singh Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning. (arXiv:2203.07656v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07656","description":"<p>Previous few-shot learning (FSL) works mostly are limited to natural images\nof general concepts and categories. These works assume very high visual\nsimilarity between the source and target classes. In contrast, the recently\nproposed cross-domain few-shot learning (CD-FSL) aims at transferring knowledge\nfrom general nature images of many labeled examples to novel domain-specific\ntarget categories of only a few labeled examples. The key challenge of CD-FSL\nlies in the huge data shift between source and target domains, which is\ntypically in the form of totally different visual styles. This makes it very\nnontrivial to directly extend the classical FSL methods to address the CD-FSL\ntask. To this end, this paper studies the problem of CD-FSL by spanning the\nstyle distributions of the source dataset. Particularly, wavelet transform is\nintroduced to enable the decomposition of visual representations into\nlow-frequency components such as shape and style and high-frequency components\ne.g., texture. To make our model robust to visual styles, the source images are\naugmented by swapping the styles of their low-frequency components with each\nother. We propose a novel Style Augmentation (StyleAug) module to implement\nthis idea. Furthermore, we present a Self-Supervised Learning (SSL) module to\nensure the predictions of style-augmented images are semantically similar to\nthe unchanged ones. This avoids the potential semantic drift problem in\nexchanging the styles. Extensive experiments on two CD-FSL benchmarks show the\neffectiveness of our method. Our codes and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Radiance Projection. (arXiv:2203.07658v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07658","description":"<p>The proposed method, Neural Radiance Projection (NeRP), addresses the three\nmost fundamental shortages of training such a convolutional neural network on\nX-ray image segmentation: dealing with missing/limited human-annotated\ndatasets; ambiguity on the per-pixel label; and the imbalance across positive-\nand negative- classes distribution. By harnessing a generative adversarial\nnetwork, we can synthesize a massive amount of physics-based X-ray images,\nso-called Variationally Reconstructed Radiographs (VRRs), alongside their\nsegmentation from more accurate labeled 3D Computed Tomography data. As a\nresult, VRRs present more faithfully than other projection methods in terms of\nphoto-realistic metrics. Adding outputs from NeRP also surpasses the vanilla\nUNet models trained on the same pairs of X-ray images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huy_P/0/1/0/all/0/1\">Pham Ngoc Huy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_T/0/1/0/all/0/1\">Tran Minh Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Molecular Subtypes Prediction on Pathological Images with Discriminative Patch Selecting and Multi-Instance Learning. (arXiv:2203.07659v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07659","description":"<p>Molecular subtypes of breast cancer are important references to personalized\nclinical treatment. For cost and labor savings, only one of the patient's\nparaffin blocks is usually selected for subsequent immunohistochemistry (IHC)\nto obtain molecular subtypes. Inevitable sampling error is risky due to tumor\nheterogeneity and could result in a delay in treatment. Molecular subtype\nprediction from conventional H&amp;E pathological whole slide images (WSI) using AI\nmethod is useful and critical to assist pathologists pre-screen proper paraffin\nblock for IHC. It's a challenging task since only WSI level labels of molecular\nsubtypes can be obtained from IHC. Gigapixel WSIs are divided into a huge\nnumber of patches to be computationally feasible for deep learning. While with\ncoarse slide-level labels, patch-based methods may suffer from abundant noise\npatches, such as folds, overstained regions, or non-tumor tissues. A weakly\nsupervised learning framework based on discriminative patch selecting and\nmulti-instance learning was proposed for breast cancer molecular subtype\nprediction from H&amp;E WSIs. Firstly, co-teaching strategy was adopted to learn\nmolecular subtype representations and filter out noise patches. Then, a\nbalanced sampling strategy was used to handle the imbalance in subtypes in the\ndataset. In addition, a noise patch filtering algorithm that used local outlier\nfactor based on cluster centers was proposed to further select discriminative\npatches. Finally, a loss function integrating patch with slide constraint\ninformation was used to finetune MIL framework on obtained discriminative\npatches and further improve the performance of molecular subtyping. The\nexperimental results confirmed the effectiveness of the proposed method and our\nmodels outperformed even senior pathologists, with potential to assist\npathologists to pre-screen paraffin blocks for IHC in clinic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Wen-Dong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_Z/0/1/0/all/0/1\">Zi-Hao Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiang-Dong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hai-Yan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Ke-Wen Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jia-Lin Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jia-Rui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_L/0/1/0/all/0/1\">Li-Lan Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_H/0/1/0/all/0/1\">Hui-Min Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Hui-Juan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuan-Song Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yue-Liang Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07662","description":"<p>In object detection, false negatives arise when a detector fails to detect a\ntarget object. To understand why object detectors produce false negatives, we\nidentify five 'false negative mechanisms', where each mechanism describes how a\nspecific component inside the detector architecture failed. Focusing on\ntwo-stage and one-stage anchor-box object detector architectures, we introduce\na framework for quantifying these false negative mechanisms. Using this\nframework, we investigate why Faster R-CNN and RetinaNet fail to detect objects\nin benchmark vision datasets and robotics datasets. We show that a detector's\nfalse negative mechanisms differ significantly between computer vision\nbenchmark datasets and robotics deployment scenarios. This has implications for\nthe translation of object detectors developed for benchmark datasets to\nrobotics applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">Dimity Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_M/0/1/0/all/0/1\">Mark Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildie_M/0/1/0/all/0/1\">Matt Wildie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurdak_R/0/1/0/all/0/1\">Raja Jurdak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can you even tell left from right? Presenting a new challenge for VQA. (arXiv:2203.07664v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07664","description":"<p>Visual Question Answering (VQA) needs a means of evaluating the strengths and\nweaknesses of models. One aspect of such an evaluation is the evaluation of\ncompositional generalisation, or the ability of a model to answer well on\nscenes whose scene-setups are different from the training set. Therefore, for\nthis purpose, we need datasets whose train and test sets differ significantly\nin composition. In this work, we present several quantitative measures of\ncompositional separation and find that popular datasets for VQA are not good\nevaluators. To solve this, we present Uncommon Objects in Unseen Configurations\n(UOUC), a synthetic dataset for VQA. UOUC is at once fairly complex while also\nbeing well-separated, compositionally. The object-class of UOUC consists of 380\nclasess taken from 528 characters from the Dungeons and Dragons game. The train\nset of UOUC consists of 200,000 scenes; whereas the test set consists of 30,000\nscenes. In order to study compositional generalisation, simple reasoning and\nmemorisation, each scene of UOUC is annotated with up to 10 novel questions.\nThese deal with spatial relationships, hypothetical changes to scenes,\ncounting, comparison, memorisation and memory-based reasoning. In total, UOUC\npresents over 2 million questions. UOUC also finds itself as a strong challenge\nto well-performing models for VQA. Our evaluation of recent models for VQA\nshows poor compositional generalisation, and comparatively lower ability\ntowards simple reasoning. These results suggest that UOUC could lead to\nadvances in research by being a strong benchmark for VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1\">Sai Raam Venkatraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rishi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S. Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorugunti_C/0/1/0/all/0/1\">Chandra Sekhar Vorugunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_R/0/1/0/all/0/1\">R. Raghunatha Sarma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SATS: Self-Attention Transfer for Continual Semantic Segmentation. (arXiv:2203.07667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07667","description":"<p>Continually learning to segment more and more types of image regions is a\ndesired capability for many intelligent systems. However, such continual\nsemantic segmentation suffers from the same catastrophic forgetting issue as in\ncontinual classification learning. While multiple knowledge distillation\nstrategies originally for continual classification have been well adapted to\ncontinual semantic segmentation, they only consider transferring old knowledge\nbased on the outputs from one or more layers of deep fully convolutional\nnetworks. Different from existing solutions, this study proposes to transfer a\nnew type of information relevant to knowledge, i.e. the relationships between\nelements (Eg. pixels or small local regions) within each image which can\ncapture both within-class and between-class knowledge. The relationship\ninformation can be effectively obtained from the self-attention maps in a\nTransformer-style segmentation model. Considering that pixels belonging to the\nsame class in each image often share similar visual properties, a\nclass-specific region pooling is applied to provide more efficient relationship\ninformation for knowledge transfer. Extensive evaluations on multiple public\nbenchmarks support that the proposed self-attention transfer method can further\neffectively alleviate the catastrophic forgetting issue, and its flexible\ncombination with one or more widely adopted strategies significantly\noutperforms state-of-the-art solu\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yiqiao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yixing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhuohao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanchong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaobin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weishi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07669","description":"<p>In this paper, we propose a new query-based detection framework for crowd\ndetection. Previous query-based detectors suffer from two drawbacks: first,\nmultiple predictions will be inferred for a single object, typically in crowded\nscenes; second, the performance saturates as the depth of the decoding stage\nincreases. Benefiting from the nature of the one-to-one label assignment rule,\nwe propose a progressive predicting method to address the above issues.\nSpecifically, we first select accepted queries prone to generate true positive\npredictions, then refine the rest noisy queries according to the previously\naccepted predictions. Experiments show that our method can significantly boost\nthe performance of query-based detectors in crowded scenes. Equipped with our\napproach, Sparse RCNN achieves 92.0\\% $\\text{AP}$, 41.4\\% $\\text{MR}^{-2}$ and\n83.2\\% $\\text{JI}$ on the challenging CrowdHuman \\cite{shao2018crowdhuman}\ndataset, outperforming the box-based method MIP \\cite{chu2020detection} that\nspecifies in handling crowded scenarios. Moreover, the proposed method, robust\nto crowdedness, can still obtain consistent improvements on moderately and\nslightly crowded datasets like CityPersons \\cite{zhang2017citypersons} and COCO\n\\cite{lin2014microsoft}. Code will be made publicly available at\nhttps://github.com/megvii-model/Iter-E2EDET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Anlin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Deep Image Dehazing Using Contrastive Disentanglement Learning. (arXiv:2203.07677v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07677","description":"<p>We present an effective unpaired learning based image dehazing network from\nan unpaired set of clear and hazy images. This paper provides a new perspective\nto treat image dehazing as a two-class separated factor disentanglement task,\ni.e, the task-relevant factor of clear image reconstruction and the\ntask-irrelevant factor of haze-relevant distribution. To achieve the\ndisentanglement of these two-class factors in deep feature space, contrastive\nlearning is introduced into a CycleGAN framework to learn disentangled\nrepresentations by guiding the generated images to be associated with latent\nfactors. With such formulation, the proposed contrastive disentangled dehazing\nmethod (CDD-GAN) first develops negative generators to cooperate with the\nencoder network to update alternately, so as to produce a queue of challenging\nnegative adversaries. Then these negative adversaries are trained end-to-end\ntogether with the backbone representation network to enhance the discriminative\ninformation and promote factor disentanglement performance by maximizing the\nadversarial contrastive loss. During the training, we further show that hard\nnegative examples can suppress the task-irrelevant factors and unpaired clear\nexemples can enhance the task-relevant factors, in order to better facilitate\nhaze removal and help image restoration. Extensive experiments on both\nsynthetic and real-world datasets demonstrate that our method performs\nfavorably against existing state-of-the-art unpaired dehazing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1\">Zhentao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Pengpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution. (arXiv:2203.07682v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07682","description":"<p>Recent vision transformers along with self-attention have achieved promising\nresults on various computer vision tasks. In particular, a pure\ntransformer-based image restoration architecture surpasses the existing\nCNN-based methods using multi-task pre-training with a large number of\ntrainable parameters. In this paper, we introduce an effective hybrid\narchitecture for super-resolution (SR) tasks, which leverages local features\nfrom CNNs and long-range dependencies captured by transformers to further\nimprove the SR results. Specifically, our architecture comprises of transformer\nand convolution branches, and we substantially elevate the performance by\nmutually fusing two branches to complement each representation. Furthermore, we\npropose a cross-scale token attention module, which allows the transformer to\nefficiently exploit the informative relationships among tokens across different\nscales. Our proposed method achieves state-of-the-art SR results on numerous\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jinsu Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae Hyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsCon:Instance Consistency Feature Representation via Self-Supervised Learning. (arXiv:2203.07688v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07688","description":"<p>Feature representation via self-supervised learning has reached remarkable\nsuccess in image-level contrastive learning, which brings impressive\nperformances on image classification tasks. While image-level feature\nrepresentation mainly focuses on contrastive learning in single instance, it\nignores the objective differences between pretext and downstream prediction\ntasks such as object detection and instance segmentation. In order to fully\nunleash the power of feature representation on downstream prediction tasks, we\npropose a new end-to-end self-supervised framework called InsCon, which is\ndevoted to capturing multi-instance information and extracting cell-instance\nfeatures for object recognition and localization. On the one hand, InsCon\nbuilds a targeted learning paradigm that applies multi-instance images as\ninput, aligning the learned feature between corresponding instance views, which\nmakes it more appropriate for multi-instance recognition tasks. On the other\nhand, InsCon introduces the pull and push of cell-instance, which utilizes cell\nconsistency to enhance fine-grained feature representation for precise boundary\nlocalization. As a result, InsCon learns multi-instance consistency on semantic\nfeature representation and cell-instance consistency on spatial feature\nrepresentation. Experiments demonstrate the method we proposed surpasses MoCo\nv2 by 1.1% AP^{bb} on COCO object detection and 1.0% AP^{mk} on COCO instance\nsegmentation using Mask R-CNN R50-FPN network structure with 90k iterations,\n2.1% APbb on PASCAL VOC objection detection using Faster R-CNN R50-C4 network\nstructure with 24k iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaolin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junfeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07694","description":"<p>Establishing a correspondence between two non-rigidly deforming shapes is one\nof the most fundamental problems in visual computing. Existing methods often\nshow weak resilience when presented with challenges innate to real-world data\nsuch as noise, outliers, self-occlusion etc. On the other hand, auto-decoders\nhave demonstrated strong expressive power in learning geometrically meaningful\nlatent embeddings. However, their use in \\emph{shape analysis} and especially\nin non-rigid shape correspondence has been limited. In this paper, we introduce\nan approach based on auto-decoder framework, that learns a continuous\nshape-wise deformation field over a fixed template. By supervising the\ndeformation field for points on-surface and regularising for points off-surface\nthrough a novel \\emph{Signed Distance Regularisation} (SDR), we learn an\nalignment between the template and shape \\emph{volumes}. Unlike classical\ncorrespondence techniques, our method is remarkably robust in the presence of\nstrong artefacts and can be generalised to arbitrary shape categories. Trained\non clean water-tight meshes, \\emph{without} any data-augmentation, we\ndemonstrate compelling performance on compromised data and real-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_R/0/1/0/all/0/1\">Ramana Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_G/0/1/0/all/0/1\">Gautam Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07697","description":"<p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaochao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APRNet: Attention-based Pixel-wise Rendering Network for Photo-Realistic Text Image Generation. (arXiv:2203.07705v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07705","description":"<p>Style-guided text image generation tries to synthesize text image by\nimitating reference image's appearance while keeping text content unaltered.\nThe text image appearance includes many aspects. In this paper, we focus on\ntransferring style image's background and foreground color patterns to the\ncontent image to generate photo-realistic text image. To achieve this goal, we\npropose 1) a content-style cross attention based pixel sampling approach to\nroughly mimicking the style text image's background; 2) a pixel-wise style\nmodulation technique to transfer varying color patterns of the style image to\nthe content image spatial-adaptively; 3) a cross attention based multi-scale\nstyle fusion approach to solving text foreground misalignment issue between\nstyle and content images; 4) an image patch shuffling strategy to create style,\ncontent and ground truth image tuples for training. Experimental results on\nChinese handwriting text image synthesis with SCUT-HCCDoc and CASIA-OLHWDB\ndatasets demonstrate that the proposed method can improve the quality of\nsynthetic text images and make them more photo-realistic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haisong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActFormer: A GAN Transformer Framework towards General Action-Conditioned 3D Human Motion Generation. (arXiv:2203.07706v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07706","description":"<p>We present a GAN Transformer framework for general action-conditioned 3D\nhuman motion generation, including not only single-person actions but also\nmulti-person interactive actions. Our approach consists of a powerful\nAction-conditioned motion transFormer (ActFormer) under a GAN training scheme,\nequipped with a Gaussian Process latent prior. Such a design combines the\nstrong spatio-temporal representation capacity of Transformer, superiority in\ngenerative modeling of GAN, and inherent temporal correlations from latent\nprior. Furthermore, ActFormer can be naturally extended to multi-person motions\nby alternately modeling temporal correlations and human interactions with\nTransformer encoders. We validate our approach by comparison with other methods\non larger-scale benchmarks, including NTU RGB+D 120 and BABEL. We also\nintroduce a new synthetic dataset of complex multi-person combat behaviors to\nfacilitate research on multi-person motion generation. Our method demonstrates\nadaptability to various human motion representations and achieves leading\nperformance over SOTA methods on both single-person and multi-person motion\ngeneration tasks, indicating a hopeful step towards a universal human motion\ngenerator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Ziyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhicheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenjing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images. (arXiv:2203.07707v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07707","description":"<p>This work presents a novel self-supervised pre-training method to learn\nefficient representations without labels on histopathology medical images\nutilizing magnification factors. Other state-of-theart works mainly focus on\nfully supervised learning approaches that rely heavily on human annotations.\nHowever, the scarcity of labeled and unlabeled data is a long-standing\nchallenge in histopathology. Currently, representation learning without labels\nremains unexplored for the histopathology domain. The proposed method,\nMagnification Prior Contrastive Similarity (MPCS), enables self-supervised\nlearning of representations without labels on small-scale breast cancer dataset\nBreakHis by exploiting magnification factor, inductive transfer, and reducing\nhuman prior. The proposed method matches fully supervised learning\nstate-of-the-art performance in malignancy classification when only 20% of\nlabels are used in fine-tuning and outperform previous works in fully\nsupervised learning settings. It formulates a hypothesis and provides empirical\nevidence to support that reducing human-prior leads to efficient representation\nlearning in self-supervision. The implementation of this work is available\nonline on GitHub -\nhttps://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chhipa_P/0/1/0/all/0/1\">Prakash Chandra Chhipa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_R/0/1/0/all/0/1\">Richa Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pihlgren_G/0/1/0/all/0/1\">Gustav Grund Pihlgren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saini_R/0/1/0/all/0/1\">Rajkumar Saini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revitalize Region Feature for Democratizing Video-Language Pre-training. (arXiv:2203.07720v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07720","description":"<p>Recent dominant methods for video-language pre-training (VLP) learn\ntransferable representations from the raw pixels in an end-to-end manner to\nachieve advanced performance on downstream video-language tasks. Despite the\nimpressive results, VLP research becomes extremely expensive with the need for\nmassive data and a long training time, preventing further explorations. In this\nwork, we revitalize region features of sparsely sampled video clips to\nsignificantly reduce both spatial and temporal visual redundancy towards\ndemocratizing VLP research at the same time achieving state-of-the-art results.\nSpecifically, to fully explore the potential of region features, we introduce a\nnovel bidirectional region-word alignment regularization that properly\noptimizes the fine-grained relations between regions and certain words in\nsentences, eliminating the domain/modality disconnections between pre-extracted\nregion features and text. Extensive results of downstream text-to-video\nretrieval and video question answering tasks on seven datasets demonstrate the\nsuperiority of our method on both effectiveness and efficiency, e.g., our\nmethod achieves competing results with 80\\% fewer data and 85\\% less\npre-training time compared to the most efficient VLP method so far. The code\nwill be available at \\url{https://github.com/CuthbertCai/DemoVLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07724","description":"<p>Contemporary deep-learning object detection methods for autonomous driving\nusually assume prefixed categories of common traffic participants, such as\npedestrians and cars. Most existing detectors are unable to detect uncommon\nobjects and corner cases (e.g., a dog crossing a street), which may lead to\nsevere accidents in some situations, making the timeline for the real-world\napplication of reliable autonomous driving uncertain. One main reason that\nimpedes the development of truly reliably self-driving systems is the lack of\npublic datasets for evaluating the performance of object detectors on corner\ncases. Hence, we introduce a challenging dataset named CODA that exposes this\ncritical problem of vision-based detectors. The dataset consists of 1500\ncarefully selected real-world driving scenes, each containing four object-level\ncorner cases (on average), spanning 30+ object categories. On CODA, the\nperformance of standard object detectors trained on large-scale autonomous\ndriving datasets significantly drops to no more than 12.8% in mAR. Moreover, we\nexperiment with the state-of-the-art open-world object detector and find that\nit also fails to reliably identify the novel objects in CODA, suggesting that a\nrobust perception system for autonomous driving is probably still far from\nreach. We expect our CODA dataset to facilitate further research in reliable\ndetection for real-world autonomous driving. Our dataset will be released at\nhttps://coda-dataset.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaican Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Ordinal Regression Forest for Medical Image Classification with Ordinal Labels. (arXiv:2203.07725v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07725","description":"<p>The performance of medical image classification has been enhanced by deep\nconvolutional neural networks (CNNs), which are typically trained with\ncross-entropy (CE) loss. However, when the label presents an intrinsic ordinal\nproperty in nature, e.g., the development from benign to malignant tumor, CE\nloss cannot take into account such ordinal information to allow for better\ngeneralization. To improve model generalization with ordinal information, we\npropose a novel meta ordinal regression forest (MORF) method for medical image\nclassification with ordinal labels, which learns the ordinal relationship\nthrough the combination of convolutional neural network and differential forest\nin a meta-learning framework. The merits of the proposed MORF come from the\nfollowing two components: a tree-wise weighting net (TWW-Net) and a grouped\nfeature selection (GFS) module. First, the TWW-Net assigns each tree in the\nforest with a specific weight that is mapped from the classification loss of\nthe corresponding tree. Hence, all the trees possess varying weights, which is\nhelpful for alleviating the tree-wise prediction variance. Second, the GFS\nmodule enables a dynamic forest rather than a fixed one that was previously\nused, allowing for random feature perturbation. During training, we\nalternatively optimize the parameters of the CNN backbone and TWW-Net in the\nmeta-learning framework through calculating the Hessian matrix. Experimental\nresults on two medical image classification datasets with ordinal labels, i.e.,\nLIDC-IDRI and Breast Ultrasound Dataset, demonstrate the superior performances\nof our MORF method over existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yiming Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haiping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Securing the Classification of COVID-19 in Chest X-ray Images: A Privacy-Preserving Deep Learning Approach. (arXiv:2203.07728v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07728","description":"<p>Deep learning (DL) is being increasingly utilized in healthcare-related\nfields due to its outstanding efficiency. However, we have to keep the\nindividual health data used by DL models private and secure. Protecting data\nand preserving the privacy of individuals has become an increasingly prevalent\nissue. The gap between the DL and privacy communities must be bridged. In this\npaper, we propose privacy-preserving deep learning (PPDL)-based approach to\nsecure the classification of Chest X-ray images. This study aims to use Chest\nX-ray images to their fullest potential without compromising the privacy of the\ndata that it contains. The proposed approach is based on two steps: encrypting\nthe dataset using partially homomorphic encryption and training/testing the DL\nalgorithm over the encrypted images. Experimental results on the COVID-19\nRadiography database show that the MobileNetV2 model achieves an accuracy of\n94.2% over the plain data and 93.3% over the encrypted data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boulila_W/0/1/0/all/0/1\">Wadii Boulila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ammar_A/0/1/0/all/0/1\">Adel Ammar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benjdira_B/0/1/0/all/0/1\">Bilel Benjdira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koubaa_A/0/1/0/all/0/1\">Anis Koubaa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular Image. (arXiv:2203.07732v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07732","description":"<p>We present a novel face reconstruction method capable of reconstructing\ndetailed face geometry, spatially varying face reflectance from a single\nmonocular image. We build our work upon the recent advances of DNN-based\nauto-encoders with differentiable ray tracing image formation, trained in\nself-supervised manner. While providing the advantage of learning-based\napproaches and real-time reconstruction, the latter methods lacked fidelity. In\nthis work, we achieve, for the first time, high fidelity face reconstruction\nusing self-supervised learning only. Our novel coarse-to-fine deep architecture\nallows us to solve the challenging problem of decoupling face reflectance from\ngeometry using a single image, at high computational speed. Compared to\nstate-of-the-art methods, our method achieves more visually appealing\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1\">Abdallah Dib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junghyun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thebault_C/0/1/0/all/0/1\">Cedric Thebault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_P/0/1/0/all/0/1\">Philippe-Henri Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevallier_L/0/1/0/all/0/1\">Louis Chevallier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Annotation-free Restoration Network for Cataractous Fundus Images. (arXiv:2203.07737v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07737","description":"<p>Cataracts are the leading cause of vision loss worldwide. Restoration\nalgorithms are developed to improve the readability of cataract fundus images\nin order to increase the certainty in diagnosis and treatment for cataract\npatients. Unfortunately, the requirement of annotation limits the application\nof these algorithms in clinics. This paper proposes a network to\nannotation-freely restore cataractous fundus images (ArcNet) so as to boost the\nclinical practicability of restoration. Annotations are unnecessary in ArcNet,\nwhere the high-frequency component is extracted from fundus images to replace\nsegmentation in the preservation of retinal structures. The restoration model\nis learned from the synthesized images and adapted to real cataract images.\nExtensive experiments are implemented to verify the performance and\neffectiveness of ArcNet. Favorable performance is achieved using ArcNet against\nstate-of-the-art algorithms, and the diagnosis of ocular fundus diseases in\ncataract patients is promoted by ArcNet. The capability of properly restoring\ncataractous images in the absence of annotated data promises the proposed\nalgorithm outstanding clinical practicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanpei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSN: Component-Supervised Network for Few-Shot Classification. (arXiv:2203.07738v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07738","description":"<p>The few-shot classification (FSC) task has been a hot research topic in\nrecent years. It aims to address the classification problem with insufficient\nlabeled data on a cross-category basis. Typically, researchers pre-train a\nfeature extractor with base data, then use it to extract the features of novel\ndata and recognize them. Notably, the novel set only has a few annotated\nsamples and has entirely different categories from the base set, which leads to\nthat the pre-trained feature extractor can not adapt to the novel data\nflawlessly. We dub this problem as Feature-Extractor-Maladaptive (FEM) problem.\nStarting from the root cause of this problem, this paper presents a new scheme,\nComponent-Supervised Network (CSN), to improve the performance of FSC. We\nbelieve that although the categories of base and novel sets are different, the\ncomposition of the sample's components is similar. For example, both cat and\ndog contain leg and head components. Actually, such entity components are\nintra-class stable. They have fine cross-category versatility and new category\ngeneralization. Therefore, we refer to WordNet, a dictionary commonly used in\nnatural language processing, to collect component information of samples and\nconstruct a component-based auxiliary task to improve the adaptability of the\nfeature extractor. We conduct experiments on two benchmark datasets\n(mini-ImageNet and tiered-ImageNet), the improvements of $0.9\\%$-$5.8\\%$\ncompared with state-of-the-arts have evaluated the efficiency of our CSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Baodi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lifei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07740","description":"<p>Arbitrary style transfer (AST) and domain generalization (DG) are important\nyet challenging visual learning tasks, which can be cast as a feature\ndistribution matching problem. With the assumption of Gaussian feature\ndistribution, conventional feature distribution matching methods usually match\nthe mean and standard deviation of features. However, the feature distributions\nof real-world data are usually much more complicated than Gaussian, which\ncannot be accurately matched by using only the first-order and second-order\nstatistics, while it is computationally prohibitive to use high-order\nstatistics for distribution matching. In this work, we, for the first time to\nour best knowledge, propose to perform Exact Feature Distribution Matching\n(EFDM) by exactly matching the empirical Cumulative Distribution Functions\n(eCDFs) of image features, which could be implemented by applying the Exact\nHistogram Matching (EHM) in the image feature space. Particularly, a fast EHM\nalgorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play\nmanner with minimal cost. The effectiveness of our proposed EFDM method is\nverified on a variety of AST and DG tasks, demonstrating new state-of-the-art\nresults. Codes are available at https://github.com/YBZh/EFDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Curve Translator for Real-Time High-Resolution Image-to-Image Translation. (arXiv:2203.07756v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07756","description":"<p>The dominant image-to-image translation methods are based on fully\nconvolutional networks, which extract and translate an image's features and\nthen reconstruct the image. However, they have unacceptable computational costs\nwhen working with high-resolution images. To this end, we present the\nMulti-Curve Translator (MCT), which not only predicts the translated pixels for\nthe corresponding input pixels but also for their neighboring pixels. And if a\nhigh-resolution image is downsampled to its low-resolution version, the lost\npixels are the remaining pixels' neighboring pixels. So MCT makes it possible\nto feed the network only the downsampled image to perform the mapping for the\nfull-resolution image, which can dramatically lower the computational cost.\nBesides, MCT is a plug-in approach that utilizes existing base models and\nrequires only replacing their output layers. Experiments demonstrate that the\nMCT variants can process 4K images in real-time and achieve comparable or even\nbetter performance than the base models on various image-to-image translation\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07772","description":"<p>The numerical wavefront backpropagation principle of digital holography\nconfers unique extended focus capabilities, without mechanical displacements\nalong z-axis. However, the determination of the correct focusing distance is a\nnon-trivial and time consuming issue. A deep learning (DL) solution is proposed\nto cast the autofocusing as a regression problem and tested over both\nexperimental and simulated holograms. Single wavelength digital holograms were\nrecorded by a Digital Holographic Microscope (DHM) with a 10$\\mathrm{x}$\nmicroscope objective from a patterned target moving in 3D over an axial range\nof 92 $\\mu$m. Tiny DL models are proposed and compared such as a tiny Vision\nTransformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The\nexperiments show that the predicted focusing distance $Z_R^{\\mathrm{Pred}}$ is\naccurately inferred with an accuracy of 1.2 $\\mu$m in average in comparison\nwith the DHM depth of field of 15 $\\mu$m. Numerical simulations show that all\ntiny models give the $Z_R^{\\mathrm{Pred}}$ with an error below 0.3 $\\mu$m. Such\na prospect would significantly improve the current capabilities of computer\nvision position sensing in applications such as 3D microscopy for life sciences\nor micro-robotics. Moreover, all models reach state of the art inference time\non CPU, less than 25 ms per inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andr&#xe9;oli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andre_A/0/1/0/all/0/1\">Antoine N. Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandoz_P/0/1/0/all/0/1\">Patrick Sandoz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laurent_G/0/1/0/all/0/1\">Guillaume J. Laurent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v1 [cs.LG])","link":"http://arxiv.org/abs/2203.07788","description":"<p>Noisy training set usually leads to the degradation of generalization and\nrobustness of neural networks. In this paper, we propose using a theoretically\nguaranteed noisy label detection framework to detect and remove noisy data for\nLearning with Noisy Labels (LNL). Specifically, we design a penalized\nregression to model the linear relation between network features and one-hot\nlabels, where the noisy data are identified by the non-zero mean shift\nparameters solved in the regression model. To make the framework scalable to\ndatasets that contain a large number of categories and training data, we\npropose a split algorithm to divide the whole training set into small pieces\nthat can be solved by the penalized regression in parallel, leading to the\nScalable Penalized Regression (SPR) framework. We provide the non-asymptotic\nprobabilistic condition for SPR to correctly identify the noisy data. While SPR\ncan be regarded as a sample selection module for standard supervised training\npipeline, we further combine it with semi-supervised algorithm to further\nexploit the support of noisy data as unlabeled data. Experimental results on\nseveral benchmark datasets and real-world noisy datasets show the effectiveness\nof our framework. Our code and pretrained models are released at\nhttps://github.com/Yikai-Wang/SPR-LNL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parking Analytics Framework using Deep Learning. (arXiv:2203.07792v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07792","description":"<p>With the number of vehicles continuously increasing, parking monitoring and\nanalysis are becoming a substantial feature of modern cities. In this study, we\npresent a methodology to monitor car parking areas and to analyze their\noccupancy in real-time. The solution is based on a combination between image\nanalysis and deep learning techniques. It incorporates four building blocks put\ninside a pipeline: vehicle detection, vehicle tracking, manual annotation of\nparking slots, and occupancy estimation using the Ray Tracing algorithm. The\naim of this methodology is to optimize the use of parking areas and to reduce\nthe time wasted by daily drivers to find the right parking slot for their cars.\nAlso, it helps to better manage the space of the parking areas and to discover\nmisuse cases. A demonstration of the provided solution is shown in the\nfollowing video link: https://www.youtube.com/watch?v=KbAt8zT14Tc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benjdira_B/0/1/0/all/0/1\">Bilel Benjdira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubaa_A/0/1/0/all/0/1\">Anis Koubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulila_W/0/1/0/all/0/1\">Wadii Boulila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammar_A/0/1/0/all/0/1\">Adel Ammar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the focusing of thermal images. (arXiv:2203.07805v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07805","description":"<p>In this paper we present a new thermographic image database suitable for the\nanalysis of automatic focus measures. This database consists of 8 different\nsets of scenes, where each scene contains one image for 96 different focus\npositions. Using this database we evaluate the usefulness of six focus measures\nwith the goal to determine the optimal focus position. Experimental results\nreveal that an accurate automatic detection of optimal focus position is\npossible, even with a low computational burden. We also present an acquisition\ntool able to help the acquisition of thermal images. To the best of our\nknowledge, this is the first study about automatic focus of thermal images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Mekyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Duro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs. (arXiv:2203.07808v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07808","description":"<p>Unstructured pruning is well suited to reduce the memory footprint of\nconvolutional neural networks (CNNs), both at training and inference time. CNNs\ncontain parameters arranged in $K \\times K$ filters. Standard unstructured\npruning (SP) reduces the memory footprint of CNNs by setting filter elements to\nzero, thereby specifying a fixed subspace that constrains the filter.\nEspecially if pruning is applied before or during training, this induces a\nstrong bias. To overcome this, we introduce interspace pruning (IP), a general\ntool to improve existing pruning methods. It uses filters represented in a\ndynamic interspace by linear combinations of an underlying adaptive filter\nbasis (FB). For IP, FB coefficients are set to zero while un-pruned\ncoefficients and FBs are trained jointly. In this work, we provide mathematical\nevidence for IP's superior performance and demonstrate that IP outperforms SP\non all tested state-of-the-art unstructured pruning methods. Especially in\nchallenging situations, like pruning for ImageNet or pruning to high sparsity,\nIP greatly exceeds SP with equal runtime and parameter costs. Finally, we show\nthat advances of IP are due to improved trainability and superior\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Quality Assessment for Magnetic Resonance Imaging. (arXiv:2203.07809v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07809","description":"<p>Image quality assessment (IQA) algorithms aim to reproduce the human's\nperception of the image quality. The growing popularity of image enhancement,\ngeneration, and recovery models instigated the development of many methods to\nassess their performance. However, most IQA solutions are designed to predict\nimage quality in the general domain, with the applicability to specific areas,\nsuch as medical imaging, remaining questionable. Moreover, the selection of\nthese IQA metrics for a specific task typically involves intentionally induced\ndistortions, such as manually added noise or artificial blurring; yet, the\nchosen metrics are then used to judge the output of real-life computer vision\nmodels. In this work, we aspire to fill these gaps by carrying out the most\nextensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date\n(14,700 subjective scores). We use outputs of neural network models trained to\nsolve problems relevant to MRI, including image reconstruction in the scan\nacceleration, motion correction, and denoising. Seven trained radiologists\nassess these distorted images, with their verdicts then correlated with 35\ndifferent image quality metrics (full-reference, no-reference, and\ndistribution-based metrics considered). Our emphasis is on reflecting the\nradiologist's perception of the reconstructed images, gauging the most\ndiagnostically influential criteria for the quality of MRI scans:\nsignal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kastryulin_S/0/1/0/all/0/1\">Segrey Kastryulin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zakirov_J/0/1/0/all/0/1\">Jamil Zakirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pezzotti_N/0/1/0/all/0/1\">Nicola Pezzotti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (arXiv:2203.07815v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07815","description":"<p>Data augmentation has been widely used in deep learning to reduce\nover-fitting and improve the robustness of models. However, traditional data\naugmentation techniques, e.g., rotation, cropping, flipping, etc., do not\nconsider \\textit{semantic} transformations, e.g., changing the age of a brain\nimage. Previous works tried to achieve semantic augmentation by generating\n\\textit{counterfactuals}, but they focused on how to train deep generative\nmodels and randomly created counterfactuals with the generative models without\nconsidering which counterfactuals are most \\textit{effective} for improving\ndownstream training. Different from these approaches, in this work, we propose\na novel adversarial counterfactual augmentation scheme that aims to find the\nmost \\textit{effective} counterfactuals to improve downstream tasks with a\npre-trained generative model. Specifically, we construct an adversarial game\nwhere we update the input \\textit{conditional factor} of the generator and the\ndownstream \\textit{classifier} with gradient backpropagation alternatively and\niteratively. The key idea is to find conditional factors that can result in\n\\textit{hard} counterfactuals for the classifier. This can be viewed as finding\nthe `\\textit{weakness}' of the classifier and purposely forcing it to\n\\textit{overcome} its weakness via the generative model. To demonstrate the\neffectiveness of the proposed approach, we validate the method with the\nclassification of Alzheimer's Disease (AD) as the downstream task based on a\npre-trained brain ageing synthesis model. We show the proposed approach\nimproves test accuracy and can alleviate spurious correlations. Code will be\nreleased upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SISL:Self-Supervised Image Signature Learning for Splicing Detection and Localization. (arXiv:2203.07824v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07824","description":"<p>Recent algorithms for image manipulation detection almost exclusively use\ndeep network models. These approaches require either dense pixelwise\ngroundtruth masks, camera ids, or image metadata to train the networks. On one\nhand, constructing a training set to represent the countless tampering\npossibilities is impractical. On the other hand, social media platforms or\ncommercial applications are often constrained to remove camera ids as well as\nmetadata from images. A self-supervised algorithm for training manipulation\ndetection models without dense groundtruth or camera/image metadata would be\nextremely useful for many forensics applications. In this paper, we propose\nself-supervised approach for training splicing detection/localization models\nfrom frequency transforms of images. To identify the spliced regions, our deep\nnetwork learns a representation to capture an image specific signature by\nenforcing (image) self consistency . We experimentally demonstrate that our\nproposed model can yield similar or better performances of multiple existing\nmethods on standard datasets without relying on labels or metadata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Susmit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prabhat Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_S/0/1/0/all/0/1\">Siddharth Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parag_T/0/1/0/all/0/1\">Toufiq Parag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_V/0/1/0/all/0/1\">Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPA-VAE: Similar-Parts-Assignment for Unsupervised 3D Point Cloud Generation. (arXiv:2203.07825v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07825","description":"<p>This paper addresses the problem of unsupervised parts-aware point cloud\ngeneration with learned parts-based self-similarity. Our SPA-VAE infers a set\nof latent canonical candidate shapes for any given object, along with a set of\nrigid body transformations for each such candidate shape to one or more\nlocations within the assembled object. In this way, noisy samples on the\nsurface of, say, each leg of a table, are effectively combined to estimate a\nsingle leg prototype. When parts-based self-similarity exists in the raw data,\nsharing data among parts in this way confers numerous advantages: modeling\naccuracy, appropriately self-similar generative outputs, precise in-filling of\nocclusions, and model parsimony. SPA-VAE is trained end-to-end using a\nvariational Bayesian approach which uses the Gumbel-softmax trick for the\nshared part assignments, along with various novel losses to provide appropriate\ninductive biases. Quantitative and qualitative analyses on ShapeNet demonstrate\nthe advantage of SPA-VAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose-MUM : Reinforcing Key Points Relationship for Semi-Supervised Human Pose Estimation. (arXiv:2203.07837v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07837","description":"<p>A well-designed strong-weak augmentation strategy and the stable teacher to\ngenerate reliable pseudo labels are essential in the teacher-student framework\nof semi-supervised learning (SSL). Considering these in mind, to suit the\nsemi-supervised human pose estimation (SSHPE) task, we propose a novel approach\nreferred to as Pose-MUM that modifies Mix/UnMix (MUM) augmentation. Like MUM in\nthe dense prediction task, the proposed Pose-MUM makes strong-weak augmentation\nfor pose estimation and leads the network to learn the relationship between\neach human key point much better than the conventional methods by adding the\nmixing process in intermediate layers in a stochastic manner. In addition, we\nemploy the exponential-moving-average-normalization (EMAN) teacher, which is\nstable and well-suited to the SSL framework and furthermore boosts the\nperformance. Extensive experiments on MS-COCO dataset show the superiority of\nour proposed method by consistently improving the performance over the previous\nmethods following SSHPE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JongMok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwijun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jaeseung Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jongkeun Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jin Young Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy. (arXiv:2203.07845v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07845","description":"<p>Large-scale datasets play a vital role in computer vision. Existing datasets\nare either collected according to heuristic label systems or annotated blindly\nwithout differentiation to samples, making them inefficient and unscalable. How\nto systematically collect, annotate and build a mega-scale dataset remains an\nopen question. In this work, we advocate building a high-quality vision dataset\nactively and continually on a comprehensive label system. Specifically, we\ncontribute Bamboo Dataset, a mega-scale and information-dense dataset for both\nclassification and detection. Bamboo aims to populate the comprehensive\ncategories with 69M image classification annotations and 170,586 object\nbounding box annotations. Compared to ImageNet22K and Objects365, models\npre-trained on Bamboo achieve superior performance among various downstream\ntasks (6.2% gains on classification and 2.1% gains on detection). In addition,\nwe provide valuable observations regarding large-scale pre-training from over\n1,000 experiments. Due to its scalable nature on both label system and\nannotation pipeline, Bamboo will continue to grow and benefit from the\ncollective efforts of the community, which we hope would pave the way for more\ngeneral vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qinghong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive 3D Segmentation of Shoulder Joint with Coarse-scanned MR Image. (arXiv:2203.07846v1 [eess.IV])","link":"http://arxiv.org/abs/2203.07846","description":"<p>For diagnosis of shoulder illness, it is essential to look at the morphology\ndeviation of scapula and humerus from the medical images that are acquired from\nMagnetic Resonance (MR) imaging. However, taking high-resolution MR images is\ntime-consuming and costly because the reduction of the physical distance\nbetween image slices causes prolonged scanning time. Moreover, due to the lack\nof training images, images from various sources must be utilized, which creates\nthe issue of high variance across the dataset. Also, there are human errors\namong the images due to the fact that it is hard to take the spatial\nrelationship into consideration when labeling the 3D image in low resolution.\nIn order to combat all obstacles stated above, we develop a fully automated\nalgorithm for segmenting the humerus and scapula bone from coarsely scanned and\nlow-resolution MR images and a recursive learning framework that iterative\nutilize the generated labels for reducing the errors among segmentations and\nincrease our dataset set for training the next round network. In this study, 50\nMR images are collected from several institutions and divided into five\nmutually exclusive sets for carrying five-fold cross-validation. Contours that\nare generated by the proposed method demonstrated a high level of accuracy when\ncompared with ground truth and the traditional method. The proposed neural\nnetwork and the recursive learning scheme improve the overall quality of the\nsegmentation on humerus and scapula on the low-resolution dataset and reduced\nincorrect segmentation in the ground truth, which could have a positive impact\non finding the cause of shoulder pain and patient's early relief.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaoxiao He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1\">Chaowei Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_V/0/1/0/all/0/1\">Virak Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Non-Rigid 3D Registration. (arXiv:2203.07858v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07858","description":"<p>Non-rigid registration computes an alignment between a source surface with a\ntarget surface in a non-rigid manner. In the past decade, with the advances in\n3D sensing technologies that can measure time-varying surfaces, non-rigid\nregistration has been applied for the acquisition of deformable shapes and has\na wide range of applications. This survey presents a comprehensive review of\nnon-rigid registration methods for 3D shapes, focusing on techniques related to\ndynamic shape acquisition and reconstruction. In particular, we review\ndifferent approaches for representing the deformation field, and the methods\nfor computing the desired deformation. Both optimization-based and\nlearning-based methods are covered. We also review benchmarks and datasets for\nevaluating non-rigid registration methods, and discuss potential future\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyke_R/0/1/0/all/0/1\">Roberto M. Dyke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Get Me Wrong: How to apply Deep Visual Interpretations to Time Series. (arXiv:2203.07861v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07861","description":"<p>The correct interpretation and understanding of deep learning models is\nessential in many applications. Explanatory visual interpretation approaches\nfor image and natural language processing allow domain experts to validate and\nunderstand almost any deep learning model. However, they fall short when\ngeneralizing to arbitrary time series data that is less intuitive and more\ndiverse. Whether a visualization explains the true reasoning or captures the\nreal features is difficult to judge. Hence, instead of blind trust we need an\nobjective evaluation to obtain reliable quality metrics. We propose a framework\nof six orthogonal metrics for gradient- or perturbation-based post-hoc visual\ninterpretation methods designed for time series classification and segmentation\ntasks. An experimental study includes popular neural network architectures for\ntime series and nine visual interpretation methods. We evaluate the visual\ninterpretation methods with diverse datasets from the UCR repository and a\ncomplex real-world dataset, and study the influence of common regularization\ntechniques during training. We show that none of the methods consistently\noutperforms any of the others on all metrics while some are ahead at times. Our\ninsights and recommendations allow experts to make informed choices of suitable\nvisualization techniques for the model and task at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loeffler_C/0/1/0/all/0/1\">Christoffer Loeffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Cheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Lukas Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1\">Christopher Mutschler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiP-Flow: Learning Inference-time Priors for Codec Avatars via Normalizing Flows in Latent Space. (arXiv:2203.07881v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07881","description":"<p>Neural face avatars that are trained from multi-view data captured in camera\ndomes can produce photo-realistic 3D reconstructions. However, at inference\ntime, they must be driven by limited inputs such as partial views recorded by\nheadset-mounted cameras or a front-facing camera, and sparse facial landmarks.\nTo mitigate this asymmetry, we introduce a prior model that is conditioned on\nthe runtime inputs and tie this prior space to the 3D face model via a\nnormalizing flow in the latent space. Our proposed model, LiP-Flow, consists of\ntwo encoders that learn representations from the rich training-time and\nimpoverished inference-time observations. A normalizing flow bridges the two\nrepresentation spaces and transforms latent samples from one domain to another,\nallowing us to define a latent likelihood objective. We trained our model\nend-to-end to maximize the similarity of both representation spaces and the\nreconstruction quality, making the 3D face model aware of the limited driving\nsignals. We conduct extensive evaluations where the latent codes are optimized\nto reconstruct 3D avatars from partial or sparse observations. We show that our\napproach leads to an expressive and effective prior, capturing facial dynamics\nand subtle expressions better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1\">Emre Aksan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shugao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Akin Caliskan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pidhorskyi_S/0/1/0/all/0/1\">Stanislav Pidhorskyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Alexander Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition. (arXiv:2203.07890v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07890","description":"<p>Visual Question Generation (VQG) is a task to generate questions from images.\nWhen humans ask questions about an image, their goal is often to acquire some\nnew knowledge. However, existing studies on VQG have mainly addressed question\ngeneration from answers or question categories, overlooking the objectives of\nknowledge acquisition. To introduce a knowledge acquisition perspective into\nVQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is\nthe first large, humanly annotated dataset in which questions regarding images\nare tied to structured knowledge. We also developed a new VQG model that can\nencode and use knowledge as the target for a question. The experiment results\nshow that our model outperforms existing models on the K-VQG dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kohei Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Rectifier Wavelet Covariance Models For Texture Synthesis. (arXiv:2203.07902v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07902","description":"<p>State-of-the-art maximum entropy models for texture synthesis are built from\nstatistics relying on image representations defined by convolutional neural\nnetworks (CNN). Such representations capture rich structures in texture images,\noutperforming wavelet-based representations in this regard. However, conversely\nto neural networks, wavelets offer meaningful representations, as they are\nknown to detect structures at multiple scales (e.g. edges) in images. In this\nwork, we propose a family of statistics built upon non-linear wavelet based\nrepresentations, that can be viewed as a particular instance of a one-layer\nCNN, using a generalized rectifier non-linearity. These statistics\nsignificantly improve the visual quality of previous classical wavelet-based\nmodels, and allow one to produce syntheses of similar quality to\nstate-of-the-art models, on both gray-scale and color textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brochard_A/0/1/0/all/0/1\">Antoine Brochard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallat_S/0/1/0/all/0/1\">St&#xe9;phane Mallat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning Based Focal Stack Camera Depth Estimation. (arXiv:2203.07904v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07904","description":"<p>We propose an unsupervised deep learning based method to estimate depth from\nfocal stack camera images. On the NYU-v2 dataset, our method achieves much\nbetter depth estimation accuracy compared to single-image based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Weizhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norris_T/0/1/0/all/0/1\">Theodore B. Norris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic SwiftNet: Pyramidal Fusion for Real-time Panoptic Segmentation. (arXiv:2203.07908v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07908","description":"<p>Dense panoptic prediction is a key ingredient in many existing applications\nsuch as autonomous driving, automated warehouses or agri-robotics. However,\nmost of these applications leverage the recovered dense semantics as an input\nto visual closed-loop control. Hence, practical deployments require real-time\ninference over large input resolutions on embedded hardware. These requirements\ncall for computationally efficient approaches which deliver high accuracy with\nlimited computational resources. We propose to achieve this goal by trading-off\nbackbone capacity for multi-scale feature extraction. In comparison with\ncontemporaneous approaches to panoptic segmentation, the main novelties of our\nmethod are scale-equivariant feature extraction and cross-scale upsampling\nthrough pyramidal fusion. Our best model achieves 55.9% PQ on Cityscapes val at\n60 FPS on full resolution 2MPx images and RTX3090 with FP16 Tensor RT\noptimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saric_J/0/1/0/all/0/1\">Josip &#x160;ari&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning with Graph Neural Network for Sensor-Based Human Activity Recognition. (arXiv:2203.07910v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07910","description":"<p>The sensor-based human activity recognition (HAR) in mobile application\nscenarios is often confronted with sensor modalities variation and annotated\ndata deficiency. Given this observation, we devised a graph-inspired deep\nlearning approach toward the sensor-based HAR tasks, which was further used to\nbuild a deep transfer learning model toward giving a tentative solution for\nthese two challenging problems. Specifically, we present a multi-layer residual\nstructure involved graph convolutional neural network (ResGCNN) toward the\nsensor-based HAR tasks, namely the HAR-ResGCNN approach. Experimental results\non the PAMAP2 and mHealth data sets demonstrate that our ResGCNN is effective\nat capturing the characteristics of actions with comparable results compared to\nother sensor-based HAR models (with an average accuracy of 98.18% and 99.07%,\nrespectively). More importantly, the deep transfer learning experiments using\nthe ResGCNN model show excellent transferability and few-shot learning\nperformance. The graph-based framework shows good meta-learning ability and is\nsupposed to be a promising solution in sensor-based HAR tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tianzheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinjin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1\">Wei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jing Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting. (arXiv:2203.07918v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07918","description":"<p>While 6D object pose estimation has recently made a huge leap forward, most\nmethods can still only handle a single or a handful of different objects, which\nlimits their applications. To circumvent this problem, category-level object\npose estimation has recently been revamped, which aims at predicting the 6D\npose as well as the 3D metric size for previously unseen instances from a given\nset of object classes. This is, however, a much more challenging task due to\nsevere intra-class shape variations. To address this issue, we propose\nGPV-Pose, a novel framework for robust category-level pose estimation,\nharnessing geometric insights to enhance the learning of category-level\npose-sensitive features. First, we introduce a decoupled confidence-driven\nrotation representation, which allows geometry-aware recovery of the associated\nrotation matrix. Second, we propose a novel geometry-guided point-wise voting\nparadigm for robust retrieval of the 3D object bounding box. Finally,\nleveraging these different output streams, we can enforce several geometric\nconsistency terms, further increasing performance, especially for non-symmetric\ncategories. GPV-Pose produces superior results to state-of-the-art competitors\non common public benchmarks, whilst almost achieving real-time inference speed\nat 20 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1\">Yan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruida Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Z/0/1/0/all/0/1\">Zhiqiang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative Pose from SIFT Features. (arXiv:2203.07930v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07930","description":"<p>This paper proposes the geometric relationship of epipolar geometry and\norientation- and scale-covariant, e.g., SIFT, features. We derive a new linear\nconstraint relating the unknown elements of the fundamental matrix and the\norientation and scale. This equation can be used together with the well-known\nepipolar constraint to, e.g., estimate the fundamental matrix from four SIFT\ncorrespondences, essential matrix from three, and to solve the semi-calibrated\ncase from three correspondences. Requiring fewer correspondences than the\nwell-known point-based approaches (e.g., 5PT, 6PT and 7PT solvers) for epipolar\ngeometry estimation makes RANSAC-like randomized robust estimation\nsignificantly faster. The proposed constraint is tested on a number of problems\nin a synthetic environment and on publicly available real-world datasets on\nmore than 80000 image pairs. It is superior to the state-of-the-art in terms of\nprocessing time while often leading to more accurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1\">Zuzana Kukelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation. (arXiv:2203.07931v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07931","description":"<p>Conversation is an essential component of virtual avatar activities in the\nmetaverse. With the development of natural language processing, textual and\nvocal conversation generation has achieved a significant breakthrough.\nFace-to-face conversations account for the vast majority of daily\nconversations. However, this task has not acquired enough attention. In this\npaper, we propose a novel task that aims to generate a realistic human avatar\nface-to-face conversation process and present a new dataset to explore this\ntarget. To tackle this novel task, we propose a new framework that utilizes a\nseries of conversation signals, e.g. audio, head pose, and expression, to\nsynthesize face-to-face conversation videos between human avatars, with all the\ninterlocutors modeled within the same network. Our method is evaluated by\nquantitative and qualitative experiments in different aspects, e.g. image\nquality, pose sequence trend, and naturalness of the rendering videos. All the\ncode, data, and models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zanwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Transformer for Image Inversion and Editing. (arXiv:2203.07932v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07932","description":"<p>Existing GAN inversion methods fail to provide latent codes for reliable\nreconstruction and flexible editing simultaneously. This paper presents a\ntransformer-based image inversion and editing model for pretrained StyleGAN\nwhich is not only with less distortions, but also of high quality and\nflexibility for editing. The proposed model employs a CNN encoder to provide\nmulti-scale image features as keys and values. Meanwhile it regards the style\ncode to be determined for different layers of the generator as queries. It\nfirst initializes query tokens as learnable parameters and maps them into W+\nspace. Then the multi-stage alternate self- and cross-attention are utilized,\nupdating queries with the purpose of inverting the input by the generator.\nMoreover, based on the inverted code, we investigate the reference- and\nlabel-based attribute editing through a pretrained latent classifier, and\nachieve flexible image-to-image translation with high quality results.\nExtensive experiments are carried out, showing better performances on both\ninversion and editing tasks within StyleGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiusheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengyi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07967","description":"<p>Neural fields have gained significant attention in the computer vision\ncommunity due to their excellent performance in novel view synthesis, geometry\nreconstruction, and generative modeling. Some of their advantages are a sound\ntheoretic foundation and an easy implementation in current deep learning\nframeworks. While neural fields have been applied to signals on manifolds,\ne.g., for texture reconstruction, their representation has been limited to\nextrinsically embedding the shape into Euclidean space. The extrinsic embedding\nignores known intrinsic manifold properties and is inflexible wrt. transfer of\nthe learned function. To overcome these limitations, this work introduces\nintrinsic neural fields, a novel and versatile representation for neural fields\non manifolds. Intrinsic neural fields combine the advantages of neural fields\nwith the spectral properties of the Laplace-Beltrami operator. We show\ntheoretically that intrinsic neural fields inherit many desirable properties of\nthe extrinsic neural field framework but exhibit additional intrinsic\nqualities, like isometry invariance. In experiments, we show intrinsic neural\nfields can reconstruct high-fidelity textures from images with state-of-the-art\nquality and are robust to the discretization of the underlying manifold. We\ndemonstrate the versatility of intrinsic neural fields by tackling various\napplications: texture transfer between deformed shapes &amp; different shapes,\ntexture reconstruction from real-world images with view dependence, and\ndiscretization-agnostic learning on meshes and point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1\">Lukas Koestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grittner_D/0/1/0/all/0/1\">Daniel Grittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOBDrone: a Drone Video Dataset for Man OverBoard Rescue. (arXiv:2203.07973v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07973","description":"<p>Modern Unmanned Aerial Vehicles (UAV) equipped with cameras can play an\nessential role in speeding up the identification and rescue of people who have\nfallen overboard, i.e., man overboard (MOB). To this end, Artificial\nIntelligence techniques can be leveraged for the automatic understanding of\nvisual data acquired from drones. However, detecting people at sea in aerial\nimagery is challenging primarily due to the lack of specialized annotated\ndatasets for training and testing detectors for this task. To fill this gap, we\nintroduce and publicly release the MOBDrone benchmark, a collection of more\nthan 125K drone-view images in a marine environment under several conditions,\nsuch as different altitudes, camera shooting angles, and illumination. We\nmanually annotated more than 180K objects, of which about 113K man overboard,\nprecisely localizing them with bounding boxes. Moreover, we conduct a thorough\nperformance analysis of several state-of-the-art object detectors on the\nMOBDrone data, serving as baselines for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafarelli_D/0/1/0/all/0/1\">Donato Cafarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1\">Luca Ciampi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadicamo_L/0/1/0/all/0/1\">Lucia Vadicamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berton_A/0/1/0/all/0/1\">Andrea Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paterni_M/0/1/0/all/0/1\">Marco Paterni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benvenuti_C/0/1/0/all/0/1\">Chiara Benvenuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passera_M/0/1/0/all/0/1\">Mirko Passera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pitfalls of Batch Normalization for End-to-End Video Learning: A Study on Surgical Workflow Analysis. (arXiv:2203.07976v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07976","description":"<p>Batch Normalization's (BN) unique property of depending on other samples in a\nbatch is known to cause problems in several tasks, including sequential\nmodeling, and has led to the use of alternatives in these fields. In video\nlearning, however, these problems are less studied, despite the ubiquitous use\nof BN in CNNs for visual feature extraction. We argue that BN's properties\ncreate major obstacles for training CNNs and temporal models end to end in\nvideo tasks. Yet, end-to-end learning seems preferable in specialized domains\nsuch as surgical workflow analysis, which lack well-pretrained feature\nextractors. While previous work in surgical workflow analysis has avoided\nBN-related issues through complex, multi-stage learning procedures, we show\nthat even simple, end-to-end CNN-LSTMs can outperform the state of the art when\nCNNs without BN are used. Moreover, we analyze in detail when BN-related issues\noccur, including a \"cheating\" phenomenon in surgical anticipation tasks. We\nhope that a deeper understanding of BN's limitations and a reconsideration of\nend-to-end approaches can be beneficial for future research in surgical\nworkflow analysis and general video learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1\">Dominik Rivoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_I/0/1/0/all/0/1\">Isabel Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic 3D Reconstruction. (arXiv:2203.07977v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07977","description":"<p>RGBD-based real-time dynamic 3D reconstruction suffers from inaccurate\ninter-frame motion estimation as errors may accumulate with online tracking.\nThis problem is even more severe for single-view-based systems due to strong\nocclusions. Based on these observations, we propose OcclusionFusion, a novel\nmethod to calculate occlusion-aware 3D motion to guide the reconstruction. In\nour technique, the motion of visible regions is first estimated and combined\nwith temporal information to infer the motion of the occluded regions through\nan LSTM-involved graph neural network. Furthermore, our method computes the\nconfidence of the estimated motion by modeling the network output with a\nprobabilistic model, which alleviates untrustworthy motions and enables robust\ntracking. Experimental results on public datasets and our own recorded data\nshow that our technique outperforms existing single-view-based real-time\nmethods by a large margin. With the reduction of the motion errors, the\nproposed technique can handle long and challenging motion sequences. Please\ncheck out the project page for sequence results:\nhttps://wenbin-lin.github.io/OcclusionFusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1\">Jun-Hai Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07980","description":"<p>Accurate uncertainty estimates are essential for deploying deep object\ndetectors in safety-critical systems. The development and evaluation of\nprobabilistic object detectors have been hindered by shortcomings in existing\nperformance measures, which tend to involve arbitrary thresholds or limit the\ndetector's choice of distributions. In this work, we propose to view object\ndetection as a set prediction task where detectors predict the distribution\nover the set of objects. Using the negative log-likelihood for random finite\nsets, we present a proper scoring rule for evaluating and training\nprobabilistic object detectors. The proposed method can be applied to existing\nprobabilistic detectors, is free from thresholds, and enables fair comparison\nbetween architectures. Three different types of detectors are evaluated on the\nCOCO dataset. Our results indicate that the training of existing detectors is\noptimized toward non-probabilistic metrics. We hope to encourage the\ndevelopment of new object detectors that can accurately estimate their own\nuncertainty. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1\">Georg Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation. (arXiv:2203.07988v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07988","description":"<p>After the great success of Vision Transformer variants (ViTs) in computer\nvision, it has also demonstrated great potential in domain adaptive semantic\nsegmentation. Unfortunately, straightforwardly applying local ViTs in domain\nadaptive semantic segmentation does not bring in expected improvement. We find\nthat the pitfall of local ViTs is due to the severe high-frequency components\ngenerated during both the pseudo-label construction and features alignment for\ntarget domains. These high-frequency components make the training of local ViTs\nvery unsmooth and hurt their transferability. In this paper, we introduce a\nlow-pass filtering mechanism, momentum network, to smooth the learning dynamics\nof target domain features and pseudo labels. Furthermore, we propose a dynamic\nof discrepancy measurement to align the distributions in the source and target\ndomains via dynamic weights to evaluate the importance of the samples. After\ntackling the above issues, extensive experiments on sim2real benchmarks show\nthat the proposed method outperforms the state-of-the-art methods. Our codes\nare available at https://github.com/alpc91/TransDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runfa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangmin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaqi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v1 [cs.SD])","link":"http://arxiv.org/abs/2203.07996","description":"<p>Training Transformer-based models demands a large amount of data, while\nobtaining parallel aligned and labelled data in multimodality is rather\ncost-demanding, especially for audio-visual speech recognition (AVSR). Thus it\nmakes a lot of sense to make use of unlabelled uni-modal data. On the other\nside, although the effectiveness of large-scale self-supervised learning is\nwell established in both audio and visual modalities, how to integrate those\npre-trained models into a multimodal scenario remains underexplored. In this\nwork, we successfully leverage uni-modal self-supervised learning to promote\nthe multimodal AVSR. In particular, we first train audio and visual encoders on\na large-scale uni-modal dataset, then we integrate components of both encoders\ninto a larger multimodal framework which learns to recognize paired\naudio-visual data into characters through a combination of CTC and seq2seq\ndecoding. We show that both components inherited from uni-modal self-supervised\nlearning cooperate well, resulting in that the multimodal framework yields\ncompetitive results through fine-tuning. Our model is experimentally validated\non both word-level and sentence-level AVSR tasks. Especially, even without an\nexternal language model, our proposed model raises the state-of-the-art\nperformances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a\nlarge margin, with a relative improvement of 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xichen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverted Pyramid Multi-task Transformer for Dense Scene Understanding. (arXiv:2203.07997v1 [cs.CV])","link":"http://arxiv.org/abs/2203.07997","description":"<p>Multi-task dense scene understanding is a thriving research domain that\nrequires simultaneous perception and reasoning on a series of correlated tasks\nwith pixel-wise prediction. Most existing works encounter a severe limitation\nof modeling in the locality due to heavy utilization of convolution operations,\nwhile learning interactions and inference in a global spatial-position and\nmulti-task context is critical for this problem. In this paper, we propose a\nnovel end-to-end Inverted Pyramid multi-task (InvPT) Transformer to perform\nsimultaneous modeling of spatial positions and multiple tasks in a unified\nframework. To the best of our knowledge, this is the first work that explores\ndesigning a transformer structure for multi-task dense prediction for scene\nunderstanding. Besides, it is widely demonstrated that a higher spatial\nresolution is remarkably beneficial for dense predictions, while it is very\nchallenging for existing transformers to go deeper with higher resolutions due\nto huge complexity to large spatial size. InvPT presents an efficient\nUP-Transformer block to learn multi-task feature interaction at gradually\nincreased resolutions, which also incorporates effective self-attention message\npassing and multi-scale feature aggregation to produce task-specific prediction\nat a high resolution. Our method achieves superior multi-task performance on\nNYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms\nprevious state-of-the-arts. Code and trained models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanrong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding. (arXiv:2203.08013v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08013","description":"<p>Natural language spatial video grounding aims to detect the relevant objects\nin video frames with descriptive sentences as the query. In spite of the great\nadvances, most existing methods rely on dense video frame annotations, which\nrequire a tremendous amount of human effort. To achieve effective grounding\nunder a limited annotation budget, we investigate one-shot video grounding, and\nlearn to ground natural language in all video frames with solely one frame\nlabeled, in an end-to-end manner. One major challenge of end-to-end one-shot\nvideo grounding is the existence of videos frames that are either irrelevant to\nthe language query or the labeled frames. Another challenge relates to the\nlimited supervision, which might result in ineffective representation learning.\nTo address these challenges, we designed an end-to-end model via Information\nTree for One-Shot video grounding (IT-OS). Its key module, the information\ntree, can eliminate the interference of irrelevant frames based on branch\nsearch and branch cropping techniques. In addition, several self-supervised\ntasks are proposed based on the information tree to improve the representation\nlearning under insufficient labeling. Experiments on the benchmark dataset\ndemonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Noise-level-aware Framework for PET Image Denoising. (arXiv:2203.08034v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08034","description":"<p>In PET, the amount of relative (signal-dependent) noise present in different\nbody regions can be significantly different and is inherently related to the\nnumber of counts present in that region. The number of counts in a region\ndepends, in principle and among other factors, on the total administered\nactivity, scanner sensitivity, image acquisition duration, radiopharmaceutical\ntracer uptake in the region, and patient local body morphometry surrounding the\nregion. In theory, less amount of denoising operations is needed to denoise a\nhigh-count (low relative noise) image than images a low-count (high relative\nnoise) image, and vice versa. The current deep-learning-based methods for PET\nimage denoising are predominantly trained on image appearance only and have no\nspecial treatment for images of different noise levels. Our hypothesis is that\nby explicitly providing the local relative noise level of the input image to a\ndeep convolutional neural network (DCNN), the DCNN can outperform itself\ntrained on image appearance only. To this end, we propose a noise-level-aware\nframework denoising framework that allows embedding of local noise level into a\nDCNN. The proposed is trained and tested on 30 and 15 patient PET images\nacquired on a GE Discovery MI PET/CT system. Our experiments showed that the\nincreases in both PSNR and SSIM from our backbone network with relative noise\nlevel embedding (NLE) versus the same network without NLE were statistically\nsignificant with p&lt;0.001, and the proposed method significantly outperformed a\nstrong baseline method by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Ye Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1\">Jianan Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_G/0/1/0/all/0/1\">Guodong Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wollenweber_S/0/1/0/all/0/1\">Scott Wollenweber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_F/0/1/0/all/0/1\">Floris Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_S/0/1/0/all/0/1\">Se-In Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyungsang Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_K/0/1/0/all/0/1\">Kuang Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for radar data exploitation of autonomous vehicle. (arXiv:2203.08038v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08038","description":"<p>Autonomous driving requires a detailed understanding of complex driving\nscenes. The redundancy and complementarity of the vehicle's sensors provide an\naccurate and robust comprehension of the environment, thereby increasing the\nlevel of performance and safety. This thesis focuses the on automotive RADAR,\nwhich is a low-cost active sensor measuring properties of surrounding objects,\nincluding their relative speed, and has the key advantage of not being impacted\nby adverse weather conditions. With the rapid progress of deep learning and the\navailability of public driving datasets, the perception ability of vision-based\ndriving systems has considerably improved. The RADAR sensor is seldom used for\nscene understanding due to its poor angular resolution, the size, noise, and\ncomplexity of RADAR raw data as well as the lack of available datasets. This\nthesis proposes an extensive study of RADAR scene understanding, from the\nconstruction of an annotated dataset to the conception of adapted deep learning\narchitectures. First, this thesis details approaches to tackle the current lack\nof data. A simple simulation as well as generative methods for creating\nannotated data will be presented. It will also describe the CARRADA dataset,\ncomposed of synchronised camera and RADAR data with a semi-automatic annotation\nmethod. This thesis then present a proposed set of deep learning architectures\nwith their associated loss functions for RADAR semantic segmentation. It also\nintroduces a method to open up research into the fusion of LiDAR and RADAR\nsensors for scene understanding. Finally, this thesis exposes a collaborative\ncontribution, the RADIal dataset with synchronised High-Definition (HD) RADAR,\nLiDAR and camera. A deep learning architecture is also proposed to estimate the\nRADAR signal processing pipeline while performing multitask learning for object\ndetection and free driving space segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1\">Arthur Ouaknine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Localisation and Mapping with Quadric Surfaces. (arXiv:2203.08040v1 [cs.RO])","link":"http://arxiv.org/abs/2203.08040","description":"<p>There are many possibilities for how to represent the map in simultaneous\nlocalisation and mapping (SLAM). While sparse, keypoint-based SLAM systems have\nachieved impressive levels of accuracy and robustness, their maps may not be\nsuitable for many robotic tasks. Dense SLAM systems are capable of producing\ndense reconstructions, but can be computationally expensive and, like sparse\nsystems, lack higher-level information about the structure of a scene.\nHuman-made environments contain a lot of structure, and we seek to take\nadvantage of this by enabling the use of quadric surfaces as features in SLAM\nsystems. We introduce a minimal representation for quadric surfaces and show\nhow this can be included in a least-squares formulation. We also show how our\nrepresentation can be easily extended to include additional constraints on\nquadrics such as those found in quadrics of revolution. Finally, we introduce a\nproof-of-concept SLAM system using our representation, and provide some\nexperimental results using an RGB-D dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-organ point cloud registration algorithm for abdominal CT registration. (arXiv:2203.08041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08041","description":"<p>Registering CT images of the chest is a crucial step for several tasks such\nas disease progression tracking or surgical planning. It is also a challenging\nstep because of the heterogeneous content of the human abdomen which implies\ncomplex deformations. In this work, we focus on accurately registering a subset\nof organs of interest. We register organ surface point clouds, as may typically\nbe extracted from an automatic segmentation pipeline, by expanding the Bayesian\nCoherent Point Drift algorithm (BCPD). We introduce MO-BCPD, a multi-organ\nversion of the BCPD algorithm which explicitly models three important aspects\nof this task: organ individual elastic properties, inter-organ motion coherence\nand segmentation inaccuracy. This model also provides an interpolation\nframework to estimate the deformation of the entire volume. We demonstrate the\nefficiency of our method by registering different patients from the LITS\nchallenge dataset. The target registration error on anatomical landmarks is\nalmost twice as small for MO-BCPD compared to standard BCPD while imposing the\nsame constraints on individual organs deformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pheiffer_T/0/1/0/all/0/1\">Thomas Pheiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_C/0/1/0/all/0/1\">Chloe Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wohlfahrt_P/0/1/0/all/0/1\">Patrick Wohlfahrt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piat_S/0/1/0/all/0/1\">Sebastien Piat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansi_T/0/1/0/all/0/1\">Tommaso Mansi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Hyperbolic Embeddings in 2D Object Detection. (arXiv:2203.08049v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08049","description":"<p>Object detection, for the most part, has been formulated in the euclidean\nspace, where euclidean or spherical geodesic distances measure the similarity\nof an image region to an object class prototype. In this work, we study whether\na hyperbolic geometry better matches the underlying structure of the object\nclassification space. We incorporate a hyperbolic classifier in two-stage,\nkeypoint-based, and transformer-based object detection architectures and\nevaluate them on large-scale, long-tailed, and zero-shot object detection\nbenchmarks. In our extensive experimental evaluations, we observe categorical\nclass hierarchies emerging in the structure of the classification space,\nresulting in lower classification errors and boosting the overall object\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Christopher Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1\">Alexander Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Commonness and Inconsistencies: A Jointly Smoothed Approach to Multi-view Subspace Clustering. (arXiv:2203.08060v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08060","description":"<p>Multi-view subspace clustering aims to discover the hidden subspace\nstructures from multiple views for robust clustering, and has been attracting\nconsiderable attention in recent years. Despite significant progress, most of\nthe previous multi-view subspace clustering algorithms are still faced with two\nlimitations. First, they usually focus on the consistency (or commonness) of\nmultiple views, yet often lack the ability to capture the cross-view\ninconsistencies in subspace representations. Second, many of them overlook the\nlocal structures of multiple views and cannot jointly leverage multiple local\nstructures to enhance the subspace representation learning. To address these\ntwo limitations, in this paper, we propose a jointly smoothed multi-view\nsubspace clustering (JSMC) approach. Specifically, we simultaneously\nincorporate the cross-view commonness and inconsistencies into the subspace\nrepresentation learning. The view-consensus grouping effect is presented to\njointly exploit the local structures of multiple views to regularize the\nview-commonness representation, which is further associated with the low-rank\nconstraint via the nuclear norm to strengthen its cluster structure. Thus the\ncross-view commonness and inconsistencies, the view-consensus grouping effect,\nand the low-rank representation are seamlessly incorporated into a unified\nobjective function, upon which an alternating optimization algorithm is\nperformed to achieve a robust subspace representation for clustering.\nExperimental results on a variety of real-world multi-view datasets have\nconfirmed the superiority of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaosha Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guang-Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionCLIP: Exposing Human Motion Generation to CLIP Space. (arXiv:2203.08063v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08063","description":"<p>We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent\nembedding that is disentangled, well behaved, and supports highly semantic\ntextual descriptions. MotionCLIP gains its unique power by aligning its latent\nspace with that of the Contrastive Language-Image Pre-training (CLIP) model.\nAligning the human motion manifold to CLIP space implicitly infuses the\nextremely rich semantic knowledge of CLIP into the manifold. In particular, it\nhelps continuity by placing semantically similar motions close to one another,\nand disentanglement, which is inherited from the CLIP-space structure.\nMotionCLIP comprises a transformer-based motion auto-encoder, trained to\nreconstruct motion while being aligned to its text label's position in\nCLIP-space. We further leverage CLIP's unique visual understanding and inject\nan even stronger signal through aligning motion to rendered frames in a\nself-supervised manner. We show that although CLIP has never seen the motion\ndomain, MotionCLIP offers unprecedented text-to-motion abilities, allowing\nout-of-domain actions, disentangled editing, and abstract language\nspecification. For example, the text prompt \"couch\" is decoded into a sitting\ndown motion, due to lingual similarity, and the prompt \"Spiderman\" results in a\nweb-swinging-like solution that is far from seen during training. In addition,\nwe show how the introduced latent space can be leveraged for motion\ninterpolation, editing and recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tevet_G/0/1/0/all/0/1\">Guy Tevet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_B/0/1/0/all/0/1\">Brian Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Amir Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08075","description":"<p>Spatial commonsense, the knowledge about spatial position and relationship\nbetween objects (like the relative size of a lion and a girl, and the position\nof a boy relative to a bicycle when cycling), is an important part of\ncommonsense knowledge. Although pretrained language models (PLMs) succeed in\nmany NLP tasks, they are shown to be ineffective in spatial commonsense\nreasoning. Starting from the observation that images are more likely to exhibit\nspatial commonsense than texts, we explore whether models with visual signals\nlearn more spatial commonsense than text-based PLMs. We propose a spatial\ncommonsense benchmark that focuses on the relative scales of objects, and the\npositional relationship between people and objects under different actions. We\nprobe PLMs and models with visual signals, including vision-language pretrained\nmodels and image synthesis models, on this benchmark, and find that image\nsynthesis models are more capable of learning accurate and consistent spatial\nknowledge than other models. The spatial knowledge from image synthesis models\nalso helps in natural language understanding tasks that require spatial\ncommonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08080","description":"<p>Quantization has been applied to multiple domains in Deep Neural Networks\n(DNNs). We propose Depthwise Quantization (DQ) where $\\textit{quantization}$ is\napplied to a decomposed sub-tensor along the $\\textit{feature axis}$ of weak\nstatistical dependence. The feature decomposition leads to an exponential\nincrease in $\\textit{representation capacity}$ with a linear increase in memory\nand parameter cost. In addition, DQ can be directly applied to existing\nencoder-decoder frameworks without modification of the DNN architecture. We use\nDQ in the context of Hierarchical Auto-Encoder and train end-to-end on an image\nfeature representation. We provide an analysis on cross-correlation between\nspatial and channel features and we propose a decomposition of the image\nfeature representation along the channel axis. The improved performance of the\ndepthwise operator is due to the increased representation capacity from\nimplicit feature decoupling. We evaluate DQ on the likelihood estimation task,\nwhere it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and\nImageNet-64. We progressively train with increasing image size a single\nhierarchical model that uses 69% less parameters and has a faster convergence\nthan the previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fostiropoulos_I/0/1/0/all/0/1\">Iordanis Fostiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boehm_B/0/1/0/all/0/1\">Barry Boehm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. (arXiv:2203.08101v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08101","description":"<p>An intuitive way to search for images is to use queries composed of an\nexample image and a complementary text. While the first provides rich and\nimplicit context for the search, the latter explicitly calls for new traits, or\nspecifies how some elements of the example image should be changed to retrieve\nthe desired target image. Current approaches typically combine the features of\neach of the two elements of the query into a single representation, which can\nthen be compared to the ones of the potential target images. Our work aims at\nshedding new light on the task by looking at it through the prism of two\nfamiliar and related frameworks: text-to-image and image-to-image retrieval.\nTaking inspiration from them, we exploit the specific relation of each query\nelement with the targeted image and derive light-weight attention mechanisms\nwhich enable to mediate between the two complementary modalities. We validate\nour approach on several retrieval benchmarks, querying with images and their\nassociated free-form text modifiers. Our method obtains state-of-the-art\nresults without resorting to side information, multi-level features, heavy\npre-training nor large architectures as in previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delmas_G/0/1/0/all/0/1\">Ginger Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From 2D to 3D: Re-thinking Benchmarking of Monocular Depth Prediction. (arXiv:2203.08122v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08122","description":"<p>There have been numerous recently proposed methods for monocular depth\nprediction (MDP) coupled with the equally rapid evolution of benchmarking\ntools. However, we argue that MDP is currently witnessing benchmark\nover-fitting and relying on metrics that are only partially helpful to gauge\nthe usefulness of the predictions for 3D applications. This limits the design\nand development of novel methods that are truly aware of - and improving\ntowards estimating - the 3D structure of the scene rather than optimizing\n2D-based distances. In this work, we aim to bring structural awareness to MDP,\nan inherently 3D task, by exhibiting the limits of evaluation metrics towards\nassessing the quality of the 3D geometry. We propose a set of metrics well\nsuited to evaluate the 3D geometry of MDP approaches and a novel indoor\nbenchmark, RIO-D3D, crucial for the proposed evaluation methodology. Our\nbenchmark is based on a real-world dataset featuring high-quality rendered\ndepth maps obtained from RGB-D reconstructions. We further demonstrate this to\nhelp benchmark the closely-tied task of 3D scene completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Shristi Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wald_J/0/1/0/all/0/1\">Johanna Wald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective. (arXiv:2203.08124v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08124","description":"<p>We discuss methods for visualizing neural network decision boundaries and\ndecision regions. We use these visualizations to investigate issues related to\nreproducibility and generalization in neural network training. We observe that\nchanges in model architecture (and its associate inductive bias) cause visible\nchanges in decision boundaries, while multiple runs with the same architecture\nyield results with strong similarities, especially in the case of wide\narchitectures. We also use decision boundary methods to visualize double\ndescent phenomena. We see that decision boundary reproducibility depends\nstrongly on model width. Near the threshold of interpolation, neural network\ndecision boundaries become fragmented into many small decision regions, and\nthese regions are non-reproducible. Meanwhile, very narrows and very wide\nnetworks have high levels of reproducibility in their decision boundaries with\nrelatively few decision regions. We discuss how our observations relate to the\ntheory of double descent phenomena in convex models. Code is available at\nhttps://github.com/somepago/dbViz\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1\">Gowthami Somepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Arpit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Chiang_P/0/1/0/all/0/1\">Ping Yeh-Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1\">Yehuda Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning. (arXiv:2203.08130v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08130","description":"<p>The current literature on self-supervised learning (SSL) focuses on\ndeveloping learning objectives to train neural networks more effectively on\nunlabeled data. The typical development process involves taking\nwell-established architectures, e.g., ResNet demonstrated on ImageNet, and\nusing them to evaluate newly developed objectives on downstream scenarios.\nWhile convenient, this does not take into account the role of architectures\nwhich has been shown to be crucial in the supervised learning literature. In\nthis work, we establish extensive empirical evidence showing that a network\narchitecture plays a significant role in SSL. We conduct a large-scale study\nwith over 100 variants of ResNet and MobileNet architectures and evaluate them\nacross 11 downstream scenarios in the SSL setting. We show that there is no one\nnetwork that performs consistently well across the scenarios. Based on this, we\npropose to learn not only network weights but also architecture topologies in\nthe SSL regime. We show that \"self-supervised architectures\" outperform popular\nhandcrafted architectures (ResNet18 and MobileNetV2) while performing\ncompetitively with the larger and computationally heavy ResNet50 on major image\nclassification benchmarks (ImageNet-1K, iNat2021, and more). Our results\nsuggest that it is time to consider moving beyond handcrafted architectures in\nSSL and start thinking about incorporating architecture search into\nself-supervised learning objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girish_S/0/1/0/all/0/1\">Sharath Girish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shital Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_C/0/1/0/all/0/1\">Caio Cesar Teodoro Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animatable Neural Implicit Surfaces for Creating Avatars from Videos. (arXiv:2203.08133v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08133","description":"<p>This paper aims to reconstruct an animatable human model from a video of very\nsparse camera views. Some recent works represent human geometry and appearance\nwith neural radiance fields and utilize parametric human models to produce\ndeformation fields for animation, which enables them to recover detailed 3D\nhuman models from videos. However, their reconstruction results tend to be\nnoisy due to the lack of surface constraints on radiance fields. Moreover, as\nthey generate the human appearance in 3D space, their rendering quality heavily\ndepends on the accuracy of deformation fields. To solve these problems, we\npropose Animatable Neural Implicit Surface (AniSDF), which models the human\ngeometry with a signed distance field and defers the appearance generation to\nthe 2D image space with a 2D neural renderer. The signed distance field\nnaturally regularizes the learned geometry, enabling the high-quality\nreconstruction of human bodies, which can be further used to improve the\nrendering speed. Moreover, the 2D neural renderer can be learned to compensate\nfor geometric errors, making the rendering more robust to inaccurate\ndeformations. Experiments on several datasets show that the proposed approach\noutperforms recent human reconstruction and synthesis methods by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_C/0/1/0/all/0/1\">Chen Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08138","description":"<p>Cryo-electron microscopy (cryo-EM) has become a tool of fundamental\nimportance in structural biology, helping us understand the basic building\nblocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the\nunknown 3D poses and the 3D electron scattering potential of a biomolecule from\nmillions of extremely noisy 2D images. Existing reconstruction algorithms,\nhowever, cannot easily keep pace with the rapidly growing size of cryo-EM\ndatasets due to their high computational and memory cost. We introduce cryoAI,\nan ab initio reconstruction algorithm for homogeneous conformations that uses\ndirect gradient-based optimization of particle poses and the electron\nscattering potential from single-particle cryo-EM data. CryoAI combines a\nlearned encoder that predicts the poses of each particle image with a\nphysics-based decoder to aggregate each particle image into an implicit\nrepresentation of the scattering potential volume. This volume is stored in the\nFourier domain for computational efficiency and leverages a modern coordinate\nnetwork architecture for memory efficiency. Combined with a symmetrized loss\nfunction, this framework achieves results of a quality on par with\nstate-of-the-art cryo-EM solvers for both simulated and experimental data, one\norder of magnitude faster for large datasets and with significantly lower\nmemory requirements than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poitevin_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Poitevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nashed_Y/0/1/0/all/0/1\">Youssef Nashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peck_A/0/1/0/all/0/1\">Ariana Peck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_D/0/1/0/all/0/1\">Daniel Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_M/0/1/0/all/0/1\">Mike Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Spatio-Temporal Downsampling for Effective Video Upscaling. (arXiv:2203.08140v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08140","description":"<p>Downsampling is one of the most basic image processing operations. Improper\nspatio-temporal downsampling applied on videos can cause aliasing issues such\nas moir\\'e patterns in space and the wagon-wheel effect in time. Consequently,\nthe inverse task of upscaling a low-resolution, low frame-rate video in space\nand time becomes a challenging ill-posed problem due to information loss and\naliasing artifacts. In this paper, we aim to solve the space-time aliasing\nproblem by learning a spatio-temporal downsampler. Towards this goal, we\npropose a neural network framework that jointly learns spatio-temporal\ndownsampling and upsampling. It enables the downsampler to retain the key\npatterns of the original video and maximizes the reconstruction performance of\nthe upsampler. To make the downsamping results compatible with popular image\nand video storage formats, the downsampling results are encoded to uint8 with a\ndifferentiable quantization layer. To fully utilize the space-time\ncorrespondences, we propose two novel modules for explicit temporal propagation\nand space-time feature rearrangement. Experimental results show that our\nproposed method significantly boosts the space-time reconstruction quality by\npreserving spatial textures and motion patterns in both downsampling and\nupscaling. Moreover, our framework enables a variety of applications, including\narbitrary video resampling, blurry frame reconstruction, and efficient video\nstorage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengarajan_V/0/1/0/all/0/1\">Vijay Rengarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_L/0/1/0/all/0/1\">Lucas Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Manipulation via Visual Target Localization. (arXiv:2203.08141v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08141","description":"<p>Object manipulation is a critical skill required for Embodied AI agents\ninteracting with the world around them. Training agents to manipulate objects,\nposes many challenges. These include occlusion of the target object by the\nagent's arm, noisy object detection and localization, and the target frequently\ngoing out of view as the agent moves around in the scene. We propose\nManipulation via Visual Object Location Estimation (m-VOLE), an approach that\nexplores the environment in search for target objects, computes their 3D\ncoordinates once they are located, and then continues to estimate their 3D\nlocations even when the objects are not visible, thus robustly aiding the task\nof manipulating these objects throughout the episode. Our evaluations show a\nmassive 3x improvement in success rate over a model that has access to the same\nsensory suite but is trained without the object location estimator, and our\nanalysis shows that our agent is robust to noise in depth perception and agent\nlocalization. Importantly, our proposed approach relaxes several assumptions\nabout idealized localization and perception that are commonly employed by\nrecent works in embodied AI -- an important step towards training agents for\nobject manipulation in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Neural Network for Photo-realistic Image Super-Resolution. (arXiv:1903.02240v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.02240","description":"<p>Recent progress in deep learning-based models has improved photo-realistic\n(or perceptual) single-image super-resolution significantly. However, despite\ntheir powerful performance, many methods are difficult to apply to real-world\napplications because of the heavy computational requirements. To facilitate the\nuse of a deep model under such demands, we focus on keeping the network\nefficient while maintaining its performance. In detail, we design an\narchitecture that implements a cascading mechanism on a residual network to\nboost the performance with limited resources via multi-level feature fusion. In\naddition, our proposed model adopts group convolution and recursive schemes in\norder to achieve extreme efficiency. We further improve the perceptual quality\nof the output by employing the adversarial learning paradigm and a multi-scale\ndiscriminator approach. The performance of our method is investigated through\nextensive internal experiments and benchmarks using various datasets. Our\nresults show that our models outperform the recent methods with similar\ncomplexity, for both traditional pixel-based and perception-based tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1\">Namhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kyung-Ah Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTP: Multi-Task Pruning for Efficient Semantic Segmentation Networks. (arXiv:2007.08386v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.08386","description":"<p>This paper focuses on channel pruning for semantic segmentation networks.\nPrevious methods to compress and accelerate deep neural networks in the\nclassification task cannot be straightforwardly applied to the semantic\nsegmentation network that involves an implicit multi-task learning problem via\npre-training. To identify the redundancy in segmentation networks, we present a\nmulti-task channel pruning approach. The importance of each convolution filter\n\\wrt the channel of an arbitrary layer will be simultaneously determined by the\nclassification and segmentation tasks. In addition, we develop an alternative\nscheme for optimizing importance scores of filters in the entire network.\nExperimental results on several benchmarks illustrate the superiority of the\nproposed algorithm over the state-of-the-art pruning methods. Notably, we can\nobtain an about $2\\times$ FLOPs reduction on DeepLabv3 with only an about $1\\%$\nmIoU drop on the PASCAL VOC 2012 dataset and an about $1.3\\%$ mIoU drop on\nCityscapes dataset, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Extrinsic Calibration Method for LiDAR and Camera Sensor Setups. (arXiv:2101.04431v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2101.04431","description":"<p>Most sensor setups for onboard autonomous perception are composed of LiDARs\nand vision systems, as they provide complementary information that improves the\nreliability of the different algorithms necessary to obtain a robust scene\nunderstanding. However, the effective use of information from different sources\nrequires an accurate calibration between the sensors involved, which usually\nimplies a tedious and burdensome process. We present a method to calibrate the\nextrinsic parameters of any pair of sensors involving LiDARs, monocular or\nstereo cameras, of the same or different modalities. The procedure is composed\nof two stages: first, reference points belonging to a custom calibration target\nare extracted from the data provided by the sensors to be calibrated, and\nsecond, the optimal rigid transformation is found through the registration of\nboth point sets. The proposed approach can handle devices with very different\nresolutions and poses, as usually found in vehicle setups. In order to assess\nthe performance of the proposed method, a novel evaluation suite built on top\nof a popular simulation framework is introduced. Experiments on the synthetic\nenvironment show that our calibration algorithm significantly outperforms\nexisting methods, whereas real data tests corroborate the results obtained in\nthe evaluation suite. Open-source code is available at\nhttps://github.com/beltransen/velo2cam_calibration\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1\">Jorge Beltr&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guindel_C/0/1/0/all/0/1\">Carlos Guindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_A/0/1/0/all/0/1\">Arturo de la Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_F/0/1/0/all/0/1\">Fernando Garc&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details. (arXiv:2102.01066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01066","description":"<p>By design, average precision (AP) for object detection aims to treat all\nclasses independently: AP is computed independently per category and averaged.\nOn one hand, this is desirable as it treats all classes equally. On the other\nhand, it ignores cross-category confidence calibration, a key property in\nreal-world use cases. Unfortunately, under important conditions (i.e., large\nvocabulary, high instance counts) the default implementation of AP is neither\ncategory independent, nor does it directly reward properly calibrated\ndetectors. In fact, we show that on LVIS the default implementation produces a\ngameable metric, where a simple, un-intuitive re-ranking policy can improve AP\nby a large margin. To address these limitations, we introduce two complementary\nmetrics. First, we present a simple fix to the default AP implementation,\nensuring that it is independent across categories as originally intended. We\nbenchmark recent LVIS detection advances and find that many reported gains do\nnot translate to improvements under our new evaluation, suggesting recent\nimprovements may arise from difficult to interpret changes to cross-category\nrankings. Given the importance of reliably benchmarking cross-category\nrankings, we consider a pooled version of AP (AP-Pool) that rewards properly\ncalibrated detectors by directly comparing cross-category rankings. Finally, we\nrevisit classical approaches for calibration and find that explicitly\ncalibrating detectors improves state-of-the-art on AP-Pool by 1.7 points\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1\">Achal Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building A Group-based Unsupervised Representation Disentanglement Framework. (arXiv:2102.10303v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10303","description":"<p>Disentangled representation learning is one of the major goals of deep\nlearning, and is a key step for achieving explainable and generalizable models.\nA well-defined theoretical guarantee still lacks for the VAE-based unsupervised\nmethods, which are a set of popular methods to achieve unsupervised\ndisentanglement. The Group Theory based definition of representation\ndisentanglement mathematically connects the data transformations to the\nrepresentations using the formalism of group. In this paper, built on the\ngroup-based definition and inspired by the n-th dihedral group, we first\npropose a theoretical framework towards achieving unsupervised representation\ndisentanglement. We then propose a model, based on existing VAE-based methods,\nto tackle the unsupervised learning problem of the framework. In the\ntheoretical framework, we prove three sufficient conditions on model, group\nstructure, and data respectively in an effort to achieve, in an unsupervised\nway, disentangled representation per group-based definition. With the first two\nof the conditions satisfied and a necessary condition derived for the third\none, we offer additional constraints, from the perspective of the group-based\ndefinition, for the existing VAE-based models. Experimentally, we train 1800\nmodels covering the most prominent VAE-based methods on five datasets to verify\nthe effectiveness of our theoretical framework. Compared to the original\nVAE-based methods, these Groupified VAEs consistently achieve better mean\nperformance with smaller variances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13027","description":"<p>Data mixing augmentation have proved to be effective for improving the\ngeneralization ability of deep neural networks. While early methods mix samples\nby hand-crafted policies (e.g., linear interpolation), recent methods utilize\nsaliency information to match the mixed samples and labels via complex offline\noptimization. However, there arises a trade-off between precise mixing policies\nand optimization complexity. To address this challenge, we propose a novel\nautomatic mixup (AutoMix) framework, where the mixup policy is parameterized\nand serves the ultimate classification goal directly. Specifically, AutoMix\nreformulates the mixup classification into two sub-tasks (i.e., mixed sample\ngeneration and mixup classification) with corresponding sub-networks and solves\nthem in a bi-level optimization framework. For the generation, a learnable\nlightweight mixup generator, Mix Block, is designed to generate mixed samples\nby modeling patch-wise relationships under the direct supervision of the\ncorresponding mixed labels. To prevent the degradation and instability of\nbi-level optimization, we further introduce a momentum pipeline to train\nAutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks\nprove the superiority of AutoMix compared with state-of-the-arts in various\nclassification scenarios and downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural RGB-D Surface Reconstruction. (arXiv:2104.04532v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04532","description":"<p>Obtaining high-quality 3D reconstructions of room-scale scenes is of\nparamount importance for upcoming applications in AR or VR. These range from\nmixed reality applications for teleconferencing, virtual measuring, virtual\nroom planing, to robotic applications. While current volume-based view\nsynthesis methods that use neural radiance fields (NeRFs) show promising\nresults in reproducing the appearance of an object or scene, they do not\nreconstruct an actual surface. The volumetric representation of the surface\nbased on densities leads to artifacts when a surface is extracted using\nMarching Cubes, since during optimization, densities are accumulated along the\nray and are not used at a single sample point in isolation. Instead of this\nvolumetric representation of the surface, we propose to represent the surface\nusing an implicit function (truncated signed distance function). We show how to\nincorporate this representation in the NeRF framework, and extend it to use\ndepth measurements from a commodity RGB-D sensor, such as a Kinect. In\naddition, we propose a pose and camera refinement technique which improves the\noverall reconstruction quality. In contrast to concurrent work on integrating\ndepth priors in NeRF which concentrates on novel view synthesis, our approach\nis able to reconstruct high-quality, metrical 3D reconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azinovic_D/0/1/0/all/0/1\">Dejan Azinovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04263","description":"<p>Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity. Code\nis available at https://github.com/Atten4Vis/DemystifyLocalViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04732","description":"<p>We extend semi-supervised learning to the problem of domain adaptation to\nlearn significantly higher-accuracy models that train on one data distribution\nand test on a different one. With the goal of generality, we introduce\nAdaMatch, a method that unifies the tasks of unsupervised domain adaptation\n(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation\n(SSDA). In an extensive experimental study, we compare its behavior with\nrespective state-of-the-art techniques from SSL, SSDA, and UDA on vision\nclassification tasks. We find AdaMatch either matches or significantly exceeds\nthe state-of-the-art in each case using the same hyper-parameters regardless of\nthe dataset or task. For example, AdaMatch nearly doubles the accuracy compared\nto that of the prior state-of-the-art on the UDA task for DomainNet and even\nexceeds the accuracy of the prior state-of-the-art obtained with pre-training\nby 6.4% when AdaMatch is trained completely from scratch. Furthermore, by\nproviding AdaMatch with just one labeled example per class from the target\ndomain (i.e., the SSDA setting), we increase the target accuracy by an\nadditional 6.1%, and with 5 labeled examples, by 13.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1\">David Berthelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alex Kurakin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models as a Data Source for Multiview Representation Learning. (arXiv:2106.05258v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05258","description":"<p>Generative models are now capable of producing highly realistic images that\nlook nearly indistinguishable from the data on which they are trained. This\nraises the question: if we have good enough generative models, do we still need\ndatasets? We investigate this question in the setting of learning\ngeneral-purpose visual representations from a black-box generative model rather\nthan directly from data. Given an off-the-shelf image generator without any\naccess to its training data, we train representations from the samples output\nby this generator. We compare several representation learning methods that can\nbe applied to this setting, using the latent space of the generator to generate\nmultiple \"views\" of the same semantic content. We show that for contrastive\nmethods, this multiview data can naturally be used to identify positive pairs\n(nearby in latent space) and negative pairs (far apart in latent space). We\nfind that the resulting representations rival or even outperform those learned\ndirectly from real data, but that good performance requires care in the\nsampling strategy applied and the training method. Generative models can be\nviewed as a compressed and organized copy of a dataset, and we envision a\nfuture where more and more \"model zoos\" proliferate while datasets become\nincreasingly unwieldy, missing, or private. This paper suggests several\ntechniques for dealing with visual representation learning in such a future.\nCode is available on our project page https://ali-design.github.io/GenRep/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1\">Ali Jahanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.12534","description":"<p>We present a coarse-to-fine discretisation method that enables the use of\ndiscrete reinforcement learning approaches in place of unstable and\ndata-inefficient actor-critic methods in continuous robotics domains. This\napproach builds on the recently released ARM algorithm, which replaces the\ncontinuous next-best pose agent with a discrete one, with coarse-to-fine\nQ-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what\npart of the scene to 'zoom' into. When this 'zooming' behaviour is applied\niteratively, it results in a near-lossless discretisation of the translation\nspace, and allows the use of a discrete action, deep Q-learning method. We show\nthat our new coarse-to-fine algorithm achieves state-of-the-art performance on\nseveral difficult sparsely rewarded RLBench vision-based robotics tasks, and\ncan train real-world policies, tabula rasa, in a matter of minutes, with as\nlittle as 3 demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From General to Specific: Online Updating for Blind Super-Resolution. (arXiv:2107.02398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02398","description":"<p>Most deep learning-based super-resolution (SR) methods are not\nimage-specific: 1) They are trained on samples synthesized by predefined\ndegradations (e.g. bicubic downsampling), regardless of the domain gap between\ntraining and testing data. 2) During testing, they super-resolve all images by\nthe same set of model weights, ignoring the degradation variety. As a result,\nmost previous methods may suffer a performance drop when the degradations of\ntest images are unknown and various (i.e. the case of blind SR). To address\nthese issues, we propose an online SR (ONSR) method. It does not rely on\npredefined degradations and allows the model weights to be updated according to\nthe degradation of the test image. Specifically, ONSR consists of two branches,\nnamely internal branch (IB) and external branch (EB). IB could learn the\nspecific degradation of the given test LR image, and EB could learn to super\nresolve images degraded by the learned degradation. In this way, ONSR could\ncustomize a specific model for each test image, and thus get more robust to\nvarious degradations. Extensive experiments on both synthesized and real-world\nimages show that ONSR can generate more visually favorable SR results and\nachieve state-of-the-art performance in blind SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengxiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuwu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08712","description":"<p>By considering the spatial correspondence, dense self-supervised\nrepresentation learning has achieved superior performance on various dense\nprediction tasks. However, the pixel-level correspondence tends to be noisy\nbecause of many similar misleading pixels, e.g., backgrounds. To address this\nissue, in this paper, we propose to explore \\textbf{set} \\textbf{sim}ilarity\n(SetSim) for dense self-supervised representation learning. We generalize\npixel-wise similarity learning to set-wise one to improve the robustness\nbecause sets contain more semantic and structure information. Specifically, by\nresorting to attentional features of views, we establish corresponding sets,\nthus filtering out noisy backgrounds that may cause incorrect correspondences.\nMeanwhile, these attentional features can keep the coherence of the same image\nacross different views to alleviate semantic inconsistency. We further search\nthe cross-view nearest neighbours of sets and employ the structured\nneighbourhood information to enhance the robustness. Empirical evaluations\ndemonstrate that SetSim is superior to state-of-the-art methods on object\ndetection, keypoint detection, instance segmentation, and semantic\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11214","description":"<p>We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization Challenge. (arXiv:2108.11269v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11269","description":"<p>Assessing the Mitotic Count has a known high degree of intra- and inter-rater\nvariability. Computer-aided systems have proven to decrease this variability\nand reduce labeling time. These systems, however, are generally highly\ndependent on their training domain and show poor applicability to unseen\ndomains. In histopathology, these domain shifts can result from various\nsources, including different slide scanning systems used to digitize histologic\nsamples. The MItosis DOmain Generalization challenge focused on this specific\ndomain shift for the task of mitotic figure detection. This work presents a\nmitotic figure detection algorithm developed as a baseline for the challenge,\nbased on domain adversarial training. On the challenge's test set, the\nalgorithm scored an F$_1$ score of 0.7183. The corresponding network weights\nand code for implementing the network are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Application of Convolutional Neural Networks for Tomographic Reconstruction of Hyperspectral Images. (arXiv:2108.13458v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13458","description":"<p>A novel method, utilizing convolutional neural networks (CNNs), is proposed\nto reconstruct hyperspectral cubes from computed tomography imaging\nspectrometer (CTIS) images. Current reconstruction algorithms are usually\nsubject to long reconstruction times and mediocre precision in cases of a large\nnumber of spectral channels. The constructed CNNs deliver higher precision and\nshorter reconstruction time than a sparse expectation maximization algorithm.\nIn addition, the network can handle two different types of real-world images at\nthe same time -- specifically ColorChecker and carrot spectral images are\nconsidered. This work paves the way toward real-time reconstruction of\nhyperspectral cubes from CTIS images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wei-Chih Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peters_M/0/1/0/all/0/1\">Mads Svanborg Peters</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlebaek_M/0/1/0/all/0/1\">Mads Juul Ahlebaek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frandsen_M/0/1/0/all/0/1\">Mads Toudal Frandsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eriksen_R/0/1/0/all/0/1\">Ren&#xe9; Lynge Eriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorgensen_B/0/1/0/all/0/1\">Bjarke J&#xf8;rgensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Distribution Alignment via Adversarial Learning for Domain Adaptive Object Detection. (arXiv:2109.09033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09033","description":"<p>Unsupervised domain adaptive object detection aims to adapt a well-trained\ndetector from its original source domain with rich labeled data to a new target\ndomain with unlabeled data. Recently, mainstream approaches perform this task\nthrough adversarial learning, yet still suffer from two limitations. First,\nthey mainly align marginal distribution by unsupervised cross-domain feature\nmatching, and ignore each feature's categorical and positional information that\ncan be exploited for conditional alignment; Second, they treat all classes as\nequally important for transferring cross-domain knowledge and ignore that\ndifferent classes usually have different transferability. In this paper, we\npropose a joint adaptive detection framework (JADF) to address the above\nchallenges. First, an end-to-end joint adversarial adaptation framework for\nobject detection is proposed, which aligns both marginal and conditional\ndistributions between domains without introducing any extra hyperparameter.\nNext, to consider the transferability of each object class, a metric for\nclass-wise transferability assessment is proposed, which is incorporated into\nthe JADF objective for domain adaptation. Further, an extended study from\nunsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation\n(UFDA) is conducted, where only a few unlabeled training images are available\nin unlabeled target domain. Extensive experiments validate that JADF is\neffective in both the UDA and UFDA settings, achieving significant performance\ngains over existing state-of-the-art cross-domain detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoyao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10115","description":"<p>We present a large-scale stereo RGB image object pose estimation dataset\nnamed the $\\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address\nchallenging cases such as object transparency, translucency, and specular\nreflection, in addition to the common challenges of occlusion, symmetry, and\nvariations in illumination and environments. In order to collect data of\nsufficient scale for modern deep learning models, we propose a novel method for\nefficiently annotating pose data in a multi-view fashion that allows data\ncapturing in complex and flexible environments. Fully annotated with 6D object\nposes, our dataset contains over 393K frames and over 1.5M annotations of 18\nobjects recorded in 182 scenes constructed in 11 different environments. The 18\nobjects include 8 symmetric objects, 7 transparent objects, and 8 reflective\nobjects. We benchmark two state-of-the-art pose estimation frameworks on\nStereOBJ-1M as baselines for future work. We also propose a novel object-level\npose optimization method for computing 6D pose from keypoint predictions in\nmultiple images. Project website: https://sites.google.com/view/stereobj-1m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMix: A Density-Aware Mixup Augmentation for Single Image Dehazing under Domain Shift. (arXiv:2109.12544v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12544","description":"<p>Deep learning-based methods have achieved considerable success on single\nimage dehazing in recent years. However, these methods are often subject to\nperformance degradation when domain shifts are confronted. Specifically, haze\ndensity gaps exist among the existing datasets, often resulting in poor\nperformance when these methods are tested across datasets. To address this\nissue, we propose a density-aware mixup augmentation (DAMix). DAMix generates\nsamples in an attempt to minimize the Wasserstein distance with the hazy images\nin the target domain. These DAMix-ed samples not only mitigate domain gaps but\nare also proven to comply with the atmospheric scattering model. Thus, DAMix\nachieves comprehensive improvements on domain adaptation. Furthermore, we show\nthat DAMix is helpful with respect to data efficiency. Specifically, a network\ntrained with half of the source dataset using DAMix can achieve even better\nadaptivity than that trained with the whole source dataset but without DAMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chia-Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Nan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.13930v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.13930","description":"<p>Semi-supervised learning has substantially advanced medical image\nsegmentation since it alleviates the heavy burden of acquiring the costly\nexpert-examined annotations. Especially, the consistency-based approaches have\nattracted more attention for their superior performance, wherein the real\nlabels are only utilized to supervise their paired images via supervised loss\nwhile the unlabeled images are exploited by enforcing the perturbation-based\n\\textit{\"unsupervised\"} consistency without explicit guidance from those real\nlabels. However, intuitively, the expert-examined real labels contain more\nreliable supervision signals. Observing this, we ask an unexplored but\ninteresting question: can we exploit the unlabeled data via explicit real label\nsupervision for semi-supervised training? To this end, we discard the previous\nperturbation-based consistency but absorb the essence of non-parametric\nprototype learning. Based on the prototypical network, we then propose a novel\ncyclic prototype consistency learning (CPCL) framework, which is constructed by\na labeled-to-unlabeled (L2U) prototypical forward process and an\nunlabeled-to-labeled (U2L) backward process. Such two processes synergistically\nenhance the segmentation network by encouraging more discriminative and compact\nfeatures. In this way, our framework turns previous \\textit{\"unsupervised\"}\nconsistency into new \\textit{\"supervised\"} consistency, obtaining the\n\\textit{\"all-around real label supervision\"} property of our method. Extensive\nexperiments on brain tumor segmentation from MRI and kidney segmentation from\nCT images show that our CPCL can effectively exploit the unlabeled data and\noutperform other state-of-the-art semi-supervised medical image segmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05922","description":"<p>\"The power of a generalization system follows directly from its biases\"\n(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --\nbut to what degree have we understood how their inductive bias influences model\ndecisions? We here attempt to disentangle the various aspects that determine\nhow a model decides. In particular, we ask: what makes one model decide\ndifferently from another? In a meticulously controlled setting, we find that\n(1.) irrespective of the network architecture or objective (e.g.\nself-supervised, semi-supervised, vision transformers, recurrent models) all\nmodels end up with a similar decision boundary. (2.) To understand these\nfindings, we analysed model decisions on the ImageNet validation set from epoch\nto epoch and image by image. We find that the ImageNet validation set, among\nothers, suffers from dichotomous data difficulty (DDD): For the range of\ninvestigated models and their accuracies, it is dominated by 46.0% \"trivial\"\nand 11.5% \"impossible\" images (beyond label errors). Only 42.5% of the images\ncould possibly be responsible for the differences between two models' decision\nboundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us\nto see pronounced differences between models. (4.) Humans are highly accurate\nat predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%).\nThis implies that in future comparisons of brains, machines and behaviour, much\nmay be gained from investigating the decisive role of images and the\ndistribution of their difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meding_K/0/1/0/all/0/1\">Kristof Meding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschoff_L/0/1/0/all/0/1\">Luca M. Schulze Buschoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1\">Felix A. Wichmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06850","description":"<p>Recently, Zhang et al. (2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\n$\\ell_\\infty$ robustness by its construction. Despite the novel design and\ntheoretical foundation, so far the model only achieved comparable performance\nto conventional networks. In this paper, we make the following two\ncontributions: $\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets\nenjoy a fundamental advantage in certified robustness over conventional\nnetworks (under typical certification approaches); $\\mathrm{(ii)}$ With an\nimproved training process we are able to significantly boost the certified\naccuracy of $\\ell_\\infty$-distance nets. Our training approach largely\nalleviates the optimization problem that arose in the previous training scheme,\nin particular, the unexpected large Lipschitz constant due to the use of a\ncrucial trick called $\\ell_p$-relaxation. The core of our training approach is\na novel objective function that combines scaled cross-entropy loss and clipped\nhinge loss with a decaying mixing coefficient. Experiments show that using the\nproposed training strategy, the certified accuracy of $\\ell_\\infty$-distance\nnet can be dramatically improved from 33.30% to 40.06% on CIFAR-10\n($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a\nlarge margin. Our results clearly demonstrate the effectiveness and potential\nof $\\ell_\\infty$-distance net for certified robustness. Codes are available at\nhttps://github.com/zbh2047/L_inf-dist-net-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Du Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07165","description":"<p>Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Abhishek Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dubber: Dubbing for Videos According to Scripts. (arXiv:2110.08243v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.08243","description":"<p>Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given video\nfrom the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that\nutilizes the lip movement in the video to control the prosody of the generated\nspeech. Furthermore, an image-based speaker embedding (ISE) module is developed\nfor the multi-speaker setting, which enables Neural Dubber to generate speech\nwith a reasonable timbre according to the speaker's face. Experiments on the\nchemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show\nthat Neural Dubber can generate speech audios on par with state-of-the-art TTS\nmodels in terms of speech quality. Most importantly, both qualitative and\nquantitative evaluations show that Neural Dubber can control the prosody of\nsynthesized speech by the video, and generate high-fidelity speech temporally\nsynchronized with the video. Our project page is at\nhttps://tsinghua-mars-lab.github.io/NeuralDubber/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Chenxu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1\">Qiao Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tingle Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08484","description":"<p>Large pre-trained vision-language (VL) models can learn a new task with a\nhandful of examples and generalize to a new task without fine-tuning. However,\nthese VL models are hard to deploy for real-world applications due to their\nimpractically huge sizes and slow inference speed. To solve this limitation, we\nstudy prompt-based low-resource learning of VL tasks with our proposed method,\nFewVLM, relatively smaller than recent few-shot learners. For FewVLM, we\npre-train a sequence-to-sequence transformer model with prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we\nanalyze the effect of diverse prompts for few-shot tasks. Experimental results\non VQA show that FewVLM with prompt-based learning outperforms Frozen which is\n31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x\nlarger model, PICa. In our analysis, we observe that (1) prompts significantly\naffect zero-shot performance but marginally affect few-shot performance, (2)\nmodels with noisy prompts learn as quickly as hand-crafted prompts given larger\ntraining data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts\ncaptioning performance. Our code is publicly available at\n\\url{https://github.com/woojeongjin/FewVLM}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification. (arXiv:2110.09759v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09759","description":"<p>Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor\nthe condition of the human heart. By using deep neural networks (DNNs),\ninterpretation of ECG signals can be fully automated for the identification of\npotential abnormalities in a patient's heart in a fraction of a second. Studies\nhave shown that given a sufficiently large amount of training data, DNN\naccuracy for ECG classification could reach human-expert cardiologist level.\nHowever, despite of the excellent performance in classification accuracy, DNNs\nare highly vulnerable to adversarial noises that are subtle changes in the\ninput of a DNN and may lead to a wrong class-label prediction. It is\nchallenging and essential to improve robustness of DNNs against adversarial\nnoises, which are a threat to life-critical applications. In this work, we\nproposed a regularization method to improve DNN robustness from the perspective\nof noise-to-signal ratio (NSR) for the application of ECG signal\nclassification. We evaluated our method on PhysioNet MIT-BIH dataset and\nCPSC2018 ECG dataset, and the results show that our method can substantially\nenhance DNN robustness against adversarial noises generated from adversarial\nattacks, with a minimal change in accuracy on clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Illiterate DALL-E Learns to Compose. (arXiv:2110.11405v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11405","description":"<p>Although DALL-E has shown an impressive ability of composition-based\nsystematic generalization in image generation, it requires the dataset of\ntext-image pairs and the compositionality is provided by the text. In contrast,\nobject-centric representation models like the Slot Attention model learn\ncomposable representations without the text prompt. However, unlike DALL-E its\nability to systematically generalize for zero-shot generation is significantly\nlimited. In this paper, we propose a simple but novel slot-based autoencoding\narchitecture, called SLATE, for combining the best of both worlds: learning\nobject-centric representations that allows systematic generalization in\nzero-shot image generation without text. As such, this model can also be seen\nas an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing\nobject-centric representation models, we propose to use the Image GPT decoder\nconditioned on the slots for capturing complex interactions among the slots and\npixels. In experiments, we show that this simple and easy-to-implement\narchitecture not requiring a text prompt achieves significant improvement in\nin-distribution and out-of-distribution (zero-shot) image generation and\nqualitatively comparable or better slot-attention structure than the models\nbased on mixture decoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SILT: Self-supervised Lighting Transfer Using Implicit Image Decomposition. (arXiv:2110.12914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12914","description":"<p>We present SILT, a Self-supervised Implicit Lighting Transfer method. Unlike\nprevious research on scene relighting, we do not seek to apply arbitrary new\nlighting configurations to a given scene. Instead, we wish to transfer the\nlighting style from a database of other scenes, to provide a uniform lighting\nstyle regardless of the input. The solution operates as a two-branch network\nthat first aims to map input images of any arbitrary lighting style to a\nunified domain, with extra guidance achieved through implicit image\ndecomposition. We then remap this unified input domain using a discriminator\nthat is presented with the generated outputs and the style reference, i.e.\nimages of the desired illumination conditions. Our method is shown to\noutperform supervised relighting solutions across two different datasets\nwithout requiring lighting supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kubiak_N/0/1/0/all/0/1\">Nikolina Kubiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1\">Armin Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillipson_G/0/1/0/all/0/1\">Graeme Phillipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolly_S/0/1/0/all/0/1\">Stephen Jolly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1\">Simon Hadfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariant Contrastive Learning. (arXiv:2111.00899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00899","description":"<p>In state-of-the-art self-supervised learning (SSL) pre-training produces\nsemantically good representations by encouraging them to be invariant under\nmeaningful transformations prescribed from human knowledge. In fact, the\nproperty of invariance is a trivial instance of a broader class called\nequivariance, which can be intuitively understood as the property that\nrepresentations transform according to the way the inputs transform. Here, we\nshow that rather than using only invariance, pre-training that encourages\nnon-trivial equivariance to some transformations, while maintaining invariance\nto other transformations, can be used to improve the semantic quality of\nrepresentations. Specifically, we extend popular SSL methods to a more general\nframework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL,\na simple additional pre-training objective encourages equivariance by\npredicting the transformations applied to the input. We demonstrate E-SSL's\neffectiveness empirically on several popular computer vision benchmarks, e.g.\nimproving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we\ndemonstrate usefulness of E-SSL for applications beyond computer vision; in\nparticular, we show its utility on regression problems in photonics science.\nOur code, datasets and pre-trained models are available at\nhttps://github.com/rdangovs/essl to aid further research in E-SSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1\">Rumen Dangovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Li Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_C/0/1/0/all/0/1\">Charlotte Loh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungwook Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Akash Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Semantic Segmentation of the Lumbar Spine: Clinical Applicability in a Multi-parametric and Multi-centre Study on Magnetic Resonance Images. (arXiv:2111.08712v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.08712","description":"<p>One of the major difficulties in medical image segmentation is the high\nvariability of these images, which is caused by their origin (multi-centre),\nthe acquisition protocols (multi-parametric), as well as the variability of\nhuman anatomy, the severity of the illness, the effect of age and gender, among\nothers. The problem addressed in this work is the automatic semantic\nsegmentation of lumbar spine Magnetic Resonance images using convolutional\nneural networks. The purpose is to assign a class label to each pixel of an\nimage. Classes were defined by radiologists and correspond to different\nstructural elements like vertebrae, intervertebral discs, nerves, blood\nvessels, and other tissues. The proposed network topologies are variants of the\nU-Net architecture. Several complementary blocks were used to define the\nvariants: Three types of convolutional blocks, spatial attention models, deep\nsupervision and multilevel feature extractor. This document describes the\ntopologies and analyses the results of the neural network designs that obtained\nthe most accurate segmentations. Several of the proposed designs outperform the\nstandard U-Net used as baseline, especially when used in ensembles where the\noutput of multiple neural networks is combined according to different\nstrategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saenz_Gamboa_J/0/1/0/all/0/1\">Jhon Jairo Saenz-Gamboa</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Domenech_J/0/1/0/all/0/1\">Julio Domenech</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Alonso_Manjarres_A/0/1/0/all/0/1\">Antonio Alonso-Manjarr&#xe9;s</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Gomez_J/0/1/0/all/0/1\">Jon A. G&#xf3;mez</a> (4), <a href=\"http://arxiv.org/find/eess/1/au:+Iglesia_Vaya_M/0/1/0/all/0/1\">Maria de la Iglesia-Vay&#xe1;</a> (1 and 5) ((1) FISABIO-CIPF Joint Research Unit in Biomedical Imaging - Val&#xe8;ncia Spain, (2) Orthopedic Surgery Department Hospital Arnau de Vilanova - Val&#xe8;ncia Spain, (3) Radiology Department Hospital Arnau de Vilanova - Val&#xe8;ncia Spain, (4) Pattern Recognition and Human Language Technology research center - Universitat Polit&#xe8;cnica de Val&#xe8;ncia, (5) Regional ministry of Universal Health and Public Health in Valencia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAANet: Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Density Level Estimation. (arXiv:2111.09515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09515","description":"<p>3D object detection from LiDAR data for autonomous driving has been making\nremarkable strides in recent years. Among the state-of-the-art methodologies,\nencoding point clouds into a bird's-eye view (BEV) has been demonstrated to be\nboth effective and efficient. Different from perspective views, BEV preserves\nrich spatial and distance information between objects; and while farther\nobjects of the same type do not appear smaller in the BEV, they contain sparser\npoint cloud features. This fact weakens BEV feature extraction using\nshared-weight convolutional neural networks. In order to address this\nchallenge, we propose Range-Aware Attention Network (RAANet), which extracts\nmore powerful BEV features and generates superior 3D object detections. The\nrange-aware attention (RAA) convolutions significantly improve feature\nextraction for near as well as far objects. Moreover, we propose a novel\nauxiliary loss for density estimation to further enhance the detection accuracy\nof RAANet for occluded objects. It is worth to note that our proposed RAA\nconvolution is lightweight and compatible to be integrated into any CNN\narchitecture used for the BEV detection. Extensive experiments on the nuScenes\ndataset demonstrate that our proposed approach outperforms the state-of-the-art\nmethods for LiDAR-based 3D object detection, with real-time inference speed of\n16 Hz for the full version and 22 Hz for the lite version. The code is publicly\navailable at an anonymous Github repository\nhttps://github.com/anonymous0522/RAAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yantao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xuetao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1\">Weiheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Muchenxuan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1\">Senem Velipasalar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUM : Mix Image Tiles and UnMix Feature Tiles for Semi-Supervised Object Detection. (arXiv:2111.10958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10958","description":"<p>Many recent semi-supervised learning (SSL) studies build teacher-student\narchitecture and train the student network by the generated supervisory signal\nfrom the teacher. Data augmentation strategy plays a significant role in the\nSSL framework since it is hard to create a weak-strong augmented input pair\nwithout losing label information. Especially when extending SSL to\nsemi-supervised object detection (SSOD), many strong augmentation methodologies\nrelated to image geometry and interpolation-regularization are hard to utilize\nsince they possibly hurt the location information of the bounding box in the\nobject detection task. To address this, we introduce a simple yet effective\ndata augmentation method, Mix/UnMix (MUM), which unmixes feature tiles for the\nmixed image tiles for the SSOD framework. Our proposed method makes mixed input\nimage tiles and reconstructs them in the feature space. Thus, MUM can enjoy the\ninterpolation-regularization effect from non-interpolated pseudo-labels and\nsuccessfully generate a meaningful weak-strong pair. Furthermore, MUM can be\neasily equipped on top of various SSOD methods. Extensive experiments on\nMS-COCO and PASCAL VOC datasets demonstrate the superiority of MUM by\nconsistently improving the mAP performance over the baseline in all the tested\nSSOD benchmark protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JongMok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jooyoung Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seunghyeon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jongkeun Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointMixer: MLP-Mixer for Point Cloud Understanding. (arXiv:2111.11187v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11187","description":"<p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs\nand transformer. Despite its simplicity compared to transformer, the concept of\nchannel-mixing MLPs and token-mixing MLPs achieves noticeable performance in\nvisual recognition tasks. Unlike images, point clouds are inherently sparse,\nunordered and irregular, which limits the direct use of MLP-Mixer for point\ncloud understanding. In this paper, we propose PointMixer, a universal point\nset operator that facilitates information sharing among unstructured 3D points.\nBy simply replacing token-mixing MLPs with a softmax function, PointMixer can\n\"mix\" features within/between point sets. By doing so, PointMixer can be\nbroadly used in the network as inter-set mixing, intra-set mixing, and pyramid\nmixing. Extensive experiments show the competitive or superior performance of\nPointMixer in semantic segmentation, classification, and point reconstruction\nagainst transformer-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Point Cloud Reconstruction. (arXiv:2111.11704v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11704","description":"<p>Point cloud obtained from 3D scanning is often sparse, noisy, and irregular.\nTo cope with these issues, recent studies have been separately conducted to\ndensify, denoise, and complete inaccurate point cloud. In this paper, we\nadvocate that jointly solving these tasks leads to significant improvement for\npoint cloud reconstruction. To this end, we propose a deep point cloud\nreconstruction network consisting of two stages: 1) a 3D sparse\nstacked-hourglass network as for the initial densification and denoising, 2) a\nrefinement via transformers converting the discrete voxels into 3D points. In\nparticular, we further improve the performance of transformer by a newly\nproposed module called amplified positional encoding. This module has been\ndesigned to differently amplify the magnitude of positional encoding vectors\nbased on the points' distances for adaptive refinements. Extensive experiments\ndemonstrate that our network achieves state-of-the-art performance among the\nrecent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets. Moreover,\nwe underline the ability of our network to generalize toward real-world and\nunmet scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joung_B/0/1/0/all/0/1\">Byeongin Joung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Object-Centric Learning from Video. (arXiv:2111.12594v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12594","description":"<p>Object-centric representations are a promising path toward more systematic\ngeneralization by providing flexible abstractions upon which compositional\nworld models can be built. Recent work on simple 2D and 3D datasets has shown\nthat models with object-centric inductive biases can learn to segment and\nrepresent meaningful objects from the statistical structure of the data alone\nwithout the need for any supervision. However, such fully-unsupervised methods\nstill fail to scale to diverse realistic data, despite the use of increasingly\ncomplex inductive biases such as priors for the size of objects or the 3D\ngeometry of the scene. In this paper, we instead take a weakly-supervised\napproach and focus on how 1) using the temporal dynamics of video data in the\nform of optical flow and 2) conditioning the model on simple object location\ncues can be used to enable segmenting and tracking objects in significantly\nmore realistic synthetic data. We introduce a sequential extension to Slot\nAttention which we train to predict optical flow for realistic looking\nsynthetic scenes and show that conditioning the initial state of this model on\na small set of hints, such as center of mass of objects in the first frame, is\nsufficient to significantly improve instance segmentation. These benefits\ngeneralize beyond the training distribution to novel objects, novel\nbackgrounds, and to longer video sequences. We also find that such\ninitial-state-conditioning can be used during inference as a flexible interface\nto query the model for specific objects or parts of objects, which could pave\nthe way for a range of weakly-supervised approaches and allow more effective\ninteraction with trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin F. Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1\">Aravindh Mahendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sara Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1\">Georg Heigold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonschkowski_R/0/1/0/all/0/1\">Rico Jonschkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greff_K/0/1/0/all/0/1\">Klaus Greff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Active Learning Using Pseudo-Domains for Limited Labelling Resources and Changing Acquisition Characteristics. (arXiv:2111.13069v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13069","description":"<p>Machine learning in medical imaging during clinical routine is impaired by\nchanges in scanner protocols, hardware, or policies resulting in a\nheterogeneous set of acquisition settings. When training a deep learning model\non an initial static training set, model performance and reliability suffer\nfrom changes of acquisition characteristics as data and targets may become\ninconsistent. Continual learning can help to adapt models to the changing\nenvironment by training on a continuous data stream. However, continual manual\nexpert labelling of medical imaging requires substantial effort. Thus, ways to\nuse labelling resources efficiently on a well chosen sub-set of new examples is\nnecessary to render this strategy feasible.\n</p>\n<p>Here, we propose a method for continual active learning operating on a stream\nof medical images in a multi-scanner setting. The approach automatically\nrecognizes shifts in image acquisition characteristics - new domains -, selects\noptimal examples for labelling and adapts training accordingly. Labelling is\nsubject to a limited budget, resembling typical real world scenarios. To\ndemonstrate generalizability, we evaluate the effectiveness of our method on\nthree tasks: cardiac segmentation, lung nodule detection and brain age\nestimation. Results show that the proposed approach outperforms other active\nlearning methods, while effectively counteracting catastrophic forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perkonigg_M/0/1/0/all/0/1\">Matthias Perkonigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmanninger_J/0/1/0/all/0/1\">Johannes Hofmanninger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prosch_H/0/1/0/all/0/1\">Helmut Prosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langs_G/0/1/0/all/0/1\">Georg Langs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confounder Identification-free Causal Visual Feature Learning. (arXiv:2111.13420v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13420","description":"<p>Confounders in deep learning are in general detrimental to model's\ngeneralization where they infiltrate feature representations. Therefore,\nlearning causal features that are free of interference from confounders is\nimportant. Most previous causal learning based approaches employ back-door\ncriterion to mitigate the adverse effect of certain specific confounder, which\nrequire the explicit identification of confounder. However, in real scenarios,\nconfounders are typically diverse and difficult to be identified. In this\npaper, we propose a novel Confounder Identification-free Causal Visual Feature\nLearning (CICF) method, which obviates the need for identifying confounders.\nCICF models the interventions among different samples based on front-door\ncriterion, and then approximates the global-scope intervening effect upon the\ninstance-level interventions from the perspective of optimization. In this way,\nwe aim to find a reliable optimization direction, which avoids the intervening\neffects of confounders, to learn causal features. Furthermore, we uncover the\nrelation between CICF and the popular meta-learning strategy MAML, and provide\nan interpretation of why MAML works from the theoretical perspective of causal\nlearning for the first time. Thanks to the effective learning of causal\nfeatures, our CICF enables models to have superior generalization capability.\nExtensive experiments on domain generalization benchmark datasets demonstrate\nthe effectiveness of our CICF, which achieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDGNet: Class Distribution Guided Network for Human Parsing. (arXiv:2111.14173v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14173","description":"<p>The objective of human parsing is to partition a human in an image into\nconstituent parts. This task involves labeling each pixel of the human image\naccording to the classes. Since the human body comprises hierarchically\nstructured parts, each body part of an image can have its sole position\ndistribution characteristic. Probably, a human head is less likely to be under\nthe feet, and arms are more likely to be near the torso. Inspired by this\nobservation, we make instance class distributions by accumulating the original\nhuman parsing label in the horizontal and vertical directions, which can be\nutilized as supervision signals. Using these horizontal and vertical class\ndistribution labels, the network is guided to exploit the intrinsic position\ndistribution of each class. We combine two guided features to form a spatial\nguidance map, which is then superimposed onto the baseline network by\nmultiplication and concatenation to distinguish the human parts precisely. We\nconducted extensive experiments to demonstrate the effectiveness and\nsuperiority of our method on three well-known benchmarks: LIP, ATR, and CIHP\ndatabases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_O/0/1/0/all/0/1\">Ouk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshUDF: Fast and Differentiable Meshing of Unsigned Distance Field Networks. (arXiv:2111.14549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14549","description":"<p>Unsigned Distance Fields (UDFs) can be used to represent non-watertight\nsurfaces. However, current approaches to converting them into explicit meshes\ntend to either be expensive or to degrade the accuracy. Here, we extend the\nmarching cube algorithm to handle UDFs, both fast and accurately. Moreover, our\napproach to surface extraction is differentiable, which is key to using\npretrained UDF networks to fit sparse data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1\">Benoit Guillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1\">Federico Stella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRIS: CLIP-Driven Referring Image Segmentation. (arXiv:2111.15174v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15174","description":"<p>Referring image segmentation aims to segment a referent via a natural\nlinguistic expression.Due to the distinct data properties between text and\nimage, it is challenging for a network to well align text and pixel-level\nfeatures. Existing approaches use pretrained models to facilitate learning, yet\nseparately transfer the language/vision knowledge from pretrained models,\nignoring the multi-modal corresponding information. Inspired by the recent\nadvance in Contrastive Language-Image Pretraining (CLIP), in this paper, we\npropose an end-to-end CLIP-Driven Referring Image Segmentation framework\n(CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to\nvision-language decoding and contrastive learning for achieving the\ntext-to-pixel alignment. More specifically, we design a vision-language decoder\nto propagate fine-grained semantic information from textual representations to\neach pixel-level activation, which promotes consistency between the two\nmodalities. In addition, we present text-to-pixel contrastive learning to\nexplicitly enforce the text feature similar to the related pixel-level features\nand dissimilar to the irrelevances. The experimental results on three benchmark\ndatasets demonstrate that our proposed framework significantly outperforms the\nstate-of-the-art performance without any post-processing. The code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xunqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation. (arXiv:2112.04011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04011","description":"<p>Despite the outstanding success of self-supervised pretraining methods for\nvideo representation learning, they generalise poorly when the unlabeled\ndataset for pretraining is small or the domain difference between unlabelled\ndata in source task (pretraining) and labeled data in target task (finetuning)\nis significant. To mitigate these issues, we propose a novel approach to\ncomplement self-supervised pretraining via an auxiliary pretraining phase,\nbased on knowledge similarity distillation, auxSKD, for better generalisation\nwith a significantly smaller amount of video data, e.g. Kinetics-100 rather\nthan Kinetics-400. Our method deploys a teacher network that iteratively\ndistills its knowledge to the student model by capturing the similarity\ninformation between segments of unlabelled video data. The student model\nmeanwhile solves a pretext task by exploiting this prior knowledge. We also\nintroduce a novel pretext task, Video Segment Pace Prediction or VSPP, which\nrequires our model to predict the playback speed of a randomly selected segment\nof the input video to provide more reliable self-supervised representations.\nOur experimental results show superior results to the state of the art on both\nUCF101 and HMDB51 datasets when pretraining on K100 in apple-to-apple\ncomparisons. Additionally, we show that our auxiliary pretraining, auxSKD, when\nadded as an extra pretraining phase to recent state of the art self-supervised\nmethods (i.e. VCOP, VideoPace, and RSPNet), improves their results on UCF101\nand HMDB51. Our code is available at https://github.com/Plrbear/auxSKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1\">Amirhossein Dadashzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whone_A/0/1/0/all/0/1\">Alan Whone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsive Listening Head Generation: A Benchmark Dataset and Baseline. (arXiv:2112.13548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13548","description":"<p>We present a new listening head generation benchmark, for synthesizing\nresponsive feedbacks of a listener (e.g., nod, smile) during a face-to-face\nconversation. As the indispensable complement to talking heads generation,\nlistening head generation has seldomly been studied in literature.\nAutomatically synthesizing listening behavior that actively responds to a\ntalking head, is critical to applications such as digital human, virtual agents\nand social robots. In this work, we propose a novel dataset \"ViCo\",\nhighlighting the listening head generation during a face-to-face conversation.\nA total number of 92 identities (67 speakers and 76 listeners) are involved in\nViCo, featuring 483 clips in a paired \"speaking-listening\" pattern, where\nlisteners show three listening styles based on their attitudes: positive,\nneutral, negative. Different from traditional speech-to-gesture or talking-head\ngeneration, listening head generation takes as input both the audio and visual\nsignals from the speaker, and gives non-verbal feedbacks (e.g., head motions,\nfacial expressions) in a real-time manner. Our dataset supports a wide range of\napplications such as human-to-human interaction, video-to-video translation,\ncross-modal understanding and generation. To encourage further research, we\nalso release a listening head generation baseline, conditioning on different\nlistening attitudes. Project page: \\url{https://project.mhzhou.com/rld}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mohan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation. (arXiv:2201.01427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01427","description":"<p>Encoder-decoder models have been widely used in RGBD semantic segmentation,\nand most of them are designed via a two-stream network. In general, jointly\nreasoning the color and geometric information from RGBD is beneficial for\nsemantic segmentation. However, most existing approaches fail to\ncomprehensively utilize multimodal information in both the encoder and decoder.\nIn this paper, we propose a novel attention-based dual supervised decoder for\nRGBD semantic segmentation. In the encoder, we design a simple yet effective\nattention-based multimodal fusion module to extract and fuse deeply multi-level\npaired complementary information. To learn more robust deep representations and\nrich multi-modal information, we introduce a dual-branch decoder to effectively\nleverage the correlations and complementary cues of different tasks. Extensive\nexperiments on NYUDv2 and SUN-RGBD datasets demonstrate that our method\nachieves superior performance against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype Guided Network for Anomaly Segmentation. (arXiv:2201.05869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05869","description":"<p>Semantic segmentation methods can not directly identify abnormal objects in\nimages. Anomaly Segmentation algorithm from this realistic setting can\ndistinguish between in-distribution objects and Out-Of-Distribution (OOD)\nobjects and output the anomaly probability for pixels. In this paper, a\nPrototype Guided Anomaly segmentation Network (PGAN) is proposed to extract\nsemantic prototypes for in-distribution training data from limited annotated\nimages. In the model, prototypes are used to model the hierarchical category\nsemantic information and distinguish OOD pixels. The proposed PGAN model\nincludes a semantic segmentation network and a prototype extraction network.\nSimilarity measures are adopted to optimize the prototypes. The learned\nsemantic prototypes are used as category semantics to compare the similarity\nwith features extracted from test images and then to generate semantic\nsegmentation prediction. The proposed prototype extraction network can also be\nintegrated into most semantic segmentation networks and recognize OOD pixels.\nOn the StreetHazards dataset, the proposed PGAN model produced mIoU of 53.4%\nfor anomaly segmentation. The experimental results demonstrate PGAN may achieve\nthe SOTA performance in the anomaly segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yiqing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Gaoyun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08959","description":"<p>This work studies the problem of few-shot object counting, which counts the\nnumber of exemplar objects (i.e., described by one or several support images)\noccurring in the query image. The major challenge lies in that the target\nobjects can be densely packed in the query image, making it hard to recognize\nevery single one. To tackle the obstacle, we propose a novel learning block,\nequipped with a similarity comparison module (SCM) and a feature enhancement\nmodule (FEM). Concretely, given a support image and a query image, we first\nderive a score map by comparing their projected features at every spatial\nposition. The score maps regarding all support images are collected together\nand normalized across both the exemplar dimension and the spatial dimensions,\nproducing a reliable similarity map. We then enhance the query feature with the\nsupport features by employing the developed point-wise similarities as the\nweighting coefficients. Such a design encourages the model to inspect the query\nimage by focusing more on the regions akin to the support images, leading to\nmuch clearer boundaries between different objects. Extensive experiments on\nvarious benchmarks and training setups suggest that our method surpasses the\nstate-of-the-art approaches by a sufficiently large margin. For instance, on\nthe very recent large-scale FSC-147 dataset, we beat the second competitor by\nimproving the mean absolute counting error from 22.08 to 14.32 (35%\n$\\uparrow$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhiyuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques. (arXiv:2202.02521v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02521","description":"<p>Estimating and understanding the surroundings of the vehicle precisely forms\nthe basic and crucial step for the autonomous vehicle. The perception system\nplays a significant role in providing an accurate interpretation of a vehicle's\nenvironment in real-time. Generally, the perception system involves various\nsubsystems such as localization, obstacle (static and dynamic) detection, and\navoidance, mapping systems, and others. For perceiving the environment, these\nvehicles will be equipped with various exteroceptive (both passive and active)\nsensors in particular cameras, Radars, LiDARs, and others. These systems are\nequipped with deep learning techniques that transform the huge amount of data\nfrom the sensors into semantic information on which the object detection and\nlocalization tasks are performed. For numerous driving tasks, to provide\naccurate results, the location and depth information of a particular object is\nnecessary. 3D object detection methods, by utilizing the additional pose data\nfrom the sensors such as LiDARs, stereo cameras, provides information on the\nsize and location of the object. Based on recent research, 3D object detection\nframeworks performing object detection and localization on LiDAR data and\nsensor fusion techniques show significant improvement in their performance. In\nthis work, a comparative study of the effect of using LiDAR data for object\ndetection frameworks and the performance improvement seen by using sensor\nfusion techniques are performed. Along with discussing various state-of-the-art\nmethods in both the cases, performing experimental analysis, and providing\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venugopala_S/0/1/0/all/0/1\">Sreenivasa Hikkal Venugopala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal unsupervised brain image registration using edge maps. (arXiv:2202.04647v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04647","description":"<p>Diffeomorphic deformable multi-modal image registration is a challenging task\nwhich aims to bring images acquired by different modalities to the same\ncoordinate space and at the same time to preserve the topology and the\ninvertibility of the transformation. Recent research has focused on leveraging\ndeep learning approaches for this task as these have been shown to achieve\ncompetitive registration accuracy while being computationally more efficient\nthan traditional iterative registration methods. In this work, we propose a\nsimple yet effective unsupervised deep learning-based {\\em multi-modal} image\nregistration approach that benefits from auxiliary information coming from the\ngradient magnitude of the image, i.e. the image edges, during the training. The\nintuition behind this is that image locations with a strong gradient are\nassumed to denote a transition of tissues, which are locations of high\ninformation value able to act as a geometry constraint. The task is similar to\nusing segmentation maps to drive the training, but the edge maps are easier and\nfaster to acquire and do not require annotations. We evaluate our approach in\nthe context of registering multi-modal (T1w to T2w) magnetic resonance (MR)\nbrain images of different subjects using three different loss functions that\nare said to assist multi-modal registration, showing that in all cases the\nauxiliary information leads to better results without compromising the runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1\">Vasiliki Sideri-Lampretsa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Denoising via Momentum Ascent in Gradient Fields. (arXiv:2202.10094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10094","description":"<p>To achieve point cloud denoising, traditional methods heavily rely on\ngeometric priors, and most learning-based approaches suffer from outliers and\nloss of details. Recently, the gradient-based method was proposed to estimate\nthe gradient fields from the noisy point clouds using neural networks, and\nrefine the position of each point according to the estimated gradient. However,\nthe predicted gradient could fluctuate, leading to perturbed and unstable\nsolutions, as well as a large inference time. To address these issues, we\ndevelop the momentum gradient ascent method that leverages the information of\nprevious iterations in determining the trajectories of the points, thus\nimproving the stability of the solution and reducing the inference time.\nExperiments demonstrate that the proposed method outperforms state-of-the-art\nmethods with a variety of point clouds and noise levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Zigzag Flattening for Image Reading. (arXiv:2202.10240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10240","description":"<p>Sequence ordering of word vector matters a lot to text reading, which has\nbeen proven in natural language processing (NLP). However, the rule of\ndifferent sequence ordering in computer vision (CV) was not well explored,\ne.g., why the \"zigzag\" flattening (ZF) is commonly utilized as a default option\nto get the image patches ordering in vision transformers (ViTs). Notably, when\ndecomposing multi-scale images, the ZF could not maintain the invariance of\nfeature point positions. To this end, we investigate the Hilbert fractal\nflattening (HF) as another method for sequence ordering in CV and contrast it\nagainst ZF. The HF has proven to be superior to other curves in maintaining\nspatial locality, when performing multi-scale transformations of dimensional\nspace. And it can be easily plugged into most deep neural networks (DNNs).\nExtensive experiments demonstrate that it can yield consistent and significant\nperformance boosts for a variety of architectures. Finally, we hope that our\nstudies spark further research about the flattening strategy of image reading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingsong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shuguang Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Cairong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Looming from LiDAR. (arXiv:2202.10972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10972","description":"<p>Looming, traditionally defined as the relative expansion of objects in the\nobserver's retina, is a fundamental visual cue for perception of threat and can\nbe used to accomplish collision free navigation. The measurement of the looming\ncue is not only limited to vision, and can also be obtained from range sensors\nlike LiDAR (Light Detection and Ranging). In this article we present two\nmethods that process raw LiDAR data to estimate the looming cue. Using looming\nvalues we show how to obtain threat zones for collision avoidance tasks. The\nmethods are general enough to be suitable for any six-degree-of-freedom motion\nand can be implemented in real-time without the need for fine matching,\npoint-cloud registration, object classification or object segmentation.\nQuantitative results using the KITTI dataset shows advantages and limitations\nof the methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yepes_J/0/1/0/all/0/1\">Juan D. Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Daniel Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12403","description":"<p>We propose a reinforcement learning based approach to query object\nlocalization, for which an agent is trained to localize objects of interest\nspecified by a small exemplary set. We learn a transferable reward signal\nformulated using the exemplary set by ordinal metric learning. Our proposed\nmethod enables test-time policy adaptation to new environments where the reward\nsignals are not readily available, and outperforms fine-tuning approaches that\nare limited to annotated images. In addition, the transferable reward allows\nrepurposing the trained agent from one specific class to another class.\nExperiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tingfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shaobo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13100","description":"<p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm\nfor training classifiers that generalize over output spaces. In contrast to\nstandard classification, which treats classes as discrete symbols, SemSup\nrepresents them as dense vector features obtained from descriptions of classes\n(e.g., \"The cat is a small carnivorous mammal\"). This allows the output space\nto be unbounded (in the space of descriptions) and enables models to generalize\nboth over unseen inputs and unseen outputs (e.g. \"The aardvark is a nocturnal\nburrowing mammal with long ears\"). Specifically, SemSup enables four types of\ngeneralization, to -- (1) unseen class descriptions, (2) unseen classes, (3)\nunseen super-classes, and (4) unseen tasks. Through experiments on four\nclassification datasets across two variants (multi-class and multi-label), two\ninput modalities (text and images), and two output description modalities (text\nand JSON), we show that our SemSup models significantly outperform standard\nsupervised models and existing models that leverage word embeddings over class\nnames. For instance, our model outperforms baselines by 40% and 15% precision\npoints on unseen descriptions and classes, respectively, on a news\ncategorization dataset (RCV1). SemSup can serve as a pathway for scaling neural\nmodels to large unbounded output spaces and enabling better generalization and\nmodel reuse for unseen tasks and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlideNet: Global, Local and Intrinsic based Dense Embedding NETwork for Multi-category Attributes Prediction. (arXiv:2203.03079v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03079","description":"<p>Attaching attributes (such as color, shape, state, action) to object\ncategories is an important computer vision problem. Attribute prediction has\nseen exciting recent progress and is often formulated as a multi-label\nclassification problem. Yet significant challenges remain in: 1) predicting\ndiverse attributes over multiple categories, 2) modeling attributes-category\ndependency, 3) capturing both global and local scene context, and 4) predicting\nattributes of objects with low pixel-count. To address these issues, we propose\na novel multi-category attribute prediction deep architecture named GlideNet,\nwhich contains three distinct feature extractors. A global feature extractor\nrecognizes what objects are present in a scene, whereas a local one focuses on\nthe area surrounding the object of interest. Meanwhile, an intrinsic feature\nextractor uses an extension of standard convolution dubbed Informed Convolution\nto retrieve features of objects with low pixel-count. GlideNet uses gating\nmechanisms with binary masks and its self-learned category embedding to combine\nthe dense embeddings. Collectively, the Global-Local-Intrinsic blocks\ncomprehend the scene's global context while attending to the characteristics of\nthe local object of interest. Finally, using the combined features, an\ninterpreter predicts the attributes, and the length of the output is determined\nby the category, thereby removing unnecessary attributes. GlideNet can achieve\ncompelling results on two recent and challenging datasets -- VAW and CAR -- for\nlarge-scale attribute prediction. For instance, it obtains more than 5\\% gain\nover state of the art in the mean recall (mR) metric. GlideNet's advantages are\nespecially apparent when predicting attributes of objects with low pixel counts\nas well as attributes that demand global context understanding. Finally, we\nshow that GlideNet excels in training starved real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metwaly_K/0/1/0/all/0/1\">Kareem Metwaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Aerin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1\">Elliot Branson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03949","description":"<p>Finding accurate correspondences among different views is the Achilles' heel\nof unsupervised Multi-View Stereo (MVS). Existing methods are built upon the\nassumption that corresponding pixels share similar photometric features.\nHowever, multi-view images in real scenarios observe non-Lambertian surfaces\nand experience occlusions. In this work, we propose a novel approach with\nneural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences\namong views. Specifically, we impose a depth rendering consistency loss to\nconstrain the geometry features close to the object surface to alleviate\nocclusions. Concurrently, we introduce a reference view synthesis loss to\ngenerate consistent supervision, even for non-Lambertian surfaces. Extensive\nexperiments on DTU and Tanks\\&amp;Temples benchmarks demonstrate that our RC-MVSNet\napproach achieves state-of-the-art performance over unsupervised MVS frameworks\nand competitive performance to many supervised methods.The trained models and\ncode will be released at https://github.com/Boese0601/RC-MVSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Di Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1\">Alja&#x17e; Bo&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03952","description":"<p>Recently, vision transformers started to show impressive results which\noutperform large convolution based models significantly. However, in the area\nof small models for mobile or resource constrained devices, ConvNet still has\nits own advantages in both performance and model complexity. We propose\nEdgeFormer, a pure ConvNet based backbone model that further strengthens these\nadvantages by fusing the merits of vision transformers into ConvNets.\nSpecifically, we propose global circular convolution (GCC) with position\nembeddings, a light-weight convolution op which boasts a global receptive field\nwhile producing location sensitive features as in local convolutions. We\ncombine the GCCs and squeeze-exictation ops to form a meta-former like model\nblock, which further has the attention mechanism like transformers. The\naforementioned block can be used in plug-and-play manner to replace relevant\nblocks in ConvNets or transformers. Experiment results show that the proposed\nEdgeFormer achieves better performance than popular light-weight ConvNets and\nvision transformer based models in common vision tasks and datasets, while\nhaving fewer parameters and faster inference speed. For classification on\nImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million\nparameters, saving 11% parameters and 13% computational cost but gaining 0.2%\nhigher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)\ncompared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%\naccuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC\nsegmentation tasks, EdgeFormer also shows better performance. Code is available\nat https://github.com/hkzhang91/EdgeFormer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dual-Output Diffusion Models. (arXiv:2203.04304v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04304","description":"<p>Iterative denoising-based generation, also known as denoising diffusion\nmodels, has recently been shown to be comparable in quality to other classes of\ngenerative models, and even surpass them. Including, in particular, Generative\nAdversarial Networks, which are currently the state of the art in many\nsub-tasks of image generation. However, a major drawback of this method is that\nit requires hundreds of iterations to produce a competitive result. Recent\nworks have proposed solutions that allow for faster generation with fewer\niterations, but the image quality gradually deteriorates with increasingly\nfewer iterations being applied during generation. In this paper, we reveal some\nof the causes that affect the generation quality of diffusion models,\nespecially when sampling with few iterations, and come up with a simple, yet\neffective, solution to mitigate them. We consider two opposite equations for\nthe iterative denoising, the first predicts the applied noise, and the second\npredicts the image directly. Our solution takes the two options and learns to\ndynamically alternate between them through the denoising process. Our proposed\nsolution is general and can be applied to any existing diffusion model. As we\nshow, when applied to various SOTA architectures, our solution immediately\nimproves their generation quality, with negligible added complexity and\nparameters. We experiment on multiple datasets and configurations and run an\nextensive ablation study to support these findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classificationtion. (arXiv:2203.04771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04771","description":"<p>Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongfan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervision semantic segmentation with uncertainty-guided self cross supervision. (arXiv:2203.05118v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05118","description":"<p>As a powerful way of realizing semi-supervised segmentation, the cross\nsupervision method learns cross consistency based on independent ensemble\nmodels using abundant unlabeled images. However, the wrong pseudo labeling\ninformation generated by cross supervision would confuse the training process\nand negatively affect the effectiveness of the segmentation model. Besides, the\ntraining process of ensemble models in such methods also multiplies the cost of\ncomputation resources and decreases the training efficiency. To solve these\nproblems, we propose a novel cross supervision method, namely\nuncertainty-guided self cross supervision (USCS). In addition to ensemble\nmodels, we first design a multi-input multi-output (MIMO) segmentation model\nwhich can generate multiple outputs with shared model and consequently impose\nconsistency over the outputs, saving the cost on parameters and calculations.\nOn the other hand, we employ uncertainty as guided information to encourage the\nmodel to focus on the high confident regions of pseudo labels and mitigate the\neffects of wrong pseudo labeling in self cross supervision, improving the\nperformance of the segmentation model. Extensive experiments show that our\nmethod achieves state-of-the-art performance while saving 40.5% and 49.1% cost\non parameters and calculations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaohu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05151","description":"<p>Current adversarial attack research reveals the vulnerability of\nlearning-based classifiers against carefully crafted perturbations. However,\nmost existing attack methods have inherent limitations in cross-dataset\ngeneralization as they rely on a classification layer with a closed set of\ncategories. Furthermore, the perturbations generated by these methods may\nappear in regions easily perceptible to the human visual system (HVS). To\ncircumvent the former problem, we propose a novel algorithm that attacks\nsemantic similarity on feature representations. In this way, we are able to\nfool classifiers without limiting attacks to a specific dataset. For\nimperceptibility, we introduce the low-frequency constraint to limit\nperturbations within high-frequency components, ensuring perceptual similarity\nbetween adversarial examples and originals. Extensive experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online\nplatforms indicate that our attack can yield misleading and transferable\nadversarial examples across architectures and datasets. Additionally,\nvisualization results and quantitative performance (in terms of four different\nmetrics) show that the proposed algorithm generates more imperceptible\nperturbations than the state-of-the-art methods. Code is made available at.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinliang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bizhu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05238","description":"<p>In this paper, we propose a weakly-supervised approach for 3D object\ndetection, which makes it possible to train strong 3D detector with\nposition-level annotations (i.e. annotations of object centers). In order to\nremedy the information loss from box annotations to centers, our method, namely\nBack to Reality (BR), makes use of synthetic 3D shapes to convert the weak\nlabels into fully-annotated virtual scenes as stronger supervision, and in turn\nutilizes the perfect virtual labels to complement and refine the real labels.\nSpecifically, we first assemble 3D shapes into physically reasonable virtual\nscenes according to the coarse scene layout extracted from position-level\nannotations. Then we go back to reality by applying a virtual-to-real domain\nadaptation method, which refine the weak labels and additionally supervise the\ntraining of detector with the virtual scenes. Furthermore, we propose a more\nchallenging benckmark for indoor 3D object detection with more diversity in\nobject sizes to better show the potential of BR. With less than 5% of the\nlabeling labor, we achieve comparable detection performance with some popular\nfully-supervised approaches on the widely used ScanNet dataset. Code is\navailable at: https://github.com/wyf-ACCEPT/BackToReality\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiuwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05340","description":"<p>With diverse presentation attacks emerging continually, generalizable face\nanti-spoofing (FAS) has drawn growing attention. Most existing methods\nimplement domain generalization (DG) on the complete representations. However,\ndifferent image statistics may have unique properties for the FAS tasks. In\nthis work, we separate the complete representation into content and style ones.\nA novel Shuffled Style Assembly Network (SSAN) is proposed to extract and\nreassemble different content and style features for a stylized feature space.\nThen, to obtain a generalized representation, a contrastive learning strategy\nis developed to emphasize liveness-related style information while suppress the\ndomain-specific one. Finally, the representations of the correct assemblies are\nused to distinguish between living and spoofing during the inferring. On the\nother hand, despite the decent performance, there still exists a gap between\nacademia and industry, due to the difference in data quantity and distribution.\nThus, a new large-scale benchmark for FAS is built up to further evaluate the\nperformance of algorithms in reality. Both qualitative and quantitative results\non existing and proposed benchmarks demonstrate the effectiveness of our\nmethods. The codes will be available at https://github.com/wangzhuo2019/SSAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Size Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep AutoAugment. (arXiv:2203.06172v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06172","description":"<p>While recent automated data augmentation methods lead to state-of-the-art\nresults, their design spaces and the derived data augmentation strategies still\nincorporate strong human priors. In this work, instead of fixing a set of\nhand-picked default augmentations alongside the searched data augmentations, we\npropose a fully automated approach for data augmentation search named Deep\nAutoAugment (DeepAA). DeepAA progressively builds a multi-layer data\naugmentation pipeline from scratch by stacking augmentation layers one at a\ntime until reaching convergence. For each augmentation layer, the policy is\noptimized to maximize the cosine similarity between the gradients of the\noriginal and augmented data along the direction with low variance. Our\nexperiments show that even without default augmentations, we can learn an\naugmentation policy that achieves strong performance with that of previous\nworks. Extensive ablation studies show that the regularized gradient matching\nis an effective search method for data augmentation policies. Our code is\navailable at: https://github.com/MSU-MLSys-Lab/DeepAA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06319","description":"<p>3D object detection plays a fundamental role in enabling autonomous driving,\nwhich is regarded as the significant key to unlocking the bottleneck of\ncontemporary transportation systems from the perspectives of safety, mobility,\nand sustainability. Most of the state-of-the-art (SOTA) object detection\nmethods from point clouds are developed based on a single onboard LiDAR, whose\nperformance will be inevitably limited by the range and occlusion, especially\nin dense traffic scenarios. In this paper, we propose \\textit{PillarGrid}, a\nnovel cooperative perception method fusing information from multiple 3D LiDARs\n(both on-board and roadside), to enhance the situation awareness for connected\nand automated vehicles (CAVs). PillarGrid consists of four main phases: 1)\ncooperative preprocessing of point clouds, 2) pillar-wise voxelization and\nfeature extraction, 3) grid-wise deep fusion of features from multiple sensors,\nand 4) convolutional neural network (CNN)-based augmented 3D object detection.\nA novel cooperative perception platform is developed for model training and\ntesting. Extensive experimentation shows that PillarGrid outperforms the SOTA\nsingle-LiDAR-based 3D object detection methods with respect to both accuracy\nand range by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisbot_E/0/1/0/all/0/1\">Emrah Akin Sisbot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06451","description":"<p>Rolling shutter (RS) distortion can be interpreted as the result of picking a\nrow of pixels from instant global shutter (GS) frames over time during the\nexposure of the RS camera. This means that the information of each instant GS\nframe is partially, yet sequentially, embedded into the row-dependent\ndistortion. Inspired by this fact, we address the challenging task of reversing\nthis process, i.e., extracting undistorted GS frames from images suffering from\nRS distortion. However, since RS distortion is coupled with other factors such\nas readout settings and the relative velocity of scene elements to the camera,\nmodels that only exploit the geometric correlation between temporally adjacent\nimages suffer from poor generality in processing data with different readout\nsettings and dynamic scenes with both camera motion and object motion. In this\npaper, instead of two consecutive frames, we propose to exploit a pair of\nimages captured by dual RS cameras with reversed RS directions for this highly\nchallenging task. Grounded on the symmetric and complementary nature of dual\nreversed distortion, we develop a novel end-to-end model, IFED, to generate\ndual optical flow sequence through iterative learning of the velocity field\nduring the RS time. Extensive experimental results demonstrate that IFED is\nsuperior to naive cascade schemes, as well as the state-of-the-art which\nutilizes adjacent RS images. Most importantly, although it is trained on a\nsynthetic dataset, IFED is shown to be effective at retrieving GS frame\nsequences from real-world RS distorted images of dynamic scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhongyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2203.06605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06605","description":"<p>Talking head video generation aims to produce a synthetic human face video\nthat contains the identity and pose information respectively from a given\nsource image and a driving video.Existing works for this task heavily rely on\n2D representations (e.g. appearance and motion) learned from the input images.\nHowever, dense 3D facial geometry (e.g. pixel-wise depth) is extremely\nimportant for this task as it is particularly beneficial for us to essentially\ngenerate accurate 3D face structures and distinguish noisy information from the\npossibly cluttered background. Nevertheless, dense 3D geometry annotations are\nprohibitively costly for videos and are typically not available for this video\ngeneration task. In this paper, we first introduce a self-supervised geometry\nlearning method to automatically recover the dense 3D geometry (i.e.depth) from\nthe face videos without the requirement of any expensive 3D annotation data.\nBased on the learned dense depth maps, we further propose to leverage them to\nestimate sparse facial keypoints that capture the critical movement of the\nhuman head. In a more dense way, the depth is also utilized to learn 3D-aware\ncross-modal (i.e. appearance and depth) attention to guide the generation of\nmotion fields for warping source image representations. All these contributions\ncompose a novel depth-aware generative adversarial network (DaGAN) for talking\nhead generation. Extensive experiments conducted demonstrate that our proposed\nmethod can generate highly realistic faces, and achieve significant results on\nthe unseen human faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show the proposed VPTSL outperforms\nother state-of-the-art methods, which demonstrates the effectiveness of visual\nprompt and the text span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity Equivariant Linear Transformation of Joint Orientation-Scale Space Representations. (arXiv:2203.06786v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06786","description":"<p>Convolution is conventionally defined as a linear operation on functions of\none or more variables which commutes with shifts. Group convolution generalizes\nthe concept to linear operations on functions of group elements representing\nmore general geometric transformations and which commute with those\ntransformations. Since similarity transformation is the most general geometric\ntransformation on images that preserves shape, the group convolution that is\nequivariant to similarity transformation is the most general shape preserving\nlinear operator. Because similarity transformations have four free parameters,\ngroup convolutions are defined on four-dimensional, joint orientation-scale\nspaces. Although prior work on equivariant linear operators has been limited to\ndiscrete groups, the similarity group is continuous. In this paper, we describe\nlinear operators on discrete representations that are equivariant to continuous\nsimilarity transformation. This is achieved by using a basis of functions that\nis it joint shiftable-twistable-scalable. These pinwheel functions use Fourier\nseries in the orientation dimension and Laplace transform in the log-scale\ndimension to form a basis of spatially localized functions that can be\ncontinuously interpolated in position, orientation and scale. Although this\nresult is potentially significant with respect to visual computation generally,\nwe present an initial demonstration of its utility by using it to compute a\nshape equivariant distribution of closed contours traced by particles\nundergoing Brownian motion in velocity. The contours are constrained by sets of\npoints and line endings representing well known bistable illusory contour\ninducing patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Lance R. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. (arXiv:2203.06947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06947","description":"<p>Recently, various multimodal networks for Visually-Rich Document\nUnderstanding(VRDU) have been proposed, showing the promotion of transformers\nby integrating visual and layout information with the text embeddings. However,\nmost existing approaches utilize the position embeddings to incorporate the\nsequence information, neglecting the noisy improper reading order obtained by\nOCR tools. In this paper, we propose a robust layout-aware multimodal network\nnamed XYLayoutLM to capture and leverage rich layout information from proper\nreading orders produced by our Augmented XY Cut. Moreover, a Dilated\nConditional Position Encoding module is proposed to deal with the input\nsequence of variable lengths, and it additionally extracts local layout\ninformation from both textual and visual modalities while generating position\nembeddings. Experiment results show that our XYLayoutLM achieves competitive\nresults on document understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhangxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Changhua Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1\">Jun Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Ming Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision and Deep Learning for Fish Classification in Underwater Habitats: A Survey. (arXiv:2203.06951v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06951","description":"<p>Marine scientists use remote underwater video recording to survey fish\nspecies in their natural habitats. This helps them understand and predict how\nfish respond to climate change, habitat degradation, and fishing pressure. This\ninformation is essential for developing sustainable fisheries for human\nconsumption, and for preserving the environment. However, the enormous volume\nof collected videos makes extracting useful information a daunting and\ntime-consuming task for a human. A promising method to address this problem is\nthe cutting-edge Deep Learning (DL) technology.DL can help marine scientists\nparse large volumes of video promptly and efficiently, unlocking niche\ninformation that cannot be obtained using conventional manual monitoring\nmethods. In this paper, we provide an overview of the key concepts of DL, while\npresenting a survey of literature on fish habitat monitoring with a focus on\nunderwater fish classification. We also discuss the main challenges faced when\ndeveloping DL for underwater image processing and propose approaches to address\nthem. Finally, we provide insights into the marine habitat monitoring research\ndomain and shed light on what the future of DL for underwater image processing\nmay hold. This paper aims to inform a wide range of readers from marine\nscientists who would like to apply DL in their research to computer scientists\nwho would like to survey state-of-the-art DL-based underwater fish habitat\nmonitoring literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1\">Alzayat Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheaves_M/0/1/0/all/0/1\">Marcus Sheaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1\">Mostafa Rahimi Azghadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots. (arXiv:2203.06967v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06967","description":"<p>Real noisy-clean pairs on a large scale are costly and difficult to obtain.\nMeanwhile, supervised denoisers trained on synthetic data perform poorly in\npractice. Self-supervised denoisers, which learn only from single noisy images,\nsolve the data collection problem. However, self-supervised denoising methods,\nespecially blindspot-driven ones, suffer sizable information loss during input\nor network design. The absence of valuable information dramatically reduces the\nupper bound of denoising performance. In this paper, we propose a simple yet\nefficient approach called Blind2Unblind to overcome the information loss in\nblindspot-driven denoising methods. First, we introduce a global-aware mask\nmapper that enables global perception and accelerates training. The mask mapper\nsamples all pixels at blind spots on denoised volumes and maps them to the same\nchannel, allowing the loss function to optimize all blind spots at once.\nSecond, we propose a re-visible loss to train the denoising network and make\nblind spots visible. The denoiser can learn directly from raw noise images\nwithout losing information or being trapped in identity mapping. We also\ntheoretically analyze the convergence of the re-visible loss. Extensive\nexperiments on synthetic and real-world datasets demonstrate the superior\nperformance of our approach compared to previous work. Code is available at\nhttps://github.com/demonsjin/Blind2Unblind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zejin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiazheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hua Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial amplitude swap towards robust image classifiers. (arXiv:2203.07138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07138","description":"<p>The vulnerability of convolutional neural networks (CNNs) to image\nperturbations such as common corruptions and adversarial perturbations has\nrecently been investigated from the perspective of frequency. In this study, we\ninvestigate the effect of the amplitude and phase spectra of adversarial images\non the robustness of CNN classifiers. Extensive experiments revealed that the\nimages generated by combining the amplitude spectrum of adversarial images and\nthe phase spectrum of clean images accommodates moderate and general\nperturbations, and training with these images equips a CNN classifier with more\ngeneral robustness, performing well under both common corruptions and\nadversarial perturbations. We also found that two types of overfitting\n(catastrophic overfitting and robust overfitting) can be circumvented by the\naforementioned spectrum recombination. We believe that these results contribute\nto the understanding and the training of truly robust classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chun Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1\">Kazuhiko Kawamoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Motion Handling for Video Camouflaged Object Detection. (arXiv:2203.07363v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07363","description":"<p>We propose a new video camouflaged object detection (VCOD) framework that can\nexploit both short-term dynamics and long-term temporal consistency to detect\ncamouflaged objects from video frames. An essential property of camouflaged\nobjects is that they usually exhibit patterns similar to the background and\nthus make them hard to identify from still images. Therefore, effectively\nhandling temporal dynamics in videos becomes the key for the VCOD task as the\ncamouflaged objects will be noticeable when they move. However, current VCOD\nmethods often leverage homography or optical flows to represent motions, where\nthe detection error may accumulate from both the motion estimation error and\nthe segmentation error. On the other hand, our method unifies motion estimation\nand object segmentation within a single optimization framework. Specifically,\nwe build a dense correlation volume to implicitly capture motions between\nneighbouring frames and utilize the final segmentation supervision to optimize\nthe implicit motion estimation and segmentation jointly. Furthermore, to\nenforce temporal consistency within a video sequence, we jointly utilize a\nspatio-temporal transformer to refine the short-term predictions. Extensive\nexperiments on VCOD benchmarks demonstrate the architectural effectiveness of\nour approach. We also provide a large-scale VCOD dataset named MoCA-Mask with\npixel-level handcrafted ground-truth masks and construct a comprehensive VCOD\nbenchmark with previous methods to facilitate research in this direction.\nDataset Link: https://xueliancheng.github.io/SLT-Net-project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuelian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Huan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}