{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-27T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Model-based Decision Making with Imagination for Autonomous Parking. (arXiv:2108.11420v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11420","description":"<p>Autonomous parking technology is a key concept within autonomous driving\nresearch. This paper will propose an imaginative autonomous parking algorithm\nto solve issues concerned with parking. The proposed algorithm consists of\nthree parts: an imaginative model for anticipating results before parking, an\nimproved rapid-exploring random tree (RRT) for planning a feasible trajectory\nfrom a given start point to a parking lot, and a path smoothing module for\noptimizing the efficiency of parking tasks. Our algorithm is based on a real\nkinematic vehicle model; which makes it more suitable for algorithm application\non real autonomous cars. Furthermore, due to the introduction of the\nimagination mechanism, the processing speed of our algorithm is ten times\nfaster than that of traditional methods, permitting the realization of\nreal-time planning simultaneously. In order to evaluate the algorithm's\neffectiveness, we have compared our algorithm with traditional RRT, within\nthree different parking scenarios. Ultimately, results show that our algorithm\nis more stable than traditional RRT and performs better in terms of efficiency\nand quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shitao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIVODL: Privacy-preserving vertical federated learning over distributed labels. (arXiv:2108.11444v1 [cs.CR])","link":"http://arxiv.org/abs/2108.11444","description":"<p>Federated learning (FL) is an emerging privacy preserving machine learning\nprotocol that allows multiple devices to collaboratively train a shared global\nmodel without revealing their private local data. Non-parametric models like\ngradient boosting decision trees (GBDT) have been commonly used in FL for\nvertically partitioned data. However, all these studies assume that all the\ndata labels are stored on only one client, which may be unrealistic for\nreal-world applications. Therefore, in this work, we propose a secure vertical\nFL framework, named PIVODL, to train GBDT with data labels distributed on\nmultiple devices. Both homomorphic encryption and differential privacy are\nadopted to prevent label information from being leaked through transmitted\ngradients and leaf values. Our experimental results show that both information\nleakage and model performance degradation of the proposed PIVODL are\nnegligible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kaitai Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11451","description":"<p>Neural-symbolic and statistical relational artificial intelligence both\nintegrate frameworks for learning with logical reasoning. This survey\nidentifies several parallels across seven different dimensions between these\ntwo fields. These cannot only be used to characterize and position\nneural-symbolic artificial intelligence approaches but also to identify a\nnumber of directions for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marra_G/0/1/0/all/0/1\">Giuseppe Marra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1\">Sebastijan Duman&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhaeve_R/0/1/0/all/0/1\">Robin Manhaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1\">Luc De Raedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"With One Voice: Composing a Travel Voice Assistant from Re-purposed Models. (arXiv:2108.11463v1 [eess.AS])","link":"http://arxiv.org/abs/2108.11463","description":"<p>Voice assistants provide users a new way of interacting with digital\nproducts, allowing them to retrieve information and complete tasks with an\nincreased sense of control and flexibility. Such products are comprised of\nseveral machine learning models, like Speech-to-Text transcription, Named\nEntity Recognition and Resolution, and Text Classification. Building a voice\nassistant from scratch takes the prolonged efforts of several teams\nconstructing numerous models and orchestrating between components. Alternatives\nsuch as using third-party vendors or re-purposing existing models may be\nconsidered to shorten time-to-market and development costs. However, each\noption has its benefits and drawbacks. We present key insights from building a\nvoice search assistant for Booking.com search and recommendation system. Our\npaper compares the achieved performance and development efforts in dedicated\ntailor-made solutions against existing re-purposed models. We share and discuss\nour data-driven decisions about implementation trade-offs and their estimated\noutcomes in hindsight, showing that a fully functional machine learning product\ncan be built from existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Poran_S/0/1/0/all/0/1\">Shachaf Poran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amsalem_G/0/1/0/all/0/1\">Gil Amsalem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beka_A/0/1/0/all/0/1\">Amit Beka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldenberg_D/0/1/0/all/0/1\">Dmitri Goldenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETA Prediction with Graph Neural Networks in Google Maps. (arXiv:2108.11482v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11482","description":"<p>Travel-time prediction constitutes a task of high importance in\ntransportation networks, with web mapping services like Google Maps regularly\nserving vast quantities of travel time queries from users and enterprises\nalike. Further, such a task requires accounting for complex spatiotemporal\ninteractions (modelling both the topological properties of the road network and\nanticipating events -- such as rush hours -- that may occur in the future).\nHence, it is an ideal target for graph representation learning at scale. Here\nwe present a graph neural network estimator for estimated time of arrival (ETA)\nwhich we have deployed in production at Google Maps. While our main\narchitecture consists of standard GNN building blocks, we further detail the\nusage of training schedule methods such as MetaGradients in order to make our\nmodel robust and production-ready. We also provide prescriptive studies:\nablating on various architectural decisions and training regimes, and\nqualitative analyses on real-world situations where our model provides a\ncompetitive edge. Our GNN proved powerful when deployed, significantly reducing\nnegative ETA outcomes in several regions compared to the previous production\nbaseline (40+% in cities like Sydney).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derrow_Pinion_A/0/1/0/all/0/1\">Austin Derrow-Pinion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">Jennifer She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">David Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_O/0/1/0/all/0/1\">Oliver Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hester_T/0/1/0/all/0/1\">Todd Hester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_L/0/1/0/all/0/1\">Luis Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunkesser_M/0/1/0/all/0/1\">Marc Nunkesser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xueying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiltshire_B/0/1/0/all/0/1\">Brett Wiltshire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter W. Battaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vishal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhongwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yujia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maneuver Identification Challenge. (arXiv:2108.11503v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11503","description":"<p>AI algorithms that identify maneuvers from trajectory data could play an\nimportant role in improving flight safety and pilot training. AI challenges\nallow diverse teams to work together to solve hard problems and are an\neffective tool for developing AI solutions. AI challenges are also a key driver\nof AI computational requirements. The Maneuver Identification Challenge hosted\nat maneuver-id.mit.edu provides thousands of trajectories collected from pilots\npracticing in flight simulators, descriptions of maneuvers, and examples of\nthese maneuvers performed by experienced pilots. Each trajectory consists of\npositions, velocities, and aircraft orientations normalized to a common\ncoordinate system. Construction of the data set required significant data\narchitecture to transform flight simulator logs into AI ready data, which\nincluded using a supercomputer for deduplication and data conditioning. There\nare three proposed challenges. The first challenge is separating physically\nplausible (good) trajectories from unfeasible (bad) trajectories. Human labeled\ngood and bad trajectories are provided to aid in this task. Subsequent\nchallenges are to label trajectories with their intended maneuvers and to\nassess the quality of those maneuvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_K/0/1/0/all/0/1\">Kaira Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1\">Vijay Gadepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAlpin_K/0/1/0/all/0/1\">Kyle McAlpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palko_K/0/1/0/all/0/1\">Kyle Palko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulk_B/0/1/0/all/0/1\">Ben Paulk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1\">Sid Samsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_H/0/1/0/all/0/1\">Ho Chit Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1\">Charles Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1\">Jeremy Kepner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey. (arXiv:2108.11510v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11510","description":"<p>Deep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have\ndemonstrated the remarkable successes of deep reinforcement learning in various\ndomains including finance, medicine, healthcare, video games, robotics, and\ncomputer vision. In this work, we provide a detailed review of recent and\nstate-of-the-art research advances of deep reinforcement learning in computer\nvision. We start with comprehending the theories of deep learning,\nreinforcement learning, and deep reinforcement learning. We then propose a\ncategorization of deep reinforcement learning methodologies and discuss their\nadvantages and limitations. In particular, we divide deep reinforcement\nlearning into seven main categories according to their applications in computer\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\nthese categories is further analyzed with reinforcement learning techniques,\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\nof the existing publicly available datasets and examine source code\navailability. Finally, we present some open issues and discuss future research\ndirections on deep reinforcement learning in computer vision\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathour_V/0/1/0/all/0/1\">Vidhiwar Singh Rathour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Effective and Efficient Embedding via an Adaptively-Masked Twins-based Layer. (arXiv:2108.11513v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11513","description":"<p>Embedding learning for categorical features is crucial for the deep\nlearning-based recommendation models (DLRMs). Each feature value is mapped to\nan embedding vector via an embedding learning process. Conventional methods\nconfigure a fixed and uniform embedding size to all feature values from the\nsame feature field. However, such a configuration is not only sub-optimal for\nembedding learning but also memory costly. Existing methods that attempt to\nresolve these problems, either rule-based or neural architecture search\n(NAS)-based, need extensive efforts on the human design or network training.\nThey are also not flexible in embedding size selection or in warm-start-based\napplications. In this paper, we propose a novel and effective embedding size\nselection scheme. Specifically, we design an Adaptively-Masked Twins-based\nLayer (AMTL) behind the standard embedding layer. AMTL generates a mask vector\nto mask the undesired dimensions for each embedding vector. The mask vector\nbrings flexibility in selecting the dimensions and the proposed layer can be\neasily added to either untrained or trained DLRMs. Extensive experimental\nevaluations show that the proposed scheme outperforms competitive baselines on\nall the benchmark tasks, and is also memory-efficient, saving 60\\% memory usage\nwithout compromising any performance metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bencheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-Chih Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Denoising Diffusion Models. (arXiv:2108.11514v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11514","description":"<p>Denoising diffusion probabilistic models (DDPMs) have emerged as competitive\ngenerative models yet brought challenges to efficient sampling. In this paper,\nwe propose novel bilateral denoising diffusion models (BDDMs), which take\nsignificantly fewer steps to generate high-quality samples. From a bilateral\nmodeling objective, BDDMs parameterize the forward and reverse processes with a\nscore network and a scheduling network, respectively. We show that a new lower\nbound tighter than the standard evidence lower bound can be derived as a\nsurrogate objective for training the two networks. In particular, BDDMs are\nefficient, simple-to-train, and capable of further improving any pre-trained\nDDPM by optimizing the inference noise schedules. Our experiments demonstrated\nthat BDDMs can generate high-fidelity samples with as few as 3 sampling steps\nand produce comparable or even higher quality samples than DDPMs using 1000\nsteps with only 16 sampling steps (a 62x speedup).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Max W. Y. Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Interpolation Approach and Corresponding Instance-Based Learning. (arXiv:2108.11530v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11530","description":"<p>Starting from finding approximate value of a function, introduces the measure\nof approximation-degree between two numerical values, proposes the concepts of\n\"strict approximation\" and \"strict approximation region\", then, derives the\ncorresponding one-dimensional interpolation methods and formulas, and then\npresents a calculation model called \"sum-times-difference formula\" for\nhigh-dimensional interpolation, thus develops a new interpolation approach,\nthat is, ADB interpolation. ADB interpolation is applied to the interpolation\nof actual functions with satisfactory results. Viewed from principle and\neffect, the interpolation approach is of novel idea, and has the advantages of\nsimple calculation, stable accuracy, facilitating parallel processing, very\nsuiting for high-dimensional interpolation, and easy to be extended to the\ninterpolation of vector valued functions. Applying the approach to\ninstance-based learning, a new instance-based learning method, learning using\nADB interpolation, is obtained. The learning method is of unique technique,\nwhich has also the advantages of definite mathematical basis, implicit distance\nweights, avoiding misclassification, high efficiency, and wide range of\napplications, as well as being interpretable, etc. In principle, this method is\na kind of learning by analogy, which and the deep learning that belongs to\ninductive learning can complement each other, and for some problems, the two\ncan even have an effect of \"different approaches but equal results\" in big data\nand cloud computing environment. Thus, the learning using ADB interpolation can\nalso be regarded as a kind of \"wide learning\" that is dual to deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shiyou Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios. (arXiv:2108.11539v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11539","description":"<p>Object detection on drone-captured scenarios is a recent popular task. As\ndrones always navigate in different altitudes, the object scale varies\nviolently, which burdens the optimization of networks. Moreover, high-speed and\nlow-altitude flight bring in the motion blur on the densely packed objects,\nwhich leads to great challenge of object distinction. To solve the two issues\nmentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more\nprediction head to detect different-scale objects. Then we replace the original\nprediction heads with Transformer Prediction Heads (TPH) to explore the\nprediction potential with self-attention mechanism. We also integrate\nconvolutional block attention model (CBAM) to find attention region on\nscenarios with dense objects. To achieve more improvement of our proposed\nTPH-YOLOv5, we provide bags of useful strategies such as data augmentation,\nmultiscale testing, multi-model integration and utilizing extra classifier.\nExtensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good\nperformance with impressive interpretability on drone-captured scenarios. On\nDET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is\nbetter than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge\n2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place\nmodel (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves\nabout 7%, which is encouraging and competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingkui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation. (arXiv:2108.11550v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11550","description":"<p>It is fundamental for personal robots to reliably navigate to a specified\ngoal. To study this task, PointGoal navigation has been introduced in simulated\nEmbodied AI environments. Recent advances solve this PointGoal navigation task\nwith near-perfect accuracy (99.6% success) in photo-realistically simulated\nenvironments, assuming noiseless egocentric vision, noiseless actuation, and\nmost importantly, perfect localization. However, under realistic noise models\nfor visual sensors and actuation, and without access to a \"GPS and Compass\nsensor,\" the 99.6%-success agents for PointGoal navigation only succeed with\n0.3%. In this work, we demonstrate the surprising effectiveness of visual\nodometry for the task of PointGoal navigation in this realistic setting, i.e.,\nwith realistic noise models for perception and actuation and without access to\nGPS and Compass sensors. We show that integrating visual odometry techniques\ninto navigation policies improves the state-of-the-art on the popular Habitat\nPointNav benchmark by a large margin, improving success from 64.5% to 71.7%\nwhile executing 6.4 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harsh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XCI-Sketch: Extraction of Color Information from Images for Generation of Colored Outlines and Sketches. (arXiv:2108.11554v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11554","description":"<p>Sketches are a medium to convey a visual scene from an individual's creative\nperspective. The addition of color substantially enhances the overall\nexpressivity of a sketch. This paper proposes two methods to mimic human-drawn\ncolored sketches by utilizing the Contour Drawing Dataset. Our first approach\nrenders colored outline sketches by applying image processing techniques aided\nby k-means color clustering. The second method uses a generative adversarial\nnetwork to develop a model that can generate colored sketches from previously\nunobserved images. We assess the results obtained through quantitative and\nqualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathod_H/0/1/0/all/0/1\">Harsh Rathod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manisimha Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Parna Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Sameer Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manushree_V/0/1/0/all/0/1\">V Manushree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Attention in Machine Reading Comprehension. (arXiv:2108.11574v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11574","description":"<p>Achieving human-level performance on some of Machine Reading Comprehension\n(MRC) datasets is no longer challenging with the help of powerful Pre-trained\nLanguage Models (PLMs). However, the internal mechanism of these artifacts\nstill remains unclear, placing an obstacle for further understanding these\nmodels. This paper focuses on conducting a series of analytical experiments to\nexamine the relations between the multi-head self-attention and the final\nperformance, trying to analyze the potential explainability in PLM-based MRC\nmodels. We perform quantitative analyses on SQuAD (English) and CMRC 2018\n(Chinese), two span-extraction MRC datasets, on top of BERT, ALBERT, and\nELECTRA in various aspects. We discover that {\\em passage-to-question} and {\\em\npassage understanding} attentions are the most important ones, showing strong\ncorrelations to the final performance than other parts. Through visualizations\nand case studies, we also observe several general findings on the attention\nmaps, which could be helpful to understand how these models solve the\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of the Resting Position Based on EGG, ECG, Respiration Rate and SpO2 Using Stacked Ensemble Learning. (arXiv:2108.11604v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11604","description":"<p>Rest is essential for a high-level physiological and psychological\nperformance. It is also necessary for the muscles to repair, rebuild, and\nstrengthen. There is a significant correlation between the quality of rest and\nthe resting posture. Therefore, identification of the resting position is of\nparamount importance to maintain a healthy life. Resting postures can be\nclassified into four basic categories: Lying on the back (supine), facing of\nthe left / right sides and free-fall position. The later position is already\nconsidered to be an unhealthy posture by researchers equivocally and hence can\nbe eliminated. In this paper, we analyzed the other three states of resting\nposition based on the data collected from the physiological parameters:\nElectrogastrogram (EGG), Electrocardiogram (ECG), Respiration Rate, Heart Rate,\nand Oxygen Saturation (SpO2). Based on these parameters, the resting position\nis classified using a hybrid stacked ensemble machine learning model designed\nusing the Decision tree, Random Forest, and Xgboost algorithms. Our study\ndemonstrates a 100% accurate prediction of the resting position using the\nhybrid model. The proposed method of identifying the resting position based on\nphysiological parameters has the potential to be integrated into wearable\ndevices. This is a low cost, highly accurate and autonomous technique to\nmonitor the body posture while maintaining the user privacy by eliminating the\nuse of RGB camera conventionally used to conduct the polysomnography (sleep\nMonitoring) or resting position studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1\">Md. Mohsin Sarker Raihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Muhammad Muinul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fairoz_F/0/1/0/all/0/1\">Fariha Fairoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shams_A/0/1/0/all/0/1\">Abdullah Bin Shams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence. (arXiv:2108.11609v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11609","description":"<p>Shape correspondence from 3D deformation learning has attracted appealing\nacademy interests recently. Nevertheless, current deep learning based methods\nrequire the supervision of dense annotations to learn per-point translations,\nwhich severely overparameterize the deformation process. Moreover, they fail to\ncapture local geometric details of original shape via global feature embedding.\nTo address these challenges, we develop a new Unsupervised Dense Deformation\nEmbedding Network (i.e., UD^2E-Net), which learns to predict deformations\nbetween non-rigid shapes from dense local features. Since it is non-trivial to\nmatch deformation-variant local features for deformation prediction, we develop\nan Extrinsic-Intrinsic Autoencoder to frst encode extrinsic geometric features\nfrom source into intrinsic coordinates in a shared canonical shape, with which\nthe decoder then synthesizes corresponding target features. Moreover, a bounded\nmaximum mean discrepancy loss is developed to mitigate the distribution\ndivergence between the synthesized and original features. To learn natural\ndeformation without dense supervision, we introduce a coarse parameterized\ndeformation graph, for which a novel trace and propagation algorithm is\nproposed to improve both the quality and effciency of the deformation. Our\nUD^2E-Net outperforms state-of-the-art unsupervised methods by 24% on Faust\nInter challenge and even supervised methods by 13% on Faust Intra challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ronghan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Visual Relationship Co-localization. (arXiv:2108.11618v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11618","description":"<p>In this paper, given a small bag of images, each containing a common but\nlatent predicate, we are interested in localizing visual subject-object pairs\nconnected via the common predicate in each of the images. We refer to this\nnovel problem as visual relationship co-localization or VRC as an abbreviation.\nVRC is a challenging task, even more so than the well-studied object\nco-localization task. This becomes further challenging when using just a few\nimages, the model has to learn to co-localize visual subject-object pairs\nconnected via unseen predicates. To solve VRC, we propose an optimization\nframework to select a common visual relationship in each image of the bag. The\ngoal of the optimization framework is to find the optimal solution by learning\nvisual relationship similarity across images in a few-shot setting. To obtain\nrobust visual relationship representation, we utilize a simple yet effective\ntechnique that learns relationship embedding as a translation vector from\nvisual subject to visual object in a shared space. Further, to learn visual\nrelationship similarity, we utilize a proven meta-learning technique commonly\nused for few-shot classification tasks. Finally, to tackle the combinatorial\ncomplexity challenge arising from an exponential number of feasible solutions,\nwe use a greedy approximation inference algorithm that selects approximately\nthe best solution.\n</p>\n<p>We extensively evaluate our proposed framework on variations of bag sizes\nobtained from two challenging public datasets, namely VrR-VG and VG-150, and\nachieve impressive visual co-localization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teotia_R/0/1/0/all/0/1\">Revant Teotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1\">Vaibhav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_M/0/1/0/all/0/1\">Mayank Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Anand Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11626","description":"<p>As the use of interactive machines grow, the task of Emotion Recognition in\nConversation (ERC) became more important. If the machine generated sentences\nreflect emotion, more human-like sympathetic conversations are possible. Since\nemotion recognition in conversation is inaccurate if the previous utterances\nare not taken into account, many studies reflect the dialogue context to\nimprove the performances. We introduce CoMPM, a context embedding module (CoM)\ncombined with a pre-trained memory module (PM) that tracks memory of the\nspeaker's previous utterances within the context, and show that the pre-trained\nmemory significantly improves the final accuracy of emotion recognition. We\nexperimented on both the multi-party datasets (MELD, EmoryNLP) and the\ndyadic-party datasets (IEMOCAP, DailyDialog), showing that our approach achieve\ncompetitive performance on all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11635","description":"<p>Meta-learning is widely used for few-shot slot tagging in the task of\nfew-shot learning. The performance of existing methods is, however, seriously\naffected by catastrophic forgetting. This phenomenon is common in deep learning\nas the training and testing modules fail to take into account historical\ninformation, i.e. previously trained episodes in the metric-based\nmeta-learning. To overcome this predicament, we propose the Memory-based\nContrastive Meta-learning (MCML) method. Specifically, we propose a\nlearn-from-memory mechanism that use explicit memory to keep track of the label\nrepresentations of previously trained episodes and propose a contrastive\nlearning method to compare the current label embedded in the few shot episode\nwith the historic ones stored in the memory, and an adaption-from memory\nmechanism to determine the output label based on the contrast between the input\nlabels embedded in the test episode and the label clusters in the memory.\nExperimental results show that MCML is scalable and outperforms metric-based\nmeta-learning and optimization-based meta-learning on all 1shot, 5-shot,\n10-shot, and 20-shot scenarios of the SNIPS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Model-based Reinforcement Learning for Autonomous Greenhouse Control. (arXiv:2108.11645v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11645","description":"<p>Due to the high efficiency and less weather dependency, autonomous\ngreenhouses provide an ideal solution to meet the increasing demand for fresh\nfood. However, managers are faced with some challenges in finding appropriate\ncontrol strategies for crop growth, since the decision space of the greenhouse\ncontrol problem is an astronomical number. Therefore, an intelligent\nclosed-loop control framework is highly desired to generate an automatic\ncontrol policy. As a powerful tool for optimal control, reinforcement learning\n(RL) algorithms can surpass human beings' decision-making and can also be\nseamlessly integrated into the closed-loop control framework. However, in\ncomplex real-world scenarios such as agricultural automation control, where the\ninteraction with the environment is time-consuming and expensive, the\napplication of RL algorithms encounters two main challenges, i.e., sample\nefficiency and safety. Although model-based RL methods can greatly mitigate the\nefficiency problem of greenhouse control, the safety problem has not got too\nmuch attention. In this paper, we present a model-based robust RL framework for\nautonomous greenhouse control to meet the sample efficiency and safety\nchallenges. Specifically, our framework introduces an ensemble of environment\nmodels to work as a simulator and assist in policy optimization, thereby\naddressing the low sample efficiency problem. As for the safety concern, we\npropose a sample dropout module to focus more on worst-case samples, which can\nhelp improve the adaptability of the greenhouse planting policy in extreme\ncases. Experimental results demonstrate that our approach can learn a more\neffective greenhouse planting policy with better robustness than existing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhicheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xi Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Networks Demystified: A Matched Filtering Perspective Based Tutorial. (arXiv:2108.11663v1 [cs.IT])","link":"http://arxiv.org/abs/2108.11663","description":"<p>Deep Neural Networks (DNN) and especially Convolutional Neural Networks (CNN)\nare a de-facto standard for the analysis of large volumes of signals and\nimages. Yet, their development and underlying principles have been largely\nperformed in an ad-hoc and black box fashion. To help demystify CNNs, we\nrevisit their operation from first principles and a matched filtering\nperspective. We establish that the convolution operation within CNNs, their\nvery backbone, represents a matched filter which examines the input\nsignal/image for the presence of pre-defined features. This perspective is\nshown to be physically meaningful, and serves as a basis for a step-by-step\ntutorial on the operation of CNNs, including pooling, zero padding, various\nways of dimensionality reduction. Starting from first principles, both the\nfeed-forward pass and the learning stage (via back-propagation) are illuminated\nin detail, both through a worked-out numerical example and the corresponding\nvisualizations. It is our hope that this tutorial will help shed new light and\nphysical intuition into the understanding and further development of deep\nneural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_L/0/1/0/all/0/1\">Ljubisa Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandic_D/0/1/0/all/0/1\">Danilo Mandic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Module Detection from Multi-Modal Node Features with a Greedy Decision Forest for Actionable Explainable AI. (arXiv:2108.11674v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11674","description":"<p>Network-based algorithms are used in most domains of research and industry in\na wide variety of applications and are of great practical use. In this work, we\ndemonstrate subnetwork detection based on multi-modal node features using a new\nGreedy Decision Forest for better interpretability. The latter will be a\ncrucial factor in retaining experts and gaining their trust in such algorithms\nin the future. To demonstrate a concrete application example, we focus in this\npaper on bioinformatics and systems biology with a special focus on\nbiomedicine. However, our methodological approach is applicable in many other\ndomains as well. Systems biology serves as a very good example of a field in\nwhich statistical data-driven machine learning enables the analysis of large\namounts of multi-modal biomedical data. This is important to reach the future\ngoal of precision medicine, where the complexity of patients is modeled on a\nsystem level to best tailor medical decisions, health practices and therapies\nto the individual patient. Our glass-box approach could help to uncover\ndisease-causing network modules from multi-omics data to better understand\ndiseases such as cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_B/0/1/0/all/0/1\">Bastian Pfeifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saranti_A/0/1/0/all/0/1\">Anna Saranti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1\">Andreas Holzinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling ODE parameters from dynamics in VAEs. (arXiv:2108.11684v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11684","description":"<p>Deep networks have become increasingly of interest in dynamical system\nprediction, but generalization remains elusive. In this work, we consider the\nphysical parameters of ODEs as factors of variation of the data generating\nprocess. By leveraging ideas from supervised disentanglement in VAEs, we aim to\nseparate the ODE parameters from the dynamics in the latent space. Experiments\nshow that supervised disentanglement allows VAEs to capture the variability in\nthe dynamics and extrapolate better to ODE parameter spaces that were not\npresent in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1\">Stathi Fotiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lino_M/0/1/0/all/0/1\">Mario Lino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cantwell_C/0/1/0/all/0/1\">Chris Cantwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1\">Anil Bharath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLIM: Explicit Slot-Intent Mapping with BERT for Joint Multi-Intent Detection and Slot Filling. (arXiv:2108.11711v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11711","description":"<p>Utterance-level intent detection and token-level slot filling are two key\ntasks for natural language understanding (NLU) in task-oriented systems. Most\nexisting approaches assume that only a single intent exists in an utterance.\nHowever, there are often multiple intents within an utterance in real-life\nscenarios. In this paper, we propose a multi-intent NLU framework, called SLIM,\nto jointly learn multi-intent detection and slot filling based on BERT. To\nfully exploit the existing annotation data and capture the interactions between\nslots and intents, SLIM introduces an explicit slot-intent classifier to learn\nthe many-to-one mapping between slots and intents. Empirical results on three\npublic multi-intent datasets demonstrate (1) the superior performance of SLIM\ncompared to the current state-of-the-art for NLU with multiple intents and (2)\nthe benefits obtained from the slot-intent classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Fengyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photos Are All You Need for Reciprocal Recommendation in Online Dating. (arXiv:2108.11714v1 [cs.IR])","link":"http://arxiv.org/abs/2108.11714","description":"<p>Recommender Systems are algorithms that predict a user's preference for an\nitem. Reciprocal Recommenders are a subset of recommender systems, where the\nitems in question are people, and the objective is therefore to predict a\nbidirectional preference relation. They are used in settings such as online\ndating services and social networks. In particular, images provided by users\nare a crucial part of user preference, and one that is not exploited much in\nthe literature. We present a novel method of interpreting user image preference\nhistory and using this to make recommendations. We train a recurrent neural\nnetwork to learn a user's preferences and make predictions of reciprocal\npreference relations that can be used to make recommendations that satisfy both\nusers. We show that our proposed system achieves an F1 score of 0.87 when using\nonly photographs to produce reciprocal recommendations on a large real world\nonline dating dataset. Our system significantly outperforms on the state of the\nart in both content-based and collaborative filtering systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neve_J/0/1/0/all/0/1\">James Neve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1\">Ryan McConville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on Bayesian inference for Gaussian mixture model. (arXiv:2108.11753v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11753","description":"<p>Clustering has become a core technology in machine learning, largely due to\nits application in the field of unsupervised learning, clustering,\nclassification, and density estimation. A frequentist approach exists to hand\nclustering based on mixture model which is known as the EM algorithm where the\nparameters of the mixture model are usually estimated into a maximum likelihood\nestimation framework. Bayesian approach for finite and infinite Gaussian\nmixture model generates point estimates for all variables as well as associated\nuncertainty in the form of the whole estimates' posterior distribution.\n</p>\n<p>The sole aim of this survey is to give a self-contained introduction to\nconcepts and mathematical tools in Bayesian inference for finite and infinite\nGaussian mixture model in order to seamlessly introduce their applications in\nsubsequent sections. However, we clearly realize our inability to cover all the\nuseful and interesting results concerning this field and given the paucity of\nscope to present this discussion, e.g., the separated analysis of the\ngeneration of Dirichlet samples by stick-breaking and Polya's Urn approaches.\nWe refer the reader to literature in the field of the Dirichlet process mixture\nmodel for a much detailed introduction to the related fields. Some excellent\nexamples include (Frigyik et al., 2010; Murphy, 2012; Gelman et al., 2014;\nHoff, 2009).\n</p>\n<p>This survey is primarily a summary of purpose, significance of important\nbackground and techniques for Gaussian mixture model, e.g., Dirichlet prior,\nChinese restaurant process, and most importantly the origin and complexity of\nthe methods which shed light on their modern applications. The mathematical\nprerequisite is a first course in probability. Other than this modest\nbackground, the development is self-contained, with rigorous proofs provided\nthroughout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling What and Where for 3D Object-Centric Representations Through Active Inference. (arXiv:2108.11762v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11762","description":"<p>Although modern object detection and classification models achieve high\naccuracy, these are typically constrained in advance on a fixed train set and\nare therefore not flexible to deal with novel, unseen object categories.\nMoreover, these models most often operate on a single frame, which may yield\nincorrect classifications in case of ambiguous viewpoints. In this paper, we\npropose an active inference agent that actively gathers evidence for object\nclassifications, and can learn novel object categories over time. Drawing\ninspiration from the human brain, we build object-centric generative models\ncomposed of two information streams, a what- and a where-stream. The\nwhat-stream predicts whether the observed object belongs to a specific\ncategory, while the where-stream is responsible for representing the object in\nits internal 3D reference frame. We show that our agent (i) is able to learn\nrepresentations for many object categories in an unsupervised way, (ii)\nachieves state-of-the-art classification accuracies, actively resolving\nambiguity when required and (iii) identifies novel object categories.\nFurthermore, we validate our system in an end-to-end fashion where the agent is\nable to search for an object at a given pose from a pixel-based rendering. We\nbelieve that this is a first step towards building modular, intelligent systems\nthat can be used for a wide range of tasks involving three dimensional objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maele_T/0/1/0/all/0/1\">Toon Van de Maele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1\">Tim Verbelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catal_O/0/1/0/all/0/1\">Ozan Catal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1\">Bart Dhoedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Sclerosis Lesions Identification/Segmentation in Magnetic Resonance Imaging using Ensemble CNN and Uncertainty Classification. (arXiv:2108.11791v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11791","description":"<p>To date, several automated strategies for identification/segmentation of\nMultiple Sclerosis (MS) lesions by Magnetic Resonance Imaging (MRI) have been\npresented which are either outperformed by human experts or, at least, whose\nresults are well distinguishable from humans. This is due to the ambiguity\noriginated by MRI instabilities, peculiar MS Heterogeneity and MRI unspecific\nnature with respect to MS. Physicians partially treat the uncertainty generated\nby ambiguity relying on personal radiological/clinical/anatomical background\nand experience.\n</p>\n<p>We present an automated framework for MS lesions identification/segmentation\nbased on three pivotal concepts to better emulate human reasoning: the modeling\nof uncertainty; the proposal of two, separately trained, CNN, one optimized\nwith respect to lesions themselves and the other to the environment surrounding\nlesions, respectively repeated for axial, coronal and sagittal directions; the\nensemble of the CNN output.\n</p>\n<p>The proposed framework is trained, validated and tested on the 2016 MSSEG\nbenchmark public data set from a single imaging modality, FLuid-Attenuated\nInversion Recovery (FLAIR). The comparison, performed on the segmented lesions\nby means of most of the metrics normally used with respect to the ground-truth\nand the 7 human raters in MSSEG, prove that there is no significant difference\nbetween the proposed framework and the other raters. Results are also shown for\nthe uncertainty, though a comparison with the other raters is impossible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Placidi_G/0/1/0/all/0/1\">Giuseppe Placidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mignosi_F/0/1/0/all/0/1\">Filippo Mignosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsinelli_M/0/1/0/all/0/1\">Matteo Polsinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Out-of-Distribution Detection Using Latent Space of $\\beta$-VAE for Cyber-Physical Systems. (arXiv:2108.11800v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11800","description":"<p>Deep Neural Networks are actively being used in the design of autonomous\nCyber-Physical Systems (CPSs). The advantage of these models is their ability\nto handle high-dimensional state-space and learn compact surrogate\nrepresentations of the operational state spaces. However, the problem is that\nthe sampled observations used for training the model may never cover the entire\nstate space of the physical environment, and as a result, the system will\nlikely operate in conditions that do not belong to the training distribution.\nThese conditions that do not belong to training distribution are referred to as\nOut-of-Distribution (OOD). Detecting OOD conditions at runtime is critical for\nthe safety of CPS. In addition, it is also desirable to identify the context or\nthe feature(s) that are the source of OOD to select an appropriate control\naction to mitigate the consequences that may arise because of the OOD\ncondition. In this paper, we study this problem as a multi-labeled time series\nOOD detection problem over images, where the OOD is defined both sequentially\nacross short time windows (change points) as well as across the training data\ndistribution. A common approach to solving this problem is the use of\nmulti-chained one-class classifiers. However, this approach is expensive for\nCPSs that have limited computational resources and require short inference\ntimes. Our contribution is an approach to design and train a single\n$\\beta$-Variational Autoencoder detector with a partially disentangled latent\nspace sensitive to variations in image features. We use the feature sensitive\nlatent variables in the latent space to detect OOD images and identify the most\nlikely feature(s) responsible for the OOD. We demonstrate our approach using an\nAutonomous Vehicle in the CARLA simulator and a real-world automotive dataset\ncalled nuImages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishna_S/0/1/0/all/0/1\">Shreyas Ramakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahiminasab_Z/0/1/0/all/0/1\">Zahra Rahiminasab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karsai_G/0/1/0/all/0/1\">Gabor Karsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhishek Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human readable network troubleshooting based on anomaly detection and feature scoring. (arXiv:2108.11807v1 [cs.NI])","link":"http://arxiv.org/abs/2108.11807","description":"<p>Network troubleshooting is still a heavily human-intensive process. To reduce\nthe time spent by human operators in the diagnosis process, we present a system\nbased on (i) unsupervised learning methods for detecting anomalies in the time\ndomain, (ii) an attention mechanism to rank features in the feature space and\nfinally (iii) an expert knowledge module able to seamlessly incorporate\npreviously collected domain-knowledge. In this paper, we thoroughly evaluate\nthe performance of the full system and of its individual building blocks:\nparticularly, we consider (i) 10 anomaly detection algorithms as well as (ii)\n10 attention mechanisms, that comprehensively represent the current state of\nthe art in the respective fields. Leveraging a unique collection of\nexpert-labeled datasets worth several months of real router telemetry data, we\nperform a thorough performance evaluation contrasting practical results in\nconstrained stream-mode settings, with the results achievable by an ideal\noracle in academic settings. Our experimental evaluation shows that (i) the\nproposed system is effective in achieving high levels of agreement with the\nexpert, and (ii) that even a simple statistical approach is able to extract\nuseful information from expert knowledge gained in past cases, significantly\nimproving troubleshooting performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1\">Jose M. Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huet_A/0/1/0/all/0/1\">Alexis Huet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When should agents explore?. (arXiv:2108.11811v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11811","description":"<p>Exploration remains a central challenge for reinforcement learning (RL).\nVirtually all existing methods share the feature of a monolithic behaviour\npolicy that changes only gradually (at best). In contrast, the exploratory\nbehaviours of animals and humans exhibit a rich diversity, namely including\nforms of switching between modes. This paper presents an initial study of\nmode-switching, non-monolithic exploration for RL. We investigate different\nmodes to switch between, at what timescales it makes sense to switch, and what\nsignals make for good switching triggers. We also propose practical algorithmic\ncomponents that make the switching mechanism adaptive and robust, which enables\nflexibility without an accompanying hyper-parameter-tuning burden. Finally, we\nreport a promising and detailed analysis on Atari, using two-mode exploration\nand switching at sub-episodic time-scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pislar_M/0/1/0/all/0/1\">Miruna P&#xee;slar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_D/0/1/0/all/0/1\">David Szepesvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostrovski_G/0/1/0/all/0/1\">Georg Ostrovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1\">Diana Borsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaul_T/0/1/0/all/0/1\">Tom Schaul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magnetic Field Sensing for Pedestrian and Robot Indoor Positioning. (arXiv:2108.11824v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11824","description":"<p>In this paper we address the problem of indoor localization using magnetic\nfield data in two setups, when data is collected by (i) human-held mobile phone\nand (ii) by localization robots that perturb magnetic data with their own\nelectromagnetic field. For the first setup, we revise the state of the art\napproaches and propose a novel extended pipeline to benefit from the presence\nof magnetic anomalies in indoor environment created by different ferromagnetic\nobjects. We capture changes of the Earth's magnetic field due to indoor\nmagnetic anomalies and transform them in multi-variate times series. We then\nconvert temporal patterns into visual ones. We use methods of Recurrence Plots,\nGramian Angular Fields and Markov Transition Fields to represent magnetic field\ntime series as image sequences. We regress the continuous values of user\nposition in a deep neural network that combines convolutional and recurrent\nlayers. For the second setup, we analyze how magnetic field data get perturbed\nby robots' electromagnetic field. We add an alignment step to the main\npipeline, in order to compensate the mismatch between train and test sets\nobtained by different robots. We test our methods on two public (MagPie and\nIPIN'20) and one proprietary (Hyundai department store) datasets. We report\nevaluation results and show that our methods outperform the state of the art\nmethods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antsfeld_L/0/1/0/all/0/1\">Leonid Antsfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gene Transformer: Transformers for the Gene Expression-based Classification of Cancer Subtypes. (arXiv:2108.11833v1 [q-bio.QM])","link":"http://arxiv.org/abs/2108.11833","description":"<p>Adenocarcinoma and squamous cell carcinoma constitute approximately 40% and\n30% of all lung cancer subtypes, respectively, and display broad heterogeneity\nin terms of clinical and molecular responses to therapy. Molecular subtyping\nhas enabled precision medicine to overcome these challenges and provide\nsignificant biological insights to predict prognosis and improve clinical\ndecision making. Over the past decade, conventional ML algorithms and DL-based\nCNNs have been espoused for the classification of cancer subtypes from gene\nexpression datasets. However, these methods are potentially biased toward\nidentification of cancer biomarkers. Recently proposed transformer-based\narchitectures that leverage the self-attention mechanism encode high throughput\ngene expressions and learn representations that are computationally complex and\nparametrically expensive. However, compared to the datasets for natural\nlanguage processing applications, gene expression consists of several hundreds\nof thousands of genes from a limited number of observations, making it\ndifficult to efficiently train transformers for bioinformatics applications.\nHence, we propose an end-to-end deep learning approach, Gene Transformer, which\naddresses the complexity of high-dimensional gene expression with a multi-head\nself-attention module by identifying relevant biomarkers across multiple cancer\nsubtypes without requiring feature selection as a prerequisite for the current\nclassification algorithms. The proposed architecture achieved an overall\nimproved performance for all evaluation metrics and had fewer misclassification\nerrors than the commonly used traditional classification algorithms. The\nclassification results show that Gene Transformer can be an efficient approach\nfor classifying cancer subtypes, indicating that any improvement in deep\nlearning models in computational biology can also be reflected well in this\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_A/0/1/0/all/0/1\">Anwar Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lee_B/0/1/0/all/0/1\">Boreom Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry Based Machining Feature Retrieval with Inductive Transfer Learning. (arXiv:2108.11838v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11838","description":"<p>Manufacturing industries have widely adopted the reuse of machine parts as a\nmethod to reduce costs and as a sustainable manufacturing practice.\nIdentification of reusable features from the design of the parts and finding\ntheir similar features from the database is an important part of this process.\nIn this project, with the help of fully convolutional geometric features, we\nare able to extract and learn the high level semantic features from CAD models\nwith inductive transfer learning. The extracted features are then compared with\nthat of other CAD models from the database using Frobenius norm and identical\nfeatures are retrieved. Later we passed the extracted features to a deep\nconvolutional neural network with a spatial pyramid pooling layer and the\nperformance of the feature retrieval increased significantly. It was evident\nfrom the results that the model could effectively capture the geometrical\nelements from machining features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_N/0/1/0/all/0/1\">N S Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HB_B/0/1/0/all/0/1\">Barathi Ganesh HB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variyar_S/0/1/0/all/0/1\">Sajith Variyar VV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1\">Sowmya V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KP_S/0/1/0/all/0/1\">Soman KP</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI at work -- Mitigating safety and discriminatory risk with technical standards. (arXiv:2108.11844v1 [cs.CY])","link":"http://arxiv.org/abs/2108.11844","description":"<p>The use of artificial intelligence (AI) and AI methods in the workplace holds\nboth great opportunities as well as risks to occupational safety and\ndiscrimination. In addition to legal regulation, technical standards will play\na key role in mitigating such risk by defining technical requirements for\ndevelopment and testing of AI systems. This paper provides an overview and\nassessment of existing international, European and German standards as well as\nthose currently under development. The paper is part of the research project\n\"ExamAI - Testing and Auditing of AI systems\" and focusses on the use of AI in\nan industrial production environment as well as in the realm of human resource\nmanagement (HR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becker_N/0/1/0/all/0/1\">Nikolas Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junginger_P/0/1/0/all/0/1\">Pauline Junginger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_L/0/1/0/all/0/1\">Lukas Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krupka_D/0/1/0/all/0/1\">Daniel Krupka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beining_L/0/1/0/all/0/1\">Leonie Beining</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Graph Contrastive Learning. (arXiv:2108.11873v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11873","description":"<p>Deep learning models are modern tools for spatio-temporal graph (STG)\nforecasting. Despite their effectiveness, they require large-scale datasets to\nachieve better performance and are vulnerable to noise perturbation. To\nalleviate these limitations, an intuitive idea is to use the popular data\naugmentation and contrastive learning techniques. However, existing graph\ncontrastive learning methods cannot be directly applied to STG forecasting due\nto three reasons. First, we empirically discover that the forecasting task is\nunable to benefit from the pretrained representations derived from contrastive\nlearning. Second, data augmentations that are used for defeating noise are less\nexplored for STG data. Third, the semantic similarity of samples has been\noverlooked. In this paper, we propose a Spatio-Temporal Graph Contrastive\nLearning framework (STGCL) to tackle these issues. Specifically, we improve the\nperformance by integrating the forecasting loss with an auxiliary contrastive\nloss rather than using a pretrained paradigm. We elaborate on four types of\ndata augmentations, which disturb data in terms of graph structure, time\ndomain, and frequency domain. We also extend the classic contrastive loss\nthrough a rule-based strategy that filters out the most semantically similar\nnegatives. Our framework is evaluated across three real-world datasets and four\nstate-of-the-art models. The consistent improvements demonstrate that STGCL can\nbe used as an off-the-shelf plug-in for existing deep models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human operator cognitive availability aware Mixed-Initiative control. (arXiv:2108.11885v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11885","description":"<p>This paper presents a Cognitive Availability Aware Mixed-Initiative\nController for remotely operated mobile robots. The controller enables dynamic\nswitching between different levels of autonomy (LOA), initiated by either the\nAI or the human operator. The controller leverages a state-of-the-art computer\nvision method and an off-the-shelf web camera to infer the cognitive\navailability of the operator and inform the AI-initiated LOA switching. This\nconstitutes a qualitative advancement over previous Mixed-Initiative (MI)\ncontrollers. The controller is evaluated in a disaster response experiment, in\nwhich human operators have to conduct an exploration task with a remote robot.\nMI systems are shown to effectively assist the operators, as demonstrated by\nquantitative and qualitative results in performance and workload. Additionally,\nsome insights into the experimental difficulties of evaluating complex MI\ncontrollers are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petousakis_G/0/1/0/all/0/1\">Giannis Petousakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1\">Manolis Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaou_G/0/1/0/all/0/1\">Grigoris Nikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolkin_R/0/1/0/all/0/1\">Rustam Stolkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Reinforcement Learning: Techniques, Applications, and Open Challenges. (arXiv:2108.11887v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11887","description":"<p>This paper presents a comprehensive survey of Federated Reinforcement\nLearning (FRL), an emerging and promising field in Reinforcement Learning (RL).\nStarting with a tutorial of Federated Learning (FL) and RL, we then focus on\nthe introduction of FRL as a new method with great potential by leveraging the\nbasic idea of FL to improve the performance of RL while preserving\ndata-privacy. According to the distribution characteristics of the agents in\nthe framework, FRL algorithms can be divided into two categories, i.e.\nHorizontal Federated Reinforcement Learning (HFRL) and Vertical Federated\nReinforcement Learning (VFRL). We provide the detailed definitions of each\ncategory by formulas, investigate the evolution of FRL from a technical\nperspective, and highlight its advantages over previous RL algorithms. In\naddition, the existing works on FRL are summarized by application fields,\nincluding edge computing, communication, control optimization, and attack\ndetection. Finally, we describe and discuss several key research directions\nthat are crucial to solving the open problems within FRL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiaju Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qihao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lei Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Discovering Effective Interaction Kernels between Celestial Bodies from Ephemerides. (arXiv:2108.11894v1 [astro-ph.EP])","link":"http://arxiv.org/abs/2108.11894","description":"<p>Building accurate and predictive models of the underlying mechanisms of\ncelestial motion has inspired fundamental developments in theoretical physics.\nCandidate theories seek to explain observations and predict future positions of\nplanets, stars, and other astronomical bodies as faithfully as possible. We use\na data-driven learning approach, extending that developed in Lu et al. ($2019$)\nand extended in Zhong et al. ($2020$), to a derive stable and accurate model\nfor the motion of celestial bodies in our Solar System. Our model is based on a\ncollective dynamics framework, and is learned from the NASA Jet Propulsion\nLab's development ephemerides. By modeling the major astronomical bodies in the\nSolar System as pairwise interacting agents, our learned model generate\nextremely accurate dynamics that preserve not only intrinsic geometric\nproperties of the orbits, but also highly sensitive features of the dynamics,\nsuch as perihelion precession rates. Our learned model can provide a unified\nexplanation to the observation data, especially in terms of reproducing the\nperihelion precession of Mars, Mercury, and the Moon. Moreover, Our model\noutperforms Newton's Law of Universal Gravitation in all cases and performs\nsimilarly to, and exceeds on the Moon, the Einstein-Infeld-Hoffman equations\nderived from Einstein's theory of general relativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Miller_J/0/1/0/all/0/1\">Jason Miller</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Maggioni_M/0/1/0/all/0/1\">Mauro Maggioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketches for Time-Dependent Machine Learning. (arXiv:2108.11923v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11923","description":"<p>Time series data can be subject to changes in the underlying process that\ngenerates them and, because of these changes, models built on old samples can\nbecome obsolete or perform poorly. In this work, we present a way to\nincorporate information about the current data distribution and its evolution\nacross time into machine learning algorithms. Our solution is based on\nefficiently maintaining statistics, particularly the mean and the variance, of\ndata features at different time resolutions. These data summarisations can be\nperformed over the input attributes, in which case they can then be fed into\nthe model as additional input features, or over latent representations learned\nby models, such as those of Recurrent Neural Networks. In classification tasks,\nthe proposed techniques can significantly outperform the prediction\ncapabilities of equivalent architectures with no feature / latent\nsummarisations. Furthermore, these modifications do not introduce notable\ncomputational and memory overhead when properly adjusted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antonanzas_J/0/1/0/all/0/1\">Jesus Antonanzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_M/0/1/0/all/0/1\">Marta Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weisfeiler-Leman in the BAMBOO: Novel AMR Graph Metrics and a Benchmark for AMR Graph Similarity. (arXiv:2108.11949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11949","description":"<p>Several metrics have been proposed for assessing the similarity of (abstract)\nmeaning representations (AMRs), but little is known about how they relate to\nhuman similarity ratings. Moreover, the current metrics have complementary\nstrengths and weaknesses: some emphasize speed, while others make the alignment\nof graph structures explicit, at the price of a costly alignment step.\n</p>\n<p>In this work we propose new Weisfeiler-Leman AMR similarity metrics that\nunify the strengths of previous metrics, while mitigating their weaknesses.\nSpecifically, our new metrics are able to match contextualized substructures\nand induce n:m alignments between their nodes. Furthermore, we introduce a\nBenchmark for AMR Metrics based on Overt Objectives (BAMBOO), the first\nbenchmark to support empirical assessment of graph-based MR similarity metrics.\nBAMBOO maximizes the interpretability of results by defining multiple overt\nobjectives that range from sentence similarity objectives to stress tests that\nprobe a metric's robustness against meaning-altering and meaning-preserving\ngraph transformations. We show the benefits of BAMBOO by profiling previous\nmetrics and our own metrics. Results indicate that our novel metrics may serve\nas a strong baseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_A/0/1/0/all/0/1\">Angel Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples. (arXiv:2009.05487v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2009.05487","description":"<p>The same method that creates adversarial examples (AEs) to fool\nimage-classifiers can be used to generate counterfactual explanations (CEs)\nthat explain algorithmic decisions. This observation has led researchers to\nconsider CEs as AEs by another name. We argue that the relationship to the true\nlabel and the tolerance with respect to proximity are two properties that\nformally distinguish CEs and AEs. Based on these arguments, we introduce CEs,\nAEs, and related concepts mathematically in a common framework. Furthermore, we\nshow connections between current methods for generating CEs and AEs, and\nestimate that the fields will merge more and more as the number of common\nuse-cases grows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Refinement Network for Point Cloud Completion with Self-supervision. (arXiv:2010.08719v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08719","description":"<p>Point clouds are often sparse and incomplete, which imposes difficulties for\nreal-world applications. Existing shape completion methods tend to generate\nrough shapes without fine-grained details. Considering this, we introduce a\ntwo-branch network for shape completion. The first branch is a cascaded shape\ncompletion sub-network to synthesize complete objects, where we propose to use\nthe partial input together with the coarse output to preserve the object\ndetails during the dense point reconstruction. The second branch is an\nauto-encoder to reconstruct the original partial input. The two branches share\na same feature extractor to learn an accurate global feature for shape\ncompletion. Furthermore, we propose two strategies to enable the training of\nour network when ground truth data are not available. This is to mitigate the\ndependence of existing approaches on large amounts of ground truth training\ndata that are often difficult to obtain in real-world applications.\nAdditionally, our proposed strategies are also able to improve the\nreconstruction quality for fully supervised learning. We verify our approach in\nself-supervised, semi-supervised and fully supervised settings with superior\nperformances. Quantitative and qualitative results on different datasets\ndemonstrate that our method achieves more realistic outputs than\nstate-of-the-art approaches on the point cloud completion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network iLQR: A Reinforcement Learning Architecture for Trajectory Optimization. (arXiv:2011.10737v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.10737","description":"<p>As a notable machine learning paradigm, the research efforts in the context\nof reinforcement learning have certainly progressed leaps and bounds. When\ncompared with reinforcement learning methods with the given system model, the\nmethodology of the reinforcement learning architecture based on the unknown\nmodel generally exhibits significantly broader universality and applicability.\nIn this work, a new reinforcement learning architecture based on iterative\nlinear quadratic regulator (iLQR) is developed and presented without the\nrequirement of any prior knowledge of the system model, which is termed as an\napproach of a \"neural network iterative linear quadratic regulator (NNiLQR)\".\nDepending solely on measurement data, this method yields a completely new\nnon-parametric routine for the establishment of the optimal policy (without the\nnecessity of system modeling) through iterative refinements of the neural\nnetwork system. Rather importantly, this approach significantly outperforms the\nclassical iLQR method in terms of the given objective function because of the\ninnovative utilization of further exploration in the methodology. As clearly\nindicated from the results attained in two illustrative examples, these\nsignificant merits of the NNiLQR method are demonstrated rather evidently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zilong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_F/0/1/0/all/0/1\">Frank L. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multistage BiCross encoder for multilingual access to COVID-19 health information. (arXiv:2101.03013v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2101.03013","description":"<p>The Coronavirus (COVID-19) pandemic has led to a rapidly growing 'infodemic'\nof health information online. This has motivated the need for accurate semantic\nsearch and retrieval of reliable COVID-19 information across millions of\ndocuments, in multiple languages. To address this challenge, this paper\nproposes a novel high precision and high recall neural Multistage BiCross\nencoder approach. It is a sequential three-stage ranking pipeline which uses\nthe Okapi BM25 retrieval algorithm and transformer-based bi-encoder and\ncross-encoder to effectively rank the documents with respect to the given\nquery. We present experimental results from our participation in the\nMultilingual Information Access (MLIA) shared task on COVID-19 multilingual\nsemantic search. The independently evaluated MLIA results validate our approach\nand demonstrate that it outperforms other state-of-the-art approaches according\nto nearly all evaluation metrics in cases of both monolingual and bilingual\nruns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Iknoor Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Knowledge Graph Forecasting with Neural ODE. (arXiv:2101.05151v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.05151","description":"<p>Learning node representation on dynamically-evolving, multi-relational graph\ndata has gained great research interest. However, most of the existing models\nfor temporal knowledge graph forecasting use Recurrent Neural Network (RNN)\nwith discrete depth to capture temporal information, while time is a continuous\nvariable. Inspired by Neural Ordinary Differential Equation (NODE), we extend\nthe idea of continuum-depth models to time-evolving multi-relational graph\ndata, and propose a novel Temporal Knowledge Graph Forecasting model with NODE.\nOur model captures temporal information through NODE and structural information\nthrough a Graph Neural Network (GNN). Thus, our graph ODE model achieves a\ncontinuous model in time and efficiently learns node representation for future\nprediction. We evaluate our model on six temporal knowledge graph datasets by\nperforming link forecasting. Experiment results show the superiority of our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yujia Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. (arXiv:2101.11075v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.11075","description":"<p>We introduce MADGRAD, a novel optimization method in the family of AdaGrad\nadaptive gradient methods. MADGRAD shows excellent performance on deep learning\noptimization problems from multiple fields, including classification and\nimage-to-image tasks in vision, and recurrent and bidirectionally-masked models\nin natural language processing. For each of these tasks, MADGRAD matches or\noutperforms both SGD and ADAM in test set performance, even on problems for\nwhich adaptive methods normally perform poorly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1\">Aaron Defazio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jelassi_S/0/1/0/all/0/1\">Samy Jelassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Solving the DeepFake Problem : An Analysis on Improving DeepFake Detection using Dynamic Face Augmentation. (arXiv:2102.09603v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09603","description":"<p>The creation of altered and manipulated faces has become more common due to\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\ndetection models' development for differentiating between a manipulated and\noriginal face from image or video content. In this paper, we focus on\nidentifying the limitations and shortcomings of existing deepfake detection\nframeworks. We identified some key problems surrounding deepfake detection\nthrough quantitative and qualitative analysis of existing methods and datasets.\nWe found that deepfake datasets are highly oversampled, causing models to\nbecome easily overfitted. The datasets are created using a small set of real\nfaces to generate multiple fake samples. When trained on these datasets, models\ntend to memorize the actors' faces and labels instead of learning fake\nfeatures. To mitigate this problem, we propose a simple data augmentation\nmethod termed Face-Cutout. Our method dynamically cuts out regions of an image\nusing the face landmark information. It helps the model selectively attend to\nonly the relevant regions of the input. Our evaluation experiments show that\nFace-Cutout can successfully improve the data variation and alleviate the\nproblem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to\n35.3% on different datasets, compared to other occlusion-based techniques.\nMoreover, we also propose a general-purpose data pre-processing guideline to\ntrain and evaluate existing architectures allowing us to improve the\ngeneralizability of these models for deepfake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seferbekov_S/0/1/0/all/0/1\">Selim Seferbekov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Arup Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Input Weight Initialization for Echo State Networks. (arXiv:2103.04710v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04710","description":"<p>Echo State Networks (ESNs) are a special type of recurrent neural networks\n(RNNs), in which the input and recurrent connections are traditionally\ngenerated randomly, and only the output weights are trained. Despite the recent\nsuccess of ESNs in various tasks of audio, image and radar recognition, we\npostulate that a purely random initialization is not the ideal way of\ninitializing ESNs. The aim of this work is to propose an unsupervised\ninitialization of the input connections using the K-Means algorithm on the\ntraining data. We show that for a large variety of datasets this initialization\nperforms equivalently or superior than a randomly initialized ESN whilst\nneeding significantly less reservoir neurons. Furthermore, we discuss that this\napproach provides the opportunity to estimate a suitable size of the reservoir\nbased on prior knowledge about the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_P/0/1/0/all/0/1\">Peter Steiner</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_A/0/1/0/all/0/1\">Azarakhsh Jalalvand</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Birkholz_P/0/1/0/all/0/1\">Peter Birkholz</a> (1) ((1) Institute for Acoustics and Speech Communication, Technische Universit&#xe4;t Dresden, Dresden, Germany, (2) IDLab, Ghent University, Belgium, (3) Aerospace Engineering department, Princeton University, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning Workloads. (arXiv:2104.05755v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.05755","description":"<p>During the past decade, novel Deep Learning (DL) algorithms/workloads and\nhardware have been developed to tackle a wide range of problems. Despite the\nadvances in workload/hardware ecosystems, the programming methodology of DL\nsystems is stagnant. DL workloads leverage either highly optimized, yet\nplatform-specific and inflexible kernels from DL libraries, or in the case of\nnovel operators, reference implementations are built via DL framework\nprimitives with underwhelming performance. This work introduces the Tensor\nProcessing Primitives (TPP), a programming abstraction striving for efficient,\nportable implementation of DL workloads with high productivity. TPPs define a\ncompact, yet versatile set of 2D-tensor operators (or a virtual Tensor ISA),\nwhich subsequently can be utilized as building blocks to construct complex\noperators on high-dimensional tensors. The TPP specification is\nplatform-agnostic, thus code expressed via TPPs is portable, whereas the TPP\nimplementation is highly optimized and platform-specific. We demonstrate the\nefficacy of our approach using standalone kernels and end-to-end DL workloads\nexpressed entirely via TPPs that outperform state-of-the-art implementations on\nmultiple platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georganas_E/0/1/0/all/0/1\">Evangelos Georganas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalamkar_D/0/1/0/all/0/1\">Dhiraj Kalamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1\">Sasikanth Avancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelman_M/0/1/0/all/0/1\">Menachem Adelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Cristina Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuer_A/0/1/0/all/0/1\">Alexander Breuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruestle_J/0/1/0/all/0/1\">Jeremy Bruestle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_N/0/1/0/all/0/1\">Narendra Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1\">Abhisek Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutnick_D/0/1/0/all/0/1\">Denise Kutnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laub_F/0/1/0/all/0/1\">Frank Laub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Md_V/0/1/0/all/0/1\">Vasimuddin Md</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1\">Sanchit Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_R/0/1/0/all/0/1\">Ramanarayan Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabst_H/0/1/0/all/0/1\">Hans Pabst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziv_B/0/1/0/all/0/1\">Barukh Ziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinecke_A/0/1/0/all/0/1\">Alexander Heinecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01883","description":"<p>We propose RepMLP, a multi-layer-perceptron-style neural network building\nblock for image recognition, which is composed of a series of fully-connected\n(FC) layers. Compared to convolutional layers, FC layers are more efficient,\nbetter at modeling the long-range dependencies and positional patterns, but\nworse at capturing the local structures, hence usually less favored for image\nrecognition. We propose a structural re-parameterization technique that adds\nlocal prior into an FC to make it powerful for image recognition. Specifically,\nwe construct convolutional layers inside a RepMLP during training and merge\nthem into the FC for inference. On CIFAR, a simple pure-MLP model shows\nperformance very close to CNN. By inserting RepMLP in traditional CNN, we\nimprove ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and\n2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight\nthat combining the global representational capacity and positional perception\nof FC with the local prior of convolution can improve the performance of neural\nnetwork with faster speed on both the tasks with translation invariance (e.g.,\nsemantic segmentation) and those with aligned images and positional patterns\n(e.g., face recognition). The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chunlong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementation and Evaluation of a Multivariate Abstraction-Based, Interval-Based Dynamic Time-Warping Method as a Similarity Measure for Longitudinal Medical Records. (arXiv:2105.08450v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2105.08450","description":"<p>We extended dynamic time warping (DTW) into interval-based dynamic time\nwarping (iDTW), including (A) interval-based representation (iRep): [1]\nabstracting raw, time-stamped data into interval-based abstractions, [2]\ncomparison-period scoping, [3] partitioning abstract intervals into a given\ntemporal granularity; (B) interval-based matching (iMatch): matching\npartitioned, abstract-concepts records, using a modified DTW. Using domain\nknowledge, we abstracted the raw data of medical records, for up to three\nconcepts out of four or five relevant concepts, into two interval types: State\nabstractions (e.g. LOW, HIGH) and Gradient abstractions (e.g. INCREASING,\nDECREASING). We created all uni-dimensional (State or Gradient) or\nmulti-dimensional (State and Gradient) abstraction combinations. Tasks:\nClassifying 161 oncology patients records as autologous or allogenic\nbone-marrow transplantation; classifying 125 hepatitis patients records as B or\nC hepatitis; predicting micro- or macro-albuminuria in the next year for 151\nType 2 diabetes patients. We used a k-Nearest-Neighbors majority, k = an odd\nnumber from 1 to SQRT(N), N = set size. 75,936 10-fold cross-validation\nexperiments were performed: 33,600 (Oncology), 28,800 (Hepatitis), 13,536\n(Diabetes). Measures: Area Under the Curve (AUC), optimal Youden's Index.\nPaired t-tests compared result vectors for equivalent configurations other than\na tested variable, to determine a significant mean accuracy difference\n(P&lt;0.05). Mean classification and prediction using abstractions was\nsignificantly better than using only raw time-stamped data. In each domain, at\nleast one abstraction combination led to a significantly better mean\nperformance than raw data. Increasing feature number and using\nMulti-dimensional abstractions enhanced performance. Unlike when using raw\ndata, optimal mean performance was often reached with k=5, using abstractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahar_Y/0/1/0/all/0/1\">Yuval Shahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lion_M/0/1/0/all/0/1\">Matan Lion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2106.06895","description":"<p>To reduce the time-to-market and access to state-of-the-art techniques, CNN\nhardware mapping and deployment on embedded accelerators are often outsourced\nto untrusted third parties, which is going to be more prevalent in futuristic\nartificial intelligence of things (AIoT) systems. These AIoT systems anticipate\nhorizontal collaboration among different resource-constrained AIoT node\ndevices, where CNN layers are partitioned and these devices collaboratively\ncompute complex CNN tasks. This horizontal collaboration opens another attack\nsurface to the CNN-based application, like inserting the hardware Trojans (HT)\ninto the embedded accelerators designed for the CNN. Therefore, there is a dire\nneed to explore this attack surface for designing secure embedded hardware\naccelerators for CNNs. Towards this goal, in this paper, we exploited this\nattack surface to propose an HT-based attack called FeSHI. Since in horizontal\ncollaboration of RC AIoT devices different sections of CNN architectures are\noutsourced to different untrusted third parties, the attacker may not know the\ninput image, but it has access to the layer-by-layer output feature maps\ninformation for the assigned sections of the CNN architecture. This attack\nexploits the statistical distribution, i.e., Gaussian distribution, of the\nlayer-by-layer feature maps of the CNN to design two triggers for stealthy HT\nwith a very low probability of triggering. Also, three different novel,\nstealthy and effective trigger designs are proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1\">Faiq Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1\">Travis Sandefur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware. (arXiv:2106.07030v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2106.07030","description":"<p>The capabilities of natural neural systems have inspired new generations of\nmachine learning algorithms as well as neuromorphic very large-scale integrated\n(VLSI) circuits capable of fast, low-power information processing. However, it\nhas been argued that most modern machine learning algorithms are not\nneurophysiologically plausible. In particular, the workhorse of modern deep\nlearning, the backpropagation algorithm, has proven difficult to translate to\nneuromorphic hardware. In this study, we present a neuromorphic, spiking\nbackpropagation algorithm based on synfire-gated dynamical information\ncoordination and processing, implemented on Intel's Loihi neuromorphic research\nprocessor. We demonstrate a proof-of-principle three-layer circuit that learns\nto classify digits from the MNIST dataset. To our knowledge, this is the first\nwork to show a Spiking Neural Network (SNN) implementation of the\nbackpropagation algorithm that is fully on-chip, without a computer in the\nloop. It is competitive in accuracy with off-chip trained SNNs and achieves an\nenergy-delay product suitable for edge computing. This implementation shows a\npath for using in-memory, massively parallel neuromorphic processors for\nlow-power, low-latency implementation of modern deep learning applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renner_A/0/1/0/all/0/1\">Alpha Renner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheldon_F/0/1/0/all/0/1\">Forrest Sheldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlotnik_A/0/1/0/all/0/1\">Anatoly Zlotnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Louis Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sornborger_A/0/1/0/all/0/1\">Andrew Sornborger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Smart and Defensive Human-Machine Approach to Code Analysis. (arXiv:2108.03294v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.03294","description":"<p>Static analysis remains one of the most popular approaches for detecting and\ncorrecting poor or vulnerable program code. It involves the examination of code\nlistings, test results, or other documentation to identify errors, violations\nof development standards, or other problems, with the ultimate goal of fixing\nthese errors so that systems and software are as secure as possible. There\nexists a plethora of static analysis tools, which makes it challenging for\nbusinesses and programmers to select a tool to analyze their program code. It\nis imperative to find ways to improve code analysis so that it can be employed\nby cyber defenders to mitigate security risks. In this research, we propose a\nmethod that employs the use of virtual assistants to work with programmers to\nensure that software are as safe as possible in order to protect\nsafety-critical systems from data breaches and other attacks. The proposed\nmethod employs a recommender system that uses various metrics to help\nprogrammers select the most appropriate code analysis tool for their project\nand guides them through the analysis process. The system further tracks the\nuser's behavior regarding the adoption of the recommended practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nembhard_F/0/1/0/all/0/1\">Fitzroy D. Nembhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1\">Marco M. Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking of AlphaStar. (arXiv:2108.03452v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.03452","description":"<p>We present a different view for AlphaStar (AS), the program achieving\nGrand-Master level in the game StarCraft II. It is considered big progress for\nAI research. However, in this paper, we present problems with the AS, some of\nwhich are the defects of it, and some of which are important details that are\nneglected in its article. These problems arise two questions. One is that what\ncan we get from the built of AS? The other is that does the battle between it\nwith humans fair? After the discussion, we present the future research\ndirections for these problems. Our study is based on a reproduction code of the\nAS, and the codes are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruo-Ze Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04238","description":"<p>Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-Optimal Planning for Quadrotor Waypoint Flight. (arXiv:2108.04537v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.04537","description":"<p>Quadrotors are among the most agile flying robots. However, planning\ntime-optimal trajectories at the actuation limit through multiple waypoints\nremains an open problem. This is crucial for applications such as inspection,\ndelivery, search and rescue, and drone racing. Early works used polynomial\ntrajectory formulations, which do not exploit the full actuator potential\nbecause of their inherent smoothness. Recent works resorted to numerical\noptimization but require waypoints to be allocated as costs or constraints at\nspecific discrete times. However, this time allocation is a priori unknown and\nrenders previous works incapable of producing truly time-optimal trajectories.\nTo generate truly time-optimal trajectories, we propose a solution to the time\nallocation problem while exploiting the full quadrotor's actuator potential. We\nachieve this by introducing a formulation of progress along the trajectory,\nwhich enables the simultaneous optimization of the time allocation and the\ntrajectory itself. We compare our method against related approaches and\nvalidate it in real-world flights in one of the world's largest motion-capture\nsystems, where we outperform human expert drone pilots in a drone-racing task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foehn_P/0/1/0/all/0/1\">Philipp Foehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Angel Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is Color: Image based Spatial Gene Expression Prediction using Neural Stain Learning. (arXiv:2108.10446v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.10446","description":"<p>\"Is it possible to predict expression levels of different genes at a given\nspatial location in the routine histology image of a tumor section by modeling\nits stain absorption characteristics?\" In this work, we propose a \"stain-aware\"\nmachine learning approach for prediction of spatial transcriptomic gene\nexpression profiles using digital pathology image of a routine Hematoxylin &amp;\nEosin (H&amp;E) histology section. Unlike recent deep learning methods which are\nused for gene expression prediction, our proposed approach termed Neural Stain\nLearning (NSL) explicitly models the association of stain absorption\ncharacteristics of the tissue with gene expression patterns in spatial\ntranscriptomics by learning a problem-specific stain deconvolution matrix in an\nend-to-end manner. The proposed method with only 11 trainable weight parameters\noutperforms both classical regression models with cellular composition and\nmorphological features as well as deep learning methods. We have found that the\ngene expression predictions from the proposed approach show higher correlations\nwith true expression values obtained through sequencing for a larger set of\ngenes in comparison to other approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1\">Muhammad Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Branson_K/0/1/0/all/0/1\">Kim Branson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz ul Amir Afsar Minhas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Word is Mightier than the Label: Learning without Pointillistic Labels using Data Programming. (arXiv:2108.10921v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.10921","description":"<p>Most advanced supervised Machine Learning (ML) models rely on vast amounts of\npoint-by-point labelled training examples. Hand-labelling vast amounts of data\nmay be tedious, expensive, and error-prone. Recently, some studies have\nexplored the use of diverse sources of weak supervision to produce competitive\nend model classifiers. In this paper, we survey recent work on weak\nsupervision, and in particular, we investigate the Data Programming (DP)\nframework. Taking a set of potentially noisy heuristics as input, DP assigns\ndenoised probabilistic labels to each data point in a dataset using a\nprobabilistic graphical model of heuristics. We analyze the math fundamentals\nbehind DP and demonstrate the power of it by applying it on two real-world text\nclassification tasks. Furthermore, we compare DP with pointillistic active and\nsemi-supervised learning techniques traditionally applied in data-sparse\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chufan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1\">Mononito Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"With One Voice: Composing a Travel Voice Assistant from Re-purposed Models. (arXiv:2108.11463v1 [eess.AS])","link":"http://arxiv.org/abs/2108.11463","description":"<p>Voice assistants provide users a new way of interacting with digital\nproducts, allowing them to retrieve information and complete tasks with an\nincreased sense of control and flexibility. Such products are comprised of\nseveral machine learning models, like Speech-to-Text transcription, Named\nEntity Recognition and Resolution, and Text Classification. Building a voice\nassistant from scratch takes the prolonged efforts of several teams\nconstructing numerous models and orchestrating between components. Alternatives\nsuch as using third-party vendors or re-purposing existing models may be\nconsidered to shorten time-to-market and development costs. However, each\noption has its benefits and drawbacks. We present key insights from building a\nvoice search assistant for Booking.com search and recommendation system. Our\npaper compares the achieved performance and development efforts in dedicated\ntailor-made solutions against existing re-purposed models. We share and discuss\nour data-driven decisions about implementation trade-offs and their estimated\noutcomes in hindsight, showing that a fully functional machine learning product\ncan be built from existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Poran_S/0/1/0/all/0/1\">Shachaf Poran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amsalem_G/0/1/0/all/0/1\">Gil Amsalem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beka_A/0/1/0/all/0/1\">Amit Beka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldenberg_D/0/1/0/all/0/1\">Dmitri Goldenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Attention in Machine Reading Comprehension. (arXiv:2108.11574v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11574","description":"<p>Achieving human-level performance on some of Machine Reading Comprehension\n(MRC) datasets is no longer challenging with the help of powerful Pre-trained\nLanguage Models (PLMs). However, the internal mechanism of these artifacts\nstill remains unclear, placing an obstacle for further understanding these\nmodels. This paper focuses on conducting a series of analytical experiments to\nexamine the relations between the multi-head self-attention and the final\nperformance, trying to analyze the potential explainability in PLM-based MRC\nmodels. We perform quantitative analyses on SQuAD (English) and CMRC 2018\n(Chinese), two span-extraction MRC datasets, on top of BERT, ALBERT, and\nELECTRA in various aspects. We discover that {\\em passage-to-question} and {\\em\npassage understanding} attentions are the most important ones, showing strong\ncorrelations to the final performance than other parts. Through visualizations\nand case studies, we also observe several general findings on the attention\nmaps, which could be helpful to understand how these models solve the\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVATAR: A Parallel Corpus for Java-Python Program Translation. (arXiv:2108.11590v1 [cs.SE])","link":"http://arxiv.org/abs/2108.11590","description":"<p>Program translation refers to migrating source code from one programming\nlanguage to another. It has a tremendous practical value in software\ndevelopment as porting software across different languages is time-consuming\nand costly. Automating program translation is of paramount importance in\nsoftware migration, and recently researchers explored unsupervised approaches\ndue to the unavailability of parallel corpora. However, the availability of\npre-trained language models for programming languages enable supervised\nfine-tuning with a small amount of labeled examples. In this work, we present a\ncorpus of 8,475 programming problems and their solutions written in two popular\nlanguages, Java and Python. We collect the dataset from competitive programming\nsites, online platforms, and open source repositories. We present several\nbaselines, including models trained from scratch or pre-trained on large-scale\nsource code collection and fine-tuned on our proposed dataset. Experiment\nresults show that while the models perform relatively well in terms of the\nlexical match, they lack in generating code that is accurate in terms of syntax\nand data-flow match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1\">Md Golam Rahman Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11591","description":"<p>Reading order detection is the cornerstone to understanding visually-rich\ndocuments (e.g., receipts and forms). Unfortunately, no existing work took\nadvantage of advanced deep learning models because it is too laborious to\nannotate a large enough dataset. We observe that the reading order of WORD\ndocuments is embedded in their XML metadata; meanwhile, it is easy to convert\nWORD documents to PDFs or images. Therefore, in an automated manner, we\nconstruct ReadingBank, a benchmark dataset that contains reading order, text,\nand layout information for 500,000 document images covering a wide spectrum of\ndocument types. This first-ever large-scale dataset unleashes the power of deep\nneural networks for reading order detection. Specifically, our proposed\nLayoutReader captures the text and layout information for reading order\nprediction using the seq2seq model. It performs almost perfectly in reading\norder detection and significantly improves both open-source and commercial OCR\nengines in ordering text lines in their results in our experiments. We will\nrelease the dataset and model at \\url{https://aka.ms/readingbank}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v1 [cs.SE])","link":"http://arxiv.org/abs/2108.11601","description":"<p>Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, \\tool, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. \\tool has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11607","description":"<p>In many situations (e.g., distant supervision), unlabeled entity problem\nseriously degrades the performances of named entity recognition (NER) models.\nRecently, this issue has been well addressed by a notable approach based on\nnegative sampling. In this work, we perform two studies along this direction.\nFirstly, we analyze why negative sampling succeeds both theoretically and\nempirically. Based on the observation that named entities are highly sparse in\ndatasets, we show a theoretical guarantee that, for a long sentence, the\nprobability of containing no unlabeled entities in sampled negatives is high.\nMissampling tests on synthetic datasets have verified our guarantee in\npractice. Secondly, to mine hard negatives and further reduce missampling\nrates, we propose a weighted and adaptive sampling distribution for negative\nsampling. Experiments on synthetic datasets and well-annotated datasets show\nthat our method significantly improves negative sampling in robustness and\neffectiveness. We also have achieved new state-of-the-art results on real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11626","description":"<p>As the use of interactive machines grow, the task of Emotion Recognition in\nConversation (ERC) became more important. If the machine generated sentences\nreflect emotion, more human-like sympathetic conversations are possible. Since\nemotion recognition in conversation is inaccurate if the previous utterances\nare not taken into account, many studies reflect the dialogue context to\nimprove the performances. We introduce CoMPM, a context embedding module (CoM)\ncombined with a pre-trained memory module (PM) that tracks memory of the\nspeaker's previous utterances within the context, and show that the pre-trained\nmemory significantly improves the final accuracy of emotion recognition. We\nexperimented on both the multi-party datasets (MELD, EmoryNLP) and the\ndyadic-party datasets (IEMOCAP, DailyDialog), showing that our approach achieve\ncompetitive performance on all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable End-to-End Training of Knowledge Graph-Enhanced Aspect Embedding for Aspect Level Sentiment Analysis. (arXiv:2108.11656v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11656","description":"<p>Aspect level sentiment classification (ALSC) is a difficult problem with\nstate-of-the-art models showing less than 80% macro-F1 score on benchmark\ndatasets. Existing models do not incorporate information on aspect-aspect\nrelations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem\nfrom inaccurate disambiguation of aspects to KG entities, and the inability to\nlearn aspect representations from the large KGs in joint training with ALSC\nmodels.\n</p>\n<p>We propose a two-level global-local entity embedding scheme that allows\nefficient joint training of KG-based aspect embeddings and ALSC models. A novel\nincorrect disambiguation detection technique addresses the problem of\ninaccuracy in aspect disambiguation. The proposed methods show a consistent\nimprovement of $2.5 - 4.1$ percentage points, over the recent BERT-based\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sk Mainul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourangshu Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technological Approaches to Detecting Online Disinformation and Manipulation. (arXiv:2108.11669v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11669","description":"<p>The move of propaganda and disinformation to the online environment is\npossible thanks to the fact that within the last decade, digital information\nchannels radically increased in popularity as a news source. The main advantage\nof such media lies in the speed of information creation and dissemination.\nThis, on the other hand, inevitably adds pressure, accelerating editorial work,\nfact-checking, and the scrutiny of source credibility. In this chapter, an\noverview of computer-supported approaches to detecting disinformation and\nmanipulative techniques based on several criteria is presented. We concentrate\non the technical aspects of automatic methods which support fact-checking,\ntopic identification, text style analysis, or message filtering on social media\nchannels. Most of the techniques employ artificial intelligence and machine\nlearning with feature extraction combining available information resources. The\nfollowing text firstly specifies the tasks related to computer detection of\nmanipulation and disinformation spreading. The second section presents concrete\nmethods of solving the tasks of the analysis, and the third sections enlists\ncurrent verification and benchmarking datasets published and used in this area\nfor evaluation and comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horak_A/0/1/0/all/0/1\">Ale&#x161; Hor&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baisa_V/0/1/0/all/0/1\">V&#xed;t Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_O/0/1/0/all/0/1\">Ond&#x159;ej Herman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Why Intermediate-Task Fine-Tuning Works. (arXiv:2108.11696v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11696","description":"<p>Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a\nwidely applied technique, which first fine-tunes the pretrained language models\non an intermediate task before on the target task of interest. While STILTs is\nable to further improve the performance of pretrained language models, it is\nstill unclear why and when it works. Previous research shows that those\nintermediate tasks involving complex inference, such as commonsense reasoning,\nwork especially well for RoBERTa. In this paper, we discover that the\nimprovement from an intermediate task could be orthogonal to it containing\nreasoning or other complex skills -- a simple real-fake discrimination task\nsynthesized by GPT2 can benefit diverse target tasks. We conduct extensive\nexperiments to study the impact of different factors on STILTs. These findings\nsuggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chi-Jen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Low-Resource Named Entity Recognition Using Backtranslation. (arXiv:2108.11703v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11703","description":"<p>The state of art natural language processing systems relies on sizable\ntraining datasets to achieve high performance. Lack of such datasets in the\nspecialized low resource domains lead to suboptimal performance. In this work,\nwe adapt backtranslation to generate high quality and linguistically diverse\nsynthetic data for low-resource named entity recognition. We perform\nexperiments on two datasets from the materials science (MaSciP) and biomedical\ndomains (S800). The empirical results demonstrate the effectiveness of our\nproposed augmentation strategy, particularly in the low-resource scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_U/0/1/0/all/0/1\">Usama Yaseen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Stefan Langer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statutory Article Retrieval Dataset in French. (arXiv:2108.11792v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11792","description":"<p>Statutory article retrieval is the task of automatically retrieving law\narticles relevant to a legal question. While recent advances in natural\nlanguage processing have sparked considerable interest in many legal tasks,\nstatutory article retrieval remains primarily untouched due to the scarcity of\nlarge-scale and high-quality annotated datasets. To address this bottleneck, we\nintroduce the Belgian Statutory Article Retrieval Dataset (BSARD), which\nconsists of 1,100+ French native legal questions labeled by experienced jurists\nwith relevant articles from a corpus of 22,600+ Belgian law articles. Using\nBSARD, we benchmark several unsupervised information retrieval methods based on\nterm weighting and pooled embeddings. Our best performing baseline achieves\n50.8% R@100, which is promising for the feasibility of the task and indicates\nthat there is still substantial room for improvement. By the specificity of the\ndata domain and addressed task, BSARD presents a unique challenge problem for\nfuture research on legal information retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Antoine Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijck_G/0/1/0/all/0/1\">Gijs Van Dijck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pretrained Language Models with Label Attention for Explainable Biomedical Text Classification. (arXiv:2108.11809v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11809","description":"<p>The massive growth of digital biomedical data is making biomedical text\nindexing and classification increasingly important. Accordingly, previous\nresearch has devised numerous techniques ranging from rule-based systems to\ndeep neural networks, with most focusing on feedforward, convolutional or\nrecurrent neural architectures. More recently, fine-tuned transformers-based\npretrained models (PTMs) have demonstrated superior performance in many natural\nlanguage processing tasks. However, the direct use of PTMs in the biomedical\ndomain is only limited to the target documents, ignoring the rich semantic\ninformation in the label descriptions. In this paper, we develop an improved\nlabel attention-based architecture to inject semantic label description into\nthe fine-tuning process of PTMs. Results on two public medical datasets show\nthat the proposed fine-tuning scheme outperforms the conventionally fine-tuned\nPTMs and prior state-of-the-art models. Furthermore, we show that fine-tuning\nwith the label attention mechanism is interpretable in the interpretability\nstudy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bruce Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Approach to Measure Empathy and Theory-of-Mind from Written Texts. (arXiv:2108.11810v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11810","description":"<p>Theory-of-mind (ToM), a human ability to infer the intentions and thoughts of\nothers, is an essential part of empathetic experiences. We provide here the\nframework for using NLP models to measure ToM expressed in written texts. For\nthis purpose, we introduce ToM-Diary, a crowdsourced 18,238 diaries with 74,014\nKorean sentences annotated with different ToM levels. Each diary was annotated\nwith ToM levels by trained psychology students and reviewed by selected\npsychology experts. The annotators first divided the diaries based on whether\nthey mentioned other people: self-focused and other-focused. Examples of\nself-focused sentences are \"I am feeling good\". The other-focused sentences\nwere further classified into different levels. These levels differ by whether\nthe writer 1) mentions the presence of others without inferring their mental\nstate(e.g., I saw a man walking down the street), 2) fails to take the\nperspective of others (e.g., I don't understand why they refuse to wear masks),\nor 3) successfully takes the perspective of others (It must have been hard for\nthem to continue working). We tested whether state-of-the-art transformer-based\nmodels (e.g., BERT) could predict underlying ToM levels in sentences. We found\nthat BERT more successfully detected self-focused sentences than other-focused\nones. Sentences that successfully take the perspective of others (the highest\nToM level) were the most difficult to predict. Our study suggests a promising\ndirection for large-scale and computational approaches for identifying the\nability of authors to empathize and take the perspective of others. The dataset\nis at [URL](https://github.com/humanfactorspsych/covid19-tom-empathy-diary)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoon Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Eun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yoonwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1\">Sowon Hahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11830","description":"<p>Dialogue models trained on human conversations inadvertently learn to\ngenerate offensive responses. Moreover, models can insult anyone by agreeing\nwith an offensive context. To understand the dynamics of contextually offensive\nlanguage, we study the stance of dialogue model responses in offensive Reddit\nconversations. Specifically, we crowd-annotate ToxiChat, a new dataset of 2,000\nReddit threads and model responses labeled with offensive language and stance.\nOur analysis reveals that 42% of user responses agree with toxic comments; 3x\ntheir agreement with safe comments (13%). Pre-trained transformer-based\nclassifiers fine-tuned on our dataset achieve 0.71 F1 for offensive labels and\n0.53 Macro-F1 for stance labels. Finally, we analyze some existing controllable\ntext generation (CTG) methods to mitigate the contextual offensive behavior of\ndialogue models. Compared to the baseline, our best CTG model obtains a 19%\nreduction in agreement with offensive context and 29% fewer offensive\nresponses. This highlights the need for future work to characterize and analyze\nmore forms of inappropriate behavior in dialogue models to help make them\nsafer. Our code and corpus are available at\nhttps://github.com/abaheti95/ToxiChat .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Exposure Bias via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11846v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11846","description":"<p>Encoder-decoder models have achieved remarkable success in abstractive text\nsummarization, which aims to compress one or more documents into a shorter\nversion without the loss of the essential content. Unfortunately, these models\nmostly suffer a discrepancy between training and inference, i.e., the exposure\nbias problem. During the training stage, with teacher forcing these models are\noptimized to maximize the likelihood of the gold summary given the gold summary\ntokens as input to the decoder, while at inference the given tokens are\nreplaced by the generated tokens. Consequently, low-quality summaries are very\nlikely to be generated. To remedy this problem, we propose to leverage\ncontrastive learning to decrease the likelihood of these low-quality summaries,\nand meanwhile increase the likelihood of the gold summary. Since our solution\nexpands the states that the model perceives during training, we expect that the\nexposure bias problem can be alleviated. We experimentally demonstrate that our\nmethod effectively improves the performance of the state-of-the-art model on\ndifferent datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Realistic Study of Auto-regressive Language Models for Named Entity Typing and Recognition. (arXiv:2108.11857v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11857","description":"<p>Despite impressive results of language models for named entity recognition\n(NER), their generalization to varied textual genres, a growing entity type\nset, and new entities remains a challenge. Collecting thousands of annotations\nin each new case for training or fine-tuning is expensive and time-consuming.\nIn contrast, humans can easily identify named entities given some simple\ninstructions. Inspired by this, we challenge the reliance on large datasets and\nstudy pre-trained language models for NER in a meta-learning setup. First, we\ntest named entity typing (NET) in a zero-shot transfer scenario. Then, we\nperform NER by giving few examples at inference. We propose a method to select\nseen and rare / unseen names when having access only to the pre-trained model\nand report results on these groups. The results show: auto-regressive language\nmodels as meta-learners can perform NET and NER fairly well especially for\nregular or seen names; name irregularity when often present for a certain\nentity type can become an effective exploitable cue; names with words foreign\nto the model have the most negative impact on results; the model seems to rely\nmore on name than context cues in few-shot NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Automated Fact-Checking. (arXiv:2108.11896v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11896","description":"<p>Fact-checking has become increasingly important due to the speed with which\nboth information and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity\nof claims. In this paper, we survey automated fact-checking stemming from\nnatural language processing, and discuss its connections to related tasks and\ndisciplines. In this process, we present an overview of existing datasets and\nmodels, aiming to unify the various definitions given and identify common\nconcepts. Finally, we highlight challenges for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning. (arXiv:2108.11912v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11912","description":"<p>Stylized image captioning systems aim to generate a caption not only\nsemantically related to a given image but also consistent with a given style\ndescription. One of the biggest challenges with this task is the lack of\nsufficient paired stylized data. Many studies focus on unsupervised approaches,\nwithout considering from the perspective of data augmentation. We begin with\nthe observation that people may recall similar emotions when they are in\nsimilar scenes, and often express similar emotions with similar style phrases,\nwhich underpins our data augmentation idea. In this paper, we propose a novel\nExtract-Retrieve-Generate data augmentation framework to extract style phrases\nfrom small-scale stylized sentences and graft them to large-scale factual\ncaptions. First, we design the emotional signal extractor to extract style\nphrases from small-scale stylized sentences. Second, we construct the plugable\nmulti-modal scene retriever to retrieve scenes represented with pairs of an\nimage and its stylized caption, which are similar to the query image or caption\nin the large-scale factual data. In the end, based on the style phrases of\nsimilar scenes and the factual description of the current scene, we build the\nemotion-aware caption generator to generate fluent and diversified stylized\ncaptions for the current scene. Extensive experimental results show that our\nframework can alleviate the data scarcity problem effectively. It also\nsignificantly boosts the performance of several existing image captioning\nmodels in both supervised and unsupervised settings, which outperforms the\nstate-of-the-art stylized image captioning methods in terms of both sentence\nrelevance and stylishness by a substantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuchen Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAN: Higher-order Attention Network for Spoken Language Understanding. (arXiv:2108.11916v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11916","description":"<p>Spoken Language Understanding (SLU), including intent detection and slot\nfilling, is a core component in human-computer interaction. The natural\nattributes of the relationship among the two subtasks make higher requirements\non fine-grained feature interaction, i.e., the token-level intent features and\nslot features. Previous works mainly focus on jointly modeling the relationship\nbetween the two subtasks with attention-based models, while ignoring the\nexploration of attention order. In this paper, we propose to replace the\nconventional attention with our proposed Bilinear attention block and show that\nthe introduced Higher-order Attention Network (HAN) brings improvement for the\nSLU task. Importantly, we conduct wide analysis to explore the effectiveness\nbrought from the higher-order attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Mediation in Armed Conflicts. (arXiv:2108.11942v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11942","description":"<p>Today's conflicts are becoming increasingly complex, fluid and fragmented,\noften involving a host of national and international actors with multiple and\noften divergent interests. This development poses significant challenges for\nconflict mediation, as mediators struggle to make sense of conflict dynamics,\nsuch as the range of conflict parties and the evolution of their political\npositions, the distinction between relevant and less relevant actors in peace\nmaking, or the identification of key conflict issues and their interdependence.\nInternational peace efforts appear increasingly ill-equipped to successfully\naddress these challenges. While technology is being increasingly used in a\nrange of conflict related fields, such as conflict predicting or information\ngathering, less attention has been given to how technology can contribute to\nconflict mediation. This case study is the first to apply state-of-the-art\nmachine learning technologies to data from an ongoing mediation process. Using\ndialogue transcripts from peace negotiations in Yemen, this study shows how\nmachine-learning tools can effectively support international mediators by\nmanaging knowledge and offering additional conflict analysis tools to assess\ncomplex information. Apart from illustrating the potential of machine learning\ntools in conflict mediation, the paper also emphasises the importance of\ninterdisciplinary and participatory research design for the development of\ncontext-sensitive and targeted tools and to ensure meaningful and responsible\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lier_F/0/1/0/all/0/1\">F.A. Van Lier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-Invariant Truecasing with a Word-and-Character Hierarchical Recurrent Neural Network. (arXiv:2108.11943v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11943","description":"<p>Truecasing is the task of restoring the correct case (uppercase or lowercase)\nof noisy text generated either by an automatic system for speech recognition or\nmachine translation or by humans. It improves the performance of downstream NLP\ntasks such as named entity recognition and language modeling. We propose a\nfast, accurate and compact two-level hierarchical word-and-character-based\nrecurrent neural network model, the first of its kind for this problem. Using\nsequence distillation, we also address the problem of truecasing while ignoring\ntoken positions in the sentence, i.e. in a position-invariant manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">You-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SASRA: Semantically-aware Spatio-temporal Reasoning Agent for Vision-and-Language Navigation in Continuous Environments. (arXiv:2108.11945v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11945","description":"<p>This paper presents a novel approach for the Vision-and-Language Navigation\n(VLN) task in continuous 3D environments, which requires an autonomous agent to\nfollow natural language instructions in unseen environments. Existing\nend-to-end learning-based VLN methods struggle at this task as they focus\nmostly on utilizing raw visual observations and lack the semantic\nspatio-temporal reasoning capabilities which is crucial in generalizing to new\nenvironments. In this regard, we present a hybrid transformer-recurrence model\nwhich focuses on combining classical semantic mapping techniques with a\nlearning-based method. Our method creates a temporal semantic memory by\nbuilding a top-down local ego-centric semantic map and performs cross-modal\ngrounding to align map and language modalities to enable effective learning of\nVLN policy. Empirical results in a photo-realistic long-horizon simulation\nenvironment show that the proposed approach outperforms a variety of\nstate-of-the-art methods and baselines with over 22% relative improvement in\nSPL in prior unseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1\">Muhammad Zubair Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zachary Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1\">Supun Samarasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAUCE: Truncated Sparse Document Signature Bit-Vectors for Fast Web-Scale Corpus Expansion. (arXiv:2108.11948v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11948","description":"<p>Recent advances in text representation have shown that training on large\namounts of text is crucial for natural language understanding. However, models\ntrained without predefined notions of topical interest typically require\ncareful fine-tuning when transferred to specialized domains. When a sufficient\namount of within-domain text may not be available, expanding a seed corpus of\nrelevant documents from large-scale web data poses several challenges. First,\ncorpus expansion requires scoring and ranking each document in the collection,\nan operation that can quickly become computationally expensive as the web\ncorpora size grows. Relying on dense vector spaces and pairwise similarity adds\nto the computational expense. Secondly, as the domain concept becomes more\nnuanced, capturing the long tail of domain-specific rare terms becomes\nnon-trivial, especially under limited seed corpora scenarios.\n</p>\n<p>In this paper, we consider the problem of fast approximate corpus expansion\ngiven a small seed corpus with a few relevant documents as a query, with the\ngoal of capturing the long tail of a domain-specific set of concept terms. To\nefficiently collect large-scale domain-specific corpora with limited relevance\nfeedback, we propose a novel truncated sparse document bit-vector\nrepresentation, termed Signature Assisted Unsupervised Corpus Expansion\n(SAUCE). Experimental results show that SAUCE can reduce the computational\nburden while ensuring high within-domain lexical coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahed_M/0/1/0/all/0/1\">Muntasir Wahed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruhl_D/0/1/0/all/0/1\">Daniel Gruhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_A/0/1/0/all/0/1\">Alfredo Alba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_A/0/1/0/all/0/1\">Anna Lisa Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristoski_P/0/1/0/all/0/1\">Petar Ristoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deluca_C/0/1/0/all/0/1\">Chad Deluca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_S/0/1/0/all/0/1\">Steve Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weisfeiler-Leman in the BAMBOO: Novel AMR Graph Metrics and a Benchmark for AMR Graph Similarity. (arXiv:2108.11949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11949","description":"<p>Several metrics have been proposed for assessing the similarity of (abstract)\nmeaning representations (AMRs), but little is known about how they relate to\nhuman similarity ratings. Moreover, the current metrics have complementary\nstrengths and weaknesses: some emphasize speed, while others make the alignment\nof graph structures explicit, at the price of a costly alignment step.\n</p>\n<p>In this work we propose new Weisfeiler-Leman AMR similarity metrics that\nunify the strengths of previous metrics, while mitigating their weaknesses.\nSpecifically, our new metrics are able to match contextualized substructures\nand induce n:m alignments between their nodes. Furthermore, we introduce a\nBenchmark for AMR Metrics based on Overt Objectives (BAMBOO), the first\nbenchmark to support empirical assessment of graph-based MR similarity metrics.\nBAMBOO maximizes the interpretability of results by defining multiple overt\nobjectives that range from sentence similarity objectives to stress tests that\nprobe a metric's robustness against meaning-altering and meaning-preserving\ngraph transformations. We show the benefits of BAMBOO by profiling previous\nmetrics and our own metrics. Results indicate that our novel metrics may serve\nas a strong baseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_A/0/1/0/all/0/1\">Angel Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision. (arXiv:2108.11950v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11950","description":"<p>Computer vision tasks such as object detection and semantic/instance\nsegmentation rely on the painstaking annotation of large training datasets. In\nthis paper, we propose LocTex that takes advantage of the low-cost localized\ntextual annotations (i.e., captions and synchronized mouse-over gestures) to\nreduce the annotation effort. We introduce a contrastive pre-training framework\nbetween images and captions and propose to supervise the cross-modal attention\nmap with rendered mouse traces to provide coarse localization signals. Our\nlearned visual features capture rich semantics (from free-form captions) and\naccurate localization (from mouse traces), which are very effective when\ntransferred to various downstream vision tasks. Compared with ImageNet\nsupervised pre-training, LocTex can reduce the size of the pre-training dataset\nby 10x or the target dataset by 2x while achieving comparable or even improved\nperformance on COCO instance segmentation. When provided with the same amount\nof annotations, LocTex achieves around 4% higher accuracy than the previous\nstate-of-the-art \"vision+language\" pre-training approach on the task of PASCAL\nVOC image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gideon_J/0/1/0/all/0/1\">John Gideon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUMBT+LaRL: Effective Multi-domain End-to-end Neural Task-oriented Dialog System. (arXiv:2009.10447v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.10447","description":"<p>The recent advent of neural approaches for developing each dialog component\nin task-oriented dialog systems has remarkably improved, yet optimizing the\noverall system performance remains a challenge. Besides, previous research on\nmodeling complicated multi-domain goal-oriented dialogs in end-to-end fashion\nhas been limited. In this paper, we present an effective multi-domain\nend-to-end trainable neural dialog system SUMBT+LaRL that incorporates two\nprevious strong models and facilitates them to be fully differentiable.\nSpecifically, the SUMBT+ estimates user-acts as well as dialog belief states,\nand the LaRL models latent system action spaces and generates responses given\nthe estimated contexts. We emphasize that the training framework of three steps\nsignificantly and stably increase dialog success rates: separately pretraining\nthe SUMBT+ and LaRL, fine-tuning the entire system, and then reinforcement\nlearning of dialog policy. We also introduce new reward criteria of\nreinforcement learning for dialog policy training. Then, we discuss\nexperimental results depending on the reward criteria and different dialog\nevaluation methods. Consequently, our model achieved the new state-of-the-art\nsuccess rate of 85.4% on corpus-based evaluation, and a comparable success rate\nof 81.40% on simulator-based evaluation provided by the DSTC8 challenge. To our\nbest knowledge, our work is the first comprehensive study of a modularized E2E\nmulti-domain dialog system that learning from each component to the entire\ndialog policy for task success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Seokhwan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyungJun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sangkeun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Adversarial Learning for Cross-Lingual Word Embeddings. (arXiv:2010.08432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08432","description":"<p>Generative adversarial networks (GANs) have succeeded in inducing\ncross-lingual word embeddings -- maps of matching words across languages --\nwithout supervision. Despite these successes, GANs' performance for the\ndifficult case of distant languages is still not satisfactory. These\nlimitations have been explained by GANs' incorrect assumption that source and\ntarget embedding spaces are related by a single linear mapping and are\napproximately isomorphic. We assume instead that, especially across distant\nlanguages, the mapping is only piece-wise linear, and propose a\nmulti-adversarial learning method. This novel method induces the seed\ncross-lingual dictionary through multiple mappings, each induced to fit the\nmapping for one subspace. Our experiments on unsupervised bilingual lexicon\ninduction show that this method improves performance over previous\nsingle-mapping methods, especially for distant languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_P/0/1/0/all/0/1\">Paola Merlo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. (arXiv:2011.10369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.10369","description":"<p>Backdoor attacks are a kind of emergent training-time threat to deep neural\nnetworks (DNNs). They can manipulate the output of DNNs and possess high\ninsidiousness. In the field of natural language processing, some attack methods\nhave been proposed and achieve very high attack success rates on multiple\npopular models. Nevertheless, there are few studies on defending against\ntextual backdoor attacks. In this paper, we propose a simple and effective\ntextual backdoor defense named ONION, which is based on outlier word detection\nand, to the best of our knowledge, is the first method that can handle all the\ntextual backdoor attack situations. Experiments demonstrate the effectiveness\nof our model in defending BiLSTM and BERT against five different backdoor\nattacks. All the code and data will be released to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09777","description":"<p>Inspired by Curriculum Learning, we propose a consecutive (i.e.\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using transformer-based architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_N/0/1/0/all/0/1\">Nicolas Perez Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frauenfelder_T/0/1/0/all/0/1\">Thomas Frauenfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujimoto_K/0/1/0/all/0/1\">Koji Fujimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1\">Michael Krauthammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Diversity of Neural Text Generation via Inverse Probability Weighting. (arXiv:2103.07649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07649","description":"<p>The neural text generation suffers from the text degeneration issue such as\nrepetition. Traditional stochastic sampling methods only focus on truncating\nthe unreliable \"tail\" of the distribution, and do not address the \"head\" part,\nwhich we show might contain tedious or even repetitive candidates with high\nprobability that lead to repetition loops. They also do not consider the issue\nthat human text does not always favor high-probability words. Inspired by\nthese, in this work we propose a heuristic sampling method. We propose to use\ninterquartile range of the predicted distribution to determine the \"head\" part,\nthen permutate and rescale the \"head\" with inverse probability. This aims at\ndecreasing the probability for the tedious and possibly repetitive candidates\nwith higher probability, and increasing the probability for the rational but\nmore surprising candidates with lower probability. The proposed algorithm\nprovides a reasonable permutation on the predicted distribution which enhances\ndiversity without compromising rationality of the distribution. We use\npre-trained language model to compare our algorithm with traditional methods.\nResults show that our algorithm can effectively increase the diversity of\ngenerated samples while achieving close resemblance to human text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07555","description":"<p>QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.16213","description":"<p>Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01198","description":"<p>In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. (arXiv:2108.08877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08877","description":"<p>We provide the first exploration of text-to-text transformers (T5) sentence\nembeddings. Sentence embeddings are broadly useful for language processing\ntasks. While T5 achieves impressive performance on language tasks cast as\nsequence-to-sequence mapping problems, it is unclear how to produce sentence\nembeddings from encoder-decoder models. We investigate three methods for\nextracting T5 sentence embeddings: two utilize only the T5 encoder and one uses\nthe full T5 encoder-decoder model. Our encoder-only models outperforms\nBERT-based sentence embeddings on both transfer tasks and semantic textual\nsimilarity (STS). Our encoder-decoder method achieves further improvement on\nSTS. Scaling up T5 from millions to billions of parameters is found to produce\nconsistent improvements on downstream tasks. Finally, we introduce a two-stage\ncontrastive learning approach that achieves a new state-of-art on STS using\nsentence embeddings, outperforming both Sentence BERT and SimCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1\">Gustavo Hern&#xe1;ndez &#xc1;brego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith B. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent multiple shared layers in Depth for Neural Machine Translation. (arXiv:2108.10417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10417","description":"<p>Learning deeper models is usually a simple and effective approach to improve\nmodel performance, but deeper models have larger model parameters and are more\ndifficult to train. To get a deeper model, simply stacking more layers of the\nmodel seems to work well, but previous works have claimed that it cannot\nbenefit the model. We propose to train a deeper model with recurrent mechanism,\nwhich loops the encoder and decoder blocks of Transformer in the depth\ndirection. To address the increasing of model parameters, we choose to share\nparameters in different recursive moments. We conduct our experiments on WMT16\nEnglish-to-German and WMT14 English-to-France translation tasks, our model\noutperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU\npoints, which is 27.23% of Transformer-Big model parameters. Compared to the\ndeep Transformer(20-layer encoder, 6-layer decoder), our model has similar\nmodel performance and infer speed, but our model parameters are 54.72% of the\nformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground Truth Refining. (arXiv:2108.11439v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11439","description":"<p>Weakly Supervised Object Detection (WSOD), aiming to train detectors with\nonly image-level annotations, has arisen increasing attention. Current\nstate-of-the-art approaches mainly follow a two-stage training strategy\nwhichintegrates a fully supervised detector (FSD) with a pure WSOD model. There\nare two main problems hindering the performance of the two-phase WSOD\napproaches, i.e., insufficient learning problem and strict reliance between the\nFSD and the pseudo ground truth (PGT) generated by theWSOD model. This paper\nproposes pseudo ground truth refinement network (PGTRNet), a simple yet\neffective method without introducing any extra learnable parameters, to cope\nwith these problems. PGTRNet utilizes multiple bounding boxes to establish the\nPGT, mitigating the insufficient learning problem. Besides, we propose a novel\nonline PGT refinement approach to steadily improve the quality of PGTby fully\ntaking advantage of the power of FSD during the second-phase training,\ndecoupling the first and second-phase models. Elaborate experiments are\nconducted on the PASCAL VOC 2007 benchmark to verify the effectiveness of our\nmethods. Experimental results demonstrate that PGTRNet boosts the backbone\nmodel by 2.074% mAP and achieves the state-of-the-art performance, showing the\nsignificant potentials of the second-phase training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hefeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge. (arXiv:2108.11441v1 [cs.AR])","link":"http://arxiv.org/abs/2108.11441","description":"<p>Massively parallel systolic arrays and resource-efficient depthwise separable\nconvolutions are two promising techniques to accelerate DNN inference on the\nedge. Interestingly, their combination is inefficient: Computational patterns\nof depthwise separable convolutions do not exhibit a rhythmic systolic flow and\nlack sufficient data reuse to saturate systolic arrays. We formally analyse\nthis inefficiency and propose an efficient operator, an optimal hardware\ndataflow, and a superior training methodology towards alleviating this. The\nefficient operator, called FuSeConv, is a drop-in replacement for depthwise\nseparable convolutions. FuSeConv factorizes convolution fully along their\nspatial and depth dimensions. The resultant computation efficiently maps to\nsystolic arrays. The optimal dataflow, called Spatial-Tiled Output Stationary\n(ST-OS), maximizes the efficiency of FuSeConv on systolic arrays. It maps\nindependent convolutions to rows of the array to maximize resource utilization\nwith negligible VLSI overheads. Neural Operator Scaffolding (NOS) scaffolds the\ntraining of FuSeConv by distilling knowledge from the expensive depthwise\nseparable convolutions. This bridges the accuracy gap between FuSeConv networks\nand baselines. Additionally, NOS can be combined with Neural Architecture\nSearch (NAS) to trade-off latency and accuracy. The HW/SW co-design of FuSeConv\nwith ST-OS achieves a significant speedup of 4.1-9.25X with state-of-the-art\nefficient networks for ImageNet. The parameter efficiency of FuSeConv and its\nsignificant out-performance over depthwise separable convolutions on systolic\narrays illustrates their promise as a strong solution on the edge. Training\nFuSeConv networks with NOS achieves accuracy comparable to the baselines.\nFurther, by combining NOS with NAS, we design networks that define\nstate-of-the-art models improving on both accuracy and latency on systolic\narrays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_V/0/1/0/all/0/1\">Vinod Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Riemannian Framework for Analysis of Human Body Surface. (arXiv:2108.11449v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11449","description":"<p>We propose a novel framework for comparing 3D human shapes under the change\nof shape and pose. This problem is challenging since 3D human shapes vary\nsignificantly across subjects and body postures. We solve this problem by using\na Riemannian approach. Our core contribution is the mapping of the human body\nsurface to the space of metrics and normals. We equip this space with a family\nof Riemannian metrics, called Ebin (or DeWitt) metrics. We treat a human body\nsurface as a point in a \"shape space\" equipped with a family of Riemmanian\nmetrics. The family of metrics is invariant under rigid motions and\nreparametrizations; hence it induces a metric on the \"shape space\" of surfaces.\nUsing the alignment of human bodies with a given template, we show that this\nfamily of metrics allows us to distinguish the changes in shape and pose. The\nproposed framework has several advantages. First, we define family of metrics\nwith desired invariant properties for the comparison of human shape. Second, we\npresent an efficient framework to compute geodesic paths between human shape\ngiven the chosen metric. Third, this framework provides some basic tools for\nstatistical shape analysis of human body surfaces. Finally, we demonstrate the\nutility of the proposed framework in pose and shape retrieval of human body.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emery Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumpach_A/0/1/0/all/0/1\">Alice-Barbara Tumpach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Label Effort: Self-Supervised meets Active Learning. (arXiv:2108.11458v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11458","description":"<p>Active learning is a paradigm aimed at reducing the annotation effort by\ntraining the model on actively selected informative and/or representative\nsamples. Another paradigm to reduce the annotation effort is self-training that\nlearns from a large amount of unlabeled data in an unsupervised way and\nfine-tunes on few labeled samples. Recent developments in self-training have\nachieved very impressive results rivaling supervised learning on some datasets.\nThe current work focuses on whether the two paradigms can benefit from each\nother. We studied object recognition datasets including CIFAR10, CIFAR100 and\nTiny ImageNet with several labeling budgets for the evaluations. Our\nexperiments reveal that self-training is remarkably more efficient than active\nlearning at reducing the labeling effort, that for a low labeling budget,\nactive learning offers no benefit to self-training, and finally that the\ncombination of active learning and self-training is fruitful when the labeling\nbudget is high. The performance gap between active learning trained either with\nself-training or from scratch diminishes as we approach to the point where\nalmost half of the dataset is labeled.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengar_J/0/1/0/all/0/1\">Javad Zolfaghari Bengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bartlomiej Twardowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Object Detection and Attribute Recognition by Feature Entanglement Reduction. (arXiv:2108.11501v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11501","description":"<p>We explore object detection with two attributes: color and material. The task\naims to simultaneously detect objects and infer their color and material. A\nstraight-forward approach is to add attribute heads at the very end of a usual\nobject detection pipeline. However, we observe that the two goals are in\nconflict: Object detection should be attribute-independent and attributes be\nlargely object-independent. Features computed by a standard detection network\nentangle the category and attribute features; we disentangle them by the use of\na two-stream model where the category and attribute features are computed\nindependently but the classification heads share Regions of Interest (RoIs).\nCompared with a traditional single-stream model, our model shows significant\nimprovements over VG-20, a subset of Visual Genome, on both supervised and\nattribute transfer tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaoheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadhu_A/0/1/0/all/0/1\">Arka Sadhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1\">Ram Nevatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maneuver Identification Challenge. (arXiv:2108.11503v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11503","description":"<p>AI algorithms that identify maneuvers from trajectory data could play an\nimportant role in improving flight safety and pilot training. AI challenges\nallow diverse teams to work together to solve hard problems and are an\neffective tool for developing AI solutions. AI challenges are also a key driver\nof AI computational requirements. The Maneuver Identification Challenge hosted\nat maneuver-id.mit.edu provides thousands of trajectories collected from pilots\npracticing in flight simulators, descriptions of maneuvers, and examples of\nthese maneuvers performed by experienced pilots. Each trajectory consists of\npositions, velocities, and aircraft orientations normalized to a common\ncoordinate system. Construction of the data set required significant data\narchitecture to transform flight simulator logs into AI ready data, which\nincluded using a supercomputer for deduplication and data conditioning. There\nare three proposed challenges. The first challenge is separating physically\nplausible (good) trajectories from unfeasible (bad) trajectories. Human labeled\ngood and bad trajectories are provided to aid in this task. Subsequent\nchallenges are to label trajectories with their intended maneuvers and to\nassess the quality of those maneuvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_K/0/1/0/all/0/1\">Kaira Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1\">Vijay Gadepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAlpin_K/0/1/0/all/0/1\">Kyle McAlpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palko_K/0/1/0/all/0/1\">Kyle Palko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulk_B/0/1/0/all/0/1\">Ben Paulk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1\">Sid Samsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_H/0/1/0/all/0/1\">Ho Chit Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1\">Charles Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1\">Jeremy Kepner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Real-World Super-Resolution through Adversarial Robustness. (arXiv:2108.11505v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11505","description":"<p>Real-world Super-Resolution (SR) has been traditionally tackled by first\nlearning a specific degradation model that resembles the noise and corruption\nartifacts in low-resolution imagery. Thus, current methods lack generalization\nand lose their accuracy when tested on unseen types of corruption. In contrast\nto the traditional proposal, we present Robust Super-Resolution (RSR), a method\nthat leverages the generalization capability of adversarial attacks to tackle\nreal-world SR. Our novel framework poses a paradigm shift in the development of\nreal-world SR methods. Instead of learning a dataset-specific degradation, we\nemploy adversarial attacks to create difficult examples that target the model's\nweaknesses. Afterward, we use these adversarial examples during training to\nimprove our model's capacity to process noisy inputs. We perform extensive\nexperimentation on synthetic and real-world images and empirically demonstrate\nthat our RSR method generalizes well across datasets without re-training for\nspecific noise priors. By using a single robust model, we outperform\nstate-of-the-art specialized methods on real-world benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Castillo_A/0/1/0/all/0/1\">Angela Castillo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escobar_M/0/1/0/all/0/1\">Mar&#xed;a Escobar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Romero_A/0/1/0/all/0/1\">Andr&#xe9;s Romero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey. (arXiv:2108.11510v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11510","description":"<p>Deep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have\ndemonstrated the remarkable successes of deep reinforcement learning in various\ndomains including finance, medicine, healthcare, video games, robotics, and\ncomputer vision. In this work, we provide a detailed review of recent and\nstate-of-the-art research advances of deep reinforcement learning in computer\nvision. We start with comprehending the theories of deep learning,\nreinforcement learning, and deep reinforcement learning. We then propose a\ncategorization of deep reinforcement learning methodologies and discuss their\nadvantages and limitations. In particular, we divide deep reinforcement\nlearning into seven main categories according to their applications in computer\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\nthese categories is further analyzed with reinforcement learning techniques,\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\nof the existing publicly available datasets and examine source code\navailability. Finally, we present some open issues and discuss future research\ndirections on deep reinforcement learning in computer vision\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathour_V/0/1/0/all/0/1\">Vidhiwar Singh Rathour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust High-Resolution Video Matting with Temporal Guidance. (arXiv:2108.11515v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11515","description":"<p>We introduce a robust, real-time, high-resolution human video matting method\nthat achieves new state-of-the-art performance. Our method is much lighter than\nprevious approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia\nGTX 1080Ti GPU. Unlike most existing methods that perform video matting\nframe-by-frame as independent images, our method uses a recurrent architecture\nto exploit temporal information in videos and achieves significant improvements\nin temporal coherence and matting quality. Furthermore, we propose a novel\ntraining strategy that enforces our network on both matting and segmentation\nobjectives. This significantly improves our model's robustness. Our method does\nnot require any auxiliary inputs such as a trimap or a pre-captured background\nimage, so it can be widely applied to existing human matting applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shanchuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleemi_I/0/1/0/all/0/1\">Imran Saleemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Soumyadip Sengupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChessMix: Spatial Context Data Augmentation for Remote Sensing Semantic Segmentation. (arXiv:2108.11535v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11535","description":"<p>Labeling semantic segmentation datasets is a costly and laborious process if\ncompared with tasks like image classification and object detection. This is\nespecially true for remote sensing applications that not only work with\nextremely high spatial resolution data but also commonly require the knowledge\nof experts of the area to perform the manual labeling. Data augmentation\ntechniques help to improve deep learning models under the circumstance of few\nand imbalanced labeled samples. In this work, we propose a novel data\naugmentation method focused on exploring the spatial context of remote sensing\nsemantic segmentation. This method, ChessMix, creates new synthetic images from\nthe existing training set by mixing transformed mini-patches across the dataset\nin a chessboard-like grid. ChessMix prioritizes patches with more examples of\nthe rarest classes to alleviate the imbalance problems. The results in three\ndiverse well-known remote sensing datasets show that this is a promising\napproach that helps to improve the networks' performance, working especially\nwell in datasets with few available data. The results also show that ChessMix\nis capable of improving the segmentation of objects with few labeled pixels\nwhen compared to the most common data augmentation methods widely used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1\">Matheus Barros Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson Alex dos Santos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios. (arXiv:2108.11539v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11539","description":"<p>Object detection on drone-captured scenarios is a recent popular task. As\ndrones always navigate in different altitudes, the object scale varies\nviolently, which burdens the optimization of networks. Moreover, high-speed and\nlow-altitude flight bring in the motion blur on the densely packed objects,\nwhich leads to great challenge of object distinction. To solve the two issues\nmentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more\nprediction head to detect different-scale objects. Then we replace the original\nprediction heads with Transformer Prediction Heads (TPH) to explore the\nprediction potential with self-attention mechanism. We also integrate\nconvolutional block attention model (CBAM) to find attention region on\nscenarios with dense objects. To achieve more improvement of our proposed\nTPH-YOLOv5, we provide bags of useful strategies such as data augmentation,\nmultiscale testing, multi-model integration and utilizing extra classifier.\nExtensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good\nperformance with impressive interpretability on drone-captured scenarios. On\nDET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is\nbetter than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge\n2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place\nmodel (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves\nabout 7%, which is encouraging and competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingkui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-and-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11544","description":"<p>An agent that can understand natural-language instruction and carry out\ncorresponding actions in the visual world is one of the long-term challenges of\nArtificial Intelligent (AI). Due to multifarious instructions from humans, it\nrequires the agent can link natural language to vision and action in\nunstructured, previously unseen environments. If the instruction given by human\nis a navigation task, this challenge is called Visual-and-Language Navigation\n(VLN). It is a booming multi-disciplinary field of increasing importance and\nwith extraordinary practicality. Instead of focusing on the details of specific\nmethods, this paper provides a comprehensive survey on VLN tasks and makes a\nclassification carefully according the different characteristics of language\ninstructions in these tasks. According to when the instructions are given, the\ntasks can be divided into single-turn and multi-turn. For single-turn tasks, we\nfurther divided them into goal-orientation and route-orientation based on\nwhether the instructions contain a route. For multi-turn tasks, we divided them\ninto imperative task and interactive task based on whether the agent responses\nto the instructions. This taxonomy enable researchers to better grasp the key\npoint of a specific task and identify directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wansen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinmeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation. (arXiv:2108.11550v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11550","description":"<p>It is fundamental for personal robots to reliably navigate to a specified\ngoal. To study this task, PointGoal navigation has been introduced in simulated\nEmbodied AI environments. Recent advances solve this PointGoal navigation task\nwith near-perfect accuracy (99.6% success) in photo-realistically simulated\nenvironments, assuming noiseless egocentric vision, noiseless actuation, and\nmost importantly, perfect localization. However, under realistic noise models\nfor visual sensors and actuation, and without access to a \"GPS and Compass\nsensor,\" the 99.6%-success agents for PointGoal navigation only succeed with\n0.3%. In this work, we demonstrate the surprising effectiveness of visual\nodometry for the task of PointGoal navigation in this realistic setting, i.e.,\nwith realistic noise models for perception and actuation and without access to\nGPS and Compass sensors. We show that integrating visual odometry techniques\ninto navigation policies improves the state-of-the-art on the popular Habitat\nPointNav benchmark by a large margin, improving success from 64.5% to 71.7%\nwhile executing 6.4 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harsh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XCI-Sketch: Extraction of Color Information from Images for Generation of Colored Outlines and Sketches. (arXiv:2108.11554v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11554","description":"<p>Sketches are a medium to convey a visual scene from an individual's creative\nperspective. The addition of color substantially enhances the overall\nexpressivity of a sketch. This paper proposes two methods to mimic human-drawn\ncolored sketches by utilizing the Contour Drawing Dataset. Our first approach\nrenders colored outline sketches by applying image processing techniques aided\nby k-means color clustering. The second method uses a generative adversarial\nnetwork to develop a model that can generate colored sketches from previously\nunobserved images. We assess the results obtained through quantitative and\nqualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathod_H/0/1/0/all/0/1\">Harsh Rathod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manisimha Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Parna Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Sameer Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manushree_V/0/1/0/all/0/1\">V Manushree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity-aware Graph Memory Network for Action Detection. (arXiv:2108.11559v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11559","description":"<p>Action detection plays an important role in high-level video understanding\nand media interpretation. Many existing studies fulfill this spatio-temporal\nlocalization by modeling the context, capturing the relationship of actors,\nobjects, and scenes conveyed in the video. However, they often universally\ntreat all the actors without considering the consistency and distinctness\nbetween individuals, leaving much room for improvement. In this paper, we\nexplicitly highlight the identity information of the actors in terms of both\nlong-term and short-term context through a graph memory network, namely\nidentity-aware graph memory network (IGMN). Specifically, we propose the\nhierarchical graph neural network (HGNN) to comprehensively conduct long-term\nrelation modeling within the same identity as well as between different ones.\nRegarding short-term context, we develop a dual attention module (DAM) to\ngenerate identity-aware constraint to reduce the influence of interference by\nthe actors of different identities. Extensive experiments on the challenging\nAVA dataset demonstrate the effectiveness of our method, which achieves\nstate-of-the-art results on AVA v2.1 and v2.2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingcheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeighCNN: A CNN based SAR Speckle Reduction using Feature preserving Loss Function. (arXiv:2108.11573v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11573","description":"<p>Coherent imaging systems like synthetic aperture radar are susceptible to\nmultiplicative noise that makes applications like automatic target recognition\nchallenging. In this paper, NeighCNN, a deep learning-based speckle reduction\nalgorithm that handles multiplicative noise with relatively simple\nconvolutional neural network architecture, is proposed. We have designed a loss\nfunction which is an unique combination of weighted sum of Euclidean,\nneighbourhood, and perceptual loss for training the deep network. Euclidean and\nneighbourhood losses take pixel-level information into account, whereas\nperceptual loss considers high-level semantic features between two images.\nVarious synthetic, as well as real SAR images, are used for testing the\nNeighCNN architecture, and the results verify the noise removal and edge\npreservation abilities of the proposed architecture. Performance metrics like\npeak-signal-to-noise ratio, structural similarity index, and universal image\nquality index are used for evaluating the efficiency of the proposed\narchitecture on synthetic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agrawal_D/0/1/0/all/0/1\">Darshan Agrawal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranjani_J/0/1/0/all/0/1\">J. Jennifer Ranjani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11575","description":"<p>Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51. Code and trained models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xuefan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence. (arXiv:2108.11609v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11609","description":"<p>Shape correspondence from 3D deformation learning has attracted appealing\nacademy interests recently. Nevertheless, current deep learning based methods\nrequire the supervision of dense annotations to learn per-point translations,\nwhich severely overparameterize the deformation process. Moreover, they fail to\ncapture local geometric details of original shape via global feature embedding.\nTo address these challenges, we develop a new Unsupervised Dense Deformation\nEmbedding Network (i.e., UD^2E-Net), which learns to predict deformations\nbetween non-rigid shapes from dense local features. Since it is non-trivial to\nmatch deformation-variant local features for deformation prediction, we develop\nan Extrinsic-Intrinsic Autoencoder to frst encode extrinsic geometric features\nfrom source into intrinsic coordinates in a shared canonical shape, with which\nthe decoder then synthesizes corresponding target features. Moreover, a bounded\nmaximum mean discrepancy loss is developed to mitigate the distribution\ndivergence between the synthesized and original features. To learn natural\ndeformation without dense supervision, we introduce a coarse parameterized\ndeformation graph, for which a novel trace and propagation algorithm is\nproposed to improve both the quality and effciency of the deformation. Our\nUD^2E-Net outperforms state-of-the-art unsupervised methods by 24% on Faust\nInter challenge and even supervised methods by 13% on Faust Intra challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ronghan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Visual Relationship Co-localization. (arXiv:2108.11618v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11618","description":"<p>In this paper, given a small bag of images, each containing a common but\nlatent predicate, we are interested in localizing visual subject-object pairs\nconnected via the common predicate in each of the images. We refer to this\nnovel problem as visual relationship co-localization or VRC as an abbreviation.\nVRC is a challenging task, even more so than the well-studied object\nco-localization task. This becomes further challenging when using just a few\nimages, the model has to learn to co-localize visual subject-object pairs\nconnected via unseen predicates. To solve VRC, we propose an optimization\nframework to select a common visual relationship in each image of the bag. The\ngoal of the optimization framework is to find the optimal solution by learning\nvisual relationship similarity across images in a few-shot setting. To obtain\nrobust visual relationship representation, we utilize a simple yet effective\ntechnique that learns relationship embedding as a translation vector from\nvisual subject to visual object in a shared space. Further, to learn visual\nrelationship similarity, we utilize a proven meta-learning technique commonly\nused for few-shot classification tasks. Finally, to tackle the combinatorial\ncomplexity challenge arising from an exponential number of feasible solutions,\nwe use a greedy approximation inference algorithm that selects approximately\nthe best solution.\n</p>\n<p>We extensively evaluate our proposed framework on variations of bag sizes\nobtained from two challenging public datasets, namely VrR-VG and VG-150, and\nachieve impressive visual co-localization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teotia_R/0/1/0/all/0/1\">Revant Teotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1\">Vaibhav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_M/0/1/0/all/0/1\">Mayank Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Anand Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Web Image Context Extraction with Graph Neural Networks and Sentence Embeddings on the DOM tree. (arXiv:2108.11629v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11629","description":"<p>Web Image Context Extraction (WICE) consists in obtaining the textual\ninformation describing an image using the content of the surrounding webpage. A\ncommon preprocessing step before performing WICE is to render the content of\nthe webpage. When done at a large scale (e.g., for search engine indexation),\nit may become very computationally costly (up to several seconds per page). To\navoid this cost, we introduce a novel WICE approach that combines Graph Neural\nNetworks (GNNs) and Natural Language Processing models. Our method relies on a\ngraph model containing both node types and text as features. The model is fed\nthrough several blocks of GNNs to extract the textual context. Since no labeled\nWICE dataset with ground truth exists, we train and evaluate the GNNs on a\nproxy task that consists in finding the semantically closest text to the image\ncaption. We then interpret importance weights to find the most relevant text\nnodes and define them as the image context. Thanks to GNNs, our model is able\nto encode both structural and semantic information from the webpage. We show\nthat our approach gives promising results to help address the large-scale WICE\nproblem using only HTML data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chen Dang</a> (QR), <a href=\"http://arxiv.org/find/cs/1/au:+Randrianarivo_H/0/1/0/all/0/1\">Hicham Randrianarivo</a> (QR), <a href=\"http://arxiv.org/find/cs/1/au:+Fournier_SNiehotta_R/0/1/0/all/0/1\">Rapha&#xeb;l Fournier-S&#x27;Niehotta</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1\">Nicolas Audebert</a> (CNAM, CEDRIC - VERTIGO)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SketchLattice: Latticed Representation for Sketch Manipulation. (arXiv:2108.11636v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11636","description":"<p>The key challenge in designing a sketch representation lies with handling the\nabstract and iconic nature of sketches. Existing work predominantly utilizes\neither, (i) a pixelative format that treats sketches as natural images\nemploying off-the-shelf CNN-based networks, or (ii) an elaborately designed\nvector format that leverages the structural information of drawing orders using\nsequential RNN-based methods. While the pixelative format lacks intuitive\nexploitation of structural cues, sketches in vector format are absent in most\ncases limiting their practical usage. Hence, in this paper, we propose a\nlattice structured sketch representation that not only removes the bottleneck\nof requiring vector data but also preserves the structural cues that vector\ndata provides. Essentially, sketch lattice is a set of points sampled from the\npixelative format of the sketch using a lattice graph. We show that our lattice\nstructure is particularly amenable to structural changes that largely benefits\nsketch abstraction modeling for generation tasks. Our lattice representation\ncould be effectively encoded using a graph model, that uses significantly fewer\nmodel parameters (13.5 times lesser) than existing state-of-the-art. Extensive\nexperiments demonstrate the effectiveness of sketch lattice for sketch\nmanipulation, including sketch healing and image-to-sketch synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yonggang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guoyao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StackMix and Blot Augmentations for Handwritten Text Recognition. (arXiv:2108.11667v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11667","description":"<p>This paper proposes a handwritten text recognition(HTR) system that\noutperforms current state-of-the-artmethods. The comparison was carried out on\nthree of themost frequently used in HTR task datasets, namely Ben-tham, IAM,\nand Saint Gall. In addition, the results on tworecently presented datasets,\nPeter the Greats manuscriptsand HKR Dataset, are provided.The paper describes\nthe architecture of the neural net-work and two ways of increasing the volume\nof train-ing data: augmentation that simulates strikethrough text(HandWritten\nBlots) and a new text generation method(StackMix), which proved to be very\neffective in HTR tasks.StackMix can also be applied to the standalone task of\ngen-erating handwritten text based on printed text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karachev_D/0/1/0/all/0/1\">Denis Karachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novopoltsev_M/0/1/0/all/0/1\">Maxim Novopoltsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potanin_M/0/1/0/all/0/1\">Mark Potanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Loss for Point Cloud Registration. (arXiv:2108.11682v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11682","description":"<p>The performance of surface registration relies heavily on the metric used for\nthe alignment error between the source and target shapes. Traditionally, such a\nmetric is based on the point-to-point or point-to-plane distance from the\npoints on the source surface to their closest points on the target surface,\nwhich is susceptible to failure due to instability of the closest-point\ncorrespondence. In this paper, we propose a novel metric based on the\nintersection points between the two shapes and a random straight line, which\ndoes not assume a specific correspondence. We verify the effectiveness of this\nmetric by extensive experiments, including its direct optimization for a single\nregistration problem as well as unsupervised learning for a set of registration\nproblems. The results demonstrate that the algorithms utilizing our proposed\nmetric outperforms the state-of-the-art optimization-based and unsupervised\nlearning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Reliability of Semantic Segmentation of Medical Images by Uncertainty Modeling with Bayesian Deep Networks and Curriculum Learning. (arXiv:2108.11693v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11693","description":"<p>In this paper we propose a novel method which leverages the uncertainty\nmeasures provided by Bayesian deep networks through curriculum learning so that\nthe uncertainty estimates are fed back to the system to resample the training\ndata more densely in areas where uncertainty is high. We show in the concrete\nsetting of a semantic segmentation task (iPS cell colony segmentation) that the\nproposed system is able to increase significantly the reliability of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_S/0/1/0/all/0/1\">Sora Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raytchev_B/0/1/0/all/0/1\">Bisser Raytchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneda_K/0/1/0/all/0/1\">Kazufumi Kaneda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning. (arXiv:2108.11694v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11694","description":"<p>The application of deep learning to medical image segmentation has been\nhampered due to the lack of abundant pixel-level annotated data. Few-shot\nSemantic Segmentation (FSS) is a promising strategy for breaking the deadlock.\nHowever, a high-performing FSS model still requires sufficient pixel-level\nannotated classes for training to avoid overfitting, which leads to its\nperformance bottleneck in medical image segmentation due to the unmet need for\nannotations. Thus, semi-supervised FSS for medical images is accordingly\nproposed to utilize unlabeled data for further performance improvement.\nNevertheless, existing semi-supervised FSS methods has two obvious defects: (1)\nneglecting the relationship between the labeled and unlabeled data; (2) using\nunlabeled data directly for end-to-end training leads to degenerated\nrepresentation learning. To address these problems, we propose a novel\nsemi-supervised FSS framework for medical image segmentation. The proposed\nframework employs Poisson learning for modeling data relationship and\npropagating supervision signals, and Spatial Consistency Calibration for\nencouraging the model to learn more coherent representations. In this process,\nunlabeled samples do not involve in end-to-end training, but provide\nsupervisory information for query image segmentation through graph-based\nlearning. We conduct extensive experiments on three medical image segmentation\ndatasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for\nMRI and abdominal organs segmentation for CT) to demonstrate the\nstate-of-the-art performance and broad applicability of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guokai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huilin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Ye Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianwei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation. (arXiv:2108.11695v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11695","description":"<p>3D to 2D retinal vessel segmentation is a challenging problem in Optical\nCoherence Tomography Angiography (OCTA) images. Accurate retinal vessel\nsegmentation is important for the diagnosis and prevention of ophthalmic\ndiseases. However, making full use of the 3D data of OCTA volumes is a vital\nfactor for obtaining satisfactory segmentation results. In this paper, we\npropose a Progressive Attention-Enhanced Network (PAENet) based on attention\nmechanisms to extract rich feature representation. Specifically, the framework\nconsists of two main parts, the three-dimensional feature learning path and the\ntwo-dimensional segmentation path. In the three-dimensional feature learning\npath, we design a novel Adaptive Pooling Module (APM) and propose a new\nQuadruple Attention Module (QAM). The APM captures dependencies along the\nprojection direction of volumes and learns a series of pooling coefficients for\nfeature fusion, which efficiently reduces feature dimension. In addition, the\nQAM reweights the features by capturing four-group cross-dimension\ndependencies, which makes maximum use of 4D feature tensors. In the\ntwo-dimensional segmentation path, to acquire more detailed information, we\npropose a Feature Fusion Module (FFM) to inject 3D information into the 2D\npath. Meanwhile, we adopt the Polarized Self-Attention (PSA) block to model the\nsemantic interdependencies in spatial and channel dimensions respectively.\nExperimentally, our extensive experiments on the OCTA-500 dataset show that our\nproposed algorithm achieves state-of-the-art performance compared with previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glimpse-Attend-and-Explore: Self-Attention for Active Visual Exploration. (arXiv:2108.11717v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11717","description":"<p>Active visual exploration aims to assist an agent with a limited field of\nview to understand its environment based on partial observations made by\nchoosing the best viewing directions in the scene. Recent methods have tried to\naddress this problem either by using reinforcement learning, which is difficult\nto train, or by uncertainty maps, which are task-specific and can only be\nimplemented for dense prediction tasks. In this paper, we propose the\nGlimpse-Attend-and-Explore model which: (a) employs self-attention to guide the\nvisual exploration instead of task-specific uncertainty maps; (b) can be used\nfor both dense and sparse prediction tasks; and (c) uses a contrastive stream\nto further improve the representations learned. Unlike previous works, we show\nthe application of our model on multiple tasks like reconstruction,\nsegmentation and classification. Our model provides encouraging results while\nbeing less dependent on dataset bias in driving the exploration. We further\nperform an ablation study to investigate the features and attention learned by\nour model. Finally, we show that our self-attention module learns to attend\ndifferent regions of the scene by minimizing the loss on the downstream task.\nCode: https://github.com/soroushseifi/glimpse-attend-explore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seifi_S/0/1/0/all/0/1\">Soroush Seifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Abhishek Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking high-fidelity pedestrian tracking systems for research, real-time monitoring and crowd control. (arXiv:2108.11719v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2108.11719","description":"<p>High-fidelity pedestrian tracking in real-life conditions has been an\nimportant tool in fundamental crowd dynamics research allowing to quantify\nstatistics of relevant observables including walking velocities, mutual\ndistances and body orientations. As this technology advances, it is becoming\nincreasingly useful also in society. In fact, continued urbanization is\noverwhelming existing pedestrian infrastructures such as transportation hubs\nand stations, generating an urgent need for real-time highly-accurate usage\ndata, aiming both at flow monitoring and dynamics understanding. To\nsuccessfully employ pedestrian tracking techniques in research and technology,\nit is crucial to validate and benchmark them for accuracy. This is not only\nnecessary to guarantee data quality, but also to identify systematic errors.\n</p>\n<p>In this contribution, we present and discuss a benchmark suite, towards an\nopen standard in the community, for privacy-respectful pedestrian tracking\ntechniques. The suite is technology-independent and is applicable to academic\nand commercial pedestrian tracking systems, operating both in lab environments\nand real-life conditions. The benchmark suite consists of 5 tests addressing\nspecific aspects of pedestrian tracking quality, including accurate crowd flux\nestimation, density estimation, position detection and trajectory accuracy. The\noutput of the tests are quality factors expressed as single numbers. We provide\nthe benchmark results for two tracking systems, both operating in real-life,\none commercial, and the other based on overhead depth-maps developed at TU\nEindhoven. We discuss the results on the basis of the quality factors and\nreport on the typical sensor and algorithmic performance. This enables us to\nhighlight the current state-of-the-art, its limitations and provide\ninstallation recommendations, with specific attention to multi-sensor setups\nand data stitching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Pouw_C/0/1/0/all/0/1\">Caspar A. S. Pouw</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Willems_J/0/1/0/all/0/1\">Joris Willems</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schadewijk_F/0/1/0/all/0/1\">Frank van Schadewijk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Thurau_J/0/1/0/all/0/1\">Jasmin Thurau</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Toschi_F/0/1/0/all/0/1\">Federico Toschi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Corbetta_A/0/1/0/all/0/1\">Alessandro Corbetta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Shoulder Muscle MRI Using a New Region and Edge based Deep Auto-Encoder. (arXiv:2108.11720v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11720","description":"<p>Automatic segmentation of shoulder muscle MRI is challenging due to the high\nvariation in muscle size, shape, texture, and spatial position of tears. Manual\nsegmentation of tear and muscle portion is hard, time-consuming, and subjective\nto pathological expertise. This work proposes a new Region and Edge-based Deep\nAuto-Encoder (RE-DAE) for shoulder muscle MRI segmentation. The proposed RE-DAE\nharmoniously employs average and max-pooling operation in the encoder and\ndecoder blocks of the Convolutional Neural Network (CNN). Region-based\nsegmentation incorporated in the Deep Auto-Encoder (DAE) encourages the network\nto extract smooth and homogenous regions. In contrast, edge-based segmentation\ntries to learn the boundary and anatomical information. These two concepts,\nsystematically combined in a DAE, generate a discriminative and sparse hybrid\nfeature space (exploiting both region homogeneity and boundaries). Moreover,\nthe concept of static attention is exploited in the proposed RE-DAE that helps\nin effectively learning the tear region. The performances of the proposed MRI\nsegmentation based DAE architectures have been tested using a 3D MRI shoulder\nmuscle dataset using the hold-out cross-validation technique. The MRI data has\nbeen collected from the Korea University Anam Hospital, Seoul, South Korea.\nExperimental comparisons have been conducted by employing innovative\ncustom-made and existing pre-trained CNN architectures both using transfer\nlearning and fine-tuning. Objective evaluation on the muscle datasets using the\nproposed SA-RE-DAE showed a dice similarity of 85.58% and 87.07%, an accuracy\nof 81.57% and 95.58% for tear and muscle regions, respectively. The high visual\nquality and the objective result suggest that the proposed SA-RE-DAE is able to\ncorrectly segment tear and muscle regions in shoulder muscle MRI for better\nclinical decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yeon Soo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Mehdi Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+jeong_W/0/1/0/all/0/1\">Woong Kyo jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Diversify for Single Domain Generalization. (arXiv:2108.11726v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11726","description":"<p>Domain generalization (DG) aims to generalize a model trained on multiple\nsource (i.e., training) domains to a distributionally different target (i.e.,\ntest) domain. In contrast to the conventional DG that strictly requires the\navailability of multiple source domains, this paper considers a more realistic\nyet challenging scenario, namely Single Domain Generalization (Single-DG),\nwhere only one source domain is available for training. In this scenario, the\nlimited diversity may jeopardize the model generalization on unseen target\ndomains. To tackle this problem, we propose a style-complement module to\nenhance the generalization power of the model by synthesizing images from\ndiverse distributions that are complementary to the source ones. More\nspecifically, we adopt a tractable upper bound of mutual information (MI)\nbetween the generated and source samples and perform a two-step optimization\niteratively: (1) by minimizing the MI upper bound approximation for each sample\npair, the generated images are forced to be diversified from the source\nsamples; (2) subsequently, we maximize the MI between the samples from the same\nsemantic category, which assists the network to learn discriminative features\nfrom diverse-styled images. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our approach, which surpasses the\nstate-of-the-art single-DG methods by up to 25.14%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Underwater Image Semantic Segmentation Method Focusing on Boundaries and a Real Underwater Scene Semantic Segmentation Dataset. (arXiv:2108.11727v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11727","description":"<p>With the development of underwater object grabbing technology, underwater\nobject recognition and segmentation of high accuracy has become a challenge.\nThe existing underwater object detection technology can only give the general\nposition of an object, unable to give more detailed information such as the\noutline of the object, which seriously affects the grabbing efficiency. To\naddress this problem, we label and establish the first underwater semantic\nsegmentation dataset of real scene(DUT-USEG:DUT Underwater Segmentation\nDataset). The DUT- USEG dataset includes 6617 images, 1487 of which have\nsemantic segmentation and instance segmentation annotations, and the remaining\n5130 images have object detection box annotations. Based on this dataset, we\npropose a semi-supervised underwater semantic segmentation network focusing on\nthe boundaries(US-Net: Underwater Segmentation Network). By designing a pseudo\nlabel generator and a boundary detection subnetwork, this network realizes the\nfine learning of boundaries between underwater objects and background, and\nimproves the segmentation effect of boundary areas. Experiments show that the\nproposed method improves by 6.7% in three categories of holothurian, echinus,\nstarfish in DUT-USEG dataset, and achieves state-of-the-art results. The DUT-\nUSEG dataset will be released at https://github.com/baxiyi/DUT-USEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yingshuang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongxuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning based dictionary learning and tomographic image reconstruction. (arXiv:2108.11730v1 [stat.ML])","link":"http://arxiv.org/abs/2108.11730","description":"<p>This work presents an approach for image reconstruction in clinical low-dose\ntomography that combines principles from sparse signal processing with ideas\nfrom deep learning. First, we describe sparse signal representation in terms of\ndictionaries from a statistical perspective and interpret dictionary learning\nas a process of aligning distribution that arises from a generative model with\nempirical distribution of true signals. As a result we can see that sparse\ncoding with learned dictionaries resembles a specific variational autoencoder,\nwhere the decoder is a linear function and the encoder is a sparse coding\nalgorithm. Next, we show that dictionary learning can also benefit from\ncomputational advancements introduced in the context of deep learning, such as\nparallelism and as stochastic optimization. Finally, we show that\nregularization by dictionaries achieves competitive performance in computed\ntomography (CT) reconstruction comparing to state-of-the-art model based and\ndata driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Rudzusika_J/0/1/0/all/0/1\">Jevgenija Rudzusika</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koehler_T/0/1/0/all/0/1\">Thomas Koehler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Dynamic Inference Network for Group Activity Recognition. (arXiv:2108.11743v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11743","description":"<p>Group activity recognition aims to understand the activity performed by a\ngroup of people. In order to solve it, modeling complex spatio-temporal\ninteractions is the key. Previous methods are limited in reasoning on a\npredefined graph, which ignores the inherent person-specific interaction\ncontext. Moreover, they adopt inference schemes that are computationally\nexpensive and easily result in the over-smoothing problem. In this paper, we\nmanage to achieve spatio-temporal person-specific inferences by proposing\nDynamic Inference Network (DIN), which composes of Dynamic Relation (DR) module\nand Dynamic Walk (DW) module. We firstly propose to initialize interaction\nfields on a primary spatio-temporal graph. Within each interaction field, we\napply DR to predict the relation matrix and DW to predict the dynamic walk\noffsets in a joint-processing manner, thus forming a person-specific\ninteraction graph. By updating features on the specific graph, a person can\npossess a global-level interaction field with a local initialization.\nExperiments indicate both modules' effectiveness. Moreover, DIN achieves\nsignificant improvement compared to previous state-of-the-art methods on two\npopular datasets under the same setting, while costing much less computation\noverhead of the reasoning module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hangjie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training and Profiling a Pediatric Emotion Recognition Classifier on Mobile Devices. (arXiv:2108.11754v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11754","description":"<p>Implementing automated emotion recognition on mobile devices could provide an\naccessible diagnostic and therapeutic tool for those who struggle to recognize\nemotion, including children with developmental behavioral conditions such as\nautism. Although recent advances have been made in building more accurate\nemotion classifiers, existing models are too computationally expensive to be\ndeployed on mobile devices. In this study, we optimized and profiled various\nmachine learning models designed for inference on edge devices and were able to\nmatch previous state of the art results for emotion recognition on children.\nOur best model, a MobileNet-V2 network pre-trained on ImageNet, achieved 65.11%\nbalanced accuracy and 64.19% F1-score on CAFE, while achieving a 45-millisecond\ninference latency on a Motorola Moto G6 phone. This balanced accuracy is only\n1.79% less than the current state of the art for CAFE, which used a model that\ncontains 26.62x more parameters and was unable to run on the Moto G6, even when\nfully optimized. This work validates that with specialized design and\noptimization techniques, machine learning models can become lightweight enough\nfor deployment on mobile devices and still achieve high accuracies on difficult\nimage classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Agnik Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_C/0/1/0/all/0/1\">Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis P. Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Semantic Grouping of Latent Concepts for Explanations: An Ante-Hoc Approach. (arXiv:2108.11761v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11761","description":"<p>Self-explainable deep models are devised to represent the hidden concepts in\nthe dataset without requiring any posthoc explanation generation technique. We\nworked with one of such models motivated by explicitly representing the\nclassifier function as a linear function and showed that by exploiting\nprobabilistic latent and properly modifying different parts of the model can\nresult better explanation as well as provide superior predictive performance.\nApart from standard visualization techniques, we proposed a new technique which\ncan strengthen human understanding towards hidden concepts. We also proposed a\ntechnique of using two different self-supervision techniques to extract\nmeaningful concepts related to the type of self-supervision considered and\nachieved significant performance boost. The most important aspect of our method\nis that it works nicely in a low data regime and reaches the desired accuracy\nin a few number of epochs. We reported exhaustive results with CIFAR10,\nCIFAR100, and AWA2 datasets to show effect of our method with moderate and\nrelatively complex datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anirban Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijaykeerthy_D/0/1/0/all/0/1\">Deepak Vijaykeerthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anindya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11765","description":"<p>Deep neural networks (DNNs) have become essential for processing the vast\namounts of aerial imagery collected using earth-observing satellite platforms.\nHowever, DNNs are vulnerable towards adversarial examples, and it is expected\nthat this weakness also plagues DNNs for aerial imagery. In this work, we\ndemonstrate one of the first efforts on physical adversarial attacks on aerial\nimagery, whereby adversarial patches were optimised, fabricated and installed\non or near target objects (cars) to significantly reduce the efficacy of an\nobject detector applied on overhead images. Physical adversarial attacks on\naerial images, particularly those captured from satellite platforms, are\nchallenged by atmospheric factors (lighting, weather, seasons) and the distance\nbetween the observer and target. To investigate the effects of these\nchallenges, we devised novel experiments and metrics to evaluate the efficacy\nof physical adversarial attacks against object detectors in aerial scenes. Our\nresults indicate the palpable threat posed by physical adversarial attacks\ntowards DNNs for processing satellite imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_A/0/1/0/all/0/1\">Andrew Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_Y/0/1/0/all/0/1\">Yee Wei Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_R/0/1/0/all/0/1\">Ramesh Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dillon Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Deep Saliency Map Generators on Multispectral Data in Object Detection. (arXiv:2108.11767v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11767","description":"<p>Deep neural networks, especially convolutional deep neural networks, are\nstate-of-the-art methods to classify, segment or even generate images, movies,\nor sounds. However, these methods lack of a good semantic understanding of what\nhappens internally. The question, why a COVID-19 detector has classified a\nstack of lung-ct images as positive, is sometimes more interesting than the\noverall specificity and sensitivity. Especially when human domain expert\nknowledge disagrees with the given output. This way, human domain experts could\nalso be advised to reconsider their choice, regarding the information pointed\nout by the system. In addition, the deep learning model can be controlled, and\na present dataset bias can be found. Currently, most explainable AI methods in\nthe computer vision domain are purely used on image classification, where the\nimages are ordinary images in the visible spectrum. As a result, there is no\ncomparison on how the methods behave with multimodal image data, as well as\nmost methods have not been investigated on how they behave when used for object\ndetection. This work tries to close the gaps. Firstly, investigating three\nsaliency map generator methods on how their maps differ across the different\nspectra. This is achieved via accurate and systematic training. Secondly, we\nexamine how they behave when used for object detection. As a practical problem,\nwe chose object detection in the infrared and visual spectrum for autonomous\ndriving. The dataset used in this work is the Multispectral Object Detection\nDataset, where each scene is available in the FIR, MIR and NIR as well as\nvisual spectrum. The results show that there are differences between the\ninfrared and visual activation maps. Further, an advanced training with both,\nthe infrared and visual data not only improves the network's output, it also\nleads to more focused spots in the saliency maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1\">Jens Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munch_D/0/1/0/all/0/1\">David M&#xfc;nch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-category Video Highlight Detection via Set-based Learning. (arXiv:2108.11770v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11770","description":"<p>Autonomous highlight detection is crucial for enhancing the efficiency of\nvideo browsing on social media platforms. To attain this goal in a data-driven\nway, one may often face the situation where highlight annotations are not\navailable on the target video category used in practice, while the supervision\non another video category (named as source video category) is achievable. In\nsuch a situation, one can derive an effective highlight detector on target\nvideo category by transferring the highlight knowledge acquired from source\nvideo category to the target one. We call this problem cross-category video\nhighlight detection, which has been rarely studied in previous works. For\ntackling such practical problem, we propose a Dual-Learner-based Video\nHighlight Detection (DL-VHD) framework. Under this framework, we first design a\nSet-based Learning module (SL-module) to improve the conventional pair-based\nlearning by assessing the highlight extent of a video segment under a broader\ncontext. Based on such learning manner, we introduce two different learners to\nacquire the basic distinction of target category videos and the characteristics\nof highlight moments on source video category, respectively. These two types of\nhighlight knowledge are further consolidated via knowledge distillation.\nExtensive experiments on three benchmark datasets demonstrate the superiority\nof the proposed SL-module, and the DL-VHD method outperforms five typical\nUnsupervised Domain Adaptation (UDA) algorithms on various cross-category\nhighlight detection tasks. Our code is available at\nhttps://github.com/ChrisAllenMing/Cross_Category_Video_Highlight .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Riheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICM-3D: Instantiated Category Modeling for 3D Instance Segmentation. (arXiv:2108.11771v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11771","description":"<p>Separating 3D point clouds into individual instances is an important task for\n3D vision. It is challenging due to the unknown and varying number of instances\nin a scene. Existing deep learning based works focus on a two-step pipeline:\nfirst learn a feature embedding and then cluster the points. Such a two-step\npipeline leads to disconnected intermediate objectives. In this paper, we\npropose an integrated reformulation of 3D instance segmentation as a per-point\nclassification problem. We propose ICM-3D, a single-step method to segment 3D\ninstances via instantiated categorization. The augmented category information\nis automatically constructed from 3D spatial positions. We conduct extensive\nexperiments to verify the effectiveness of ICM-3D and show that it obtains\ninspiring performance across multiple frameworks, backbones and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1\">Ruihang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modulation Network for Audio-Visual Event Localization. (arXiv:2108.11773v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11773","description":"<p>We study the problem of localizing audio-visual events that are both audible\nand visible in a video. Existing works focus on encoding and aligning audio and\nvisual features at the segment level while neglecting informative correlation\nbetween segments of the two modalities and between multi-scale event proposals.\nWe propose a novel MultiModulation Network (M2N) to learn the above correlation\nand leverage it as semantic guidance to modulate the related auditory, visual,\nand fused features. In particular, during feature encoding, we propose\ncross-modal normalization and intra-modal normalization. The former modulates\nthe features of two modalities by establishing and exploiting the cross-modal\nrelationship. The latter modulates the features of a single modality with the\nevent-relevant semantic guidance of the same modality. In the fusion stage,we\npropose a multi-scale proposal modulating module and a multi-alignment segment\nmodulating module to introduce multi-scale event proposals and enable dense\nmatching between cross-modal segments. With the auditory, visual, and fused\nfeatures modulated by the correlation information regarding audio-visual\nevents, M2N performs accurate event localization. Extensive experiments\nconducted on the AVE dataset demonstrate that our proposed method outperforms\nthe state of the art in both supervised event localization and cross-modality\nlocalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quadratic mutual information regularization in real-time deep CNN models. (arXiv:2108.11774v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11774","description":"<p>In this paper, regularized lightweight deep convolutional neural network\nmodels, capable of effectively operating in real-time on devices with\nrestricted computational power for high-resolution video input are proposed.\nFurthermore, a novel regularization method motivated by the Quadratic Mutual\nInformation, in order to improve the generalization ability of the utilized\nmodels is proposed. Extensive experiments on various binary classification\nproblems involved in autonomous systems are performed, indicating the\neffectiveness of the proposed models as well as of the proposed regularizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepi_M/0/1/0/all/0/1\">Maria Tzelepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">Anastasios Tefas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Assessment of Adversarial Severity. (arXiv:2108.11785v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11785","description":"<p>Adversarial Robustness is a growing field that evidences the brittleness of\nneural networks. Although the literature on adversarial robustness is vast, a\ndimension is missing in these studies: assessing how severe the mistakes are.\nWe call this notion \"Adversarial Severity\" since it quantifies the downstream\nimpact of adversarial corruptions by computing the semantic error between the\nmisclassification and the proper label. We propose to study the effects of\nadversarial noise by measuring the Robustness and Severity into a large-scale\ndataset: iNaturalist-H. Our contributions are: (i) we introduce novel\nHierarchical Attacks that harness the rich structured space of labels to create\nadversarial examples. (ii) These attacks allow us to benchmark the Adversarial\nRobustness and Severity of classification models. (iii) We enhance the\ntraditional adversarial training with a simple yet effective Hierarchical\nCurriculum Training to learn these nodes gradually within the hierarchical\ntree. We perform extensive experiments showing that hierarchical defenses allow\ndeep models to boost the adversarial Robustness by 1.85% and reduce the\nseverity of all attacks by 0.17, on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1\">Guillaume Jeanneret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbelaez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State of the Art: Image Hashing. (arXiv:2108.11794v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11794","description":"<p>Perceptual image hashing methods are often applied in various objectives,\nsuch as image retrieval, finding duplicate or near-duplicate images, and\nfinding similar images from large-scale image content. The main challenge in\nimage hashing techniques is robust feature extraction, which generates the same\nor similar hashes in images that are visually identical. In this article, we\npresent a short review of the state-of-the-art traditional perceptual hashing\nand deep learning-based perceptual hashing methods, identifying the best\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1\">Rubel Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Medina_P/0/1/0/all/0/1\">Pablo Blanco-Medina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient training of lightweight neural networks using Online Self-Acquired Knowledge Distillation. (arXiv:2108.11798v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11798","description":"<p>Knowledge Distillation has been established as a highly promising approach\nfor training compact and faster models by transferring knowledge from\nheavyweight and powerful models. However, KD in its conventional version\nconstitutes an enduring, computationally and memory demanding process. In this\npaper, Online Self-Acquired Knowledge Distillation (OSAKD) is proposed, aiming\nto improve the performance of any deep neural model in an online manner. We\nutilize k-nn non-parametric density estimation technique for estimating the\nunknown probability distributions of the data samples in the output feature\nspace. This allows us for directly estimating the posterior class probabilities\nof the data samples, and we use them as soft labels that encode explicit\ninformation about the similarities of the data with the classes, negligibly\naffecting the computational cost. The experimental evaluation on four datasets\nvalidates the effectiveness of proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepi_M/0/1/0/all/0/1\">Maria Tzelepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">Anastasios Tefas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the OR. (arXiv:2108.11801v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11801","description":"<p>The fine-grained localization of clinicians in the operating room (OR) is a\nkey component to design the new generation of OR support systems. Computer\nvision models for person pixel-based segmentation and body-keypoints detection\nare needed to better understand the clinical activities and the spatial layout\nof the OR. This is challenging, not only because OR images are very different\nfrom traditional vision datasets, but also because data and annotations are\nhard to collect and generate in the OR due to privacy concerns. To address\nthese concerns, we first study how joint person pose estimation and instance\nsegmentation can be performed on low resolutions images from 1x to 12x. Second,\nto address the domain shift and the lack of annotations, we propose a novel\nunsupervised domain adaptation method, called \\emph{AdaptOR}, to adapt a model\nfrom an \\emph{in-the-wild} labeled source domain to a statistically different\nunlabeled target domain. We propose to exploit explicit geometric constraints\non the different augmentations of the unlabeled target domain image to generate\naccurate pseudo labels, and using these pseudo labels to train the model on\nhigh- and low-resolution OR images in a \\emph{self-training} framework.\nFurthermore, we propose \\emph{disentangled feature normalization} to handle the\nstatistically different source and target domain data. Extensive experimental\nresults with detailed ablation studies on the two OR datasets \\emph{MVOR+} and\n\\emph{TUM-OR-test} show the effectiveness of our approach against strongly\nconstructed baselines, especially on the low-resolution privacy-preserving OR\nimages. Finally, we show the generality of our method as a semi-supervised\nlearning (SSL) method on the large-scale \\emph{COCO} dataset, where we achieve\ncomparable results with as few as \\textbf{1\\%} of labeled supervision against a\nmodel trained with 100\\% labeled supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Contextual Information Beyond Image for Semantic Segmentation. (arXiv:2108.11819v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11819","description":"<p>This paper studies the context aggregation problem in semantic image\nsegmentation. The existing researches focus on improving the pixel\nrepresentations by aggregating the contextual information within individual\nimages. Though impressive, these methods neglect the significance of the\nrepresentations of the pixels of the corresponding class beyond the input\nimage. To address this, this paper proposes to mine the contextual information\nbeyond individual images to further augment the pixel representations. We first\nset up a feature memory module, which is updated dynamically during training,\nto store the dataset-level representations of various categories. Then, we\nlearn class probability distribution of each pixel representation under the\nsupervision of the ground-truth segmentation. At last, the representation of\neach pixel is augmented by aggregating the dataset-level representations based\non the corresponding class probability distribution. Furthermore, by utilizing\nthe stored dataset-level representations, we also propose a representation\nconsistent learning strategy to make the classification head better address\nintra-class compactness and inter-class dispersion. The proposed method could\nbe effortlessly incorporated into existing segmentation frameworks (e.g., FCN,\nPSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements.\nMining contextual information beyond image allows us to report state-of-the-art\nperformance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jie Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State of the Art: Face Recognition. (arXiv:2108.11821v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11821","description":"<p>Working with Child Sexual Exploitation Material (CSEM) in forensic\napplications might be benefited from the progress in automatic face\nrecognition. However, discriminative parts of a face in CSEM, i.e., mostly the\neyes, could be often occluded to difficult the victim's identification. Most of\nthe face recognition approaches cannot deal with such kind of occlusions,\nresulting in inaccurate face recognition results. This document presents a\nshort review face recognition methods for images with natural and eye occlude\nfaces. The purpose is to select the best baseline approach for solving\nautomatic face recognition of occluded faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_R/0/1/0/all/0/1\">Rubel Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Medina_P/0/1/0/all/0/1\">Pablo Blanco-Medina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Flexible Human Pose Estimation with HyperPose. (arXiv:2108.11826v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11826","description":"<p>Estimating human pose is an important yet challenging task in multimedia\napplications. Existing pose estimation libraries target reproducing standard\npose estimation algorithms. When it comes to customising these algorithms for\nreal-world applications, none of the existing libraries can offer both the\nflexibility of developing custom pose estimation algorithms and the\nhigh-performance of executing these algorithms on commodity devices. In this\npaper, we introduce Hyperpose, a novel flexible and high-performance pose\nestimation library. Hyperpose provides expressive Python APIs that enable\ndevelopers to easily customise pose estimation algorithms for their\napplications. It further provides a model inference engine highly optimised for\nreal-time pose estimation. This engine can dynamically dispatch carefully\ndesigned pose estimation tasks to CPUs and GPUs, thus automatically achieving\nhigh utilisation of hardware resources irrespective of deployment environments.\nExtensive evaluation results show that Hyperpose can achieve up to 3.1x~7.3x\nhigher pose estimation throughput compared to state-of-the-art pose estimation\nlibraries without compromising estimation accuracy. By 2021, Hyperpose has\nreceived over 1000 stars on GitHub and attracted users from both industry and\nacademy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yixiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Luo Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry Based Machining Feature Retrieval with Inductive Transfer Learning. (arXiv:2108.11838v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11838","description":"<p>Manufacturing industries have widely adopted the reuse of machine parts as a\nmethod to reduce costs and as a sustainable manufacturing practice.\nIdentification of reusable features from the design of the parts and finding\ntheir similar features from the database is an important part of this process.\nIn this project, with the help of fully convolutional geometric features, we\nare able to extract and learn the high level semantic features from CAD models\nwith inductive transfer learning. The extracted features are then compared with\nthat of other CAD models from the database using Frobenius norm and identical\nfeatures are retrieved. Later we passed the extracted features to a deep\nconvolutional neural network with a spatial pyramid pooling layer and the\nperformance of the feature retrieval increased significantly. It was evident\nfrom the results that the model could effectively capture the geometrical\nelements from machining features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_N/0/1/0/all/0/1\">N S Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HB_B/0/1/0/all/0/1\">Barathi Ganesh HB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variyar_S/0/1/0/all/0/1\">Sajith Variyar VV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1\">Sowmya V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KP_S/0/1/0/all/0/1\">Soman KP</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11845","description":"<p>This paper is concerned with image classification based on deep convolutional\nneural networks (CNNs). The focus is centered around the following question:\ngiven a set of candidate CNN models, how to select the right one that has the\nbest generalization property for the current task? Present model selection\nmethods require access to a batch of labeled data for defining a performance\nmetric, such as the cross-entropy loss, the classification error rate, the\nnegative log-likelihood, and so on. In many practical cases, however, labeled\ndata are not available in time as labeling itself is a time-consuming and\nexpensive task. To this end, this paper presents an approach to CNN model\nselection using only unlabeled data. This method is developed based on a\nprinciple termed consistent relative confidence (CRC). The effectiveness and\nefficiency of the presented method are demonstrated by extensive experimental\nstudies based on datasets MNIST and FasionMNIST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Compression for Resource-constrained Edge Computing Systems. (arXiv:2108.11898v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11898","description":"<p>There has been much interest in deploying deep learning algorithms on\nlow-powered devices, including smartphones, drones, and medical sensors.\nHowever, full-scale deep neural networks are often too resource-intensive in\nterms of energy and storage. As a result, the bulk part of the machine learning\noperation is therefore often carried out on an edge server, where the data is\ncompressed and transmitted. However, compressing data (such as images) leads to\ntransmitting information irrelevant to the supervised task. Another popular\napproach is to split the deep network between the device and the server while\ncompressing intermediate features. To date, however, such split computing\nstrategies have barely outperformed the aforementioned naive data compression\nbaselines due to their inefficient approaches to feature compression. This\npaper adopts ideas from knowledge distillation and neural image compression to\ncompress intermediate feature representations more efficiently. Our supervised\ncompression approach uses a teacher model and a student model with a stochastic\nbottleneck and learnable prior for entropy coding. We compare our approach to\nvarious neural image and feature compression baselines in three vision tasks\nand found that it achieves better supervised rate-distortion performance while\nalso maintaining smaller end-to-end latency. We furthermore show that the\nlearned feature representations can be tuned to serve multiple downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Multi-scale Consistency for Weakly Supervised Segmentation Learning. (arXiv:2108.11900v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11900","description":"<p>Collecting large-scale medical datasets with fine-grained annotations is\ntime-consuming and requires experts. For this reason, weakly supervised\nlearning aims at optimising machine learning models using weaker forms of\nannotations, such as scribbles, which are easier and faster to collect.\nUnfortunately, training with weak labels is challenging and needs\nregularisation. Herein, we introduce a novel self-supervised multi-scale\nconsistency loss, which, coupled with an attention mechanism, encourages the\nsegmentor to learn multi-scale relationships between objects and improves\nperformance. We show state-of-the-art performance on several medical and\nnon-medical datasets. The code used for the experiments is available at\nhttps://vios-s.github.io/multiscale-pyag.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning. (arXiv:2108.11912v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11912","description":"<p>Stylized image captioning systems aim to generate a caption not only\nsemantically related to a given image but also consistent with a given style\ndescription. One of the biggest challenges with this task is the lack of\nsufficient paired stylized data. Many studies focus on unsupervised approaches,\nwithout considering from the perspective of data augmentation. We begin with\nthe observation that people may recall similar emotions when they are in\nsimilar scenes, and often express similar emotions with similar style phrases,\nwhich underpins our data augmentation idea. In this paper, we propose a novel\nExtract-Retrieve-Generate data augmentation framework to extract style phrases\nfrom small-scale stylized sentences and graft them to large-scale factual\ncaptions. First, we design the emotional signal extractor to extract style\nphrases from small-scale stylized sentences. Second, we construct the plugable\nmulti-modal scene retriever to retrieve scenes represented with pairs of an\nimage and its stylized caption, which are similar to the query image or caption\nin the large-scale factual data. In the end, based on the style phrases of\nsimilar scenes and the factual description of the current scene, we build the\nemotion-aware caption generator to generate fluent and diversified stylized\ncaptions for the current scene. Extensive experimental results show that our\nframework can alleviate the data scarcity problem effectively. It also\nsignificantly boosts the performance of several existing image captioning\nmodels in both supervised and unsupervised settings, which outperforms the\nstate-of-the-art stylized image captioning methods in terms of both sentence\nrelevance and stylishness by a substantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuchen Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Centric Semi-Automated Infographics Authoring and Recommendation. (arXiv:2108.11914v1 [cs.HC])","link":"http://arxiv.org/abs/2108.11914","description":"<p>Designing infographics can be a tedious process for non-experts and\ntime-consuming even for professional designers. Based on the literature and a\nformative study, we propose a flexible framework for automated and\nsemi-automated infographics design. This framework captures the main design\ncomponents in infographics and streamlines the generation workflow into three\nsteps, allowing users to control and optimize each aspect independently. Based\non the framework, we also propose an interactive tool, \\name{}, for assisting\nnovice designers with creating high-quality infographics from an input in a\nmarkdown format by offering recommendations of different design components of\ninfographics. Simultaneously, more experienced designers can provide custom\ndesigns and layout ideas to the tool using a canvas to control the automated\ngeneration process partially. As part of our work, we also contribute an\nindividual visual group (VG) and connection designs dataset (in SVG), along\nwith a 1k complete infographic image dataset with segmented VGs. This dataset\nplays a crucial role in diversifying the infographic designs created by our\nframework. We evaluate our approach with a comparison against similar tools, a\nuser study with novice and expert designers, and a case study. Results confirm\nthat our framework and \\name{} excel in creating customized infographics and\nexploring a large variety of designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Anjul Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pushkar Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Swasti Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11926","description":"<p>Thanks to their ability to learn flexible data-driven losses, Generative\nAdversarial Networks (GANs) are an integral part of many semi- and\nweakly-supervised methods for medical image segmentation. GANs jointly optimise\na generator and an adversarial discriminator on a set of training data. After\ntraining has completed, the discriminator is usually discarded and only the\ngenerator is used for inference. But should we discard discriminators? In this\nwork, we argue that training stable discriminators produces expressive loss\nfunctions that we can re-use at inference to detect and correct segmentation\nmistakes. First, we identify key challenges and suggest possible solutions to\nmake discriminators re-usable at inference. Then, we show that we can combine\ndiscriminators with image reconstruction costs (via decoders) to further\nimprove the model. Our method is simple and improves the test-time performance\nof pre-trained GANs. Moreover, we show that it is compatible with standard\npost-processing techniques and it has potentials to be used for Online\nContinual Learning. With our work, we open new research avenues for re-using\nadversarial discriminators at inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Coherent Out-of-Distribution Detection. (arXiv:2108.11941v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11941","description":"<p>Current out-of-distribution (OOD) detection benchmarks are commonly built by\ndefining one dataset as in-distribution (ID) and all others as OOD. However,\nthese benchmarks unfortunately introduce some unwanted and impractical goals,\ne.g., to perfectly distinguish CIFAR dogs from ImageNet dogs, even though they\nhave the same semantics and negligible covariate shifts. These unrealistic\ngoals will result in an extremely narrow range of model capabilities, greatly\nlimiting their use in real applications. To overcome these drawbacks, we\nre-design the benchmarks and propose the semantically coherent\nout-of-distribution detection (SC-OOD). On the SC-OOD benchmarks, existing\nmethods suffer from large performance degradation, suggesting that they are\nextremely sensitive to low-level discrepancy between data sources while\nignoring their inherent semantics. To develop an effective SC-OOD detection\napproach, we leverage an external unlabeled set and design a concise framework\nfeatured by unsupervised dual grouping (UDG) for the joint modeling of ID and\nOOD data. The proposed UDG can not only enrich the semantic knowledge of the\nmodel by exploiting unlabeled data in an unsupervised manner, but also\ndistinguish ID/OOD samples to enhance ID classification and OOD detection tasks\nsimultaneously. Extensive experiments demonstrate that our approach achieves\nstate-of-the-art performance on SC-OOD benchmarks. Code and benchmarks are\nprovided on our project page: https://jingkang50.github.io/projects/scood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Litong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiaopeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huabin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Modeling for Human Mesh Recovery. (arXiv:2108.11944v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11944","description":"<p>This paper focuses on the problem of 3D human reconstruction from 2D\nevidence. Although this is an inherently ambiguous problem, the majority of\nrecent works avoid the uncertainty modeling and typically regress a single\nestimate for a given input. In contrast to that, in this work, we propose to\nembrace the reconstruction ambiguity and we recast the problem as learning a\nmapping from the input to a distribution of plausible 3D poses. Our approach is\nbased on the normalizing flows model and offers a series of advantages. For\nconventional applications, where a single 3D estimate is required, our\nformulation allows for efficient mode computation. Using the mode leads to\nperformance that is comparable with the state of the art among deterministic\nunimodal regression models. Simultaneously, since we have access to the\nlikelihood of each sample, we demonstrate that our model is useful in a series\nof downstream tasks, where we leverage the probabilistic nature of the\nprediction as a tool for more accurate estimation. These tasks include\nreconstruction from multiple uncalibrated views, as well as human model\nfitting, where our model acts as a powerful image-based prior for mesh\nrecovery. Our results validate the importance of probabilistic modeling, and\nindicate state-of-the-art performance across a variety of settings. Code and\nmodels are available at: https://www.seas.upenn.edu/~nkolot/projects/prohmr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1\">Nikos Kolotouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1\">Georgios Pavlakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SASRA: Semantically-aware Spatio-temporal Reasoning Agent for Vision-and-Language Navigation in Continuous Environments. (arXiv:2108.11945v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11945","description":"<p>This paper presents a novel approach for the Vision-and-Language Navigation\n(VLN) task in continuous 3D environments, which requires an autonomous agent to\nfollow natural language instructions in unseen environments. Existing\nend-to-end learning-based VLN methods struggle at this task as they focus\nmostly on utilizing raw visual observations and lack the semantic\nspatio-temporal reasoning capabilities which is crucial in generalizing to new\nenvironments. In this regard, we present a hybrid transformer-recurrence model\nwhich focuses on combining classical semantic mapping techniques with a\nlearning-based method. Our method creates a temporal semantic memory by\nbuilding a top-down local ego-centric semantic map and performs cross-modal\ngrounding to align map and language modalities to enable effective learning of\nVLN policy. Empirical results in a photo-realistic long-horizon simulation\nenvironment show that the proposed approach outperforms a variety of\nstate-of-the-art methods and baselines with over 22% relative improvement in\nSPL in prior unseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1\">Muhammad Zubair Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zachary Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1\">Supun Samarasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision. (arXiv:2108.11950v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11950","description":"<p>Computer vision tasks such as object detection and semantic/instance\nsegmentation rely on the painstaking annotation of large training datasets. In\nthis paper, we propose LocTex that takes advantage of the low-cost localized\ntextual annotations (i.e., captions and synchronized mouse-over gestures) to\nreduce the annotation effort. We introduce a contrastive pre-training framework\nbetween images and captions and propose to supervise the cross-modal attention\nmap with rendered mouse traces to provide coarse localization signals. Our\nlearned visual features capture rich semantics (from free-form captions) and\naccurate localization (from mouse traces), which are very effective when\ntransferred to various downstream vision tasks. Compared with ImageNet\nsupervised pre-training, LocTex can reduce the size of the pre-training dataset\nby 10x or the target dataset by 2x while achieving comparable or even improved\nperformance on COCO instance segmentation. When provided with the same amount\nof annotations, LocTex achieves around 4% higher accuracy than the previous\nstate-of-the-art \"vision+language\" pre-training approach on the task of PASCAL\nVOC image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gideon_J/0/1/0/all/0/1\">John Gideon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpriortized Autoencoder For Image Generation. (arXiv:1902.04294v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1902.04294","description":"<p>In this paper, we treat the image generation task using an autoencoder, a\nrepresentative latent model. Unlike many studies regularizing the latent\nvariable's distribution by assuming a manually specified prior, we approach the\nimage generation task using an autoencoder by directly estimating the latent\ndistribution. To this end, we introduce 'latent density estimator' which\ncaptures latent distribution explicitly and propose its structure. Through\nexperiments, we show that our generative model generates images with the\nimproved visual quality compared to previous autoencoder-based generative\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaeyoung Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Colonoscopy using Extended and Directional CycleGAN for Lossy Image Translation. (arXiv:2003.12473v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2003.12473","description":"<p>Colorectal cancer screening modalities, such as optical colonoscopy (OC) and\nvirtual colonoscopy (VC), are critical for diagnosing and ultimately removing\npolyps (precursors of colon cancer). The non-invasive VC is normally used to\ninspect a 3D reconstructed colon (from CT scans) for polyps and if found, the\nOC procedure is performed to physically traverse the colon via endoscope and\nremove these polyps. In this paper, we present a deep learning framework,\nExtended and Directional CycleGAN, for lossy unpaired image-to-image\ntranslation between OC and VC to augment OC video sequences with\nscale-consistent depth information from VC, and augment VC with\npatient-specific textures, color and specular highlights from OC (e.g, for\nrealistic polyp synthesis). Both OC and VC contain structural information, but\nit is obscured in OC by additional patient-specific texture and specular\nhighlights, hence making the translation from OC to VC lossy. The existing\nCycleGAN approaches do not handle lossy transformations. To address this\nshortcoming, we introduce an extended cycle consistency loss, which compares\nthe geometric structures from OC in the VC domain. This loss removes the need\nfor the CycleGAN to embed OC information in the VC domain. To handle a stronger\nremoval of the textures and lighting, a Directional Discriminator is introduced\nto differentiate the direction of translation (by creating paired information\nfor the discriminator), as opposed to the standard CycleGAN which is\ndirection-agnostic. Combining the extended cycle consistency loss and the\nDirectional Discriminator, we show state-of-the-art results on scale-consistent\ndepth inference for phantom, textured VC and for real polyp and normal colon\nvideo sequences. We also present results for realistic pendunculated and flat\npolyp synthesis from bumps introduced in 3D VC models. Code/models:\nhttps://github.com/nadeemlab/CEP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1\">Sruti Kumari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sensory Substitution: Noninvasively Enabling Biological Neural Networks to Receive Input from Artificial Neural Networks. (arXiv:2005.13291v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2005.13291","description":"<p>As is expressed in the adage \"a picture is worth a thousand words\", when\nusing spoken language to communicate visual information, brevity can be a\nchallenge. This work describes a novel technique for leveraging machine-learned\nfeature embeddings to sonify visual (and other types of) information into a\nperceptual audio domain, allowing users to perceive this information using only\ntheir aural faculty. The system uses a pretrained image embedding network to\nextract visual features and embed them in a compact subset of Euclidean space\n-- this converts the images into feature vectors whose $L^2$ distances can be\nused as a meaningful measure of similarity. A generative adversarial network\n(GAN) is then used to find a distance preserving map from this metric space of\nfeature vectors into the metric space defined by a target audio dataset\nequipped with either the Euclidean metric or a mel-frequency cepstrum-based\npsychoacoustic distance metric. We demonstrate this technique by sonifying\nimages of faces into human speech-like audio. For both target audio metrics,\nthe GAN successfully found a metric preserving mapping, and in human subject\ntests, users were able to accurately classify audio sonifications of faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Port_A/0/1/0/all/0/1\">Andrew Port</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chelhwon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_M/0/1/0/all/0/1\">Mitesh Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.08428","description":"<p>Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is unexplored, i.e., the complex topology of\nthe neural network architecture. In this work, we address the following\nquestion: Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?. We answer this\nempirically by experimenting with different hand-crafted and NAS-based\narchitectures. Our findings show that, for small-scale attacks, NAS-based\narchitectures are more robust for small-scale datasets and simple tasks than\nhand-crafted architectures. However, as the size of the dataset or the\ncomplexity of task increases, hand-crafted architectures are more robust than\nNAS-based architectures. Our work is the first large-scale study to understand\nadversarial robustness purely from an architectural perspective. Our study\nshows that random sampling in the search space of DARTS (a popular NAS method)\nwith simple ensembling can improve the robustness to PGD attack by nearly~12\\%.\nWe show that NAS, which is popular for achieving SoTA accuracy, can provide\nadversarial accuracy as a free add-on without any form of adversarial training.\nOur results show that leveraging the search space of NAS methods with methods\nlike ensembles can be an excellent way to achieve adversarial robustness\nwithout any form of adversarial training. We also introduce a metric that can\nbe used to calculate the trade-off between clean accuracy and adversarial\nrobustness. Code and pre-trained models will be made available at\n\\url{https://github.com/tdchaitanya/nas-robustness}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalani_P/0/1/0/all/0/1\">Pulkit Gopalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Claustrum Segmentation in Human Brain MRI Using Deep Learning. (arXiv:2008.03465v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.03465","description":"<p>In the last two decades, neuroscience has produced intriguing evidence for a\ncentral role of the claustrum in mammalian forebrain structure and function.\nHowever, relatively few in vivo studies of the claustrum exist in humans. A\nreason for this may be the delicate and sheet-like structure of the claustrum\nlying between the insular cortex and the putamen, which makes it not amenable\nto conventional segmentation methods. Recently, Deep Learning (DL) based\napproaches have been successfully introduced for automated segmentation of\ncomplex, subcortical brain structures. In the following, we present a\nmulti-view DL-based approach to segment the claustrum in T1-weighted MRI scans.\nWe trained and evaluated the proposed method in 181 individuals, using\nbilateral manual claustrum annotations by an expert neuroradiologist as the\nreference standard. Cross-validation experiments yielded median volumetric\nsimilarity, robust Hausdorff distance, and Dice score of 93.3%, 1.41mm, and\n71.8%, respectively, representing equal or superior segmentation performance\ncompared to human intra-rater reliability. The leave-one-scanner-out evaluation\nshowed good transferability of the algorithm to images from unseen scanners at\nslightly inferior performance. Furthermore, we found that DL-based claustrum\nsegmentation benefits from multi-view information and requires a sample size of\naround 75 MRI scans in the training set. We conclude that the developed\nalgorithm allows for robust automated claustrum segmentation and thus yields\nconsiderable potential for facilitating MRI-based research of the human\nclaustrum. The software and models of our method are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menegaux_A/0/1/0/all/0/1\">Aurore Menegaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmitz_Koep_B/0/1/0/all/0/1\">Benita Schmitz-Koep</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neubauer_A/0/1/0/all/0/1\">Antonia Neubauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+B%7Fauerlein_F/0/1/0/all/0/1\">Felix JB B&#xe4;uerlein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorg_C/0/1/0/all/0/1\">Christian Sorg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hedderich_D/0/1/0/all/0/1\">Dennis Hedderich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized Deep Encoder-Decoder Methods for Crack Segmentation. (arXiv:2008.06266v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.06266","description":"<p>Surface crack segmentation poses a challenging computer vision task as\nbackground, shape, colour and size of cracks vary. In this work we propose\noptimized deep encoder-decoder methods consisting of a combination of\ntechniques which yield an increase in crack segmentation performance.\nSpecifically we propose a decoder-part for an encoder-decoder based deep\nlearning architecture for semantic segmentation and study its components to\nachieve increased performance. We also examine the use of different encoder\nstrategies and introduce a data augmentation policy to increase the amount of\navailable training data. The performance evaluation of our method is carried\nout on four publicly available crack segmentation datasets. Additionally, we\nintroduce two techniques into the field of surface crack segmentation,\npreviously not used there: Generating results using test-time-augmentation and\nperforming a statistical result analysis over multiple training runs. The\nformer approach generally yields increased performance results, whereas the\nlatter allows for more reproducible and better representability of a methods\nresults. Using those aforementioned strategies with our proposed\nencoder-decoder architecture we are able to achieve new state of the art\nresults in all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_J/0/1/0/all/0/1\">Jacob K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannion_M/0/1/0/all/0/1\">Mike Mannion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrie_P/0/1/0/all/0/1\">Peter Barrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heatmap Regression via Randomized Rounding. (arXiv:2009.00225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.00225","description":"<p>Heatmap regression has become the mainstream methodology for deep\nlearning-based semantic landmark localization, including in facial landmark\nlocalization and human pose estimation. Though heatmap regression is robust to\nlarge variations in pose, illumination, and occlusion in unconstrained\nsettings, it usually suffers from a sub-pixel localization problem.\nSpecifically, considering that the activation point indices in heatmaps are\nalways integers, quantization error thus appears when using heatmaps as the\nrepresentation of numerical coordinates. Previous methods to overcome the\nsub-pixel localization problem usually rely on high-resolution heatmaps. As a\nresult, there is always a trade-off between achieving localization accuracy and\ncomputational cost, where the computational complexity of heatmap regression\ndepends on the heatmap resolution in a quadratic manner. In this paper, we\nformally analyze the quantization error of vanilla heatmap regression and\npropose a simple yet effective quantization system to address the sub-pixel\nlocalization problem. The proposed quantization system induced by the\nrandomized rounding operation 1) encodes the fractional part of numerical\ncoordinates into the ground truth heatmap using a probabilistic approach during\ntraining; and 2) decodes the predicted numerical coordinates from a set of\nactivation points during testing. We prove that the proposed quantization\nsystem for heatmap regression is unbiased and lossless. Experimental results on\npopular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW) and\nhuman pose estimation datasets (MPII and COCO) demonstrate the effectiveness of\nthe proposed method for efficient and accurate semantic landmark localization.\nCode is available at <a href=\"http://github.com/baoshengyu/H3R.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Refinement Network for Point Cloud Completion with Self-supervision. (arXiv:2010.08719v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08719","description":"<p>Point clouds are often sparse and incomplete, which imposes difficulties for\nreal-world applications. Existing shape completion methods tend to generate\nrough shapes without fine-grained details. Considering this, we introduce a\ntwo-branch network for shape completion. The first branch is a cascaded shape\ncompletion sub-network to synthesize complete objects, where we propose to use\nthe partial input together with the coarse output to preserve the object\ndetails during the dense point reconstruction. The second branch is an\nauto-encoder to reconstruct the original partial input. The two branches share\na same feature extractor to learn an accurate global feature for shape\ncompletion. Furthermore, we propose two strategies to enable the training of\nour network when ground truth data are not available. This is to mitigate the\ndependence of existing approaches on large amounts of ground truth training\ndata that are often difficult to obtain in real-world applications.\nAdditionally, our proposed strategies are also able to improve the\nreconstruction quality for fully supervised learning. We verify our approach in\nself-supervised, semi-supervised and fully supervised settings with superior\nperformances. Quantitative and qualitative results on different datasets\ndemonstrate that our method achieves more realistic outputs than\nstate-of-the-art approaches on the point cloud completion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers. (arXiv:2011.02910v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.02910","description":"<p>Stereo depth estimation relies on optimal correspondence matching between\npixels on epipolar lines in the left and right images to infer depth. In this\nwork, we revisit the problem from a sequence-to-sequence correspondence\nperspective to replace cost volume construction with dense pixel matching using\nposition information and attention. This approach, named STereo TRansformer\n(STTR), has several advantages: It 1) relaxes the limitation of a fixed\ndisparity range, 2) identifies occluded regions and provides confidence\nestimates, and 3) imposes uniqueness constraints during the matching process.\nWe report promising results on both synthetic and real-world datasets and\ndemonstrate that STTR generalizes across different domains, even without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingtong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenkow_N/0/1/0/all/0/1\">Nathan Drenkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Andy Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creighton_F/0/1/0/all/0/1\">Francis X. Creighton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRD: Neural Reflectance Decomposition from Image Collections. (arXiv:2012.03918v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03918","description":"<p>Decomposing a scene into its shape, reflectance, and illumination is a\nchallenging but important problem in computer vision and graphics. This problem\nis inherently more challenging when the illumination is not a single light\nsource under laboratory conditions but is instead an unconstrained\nenvironmental illumination. Though recent work has shown that implicit\nrepresentations can be used to model the radiance field of an object, most of\nthese techniques only enable view synthesis and not relighting. Additionally,\nevaluating these radiance fields is resource and time-intensive. We propose a\nneural reflectance decomposition (NeRD) technique that uses physically-based\nrendering to decompose the scene into spatially varying BRDF material\nproperties. In contrast to existing techniques, our input images can be\ncaptured under different illumination conditions. In addition, we also propose\ntechniques to convert the learned reflectance volume into a relightable\ntextured mesh enabling fast real-time rendering with novel illuminations. We\ndemonstrate the potential of the proposed approach with experiments on both\nsynthetic and real datasets, where we are able to obtain high-quality\nrelightable 3D assets from image collections. The datasets and code is\navailable on the project page: https://markboss.me/publication/2021-nerd/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boss_M/0/1/0/all/0/1\">Mark Boss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Raphael Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P.A. Lensch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Developments in Detection of Central Serous Retinopathy through Imaging and Artificial Intelligence Techniques A Review. (arXiv:2012.10961v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.10961","description":"<p>Central Serous Retinopathy (CSR) or Central Serous Chorioretinopathy (CSC) is\na significant disease that causes blindness and vision loss among millions of\npeople worldwide. It transpires as a result of accumulation of watery fluids\nbehind the retina. Therefore, detection of CSR at early stages allows\npreventive measures to avert any impairment to the human eye. Traditionally,\nseveral manual methods for detecting CSR have been developed in the past;\nhowever, they have shown to be imprecise and unreliable. Consequently,\nArtificial Intelligence (AI) services in the medical field, including automated\nCSR detection, are now possible to detect and cure this disease. This review\nassessed a variety of innovative technologies and researches that contribute to\nthe automatic detection of CSR. In this review, various CSR disease detection\ntechniques, broadly classified into two categories: a) CSR detection based on\nclassical imaging technologies, and b) CSR detection based on Machine/Deep\nLearning methods, have been reviewed after an elaborated evaluation of 29\ndifferent relevant articles. Additionally, it also goes over the advantages,\ndrawbacks and limitations of a variety of traditional imaging techniques, such\nas Optical Coherence Tomography Angiography (OCTA), Fundus Imaging and more\nrecent approaches that utilize Artificial Intelligence techniques. Finally, it\nis concluded that the most recent Deep Learning (DL) classifiers deliver\naccurate, fast, and reliable CSR detection. However, more research needs to be\nconducted on publicly available datasets to improve computation complexity for\nthe reliable detection and diagnosis of CSR disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hassan_S/0/1/0/all/0/1\">Syed Ale Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbar_S/0/1/0/all/0/1\">Shahzad Akbar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rehman_A/0/1/0/all/0/1\">Amjad Rehman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saba_T/0/1/0/all/0/1\">Tanzila Saba</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolivand_H/0/1/0/all/0/1\">Hoshang Kolivand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bahaj_S/0/1/0/all/0/1\">Saeed Ali Bahaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Solving the DeepFake Problem : An Analysis on Improving DeepFake Detection using Dynamic Face Augmentation. (arXiv:2102.09603v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09603","description":"<p>The creation of altered and manipulated faces has become more common due to\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\ndetection models' development for differentiating between a manipulated and\noriginal face from image or video content. In this paper, we focus on\nidentifying the limitations and shortcomings of existing deepfake detection\nframeworks. We identified some key problems surrounding deepfake detection\nthrough quantitative and qualitative analysis of existing methods and datasets.\nWe found that deepfake datasets are highly oversampled, causing models to\nbecome easily overfitted. The datasets are created using a small set of real\nfaces to generate multiple fake samples. When trained on these datasets, models\ntend to memorize the actors' faces and labels instead of learning fake\nfeatures. To mitigate this problem, we propose a simple data augmentation\nmethod termed Face-Cutout. Our method dynamically cuts out regions of an image\nusing the face landmark information. It helps the model selectively attend to\nonly the relevant regions of the input. Our evaluation experiments show that\nFace-Cutout can successfully improve the data variation and alleviate the\nproblem of overfitting. Our method achieves a reduction in LogLoss of 15.2% to\n35.3% on different datasets, compared to other occlusion-based techniques.\nMoreover, we also propose a general-purpose data pre-processing guideline to\ntrain and evaluate existing architectures allowing us to improve the\ngeneralizability of these models for deepfake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seferbekov_S/0/1/0/all/0/1\">Selim Seferbekov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Arup Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning. (arXiv:2103.09504v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.09504","description":"<p>The predictive learning of spatiotemporal sequences aims to generate future\nimages by learning from the historical context, where the visual dynamics are\nbelieved to have modular structures that can be learned with compositional\nsubsystems. This paper models these structures by presenting PredRNN, a new\nrecurrent network, in which a pair of memory cells are explicitly decoupled,\noperate in nearly independent transition manners, and finally form unified\nrepresentations of the complex environment. Concretely, besides the original\nmemory cell of LSTM, this network is featured by a zigzag memory flow that\npropagates in both bottom-up and top-down directions across all layers,\nenabling the learned visual dynamics at different levels of RNNs to\ncommunicate. It also leverages a memory decoupling loss to keep the memory\ncells from learning redundant features. We further propose a new curriculum\nlearning strategy to force PredRNN to learn long-term dynamics from context\nframes, which can be generalized to most sequence-to-sequence models. We\nprovide detailed ablation studies to verify the effectiveness of each\ncomponent. Our approach is shown to obtain highly competitive results on five\ndatasets for both action-free and action-conditioned predictive learning\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud. (arXiv:2103.10339v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10339","description":"<p>This paper investigates the indistinguishable points (difficult to predict\nlabel) in semantic segmentation for large-scale 3D point clouds. The\nindistinguishable points consist of those located in complex boundary, points\nwith similar local textures but different categories, and points in isolate\nsmall hard areas, which largely harm the performance of 3D semantic\nsegmentation. To address this challenge, we propose a novel Indistinguishable\nArea Focalization Network (IAF-Net), which selects indistinguishable points\nadaptively by utilizing the hierarchical semantic features and enhances\nfine-grained features for points especially those indistinguishable points. We\nalso introduce multi-stage loss to improve the feature representation in a\nprogressive way. Moreover, in order to analyze the segmentation performances of\nindistinguishable areas, we propose a new evaluation metric called\nIndistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the\ncomparable results with state-of-the-art performance on several popular 3D\npoint cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other\nmethods on IPBM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style. (arXiv:2103.13722v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13722","description":"<p>Conditional image synthesis from layout has recently attracted much interest.\nPrevious approaches condition the generator on object locations as well as\nclass labels but lack fine-grained control over the diverse appearance aspects\nof individual objects. Gaining control over the image generation process is\nfundamental to build practical applications with a user-friendly interface. In\nthis paper, we propose a method for attribute controlled image synthesis from\nlayout which allows to specify the appearance of individual objects without\naffecting the rest of the image. We extend a state-of-the-art approach for\nlayout-to-image generation to additionally condition individual objects on\nattributes. We create and experiment on a synthetic, as well as the challenging\nVisual Genome dataset. Our qualitative and quantitative results show that our\nmethod can successfully control the fine-grained details of individual objects\nwhen modelling complex scenes with multiple objects. Source code, dataset and\npre-trained models are publicly available\n(https://github.com/stanifrolov/AttrLostGAN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Avneesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayil_T/0/1/0/all/0/1\">Tushar Karayil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02846","description":"<p>Aerial scene recognition is a fundamental research problem in interpreting\nhigh-resolution aerial imagery. Over the past few years, most studies focus on\nclassifying an image into one scene category, while in real-world scenarios, it\nis more often that a single image contains multiple scenes. Therefore, in this\npaper, we investigate a more practical yet underexplored task -- multi-scene\nrecognition in single images. To this end, we create a large-scale dataset,\ncalled MultiScene, composed of 100,000 unconstrained high-resolution aerial\nimages. Considering that manually labeling such images is extremely arduous, we\nresort to low-cost annotations from crowdsourcing platforms, e.g.,\nOpenStreetMap (OSM). However, OSM data might suffer from incompleteness and\nincorrectness, which introduce noise into image labels. To address this issue,\nwe visually inspect 14,000 images and correct their scene labels, yielding a\nsubset of cleanly-annotated images, named MultiScene-Clean. With it, we can\ndevelop and evaluate deep networks for multi-scene recognition using clean\ndata. Moreover, we provide crowdsourced annotations of all images for the\npurpose of studying network learning with noisy labels. We conduct experiments\nwith extensive baseline models on both MultiScene-Clean and MultiScene to offer\nbenchmarks for multi-scene recognition in single images and learning from noisy\nlabels for this task, respectively. To facilitate progress, we make our dataset\nand trained models available on\nhttps://github.com/Hua-YS/Multi-Scene-Recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Scale Conv-Attentional Image Transformers. (arXiv:2104.06399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06399","description":"<p>In this paper, we present Co-scale conv-attentional image Transformers\n(CoaT), a Transformer-based image classifier equipped with co-scale and\nconv-attentional mechanisms. First, the co-scale mechanism maintains the\nintegrity of Transformers' encoder branches at individual scales, while\nallowing representations learned at different scales to effectively communicate\nwith each other; we design a series of serial and parallel blocks to realize\nthe co-scale mechanism. Second, we devise a conv-attentional mechanism by\nrealizing a relative position embedding formulation in the factorized attention\nmodule with an efficient convolution-like implementation. CoaT empowers image\nTransformers with enriched multi-scale and contextual modeling capabilities. On\nImageNet, relatively small CoaT models attain superior classification results\ncompared with similar-sized convolutional neural networks and image/vision\nTransformers. The effectiveness of CoaT's backbone is also illustrated on\nobject detection and instance segmentation, demonstrating its applicability to\ndownstream computer vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00916","description":"<p>This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition. (arXiv:2105.01883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01883","description":"<p>We propose RepMLP, a multi-layer-perceptron-style neural network building\nblock for image recognition, which is composed of a series of fully-connected\n(FC) layers. Compared to convolutional layers, FC layers are more efficient,\nbetter at modeling the long-range dependencies and positional patterns, but\nworse at capturing the local structures, hence usually less favored for image\nrecognition. We propose a structural re-parameterization technique that adds\nlocal prior into an FC to make it powerful for image recognition. Specifically,\nwe construct convolutional layers inside a RepMLP during training and merge\nthem into the FC for inference. On CIFAR, a simple pure-MLP model shows\nperformance very close to CNN. By inserting RepMLP in traditional CNN, we\nimprove ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and\n2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight\nthat combining the global representational capacity and positional perception\nof FC with the local prior of convolution can improve the performance of neural\nnetwork with faster speed on both the tasks with translation invariance (e.g.,\nsemantic segmentation) and those with aligned images and positional patterns\n(e.g., face recognition). The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chunlong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Move2Hear: Active Audio-Visual Source Separation. (arXiv:2105.07142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07142","description":"<p>We introduce the active audio-visual source separation problem, where an\nagent must move intelligently in order to better isolate the sounds coming from\nan object of interest in its environment. The agent hears multiple audio\nsources simultaneously (e.g., a person speaking down the hall in a noisy\nhousehold) and it must use its eyes and ears to automatically separate out the\nsounds originating from a target object within a limited time budget. Towards\nthis goal, we introduce a reinforcement learning approach that trains movement\npolicies controlling the agent's camera and microphone placement over time,\nguided by the improvement in predicted audio separation quality. We demonstrate\nour approach in scenarios motivated by both augmented reality (system is\nalready co-located with the target object) and mobile robotics (agent begins\narbitrarily far from the target object). Using state-of-the-art realistic\naudio-visual simulations in 3D environments, we demonstrate our model's ability\nto find minimal movement sequences with maximal payoff for audio source\nseparation. Project: <a href=\"http://vision.cs.utexas.edu/projects/move2hear.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Sagnik Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training. (arXiv:2106.03640v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03640","description":"<p>Much recent research has been dedicated to improving the efficiency of\ntraining and inference for image classification. This effort has commonly\nfocused on explicitly improving theoretical efficiency, often measured as\nImageNet validation accuracy per FLOP. These theoretical savings have, however,\nproven challenging to achieve in practice, particularly on high-performance\ntraining accelerators.\n</p>\n<p>In this work, we focus on improving the practical efficiency of the\nstate-of-the-art EfficientNet models on a new class of accelerator, the\nGraphcore IPU. We do this by extending this family of models in the following\nways: (i) generalising depthwise convolutions to group convolutions; (ii)\nadding proxy-normalized activations to match batch normalization performance\nwith batch-independent statistics; (iii) reducing compute by lowering the\ntraining resolution and inexpensively fine-tuning at higher resolution. We find\nthat these three methods improve the practical efficiency for both training and\ninference. Code available at\nhttps://github.com/graphcore/graphcore-research/tree/main/Making_EfficientNet_More_Efficient .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1\">Dominic Masters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1\">Antoine Labatie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1\">Zach Eaton-Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Transformer Networks for Semantic Image Segmentation. (arXiv:2106.04108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04108","description":"<p>Transformers have shown impressive performance in various natural language\nprocessing and computer vision tasks, due to the capability of modeling\nlong-range dependencies. Recent progress has demonstrated to combine such\ntransformers with CNN-based semantic image segmentation models is very\npromising. However, it is not well studied yet on how well a pure transformer\nbased approach can achieve for image segmentation. In this work, we explore a\nnovel framework for semantic image segmentation, which is encoder-decoder based\nFully Transformer Networks (FTN). Specifically, we first propose a Pyramid\nGroup Transformer (PGT) as the encoder for progressively learning hierarchical\nfeatures, while reducing the computation complexity of the standard visual\ntransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse\nsemantic-level and spatial-level information from multiple levels of the PGT\nencoder for semantic image segmentation. Surprisingly, this simple baseline can\nachieve new state-of-the-art results on multiple challenging semantic\nsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The\nsource code will be released upon the publication of this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos. (arXiv:2106.12522v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.12522","description":"<p>Haustral folds are colon wall protrusions implicated for high polyp miss rate\nduring optical colonoscopy procedures. If segmented accurately, haustral folds\ncan allow for better estimation of missed surface and can also serve as\nvaluable landmarks for registering pre-treatment virtual (CT) and optical\ncolonoscopies, to guide navigation towards the anomalies found in pre-treatment\nscans. We present a novel generative adversarial network, FoldIt, for\nfeature-consistent image translation of optical colonoscopy videos to virtual\ncolonoscopy renderings with haustral fold overlays. A new transitive loss is\nintroduced in order to leverage ground truth information between haustral fold\nannotations and virtual colonoscopy renderings. We demonstrate the\neffectiveness of our model on real challenging optical colonoscopy videos as\nwell as on textured virtual colonoscopy videos with clinician-verified haustral\nfold annotations. All code and scripts to reproduce the experiments of this\npaper will be made available via our Computational Endoscopy Platform at\nhttps://github.com/nadeemlab/CEP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-millisecond Video Synchronization of Multiple Android Smartphones. (arXiv:2107.00987v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00987","description":"<p>This paper addresses the problem of building an affordable easy-to-setup\nsynchronized multi-view camera system, which is in demand for many Computer\nVision and Robotics applications in high-dynamic environments. In our work, we\npropose a solution for this problem -- a publicly-available Android application\nfor synchronized video recording on multiple smartphones with sub-millisecond\naccuracy. We present a generalized mathematical model of timestamping for\nAndroid smartphones and prove its applicability on 47 different physical\ndevices. Also, we estimate the time drift parameter for those smartphones,\nwhich is less than 1.2 msec per minute for most of the considered devices, that\nmakes smartphones' camera system a worthy analog for professional multi-view\nsystems. Finally, we demonstrate Android-app performance on the camera system\nbuilt from Android smartphones quantitatively on setup with lights and\nqualitatively -- on panorama stitching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozo_D/0/1/0/all/0/1\">David Pozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point Defects. (arXiv:2107.02016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02016","description":"<p>The internet is filled with fake face images and videos synthesized by deep\ngenerative models. These realistic DeepFakes pose a challenge to determine the\nauthenticity of multimedia content. As countermeasures, artifact-based\ndetection methods suffer from insufficiently fine-grained features that lead to\nlimited detection performance. DNN-based detection methods are not efficient\nenough, given that a DeepFake can be created easily by mobile apps and\nDNN-based models require high computational resources. For the first time, we\nshow that DeepFake faces have fewer feature points than real ones, especially\nin certain facial regions. Inspired by feature point detector-descriptors to\nextract discriminative features at the pixel level, we propose the Fused Facial\nRegion_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection.\nFFR_FD is only a vector extracted from the face, and it can be constructed from\nany feature point detector-descriptors. We train a random forest classifier\nwith FFR_FD and conduct extensive experiments on six large-scale DeepFake\ndatasets, whose results demonstrate that our method is superior to most state\nof the art DNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaojian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaohui Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04238","description":"<p>Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Multi-Granularity Distillation for GAN Compression. (arXiv:2108.06908v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06908","description":"<p>Generative Adversarial Networks (GANs) have witnessed prevailing success in\nyielding outstanding images, however, they are burdensome to deploy on\nresource-constrained devices due to ponderous computational costs and hulking\nmemory usage. Although recent efforts on compressing GANs have acquired\nremarkable results, they still exist potential model redundancies and can be\nfurther compressed. To solve this issue, we propose a novel online\nmulti-granularity distillation (OMGD) scheme to obtain lightweight GANs, which\ncontributes to generating high-fidelity images with low computational demands.\nWe offer the first attempt to popularize single-stage online distillation for\nGAN-oriented compression, where the progressively promoted teacher generator\nhelps to refine the discriminator-free based student generator. Complementary\nteacher generators and network layers provide comprehensive and\nmulti-granularity concepts to enhance visual fidelity from diverse dimensions.\nExperimental results on four benchmark datasets demonstrate that OMGD successes\nto compress 40x MACs and 82.5X parameters on Pix2Pix and CycleGAN, without loss\nof image quality. It reveals that OMGD provides a feasible solution for the\ndeployment of real-time image translation on resource-constrained devices. Our\ncode and models are made public at: https://github.com/bytedance/OMGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianchao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thermal Image Processing via Physics-Inspired Deep Networks. (arXiv:2108.07973v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07973","description":"<p>We introduce DeepIR, a new thermal image processing framework that combines\nphysically accurate sensor modeling with deep network-based image\nrepresentation. Our key enabling observations are that the images captured by\nthermal sensors can be factored into slowly changing, scene-independent sensor\nnon-uniformities (that can be accurately modeled using physics) and a\nscene-specific radiance flux (that is well-represented using a deep\nnetwork-based regularizer). DeepIR requires neither training data nor periodic\nground-truth calibration with a known black body target--making it well suited\nfor practical computer vision tasks. We demonstrate the power of going DeepIR\nby developing new denoising and super-resolution algorithms that exploit\nmultiple images of the scene captured with camera jitter. Simulated and real\ndata experiments demonstrate that DeepIR can perform high-quality\nnon-uniformity correction with as few as three images, achieving a 10dB PSNR\nimprovement over competing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dave_A/0/1/0/all/0/1\">Akshat Dave</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Urban Driving by Imitating a Reinforcement Learning Coach. (arXiv:2108.08265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08265","description":"<p>End-to-end approaches to autonomous driving commonly rely on expert\ndemonstrations. Although humans are good drivers, they are not good coaches for\nend-to-end algorithms that demand dense on-policy supervision. On the contrary,\nautomated experts that leverage privileged information can efficiently generate\nlarge scale on-policy and off-policy demonstrations. However, existing\nautomated experts for urban driving make heavy use of hand-crafted rules and\nperform suboptimally even on driving simulators, where ground-truth information\nis available. To address these issues, we train a reinforcement learning expert\nthat maps bird's-eye view images to continuous low-level actions. While setting\na new performance upper-bound on CARLA, our expert is also a better coach that\nprovides informative supervision signals for imitation learning agents to learn\nfrom. Supervised by our reinforcement learning coach, a baseline end-to-end\nagent with monocular camera-input achieves expert-level performance. Our\nend-to-end agent achieves a 78% success rate while generalizing to a new town\nand new weather on the NoCrash-dense benchmark and state-of-the-art performance\non the more challenging CARLA LeaderBoard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data. (arXiv:2108.08895v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08895","description":"<p>Coronavirus has caused hundreds of thousands of deaths. Fatalities could\ndecrease if every patient could get suitable treatment by the healthcare\nsystem. Machine learning, especially computer vision methods based on deep\nlearning, can help healthcare professionals diagnose and treat COVID-19\ninfected cases more efficiently. Hence, infected patients can get better\nservice from the healthcare system and decrease the number of deaths caused by\nthe coronavirus. This research proposes a method for segmenting infected lung\nregions in a CT image. For this purpose, a convolutional neural network with an\nattention mechanism is used to detect infected areas with complex patterns.\nAttention blocks improve the segmentation accuracy by focusing on informative\nparts of the image. Furthermore, a generative adversarial network generates\nsynthetic images for data augmentation and expansion of small available\ndatasets. Experimental results show the superiority of the proposed method\ncompared to some existing procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yazdekhasty_P/0/1/0/all/0/1\">Parham Yazdekhasty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zindari_A/0/1/0/all/0/1\">Ali Zindari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1\">Zahra Nabizadeh-ShahreBabak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Quality LFW: A Database for Analyzing Cross-Resolution Image Face Recognition in Unconstrained Environments. (arXiv:2108.10290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10290","description":"<p>Real-world face recognition applications often deal with suboptimal image\nquality or resolution due to different capturing conditions such as various\nsubject-to-camera distances, poor camera settings, or motion blur. This\ncharacteristic has an unignorable effect on performance. Recent\ncross-resolution face recognition approaches used simple, arbitrary, and\nunrealistic down- and up-scaling techniques to measure robustness against\nreal-world edge-cases in image quality. Thus, we propose a new standardized\nbenchmark dataset and evaluation protocol derived from the famous Labeled Faces\nin the Wild (LFW). In contrast to previous derivatives, which focus on pose,\nage, similarity, and adversarial attacks, our Cross-Quality Labeled Faces in\nthe Wild (XQLFW) maximizes the quality difference. It contains only more\nrealistic synthetically degraded images when necessary. Our proposed dataset is\nthen used to further investigate the influence of image quality on several\nstate-of-the-art approaches. With XQLFW, we show that these models perform\ndifferently in cross-quality cases, and hence, the generalizing capability is\nnot accurately predicted by their performance on LFW. Additionally, we report\nbaseline accuracy with recent deep learning models explicitly trained for\ncross-resolution applications and evaluate the susceptibility to image quality.\nTo encourage further research in cross-resolution face recognition and incite\nthe assessment of image quality robustness, we publish the database and code\nfor evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is Color: Image based Spatial Gene Expression Prediction using Neural Stain Learning. (arXiv:2108.10446v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.10446","description":"<p>\"Is it possible to predict expression levels of different genes at a given\nspatial location in the routine histology image of a tumor section by modeling\nits stain absorption characteristics?\" In this work, we propose a \"stain-aware\"\nmachine learning approach for prediction of spatial transcriptomic gene\nexpression profiles using digital pathology image of a routine Hematoxylin &amp;\nEosin (H&amp;E) histology section. Unlike recent deep learning methods which are\nused for gene expression prediction, our proposed approach termed Neural Stain\nLearning (NSL) explicitly models the association of stain absorption\ncharacteristics of the tissue with gene expression patterns in spatial\ntranscriptomics by learning a problem-specific stain deconvolution matrix in an\nend-to-end manner. The proposed method with only 11 trainable weight parameters\noutperforms both classical regression models with cellular composition and\nmorphological features as well as deep learning methods. We have found that the\ngene expression predictions from the proposed approach show higher correlations\nwith true expression values obtained through sequencing for a larger set of\ngenes in comparison to other approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1\">Muhammad Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Branson_K/0/1/0/all/0/1\">Kim Branson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz ul Amir Afsar Minhas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Object Detection by Label Assignment Distillation. (arXiv:2108.10520v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10520","description":"<p>Label assignment in object detection aims to assign targets, foreground or\nbackground, to sampled regions in an image. Unlike labeling for image\nclassification, this problem is not well defined due to the object's bounding\nbox. In this paper, we investigate the problem from a perspective of\ndistillation, hence we call Label Assignment Distillation (LAD). Our initial\nmotivation is very simple, we use a teacher network to generate labels for the\nstudent. This can be achieved in two ways: either using the teacher's\nprediction as the direct targets (soft label), or through the hard labels\ndynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD\nis more effective than soft-label, but they are complementary. (ii) Using LAD,\na smaller teacher can also improve a larger student significantly, while\nsoft-label can't. We then introduce Co-learning LAD, in which two networks\nsimultaneously learn from scratch and the role of teacher and student are\ndynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques\ncan improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \\rm AP$ and\n$47.5\\rm AP$ on the COCO test-dev set. With a strong teacher PAA-SwinB, we\nimprove the PAA-ResNet50 to $43.9\\rm AP$ with only \\1x schedule training, and\nPAA-ResNet101 to $47.9\\rm AP$, significantly surpassing the current methods.\nOur source code and checkpoints will be released at\nhttps://github.com/cybercore-co-ltd/CoLAD_paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy C. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tuan N. Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">Nam L.H. Phan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games. (arXiv:2108.11149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11149","description":"<p>The automatic detection of events in complex sports games like soccer and\nhandball using positional or video data is of large interest in research and\nindustry. One requirement is a fundamental understanding of underlying\nconcepts, i.e., events that occur on the pitch. Previous work often deals only\nwith so-called low-level events based on well-defined rules such as free kicks,\nfree throws, or goals. High-level events, such as passes, are less frequently\napproached due to a lack of consistent definitions. This introduces a level of\nambiguity that necessities careful validation when regarding event annotations.\nYet, this validation step is usually neglected as the majority of studies adopt\nannotations from commercial providers on private datasets of unknown quality\nand focuses on soccer only. To address these issues, we present (1) a universal\ntaxonomy that covers a wide range of low and high-level events for invasion\ngames and is exemplarily refined to soccer and handball, and (2) release two\nmulti-modal datasets comprising video and positional data with gold-standard\nannotations to foster research in fine-grained and ball-centered event\nspotting. Experiments on human performance demonstrate the robustness of the\nproposed taxonomy, and that disagreements and ambiguities in the annotation\nincrease with the complexity of the event. An I3D model for video\nclassification is adopted for event spotting and reveals the potential for\nbenchmarking. Datasets are available at: https://github.com/mm4spa/eigd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biermann_H/0/1/0/all/0/1\">Henrik Biermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theiner_J/0/1/0/all/0/1\">Jonas Theiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassek_M/0/1/0/all/0/1\">Manuel Bassek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raabe_D/0/1/0/all/0/1\">Dominik Raabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memmert_D/0/1/0/all/0/1\">Daniel Memmert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}