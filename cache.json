{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DDXPlus: A new Dataset for Medical Automatic Diagnosis. (arXiv:2205.09148v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09148","description":"<p>There has been rapidly growing interests in Automatic Diagnosis (AD) and\nAutomatic Symptom Detection (ASD) systems in the machine learning research\nliterature, aiming to assist doctors in telemedicine services. These systems\nare designed to interact with patients, collect evidence relevant to their\nconcerns, and make predictions about the underlying diseases. Doctors would\nreview the interaction, including the evidence and the predictions, before\nmaking their final decisions. Despite the recent progress, an important piece\nof doctors' interactions with patients is missing in the design of AD and ASD\nsystems, namely the differential diagnosis. Its absence is largely due to the\nlack of datasets that include such information for models to train on. In this\nwork, we present a large-scale synthetic dataset that includes a differential\ndiagnosis, along with the ground truth pathology, for each patient. In\naddition, this dataset includes more pathologies, as well as types of symtoms\nand antecedents. As a proof-of-concept, we extend several existing AD and ASD\nsystems to incorporate differential diagnosis, and provide empirical evidence\nthat using differentials in training signals is essential for such systems to\nlearn to predict differentials. Dataset available at\nhttps://github.com/bruzwen/ddxplus\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchango_A/0/1/0/all/0/1\">Arsene Fansi Tchango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rishab Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosn_J/0/1/0/all/0/1\">Joumana Ghosn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval. (arXiv:2205.09153v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09153","description":"<p>Neural retrievers based on pre-trained language models (PLMs), such as\ndual-encoders, have achieved promising performance on the task of open-domain\nquestion answering (QA). Their effectiveness can further reach new\nstate-of-the-arts by incorporating cross-architecture knowledge distillation.\nHowever, most of the existing studies just directly apply conventional\ndistillation methods. They fail to consider the particular situation where the\nteacher and student have different structures. In this paper, we propose a\nnovel distillation method that significantly advances cross-architecture\ndistillation for dual-encoders. Our method 1) introduces a self on-the-fly\ndistillation method that can effectively distill late interaction (i.e.,\nColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation\nprocess to further improve the performance with a cross-encoder teacher.\nExtensive experiments are conducted to validate that our proposed solution\noutperforms strong baselines and establish a new state-of-the-art on\nopen-domain QA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuxiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shikun Feng Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Overmodification Emerges from Data-Driven Learning and Pragmatic Reasoning. (arXiv:2205.09172v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09172","description":"<p>Speakers' referential expressions often depart from communicative ideals in\nways that help illuminate the nature of pragmatic language use. Patterns of\novermodification, in which a speaker uses a modifier that is redundant given\ntheir communicative goal, have proven especially informative in this regard. It\nseems likely that these patterns are shaped by the environment a speaker is\nexposed to in complex ways. Unfortunately, systematically manipulating these\nfactors during human language acquisition is impossible. In this paper, we\npropose to address this limitation by adopting neural networks (NN) as learning\nagents. By systematically varying the environments in which these agents are\ntrained, while keeping the NN architecture constant, we show that\novermodification is more likely with environmental features that are infrequent\nor salient. We show that these findings emerge naturally in the context of a\nprobabilistic model of pragmatic communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Kunal Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carbon Figures of Merit Knowledge Creation with a Hybrid Solution and Carbon Tables API. (arXiv:2205.09175v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09175","description":"<p>Nowadays there are algorithms, methods, and platforms that are being created\nto accelerate the discovery of materials that are able to absorb or adsorb\n$CO_2$ molecules that are in the atmosphere or during the combustion in power\nplants, for instance. In this work an asynchronous REST API is described to\naccelerate the creation of Carbon figures of merit knowledge, called Carbon\nTables, because the knowledge is created from tables in scientific PDF\ndocuments and stored in knowledge graphs. The figures of merit knowledge\ncreation solution uses a hybrid approach, in which heuristics and machine\nlearning are part of. As a result, one can search the knowledge with mature and\nsophisticated cognitive tools, and create more with regards to Carbon figures\nof merit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayser_M/0/1/0/all/0/1\">Maira Gatti de Bayser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreQuEL: Quality Estimation of Machine Translation Outputs in Advance. (arXiv:2205.09178v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09178","description":"<p>We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL\nsystem predicts how well a given sentence will be translated, without recourse\nto the actual translation, thus eschewing unnecessary resource allocation when\ntranslation quality is bound to be low. PreQuEL can be defined relative to a\ngiven MT system (e.g., some industry service) or generally relative to the\nstate-of-the-art. From a theoretical perspective, PreQuEL places the focus on\nthe source text, tracing properties, possibly linguistic features, that make a\nsentence harder to machine translate.\n</p>\n<p>We develop a baseline model for the task and analyze its performance. We also\ndevelop a data augmentation method (from parallel corpora), that improves\nresults substantially. We show that this augmentation method can improve the\nperformance of the Quality-Estimation task as well. We investigate the\nproperties of the input text that our model is sensitive to, by testing it on\nchallenge sets and different languages. We conclude that it is aware of\nsyntactic and semantic distinctions, and correlates and even over-emphasizes\nthe importance of standard NLP features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehiya_S/0/1/0/all/0/1\">Shachar Don-Yehiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeRaC: Learning Rate Curriculum. (arXiv:2205.09180v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09180","description":"<p>Most curriculum learning methods require an approach to sort the data samples\nby difficulty, which is often cumbersome to perform. In this work, we propose a\nnovel curriculum learning approach termed Learning Rate Curriculum (LeRaC),\nwhich leverages the use of a different learning rate for each layer of a neural\nnetwork to create a data-free curriculum during the initial training epochs.\nMore specifically, LeRaC assigns higher learning rates to neural layers closer\nto the input, gradually decreasing the learning rates as the layers are placed\nfarther away from the input. The learning rates increase at various paces\nduring the first training iterations, until they all reach the same value. From\nthis point on, the neural model is trained as usual. This creates a model-level\ncurriculum learning strategy that does not require sorting the examples by\ndifficulty and is compatible with any neural network, generating higher\nperformance levels regardless of the architecture. We conduct comprehensive\nexperiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100,\nTiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D)\ndomains, considering various convolutional (ResNet-18, Wide-ResNet-50,\nDenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr)\narchitectures, comparing our approach with the conventional training regime.\nMoreover, we also compare with Curriculum by Smoothing (CBS), a\nstate-of-the-art data-free curriculum learning approach. Unlike CBS, our\nperformance improvements over the standard training regime are consistent\nacross all datasets and models. Furthermore, we significantly surpass CBS in\nterms of training time (there is no additional cost over the standard training\nregime for LeRaC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm sorry to hear that\": finding bias in language models with a holistic descriptor dataset. (arXiv:2205.09209v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09209","description":"<p>As language models grow in popularity, their biases across all possible\nmarkers of demographic identity should be measured and addressed in order to\navoid perpetuating existing societal harms. Many datasets for measuring bias\ncurrently exist, but they are restricted in their coverage of demographic axes,\nand are commonly used with preset bias tests that presuppose which types of\nbiases the models exhibit. In this work, we present a new, more inclusive\ndataset, HOLISTICBIAS, which consists of nearly 600 descriptor terms across 13\ndifferent demographic axes. HOLISTICBIAS was assembled in conversation with\nexperts and community members with lived experience through a participatory\nprocess. We use these descriptors combinatorially in a set of bias measurement\ntemplates to produce over 450,000 unique sentence prompts, and we use these\nprompts to explore, identify, and reduce novel forms of bias in several\ngenerative models. We demonstrate that our dataset is highly efficacious for\nmeasuring previously unmeasurable biases in token likelihoods and generations\nfrom language models, as well as in an offensiveness classifier. We will invite\nadditions and amendments to the dataset, and we hope it will help serve as a\nbasis for easy-to-use and more standardized methods for evaluating bias in NLP\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambadur_M/0/1/0/all/0/1\">Melissa Hall Melanie Kambadur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presani_E/0/1/0/all/0/1\">Eleonora Presani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a> (Meta AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner. (arXiv:2205.09224v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09224","description":"<p>Large language models have achieved high performance on various question\nanswering (QA) benchmarks, but the explainability of their output remains\nelusive. Structured explanations, called entailment trees, were recently\nsuggested as a way to explain and inspect a QA system's answer. In order to\nbetter generate such entailment trees, we propose an architecture called\nIterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a\ngiven hypothesis by systematically generating a step-by-step explanation from\ntextual premises. The IRGR model iteratively searches for suitable premises,\nconstructing a single entailment step at a time. Contrary to previous\napproaches, our method combines generation steps and retrieval of premises,\nallowing the model to leverage intermediate conclusions, and mitigating the\ninput size limit of baseline encoder-decoder models. We conduct experiments\nusing the EntailmentBank dataset, where we outperform existing benchmarks on\npremise retrieval and entailment tree generation, with around 300% gain in\noverall correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_D/0/1/0/all/0/1\">Danilo Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Rui Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Multi-hop Question Answering as Single Sequence Prediction. (arXiv:2205.09226v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09226","description":"<p>Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question\nanswering (QA) model that leverages passage retrieval with a pre-trained\ntransformer and pushed the state of the art on single-hop QA. However, the\ncomplexity of multi-hop QA hinders the effectiveness of the generative QA\napproach. In this work, we propose a simple generative approach (PathFid) that\nextends the task beyond just answer generation by explicitly modeling the\nreasoning process to resolve the answer for multi-hop questions. By linearizing\nthe hierarchical reasoning path of supporting passages, their key sentences,\nand finally the factoid answer, we cast the problem as a single sequence\nprediction task. To facilitate complex reasoning with multiple clues, we\nfurther extend the unified flat representation of multiple input documents by\nencoding cross-passage interactions. Our extensive experiments demonstrate that\nPathFid leads to strong performance gains on two multi-hop QA datasets:\nHotpotQA and IIRC. Besides the performance gains, PathFid is more\ninterpretable, which in turn yields answers that are more faithfully grounded\nto the supporting passages and facts compared to the baseline Fid model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskar_N/0/1/0/all/0/1\">Nitish Shirish Keskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptDA: Label-guided Data Augmentation for Prompt-based Few Shot Learners. (arXiv:2205.09229v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09229","description":"<p>Recent advances on large pre-trained language models (PLMs) lead impressive\ngains on natural language understanding (NLU) tasks with task-specific\nfine-tuning. However, direct fine-tuning PLMs heavily relies on large amount of\nlabeled instances, which are expensive and time-consuming to obtain.\nPrompt-based tuning on PLMs has proven valuable for few shot tasks. Existing\nworks studying prompt-based tuning for few-shot NLU mainly focus on deriving\nproper label words with a verbalizer or generating prompt templates for\neliciting semantics from PLMs. In addition, conventional data augmentation\nmethods have also been verified useful for few-shot tasks. However, there\ncurrently are few data augmentation methods designed for the prompt-based\ntuning paradigm. Therefore, we study a new problem of data augmentation for\nprompt-based few shot learners. Since label semantics are helpful in\nprompt-based tuning, we propose a novel label-guided data augmentation method\nPromptDA which exploits the enriched label semantic information for data\naugmentation. Experimental results on several few shot text classification\ntasks show that our proposed framework achieves superior performance by\neffectively leveraging label semantics and data augmentation in language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Canyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets. (arXiv:2205.09249v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09249","description":"<p>Natural language guided embodied task completion is a challenging problem\nsince it requires understanding natural language instructions, aligning them\nwith egocentric visual observations, and choosing appropriate actions to\nexecute in the environment to produce desired changes. We experiment with\naugmenting a transformer model for this task with modules that effectively\nutilize a wider field of view and learn to choose whether the next step\nrequires a navigation or manipulation action. We observed that the proposed\nmodules resulted in improved, and in fact state-of-the-art performance on an\nunseen validation set of a popular benchmark dataset, ALFRED. However, our best\nmodel selected using the unseen validation set underperforms on the unseen test\nsplit of ALFRED, indicating that performance on the unseen validation set may\nnot in itself be a sufficient indicator of whether model improvements\ngeneralize to unseen test sets. We highlight this result as we believe it may\nbe a wider phenomenon in machine learning tasks but primarily noticeable only\nin benchmarks that limit evaluations on test splits, and highlights the need to\nmodify benchmark design to better account for variance in model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twist Decoding: Diverse Generators Guide Each Other. (arXiv:2205.09273v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09273","description":"<p>Natural language generation technology has recently seen remarkable progress\nwith large-scale training, and many natural language applications are now built\nupon a wide range of generation models. Combining diverse models may lead to\nfurther progress, but conventional ensembling (e.g., shallow fusion) requires\nthat they share vocabulary/tokenization schemes. We introduce Twist decoding, a\nsimple and general inference algorithm that generates text while benefiting\nfrom diverse models. Our method does not assume the vocabulary, tokenization or\neven generation order is shared. Our extensive evaluations on machine\ntranslation and scientific paper summarization demonstrate that Twist decoding\nsubstantially outperforms each model decoded in isolation over various\nscenarios, including cases where domain-specific and general-purpose models are\nboth available. Twist decoding also consistently outperforms the popular\nreranking heuristic where output candidates from one model is rescored by\nanother. We hope that our work will encourage researchers and practitioners to\nexamine generation models collectively, not just independently, and to seek out\nmodels with complementary strengths to the currently available models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Exemplification in Long-form Question Answering via Retrieval. (arXiv:2205.09278v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09278","description":"<p>Exemplification is a process by which writers explain or clarify a concept by\nproviding an example. While common in all forms of writing, exemplification is\nparticularly useful in the task of long-form question answering (LFQA), where a\ncomplicated answer can be made more understandable through simple examples. In\nthis paper, we provide the first computational study of exemplification in QA,\nperforming a fine-grained annotation of different types of examples (e.g.,\nhypotheticals, anecdotes) in three corpora. We show that not only do\nstate-of-the-art LFQA models struggle to generate relevant examples, but also\nthat standard evaluation metrics such as ROUGE are insufficient to judge\nexemplification quality. We propose to treat exemplification as a\n\\emph{retrieval} problem in which a partially-written answer is used to query a\nlarge set of human-written examples extracted from a corpus. Our approach\nallows a reliable ranking-type automatic metrics that correlates well with\nhuman evaluation. A human evaluation shows that our model's retrieved examples\nare more relevant than examples generated from a state-of-the-art LFQA model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1\">Laure Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Prompt-based Models Clueless?. (arXiv:2205.09295v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09295","description":"<p>Finetuning large pre-trained language models with a task-specific head has\nadvanced the state-of-the-art on many natural language understanding\nbenchmarks. However, models with a task-specific head require a lot of training\ndata, making them susceptible to learning and exploiting dataset-specific\nsuperficial cues that do not generalize to other datasets. Prompting has\nreduced the data requirement by reusing the language model head and formatting\nthe task input to match the pre-training objective. Therefore, it is expected\nthat few-shot prompt-based models do not exploit superficial cues. This paper\npresents an empirical examination of whether few-shot prompt-based models also\nexploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,\nHANS, and COPA has revealed that prompt-based models also exploit superficial\ncues. While the models perform well on instances with superficial cues, they\noften underperform or only marginally outperform random accuracy on instances\nwithout superficial cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kavumba_P/0/1/0/all/0/1\">Pride Kavumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryo Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oda_Y/0/1/0/all/0/1\">Yasuke Oda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation. (arXiv:2205.09314v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09314","description":"<p>Target-guided response generation enables dialogue systems to smoothly\ntransition a conversation from a dialogue context toward a target sentence.\nSuch control is useful for designing dialogue systems that direct a\nconversation toward specific goals, such as creating non-obtrusive\nrecommendations or introducing new topics in the conversation. In this paper,\nwe introduce a new technique for target-guided response generation, which first\nfinds a bridging path of commonsense knowledge concepts between the source and\nthe target, and then uses the identified bridging path to generate transition\nresponses. Additionally, we propose techniques to re-purpose existing dialogue\ndatasets for target-guided generation. Experiments reveal that the proposed\ntechniques outperform various baselines on this task. Finally, we observe that\nthe existing automated metrics for this task correlate poorly with human\njudgement ratings. We propose a novel evaluation metric that we demonstrate is\nmore reliable for target-guided response evaluation. Our work generally enables\ndialogue system designers to exercise more control over the conversations that\ntheir systems produce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer. (arXiv:2205.09324v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09324","description":"<p>Text style transfer is an important task in controllable language generation.\nSupervised approaches have pushed performance improvement on style-oriented\nrewriting such as formality conversion. However, challenges remain due to the\nscarcity of large-scale parallel data in many domains. While unsupervised\napproaches do not rely on annotated sentence pairs for each style, they are\noften plagued with instability issues such as mode collapse or quality\ndegradation. To take advantage of both supervised and unsupervised paradigms\nand tackle the challenges, in this work, we propose a semi-supervised framework\nfor text style transfer. First, the learning process is bootstrapped with\nsupervision guided by automatically constructed pseudo-parallel pairs using\nlexical and semantic-based methods. Then the model learns from unlabeled data\nvia reinforcement rewards. Specifically, we propose to improve the\nsequence-to-sequence policy gradient via stepwise reward optimization,\nproviding fine-grained learning signals and stabilizing the reinforced learning\nprocess. Experimental results show that the proposed approach achieves\nstate-of-the-art performance on multiple datasets, and produces effective\ngeneration with as minimal as 10\\% of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Talk! Striking Up Conversations via Conversational Visual Question Generation. (arXiv:2205.09327v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09327","description":"<p>An engaging and provocative question can open up a great conversation. In\nthis work, we explore a novel scenario: a conversation agent views a set of the\nuser's photos (for example, from social media platforms) and asks an engaging\nquestion to initiate a conversation with the user. The existing\nvision-to-question models mostly generate tedious and obvious questions, which\nmight not be ideals conversation starters. This paper introduces a two-phase\nframework that first generates a visual story for the photo set and then uses\nthe story to produce an interesting question. The human evaluation shows that\nour framework generates more response-provoking questions for starting\nconversations than other vision-to-question baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Shih-Han Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsai-Lun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yun-Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chi-Yang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Y/0/1/0/all/0/1\">Yu-Shian Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Inflection as a Data Augmentation Method for Parsing. (arXiv:2205.09350v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09350","description":"<p>We propose a morphology-based method for low-resource (LR) dependency\nparsing. We train a morphological inflector for target LR languages, and apply\nit to related rich-resource (RR) treebanks to create cross-lingual\n(x-inflected) treebanks that resemble the target LR language. We use such\ninflected treebanks to train parsers in zero- (training on x-inflected\ntreebanks) and few-shot (training on x-inflected and target language treebanks)\nsetups. The results show that the method sometimes improves the baselines, but\nnot consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Subtitle Segmentation for End-to-end Generation Systems. (arXiv:2205.09360v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09360","description":"<p>Subtitles appear on screen as short pieces of text, segmented based on formal\nconstraints (length) and syntactic/semantic criteria. Subtitle segmentation can\nbe evaluated with sequence segmentation metrics against a human reference.\nHowever, standard segmentation metrics cannot be applied when systems generate\noutputs different than the reference, e.g. with end-to-end subtitling systems.\nIn this paper, we study ways to conduct reference-based evaluations of\nsegmentation accuracy irrespective of the textual content. We first conduct a\nsystematic analysis of existing metrics for evaluating subtitle segmentation.\nWe then introduce $Sigma$, a new Subtitle Segmentation Score derived from an\napproximate upper-bound of BLEU on segmentation boundaries, which allows us to\ndisentangle the effect of good segmentation from text quality. To compare\n$Sigma$ with existing metrics, we further propose a boundary projection method\nfrom imperfect hypotheses to the true reference. Results show that all metrics\nare able to reward high quality output but for similar outputs system ranking\ndepends on each metric's sensitivity to error type. Our thorough analyses\nsuggest $Sigma$ is a promising segmentation candidate but its reliability over\nother segmentation metrics remains to be validated through correlations with\nhuman judgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buet_F/0/1/0/all/0/1\">Fran&#xe7;ois Buet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cettolo_M/0/1/0/all/0/1\">Mauro Cettolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers as Neural Augmentors: Class Conditional Sentence Generation via Variational Bayes. (arXiv:2205.09391v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09391","description":"<p>Data augmentation methods for Natural Language Processing tasks are explored\nin recent years, however they are limited and it is hard to capture the\ndiversity on sentence level. Besides, it is not always possible to perform data\naugmentation on supervised tasks. To address those problems, we propose a\nneural data augmentation method, which is a combination of Conditional\nVariational Autoencoder and encoder-decoder Transformer model. While encoding\nand decoding the input sentence, our model captures the syntactic and semantic\nrepresentation of the input language with its class condition. Following the\ndevelopments in the past years on pre-trained language models, we train and\nevaluate our models on several benchmarks to strengthen the downstream tasks.\nWe compare our method with 3 different augmentation techniques. The presented\nresults show that, our model increases the performance of current models\ncompared to other data augmentation techniques with a small amount of\ncomputation power.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bilici_M/0/1/0/all/0/1\">M. &#x15e;afak Bilici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1\">Mehmet Fatih Amasyali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Step Question Retrieval for Open-Domain QA. (arXiv:2205.09393v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09393","description":"<p>The retriever-reader pipeline has shown promising performance in open-domain\nQA but suffers from a very slow inference speed. Recently proposed question\nretrieval models tackle this problem by indexing question-answer pairs and\nsearching for similar questions. These models have shown a significant increase\nin inference speed, but at the cost of lower QA performance compared to the\nretriever-reader models. This paper proposes a two-step question retrieval\nmodel, SQuID (Sequential Question-Indexed Dense retrieval) and distant\nsupervision for training. SQuID uses two bi-encoders for question retrieval.\nThe first-step retriever selects top-k similar questions, and the second-step\nretriever finds the most similar question from the top-k questions. We evaluate\nthe performance and the computational efficiency of SQuID. The results show\nthat SQuID significantly increases the performance of existing question\nretrieval models with a negligible loss on inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1\">Yeon Seonwoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Juhee Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly-Supervised Iterative Graph-Based Approach to Retrieve COVID-19 Misinformation Topics. (arXiv:2205.09416v1 [cs.HC])","link":"http://arxiv.org/abs/2205.09416","description":"<p>The COVID-19 pandemic has been accompanied by an `infodemic' -- of accurate\nand inaccurate health information across social media. Detecting misinformation\namidst dynamically changing information landscape is challenging; identifying\nrelevant keywords and posts is arduous due to the large amount of human effort\nrequired to inspect the content and sources of posts. We aim to reduce the\nresource cost of this process by introducing a weakly-supervised iterative\ngraph-based approach to detect keywords, topics, and themes related to\nmisinformation, with a focus on COVID-19. Our approach can successfully detect\nspecific topics from general misinformation-related seed words in a few seed\ntexts. Our approach utilizes the BERT-based Word Graph Search (BWGS) algorithm\nthat builds on context-based neural network embeddings for retrieving\nmisinformation-related posts. We utilize Latent Dirichlet Allocation (LDA)\ntopic modeling for obtaining misinformation-related themes from the texts\nreturned by BWGS. Furthermore, we propose the BERT-based Multi-directional Word\nGraph Search (BMDWGS) algorithm that utilizes greater starting context\ninformation for misinformation extraction. In addition to a qualitative\nanalysis of our approach, our quantitative analyses show that BWGS and BMDWGS\nare effective in extracting misinformation-related content compared to common\nbaselines in low data resource settings. Extracting such content is useful for\nuncovering prevalent misconceptions and concerns and for facilitating precision\npublic health messaging campaigns to improve health behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Insights on Neural Representations for End-to-End Speech Recognition. (arXiv:2205.09456v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09456","description":"<p>End-to-end automatic speech recognition (ASR) models aim to learn a\ngeneralised speech representation. However, there are limited tools available\nto understand the internal functions and the effect of hierarchical\ndependencies within the model architecture. It is crucial to understand the\ncorrelations between the layer-wise representations, to derive insights on the\nrelationship between neural representations and performance.\n</p>\n<p>Previous investigations of network similarities using correlation analysis\ntechniques have not been explored for End-to-End ASR models. This paper\nanalyses and explores the internal dynamics between layers during training with\nCNN, LSTM and Transformer based approaches using Canonical correlation analysis\n(CCA) and centered kernel alignment (CKA) for the experiments. It was found\nthat neural representations within CNN layers exhibit hierarchical correlation\ndependencies as layer depth increases but this is mostly limited to cases where\nneural representation correlates more closely. This behaviour is not observed\nin LSTM architecture, however there is a bottom-up pattern observed across the\ntraining process, while Transformer encoder layers exhibit irregular\ncoefficiency correlation as neural depth increases. Altogether, these results\nprovide new insights into the role that neural architectures have upon speech\nrecognition performance. More specifically, these techniques can be used as\nindicators to build better performing speech recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollerenshaw_A/0/1/0/all/0/1\">Anna Ollerenshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Md Asif Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why only Micro-F1? Class Weighting of Measures for Relation Classification. (arXiv:2205.09460v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09460","description":"<p>Relation classification models are conventionally evaluated using only a\nsingle measure, e.g., micro-F1, macro-F1 or AUC. In this work, we analyze\nweighting schemes, such as micro and macro, for imbalanced datasets. We\nintroduce a framework for weighting schemes, where existing schemes are\nextremes, and two new intermediate schemes. We show that reporting results of\ndifferent weighting schemes better highlights strengths and weaknesses of a\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harbecke_D/0/1/0/all/0/1\">David Harbecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alt_C/0/1/0/all/0/1\">Christoph Alt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psychiatric Scale Guided Risky Post Screening for Early Detection of Depression. (arXiv:2205.09497v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09497","description":"<p>Depression is a prominent health challenge to the world, and early risk\ndetection (ERD) of depression from online posts can be a promising technique\nfor combating the threat. Early depression detection faces the challenge of\nefficiently tackling streaming data, balancing the tradeoff between timeliness,\naccuracy and explainability. To tackle these challenges, we propose a\npsychiatric scale guided risky post screening method that can capture risky\nposts related to the dimensions defined in clinical depression scales, and\nproviding interpretable diagnostic basis. A Hierarchical Attentional Network\nequipped with BERT (HAN-BERT) is proposed to further advance explainable\npredictions. For ERD, we propose an online algorithm based on an evolving queue\nof risky posts that can significantly reduce the number of model inferences to\nboost efficiency. Experiments show that our method outperforms the competitive\nfeature-based and neural models under conventional depression detection\nsettings, and achieves simultaneous improvement in both efficacy and efficiency\nfor ERD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDS-200: A Swiss German Speech to Standard German Text Corpus. (arXiv:2205.09501v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09501","description":"<p>We present SDS-200, a corpus of Swiss German dialectal speech with Standard\nGerman text translations, annotated with dialect, age, and gender information\nof the speakers. The dataset allows for training speech translation, dialect\nrecognition, and speech synthesis systems, among others. The data was collected\nusing a web recording tool that is open to the public. Each participant was\ngiven a text in Standard German and asked to translate it to their Swiss German\ndialect before recording it. To increase the corpus quality, recordings were\nvalidated by other participants. The data consists of 200 hours of speech by\naround 4000 different speakers and covers a large part of the Swiss-German\ndialect landscape. We release SDS-200 alongside a baseline speech translation\nmodel, which achieves a word error rate (WER) of 30.3 and a BLEU score of 53.1\non the SDS-200 test set. Furthermore, we use SDS-200 to fine-tune a pre-trained\nXLS-R model, achieving 21.6 WER and 64.0 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1\">Michel Pl&#xfc;ss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurlimann_M/0/1/0/all/0/1\">Manuela H&#xfc;rlimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuny_M/0/1/0/all/0/1\">Marc Cuny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockli_A/0/1/0/all/0/1\">Alla St&#xf6;ckli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapotis_N/0/1/0/all/0/1\">Nikolaos Kapotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_J/0/1/0/all/0/1\">Julia Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulasik_M/0/1/0/all/0/1\">Malgorzata Anna Ulasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Amit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_J/0/1/0/all/0/1\">Jan Deriu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cieliebak_M/0/1/0/all/0/1\">Mark Cieliebak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1\">Manfred Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of COVID-19 Pandemic on LGBTQ Online Communitie. (arXiv:2205.09511v1 [cs.SI])","link":"http://arxiv.org/abs/2205.09511","description":"<p>The COVID-19 pandemic has disproportionately impacted the lives of\nminorities, such as members of the LGBTQ community (lesbian, gay, bisexual,\ntransgender, and queer) due to pre-existing social disadvantages and health\ndisparities. Although extensive research has been carried out on the impact of\nthe COVID-19 pandemic on different aspects of the general population's lives,\nfew studies are focused on the LGBTQ population. In this paper, we identify a\ngroup of Twitter users who self-disclose to belong to the LGBTQ community. We\ndevelop and evaluate two sets of machine learning classifiers using a\npre-pandemic and a during pandemic dataset to identify Twitter posts exhibiting\nminority stress, which is a unique pressure faced by the members of the LGBTQ\npopulation due to their sexual and gender identities. For this task, we collect\na set of 20,593,823 posts by 7,241 self-disclosed LGBTQ users and annotate a\nrandomly selected subset of 2800 posts. We demonstrate that our best\npre-pandemic and during pandemic models show strong and stable performance for\ndetecting posts that contain minority stress. We investigate the linguistic\ndifferences in minority stress posts across pre- and during-pandemic periods.\nWe find that anger words are strongly associated with minority stress during\nthe COVID-19 pandemic. We explore the impact of the pandemic on the emotional\nstates of the LGBTQ population by conducting controlled comparisons with the\ngeneral population. We adopt propensity score-based matching to perform a\ncausal analysis. The results show that the LBGTQ population have a greater\nincrease in the usage of cognitive words and worsened observable attribute in\nthe usage of positive emotion words than the group of the general population\nwith similar pre-pandemic behavioral attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_B/0/1/0/all/0/1\">Barbara Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aledavood_T/0/1/0/all/0/1\">Talayeh Aledavood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction. (arXiv:2205.09536v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09536","description":"<p>Few-Shot Relation Extraction aims at predicting the relation for a pair of\nentities in a sentence by training with a few labelled examples in each\nrelation. Some recent works have introduced relation information (i.e.,\nrelation labels or descriptions) to assist model learning based on Prototype\nNetwork. However, most of them constrain the prototypes of each relation class\nimplicitly with relation information, generally through designing complex\nnetwork structures, like generating hybrid features, combining with contrastive\nlearning or attention networks. We argue that relation information can be\nintroduced more explicitly and effectively into the model. Thus, this paper\nproposes a direct addition approach to introduce relation information.\nSpecifically, for each relation class, the relation representation is first\ngenerated by concatenating two views of relations (i.e., [CLS] token embedding\nand the mean value of embeddings of all tokens) and then directly added to the\noriginal prototype for both train and prediction. Experimental results on the\nbenchmark dataset FewRel 1.0 show significant improvements and achieve\ncomparable results to the state-of-the-art, which demonstrates the\neffectiveness of our proposed approach. Besides, further analyses verify that\nthe direct addition is a much more effective way to integrate the relation\nrepresentations and the original prototypes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Spoken Language Identification using a Time-Delay Neural Network. (arXiv:2205.09564v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09564","description":"<p>Closed-set spoken language identification is the task of recognizing the\nlanguage being spoken in a recorded audio clip from a set of known languages.\nIn this study, a language identification system was built and trained to\ndistinguish between Arabic, Spanish, French, and Turkish based on nothing more\nthan recorded speech. A pre-existing multilingual dataset was used to train a\nseries of acoustic models based on the Tedlium TDNN model to perform automatic\nspeech recognition. The system was provided with a custom multilingual language\nmodel and a specialized pronunciation lexicon with language names prepended to\nphones. The trained model was used to generate phone alignments to test data\nfrom all four languages, and languages were predicted based on a voting scheme\nchoosing the most common language prepend in an utterance. Accuracy was\nmeasured by comparing predicted languages to known languages, and was\ndetermined to be very high in identifying Spanish and Arabic, and somewhat\nlower in identifying Turkish and French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kepecs_B/0/1/0/all/0/1\">Benjamin Kepecs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beigi_H/0/1/0/all/0/1\">Homayoon Beigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments. (arXiv:2205.09570v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09570","description":"<p>This work describes an implementation of the \"extended alignment\" (or\n\"multitiers\") approach for cognate reflex prediction, submitted to \"Prediction\nof Cognate Reflexes\" shared task. Similarly to List2022d, the technique\ninvolves an automatic extension of sequence alignments with multilayered\nvectors that encode informational tiers on both site-specific traits, such as\nsound classes and distinctive features, as well as contextual and\nsuprasegmental ones, conveyed by cross-site referrals and replication. The\nmethod allows to generalize the problem of cognate reflex prediction as a\nclassification problem, with models trained using a parallel corpus of cognate\nsets. A model using random forests is trained and evaluated on the shared task\nfor reflex prediction, and the experimental results are presented and discussed\nalong with some differences to other implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tresoldi_T/0/1/0/all/0/1\">Tiago Tresoldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A machine transliteration tool between Uzbek alphabets. (arXiv:2205.09578v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09578","description":"<p>Machine transliteration, as defined in this paper, is a process of\nautomatically transforming written script of words from a source alphabet into\nwords of another target alphabet within the same language, while preserving\ntheir meaning, as well as pronunciation. The main goal of this paper is to\npresent a machine transliteration tool between three common scripts used in\nlow-resource Uzbek language: the old Cyrillic, currently official Latin, and\nnewly announced New Latin alphabets. The tool has been created using a\ncombination of rule-based and fine-tuning approaches. The created tool is\navailable as an open-source Python package, as well as a web-based application\nincluding a public API. To our knowledge, this is the first machine\ntransliteration tool that supports the newly announced Latin alphabet of the\nUzbek language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaev_U/0/1/0/all/0/1\">Ulugbek Salaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuriyozov_E/0/1/0/all/0/1\">Elmurod Kuriyozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing. (arXiv:2205.09607v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09607","description":"<p>Semantic parsing is the task of producing structured meaning representations\nfor natural language sentences. Recent research has pointed out that the\ncommonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to\ngeneralize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation directly\nas a graph and not as a sequence. To this end we propose LAGr (Label Aligned\nGraphs), a general framework to produce semantic parses by independently\npredicting node and edge labels for a complete multi-layer input-aligned graph.\nThe strongly-supervised LAGr algorithm requires aligned graphs as inputs,\nwhereas weakly-supervised LAGr infers alignments for originally unaligned\ntarget graphs using approximate maximum-a-posteriori inference. Experiments\ndemonstrate that LAGr achieves significant improvements in systematic\ngeneralization upon the baseline seq2seq parsers in both strongly- and\nweakly-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jambor_D/0/1/0/all/0/1\">Dora Jambor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acceptability Judgements via Examining the Topology of Attention Maps. (arXiv:2205.09630v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09630","description":"<p>The role of the attention mechanism in encoding linguistic knowledge has\nreceived special interest in NLP. However, the ability of the attention heads\nto judge the grammatical acceptability of a sentence has been underexplored.\nThis paper approaches the paradigm of acceptability judgments with topological\ndata analysis (TDA), showing that the geometric properties of the attention\ngraph can be efficiently exploited for two standard practices in linguistics:\nbinary judgments and linguistic minimal pairs. Topological features enhance the\nBERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three\nlanguages (English, Italian, and Swedish). By revealing the topological\ndiscrepancy between attention maps of minimal pairs, we achieve the human-level\nperformance on the BLiMP benchmark, outperforming nine statistical and\nTransformer LM baselines. At the same time, TDA provides the foundation for\nanalyzing the linguistic functions of attention heads and interpreting the\ncorrespondence between the graph features and grammatical phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurina_I/0/1/0/all/0/1\">Irina Proskurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phylogeny-Inspired Adaptation of Multilingual Models to New Languages. (arXiv:2205.09634v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09634","description":"<p>Large pretrained multilingual models, trained on dozens of languages, have\ndelivered promising results due to cross-lingual learning capabilities on\nvariety of language tasks. Further adapting these models to specific languages,\nespecially ones unseen during pre-training, is an important goal towards\nexpanding the coverage of language technologies. In this study, we show how we\ncan use language phylogenetic information to improve cross-lingual transfer\nleveraging closely related languages in a structured, linguistically-informed\nmanner. We perform adapter-based training on languages from diverse language\nfamilies (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic\nand semantic tasks, obtaining more than 20% relative performance improvements\nover strong commonly used baselines, especially on languages unseen during\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNaC: Coherence Error Detection for Narrative Summarization. (arXiv:2205.09641v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09641","description":"<p>Progress in summarizing long texts is inhibited by the lack of appropriate\nevaluation frameworks. When a long summary must be produced to appropriately\ncover the facets of that text, that summary needs to present a coherent\nnarrative to be understandable by a reader, but current automatic and human\nevaluation methods fail to identify gaps in coherence. In this work, we\nintroduce SNaC, a narrative coherence evaluation framework rooted in\nfine-grained annotations for long summaries. We develop a taxonomy of coherence\nerrors in generated narrative summaries and collect span-level annotations for\n6.6k sentences across 150 book and movie screenplay summaries. Our work\nprovides the first characterization of coherence errors generated by\nstate-of-the-art summarization models and a protocol for eliciting coherence\njudgments from crowd annotators. Furthermore, we show that the collected\nannotations allow us to train a strong classifier for automatically localizing\ncoherence errors in generated summaries as well as benchmarking past work in\ncoherence modeling. Finally, our SNaC framework can support future work in long\ndocument summarization and coherence evaluation, including improved\nsummarization modeling and post-hoc summary correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models. (arXiv:2205.09646v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09646","description":"<p>The energy requirements of current natural language processing models\ncontinue to grow at a rapid, unsustainable pace. Recent works highlighting this\nproblem conclude there is an urgent need for methods that reduce the energy\nneeds of NLP and machine learning more broadly. In this article, we investigate\ntechniques that can be used to reduce the energy consumption of common NLP\napplications. In particular, we focus on techniques to measure energy usage and\ndifferent hardware and datacenter-oriented settings that can be tuned to reduce\nenergy consumption for training and inference for language models. We\ncharacterize the impact of these settings on metrics such as computational\nperformance and energy consumption through experiments conducted on a high\nperformance computing system as well as popular cloud computing platforms.\nThese techniques can lead to significant reduction in energy consumption when\ntraining language models or their use for inference. For example,\npower-capping, which limits the maximum power a GPU can consume, can enable a\n15\\% decrease in energy usage with marginal increase in overall computation\ntime when training a transformer-based language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">Joseph McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_N/0/1/0/all/0/1\">Nathan Frey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_D/0/1/0/all/0/1\">Devesh Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1\">Vijay Gadepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1\">Siddharth Samsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition, Multi-Task Learning, Nested Entities, BERT, Arabic NER Corpus. (arXiv:2205.09651v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09651","description":"<p>This paper presents Wojood, a corpus for Arabic nested Named Entity\nRecognition (NER). Nested entities occur when one entity mention is embedded\ninside another entity mention. Wojood consists of about 550K Modern Standard\nArabic (MSA) and dialect tokens that are manually annotated with 21 entity\ntypes including person, organization, location, event and date. More\nimportantly, the corpus is annotated with nested entities instead of the more\ncommon flat annotations. The data contains about 75K entities and 22.5% of\nwhich are nested. The inter-annotator evaluation of the corpus demonstrated a\nstrong agreement with Cohen's Kappa of 0.979 and an F1-score of 0.976. To\nvalidate our data, we used the corpus to train a nested NER model based on\nmulti-task learning and AraBERT (Arabic BERT). The model achieved an overall\nmicro F1-score of 0.884. Our corpus, the annotation guidelines, the source code\nand the pre-trained model are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalilia_M/0/1/0/all/0/1\">Mohammed Khalilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_S/0/1/0/all/0/1\">Sana Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-augmented Data Selection for Few-shot Dialogue Generation. (arXiv:2205.09661v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09661","description":"<p>The natural language generation (NLG) module in task-oriented dialogue\nsystems translates structured meaning representations (MRs) into text\nresponses, which has a great impact on users' experience as the human-machine\ninteraction interface. However, in practice, developers often only have a few\nwell-annotated data and confront a high data collection cost to build the NLG\nmodule. In this work, we adopt the self-training framework to deal with the\nfew-shot MR-to-Text generation problem. We leverage the pre-trained language\nmodel to self-augment many pseudo-labeled data. To prevent the gradual drift\nfrom target data distribution to noisy augmented data distribution, we propose\na novel data selection strategy to select the data that our generation model is\nmost uncertain about. Compared with existing data selection methods, our method\nis: (1) parameter-efficient, which does not require training any additional\nneural models, (2) computation-efficient, which only needs to apply several\nstochastic forward passes of the model to estimate the uncertainty. We conduct\nempirical experiments on two benchmark datasets: FewShotWOZ and FewShotSGD, and\nshow that our proposed framework consistently outperforms other baselines in\nterms of BLEU and ERR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content. (arXiv:2205.09664v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09664","description":"<p>We present a formal Arabic wordnet built on the basis of a carefully designed\nontology hereby referred to as the Arabic Ontology. The ontology provides a\nformal representation of the concepts that the Arabic terms convey, and its\ncontent was built with ontological analysis in mind, and benchmarked to\nscientific advances and rigorous knowledge sources as much as this is possible,\nrather than to only speakers' beliefs as lexicons typically are. A\ncomprehensive evaluation was conducted thereby demonstrating that the current\nversion of the top-levels of the ontology can top the majority of the Arabic\nmeanings. The ontology consists currently of about 1,300 well-investigated\nconcepts in addition to 11,000 concepts that are partially validated. The\nontology is accessible and searchable through a lexicographic search engine\n(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual\nlexicons, and which are being mapped and enriched using the ontology. The\nontology is fully mapped with Princeton WordNet, Wikidata, and other resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Crossword Solving. (arXiv:2205.09665v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09665","description":"<p>We present the Berkeley Crossword Solver, a state-of-the-art approach for\nautomatically solving crossword puzzles. Our system works by generating answer\ncandidates for each crossword clue using neural question answering models and\nthen combines loopy belief propagation with local search to find full puzzle\nsolutions. Compared to existing approaches, our system improves exact puzzle\naccuracy from 57% to 82% on crosswords from The New York Times and obtains\n99.9% letter accuracy on themeless puzzles. Our system also won first place at\nthe top human crossword tournament, which marks the first time that a computer\nprogram has surpassed human performance at this event. To facilitate research\non question answering and crossword solving, we analyze our system's remaining\nerrors and release a dataset of over six million question-answer pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Albert Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_E/0/1/0/all/0/1\">Eshaan Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsberg_M/0/1/0/all/0/1\">Matthew Ginsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Active and Passive Cosponsorship in the U.S. Congress. (arXiv:2205.09674v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09674","description":"<p>In the U.S. Congress, legislators can use active and passive cosponsorship to\nsupport bills. We show that these two types of cosponsorship are driven by two\ndifferent motivations: the backing of political colleagues and the backing of\nthe bill's content. To this end, we develop an Encoder+RGCN based model that\nlearns legislator representations from bill texts and speech transcripts. These\nrepresentations predict active and passive cosponsorship with an F1-score of\n0.88. Applying our representations to predict voting decisions, we show that\nthey are interpretable and generalize to unseen tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1\">Giuseppe Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gote_C/0/1/0/all/0/1\">Christoph Gote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandenberger_L/0/1/0/all/0/1\">Laurence Brandenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlosser_S/0/1/0/all/0/1\">Sophia Schlosser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweitzer_F/0/1/0/all/0/1\">Frank Schweitzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArabGlossBERT: Fine-Tuning BERT on Context-Gloss Pairs for WSD. (arXiv:2205.09685v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09685","description":"<p>Using pre-trained transformer models such as BERT has proven to be effective\nin many NLP tasks. This paper presents our work to fine-tune BERT models for\nArabic Word Sense Disambiguation (WSD). We treated the WSD task as a\nsentence-pair binary classification task. First, we constructed a dataset of\nlabeled Arabic context-gloss pairs (~167k pairs) we extracted from the Arabic\nOntology and the large lexicographic database available at Birzeit University.\nEach pair was labeled as True or False and target words in each context were\nidentified and annotated. Second, we used this dataset for fine-tuning three\npre-trained Arabic BERT models. Third, we experimented the use of different\nsupervised signals used to emphasize target words in context. Our experiments\nachieved promising results (accuracy of 84%) although we used a large set of\nsenses in the experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hajj_M/0/1/0/all/0/1\">Moustafa Al-Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curras + Baladi: Towards a Levantine Corpus. (arXiv:2205.09692v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09692","description":"<p>The processing of the Arabic language is a complex field of research. This is\ndue to many factors, including the complex and rich morphology of Arabic, its\nhigh degree of ambiguity, and the presence of several regional varieties that\nneed to be processed while taking into account their unique characteristics.\nWhen its dialects are taken into account, this language pushes the limits of\nNLP to find solutions to problems posed by its inherent nature. It is a\ndiglossic language; the standard language is used in formal settings and in\neducation and is quite different from the vernacular languages spoken in the\ndifferent regions and influenced by older languages that were historically\nspoken in those regions. This should encourage NLP specialists to create\ndialect-specific corpora such as the Palestinian morphologically annotated\nCurras corpus of Birzeit University. In this work, we present the Lebanese\nCorpus Baladi that consists of around 9.6K morphologically annotated tokens.\nSince Lebanese and Palestinian dialects are part of the same Levantine\ndialectal continuum, and thus highly mutually intelligible, our proposed corpus\nwas constructed to be used to (1) enrich Curras and transform it into a more\ngeneral Levantine corpus and (2) improve Curras by solving detected errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haff_K/0/1/0/all/0/1\">Karim El Haff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammouda_T/0/1/0/all/0/1\">Tymaa Hammouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaraket_F/0/1/0/all/0/1\">Fadi Zaraket</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLAID: An Efficient Engine for Late Interaction Retrieval. (arXiv:2205.09707v1 [cs.IR])","link":"http://arxiv.org/abs/2205.09707","description":"<p>Pre-trained language models are increasingly important components across\nmultiple information retrieval (IR) paradigms. Late interaction, introduced\nwith the ColBERT model and recently refined in ColBERTv2, is a popular paradigm\nthat holds state-of-the-art status across many benchmarks. To dramatically\nspeed up the search latency of late interaction, we introduce the\nPerformance-optimized Late Interaction Driver (PLAID). Without impacting\nquality, PLAID swiftly eliminates low-scoring passages using a novel centroid\ninteraction mechanism that treats every passage as a lightweight bag of\ncentroids. PLAID uses centroid interaction as well as centroid pruning, a\nmechanism for sparsifying the bag of centroids, within a highly-optimized\nengine to reduce late interaction search latency by up to 7$\\times$ on a GPU\nand 45$\\times$ on a CPU against vanilla ColBERTv2, while continuing to deliver\nstate-of-the-art retrieval quality. This allows the PLAID engine with ColBERTv2\nto achieve latency of tens of milliseconds on a GPU and tens or just few\nhundreds of milliseconds on a CPU at large scale, even at the largest scales we\nevaluate with 140M passages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-LSTM Scoring Based Similarity Measurement with Agglomerative Hierarchical Clustering (AHC) for Speaker Diarization. (arXiv:2205.09709v1 [eess.AS])","link":"http://arxiv.org/abs/2205.09709","description":"<p>Majority of speech signals across different scenarios are never available\nwith well-defined audio segments containing only a single speaker. A typical\nconversation between two speakers consists of segments where their voices\noverlap, interrupt each other or halt their speech in between multiple\nsentences. Recent advancements in diarization technology leverage neural\nnetwork-based approaches to improvise multiple subsystems of speaker\ndiarization system comprising of extracting segment-wise embedding features and\ndetecting changes in the speaker during conversation. However, to identify\nspeaker through clustering, models depend on methodologies like PLDA to\ngenerate similarity measure between two extracted segments from a given\nconversational audio. Since these algorithms ignore the temporal structure of\nconversations, they tend to achieve a higher Diarization Error Rate (DER), thus\nleading to misdetections both in terms of speaker and change identification.\nTherefore, to compare similarity of two speech segments both independently and\nsequentially, we propose a Bi-directional Long Short-term Memory network for\nestimating the elements present in the similarity matrix. Once the similarity\nmatrix is generated, Agglomerative Hierarchical Clustering (AHC) is applied to\nfurther identify speaker segments based on thresholding. To evaluate the\nperformance, Diarization Error Rate (DER%) metric is used. The proposed model\nachieves a low DER of 34.80% on a test set of audio samples derived from ICSI\nMeeting Corpus as compared to traditional PLDA based similarity measurement\nmechanism which achieved a DER of 39.90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nijhawan_S/0/1/0/all/0/1\">Siddharth S. Nijhawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beigi_H/0/1/0/all/0/1\">Homayoon Beigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-informed Language Grounding. (arXiv:2205.09710v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09710","description":"<p>Natural language applied to natural 2D images describes a fundamentally 3D\nworld. We present the Voxel-informed Language Grounder (VLG), a language\ngrounding model that leverages 3D geometric information in the form of voxel\nmaps derived from the visual input using a volumetric reconstruction model. We\nshow that VLG significantly improves grounding accuracy on SNARE, an object\nreference game task. At the time of writing, VLG holds the top place on the\nSNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corona_R/0/1/0/all/0/1\">Rodolfo Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shizhan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning. (arXiv:2205.09712v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09712","description":"<p>Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_I/0/1/0/all/0/1\">Irina Higgins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankGen: Improving Text Generation with Large Ranking Models. (arXiv:2205.09726v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09726","description":"<p>Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues, we present RankGen, an encoder model (1.2B\nparameters) that scores model generations given a prefix. RankGen can be\nflexibly incorporated as a scoring function in beam search and used to decode\nfrom any pretrained language model. We train RankGen using large-scale\ncontrastive learning to map a prefix close to the ground-truth sequence that\nfollows it and far away from two types of negatives: (1) random sequences from\nthe same document as the prefix, and, which discourage topically-similar but\nirrelevant generations; (2) sequences generated from a large language model\nconditioned on the prefix, which discourage repetition and hallucination.\nExperiments across four different language models (345M-11B parameters) and two\ndomains show that RankGen significantly outperforms decoding algorithms like\nnucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3\nMAUVE) as well as human evaluations with English writers (74.5% human\npreference over nucleus sampling). Analysis reveals that RankGen outputs are\nmore relevant to the prefix and improve continuity and coherence compared to\nbaselines. We open source our model checkpoints, code, and human preferences\nwith detailed explanations for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yapei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Slot Tagging with Intent Features for Task Oriented Natural Language Understanding using BERT. (arXiv:2205.09732v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09732","description":"<p>Recent joint intent detection and slot tagging models have seen improved\nperformance when compared to individual models. In many real-world datasets,\nthe slot labels and values have a strong correlation with their intent labels.\nIn such cases, the intent label information may act as a useful feature to the\nslot tagging model. In this paper, we examine the effect of leveraging intent\nlabel features through 3 techniques in the slot tagging task of joint intent\nand slot detection models. We evaluate our techniques on benchmark spoken\nlanguage datasets SNIPS and ATIS, as well as over a large private Bixby dataset\nand observe an improved slot-tagging performance over state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_S/0/1/0/all/0/1\">Shruthi Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vignesh Kumar Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh/0/1/0/all/0/1\">Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarapanahalli_J/0/1/0/all/0/1\">Jayantha Gowda Sarapanahalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Harrington Yowlumne Narrative Corpus. (arXiv:2102.00610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00610","description":"<p>Minority languages continue to lack adequate resources for their development,\nespecially in the technological domain. Likewise, the J.P. Harrington Papers\ncollection at the Smithsonian Institution are difficult to access in practical\nterms for community members and researchers due to its handwritten and\ndisorganized format. Our current work seeks to make a portion of this\npublicly-available yet problematic material practically accessible for natural\nlanguage processing use. Here, we present the Harrington Yowlumne Narrative\nCorpus, a corpus of 20 narrative texts that derive from the Tejone\\~no Yowlumne\ncommunity of the Tinliw rancheria in Kern County, California between 1910 and\n1925. We digitally transcribe the texts and, through a Levenshtein\ndistance-based algorithm and manual checking, we provide gold-standard aligned\nnormalized and lemmatized text. We likewise provide POS tags for each\nlemmatized token via a lexicon-based deterministic approach. Altogether, the\ncorpus contains 57,136 transcribed characters aligned with 10,719 gold standard\ntext-normalized words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_N/0/1/0/all/0/1\">Nathan M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_Rodriguez_T/0/1/0/all/0/1\">Timothy Henry-Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer of Monolingual Models. (arXiv:2109.07348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07348","description":"<p>Recent studies in zero-shot cross-lingual learning using multilingual models\nhave falsified the previous hypothesis that shared vocabulary and joint\npre-training are the keys to cross-lingual generalization. Inspired by this\nadvancement, we introduce a cross-lingual transfer method for monolingual\nmodels based on domain adaptation. We study the effects of such transfer from\nfour different languages to English. Our experimental results on GLUE show that\nthe transferred models outperform the native English model independently of the\nsource language. After probing the English linguistic knowledge encoded in the\nrepresentations before and after transfer, we find that semantic information is\nretained from the source language, while syntactic information is learned\nduring transfer. Additionally, the results of evaluating the transferred models\nin source language tasks reveal that their performance in the source domain\ndeteriorates after transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1\">Evangelia Gogoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekgren_A/0/1/0/all/0/1\">Ariel Ekgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isbister_T/0/1/0/all/0/1\">Tim Isbister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Spread of Propaganda by Coordinated Communities on Social Media. (arXiv:2109.13046v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2109.13046","description":"<p>Large-scale manipulations on social media have two important characteristics:\n(i) use of propaganda to influence others, and (ii) adoption of coordinated\nbehavior to spread it and to amplify its impact. Despite the connection between\nthem, these two characteristics have so far been considered in isolation. Here\nwe aim to bridge this gap. In particular, we analyze the spread of propaganda\nand its interplay with coordinated behavior on a large Twitter dataset about\nthe 2019 UK general election. We first propose and evaluate several metrics for\nmeasuring the use of propaganda on Twitter. Then, we investigate the use of\npropaganda by different coordinated communities that participated in the online\ndebate. The combination of the use of propaganda and coordinated behavior\nallows us to uncover the authenticity and harmfulness of the different\ncommunities. Finally, we compare our measures of propaganda and coordination\nwith automation (i.e., bot) scores and Twitter suspensions, revealing\ninteresting trends. From a theoretical viewpoint, we introduce a methodology\nfor analyzing several important dimensions of online behavior that are seldom\nconjointly considered. From a practical viewpoint, we provide new insights into\nauthentic and inauthentic online activities during the 2019 UK general\nelection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hristakieva_K/0/1/0/all/0/1\">Kristina Hristakieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cresci_S/0/1/0/all/0/1\">Stefano Cresci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis. (arXiv:2110.04482v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.04482","description":"<p>This work presents a lifelong learning approach to train a multilingual\nText-To-Speech (TTS) system, where each language was seen as an individual task\nand was learned sequentially and continually. It does not require pooled data\nfrom all languages altogether, and thus alleviates the storage and computation\nburden. One of the challenges of lifelong learning methods is \"catastrophic\nforgetting\": in TTS scenario it means that model performance quickly degrades\non previous languages when adapted to a new language. We approach this problem\nvia a data-replay-based lifelong learning method. We formulate the replay\nprocess as a supervised learning problem, and propose a simple yet effective\ndual-sampler framework to tackle the heavily language-imbalanced training\nsamples. Through objective and subjective evaluations, we show that this\nsupervised learning formulation outperforms other gradient-based and\nregularization-based lifelong learning methods, achieving 43% Mel-Cepstral\nDistortion reduction compared to a fine-tuning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiST: Lite Prompted Self-training Makes Parameter-Efficient Few-shot Learners. (arXiv:2110.06274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06274","description":"<p>We present a new method LiST is short for Lite Prompted Self-Training for\nparameter-efficient fine-tuning of large pre-trained language models (PLMs) for\nfew-shot learning. LiST improves over recent methods that adopt prompt-based\nfine-tuning (FN) using two key techniques. The first is the use of\nself-training to leverage large amounts of unlabeled data for prompt-based FN\nin few-shot settings. We use self-training in conjunction with meta-learning\nfor re-weighting noisy pseudo-prompt labels. Self-training is expensive as it\nrequires updating all the model parameters repetitively. Therefore, we use a\nsecond technique for light-weight fine-tuning where we introduce a small number\nof task-specific parameters that are fine-tuned during self-training while\nkeeping the PLM encoder frozen. Our experiments show that LiST can effectively\nleverage unlabeled data to improve the model performance for few-shot learning.\nAdditionally, the fine-tuning is efficient as it only updates a small\npercentage of parameters and the overall model footprint is reduced since\nseveral tasks can share a common PLM encoder as backbone. A comprehensive study\non six NLU tasks demonstrate LiST to improve by 35% over classic fine-tuning\nand 6% over prompt-based FN with 96% reduction in number of trainable\nparameters when fine-tuned with no more than 30 labeled examples from each\ntask. With only 14M tunable parameters, LiST outperforms GPT-3 in-context\nlearning by 33% on few-shot NLU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speeding Up Entmax. (arXiv:2111.06832v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06832","description":"<p>Softmax is the de facto standard in modern neural networks for language\nprocessing when it comes to normalizing logits. However, by producing a dense\nprobability distribution each token in the vocabulary has a nonzero chance of\nbeing selected at each generation step, leading to a variety of reported\nproblems in text generation. $\\alpha$-entmax of Peters et al. (2019,\n<a href=\"/abs/1905.05702\">arXiv:1905.05702</a>) solves this problem, but is considerably slower than softmax.\n</p>\n<p>In this paper, we propose an alternative to $\\alpha$-entmax, which keeps its\nvirtuous characteristics, but is as fast as optimized softmax and achieves on\npar or better performance in machine translation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1\">Maxat Tezekbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1\">Zhenisbek Assylbekov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Human Evaluation for Image Captioning. (arXiv:2111.08940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08940","description":"<p>We establish THumB, a rubric-based human evaluation protocol for image\ncaptioning models. Our scoring rubrics and their definitions are carefully\ndeveloped based on machine- and human-generated captions on the MSCOCO dataset.\nEach caption is evaluated along two main dimensions in a tradeoff (precision\nand recall) as well as other aspects that measure the text quality (fluency,\nconciseness, and inclusive language). Our evaluations demonstrate several\ncritical problems of the current evaluation practice. Human-generated captions\nshow substantially higher quality than machine-generated ones, especially in\ncoverage of salient information (i.e., recall), while most automatic metrics\nsay the opposite. Our rubric-based results reveal that CLIPScore, a recent\nmetric that uses image features, better correlates with human judgments than\nconventional text-only metrics because it is more sensitive to recall. We hope\nthat this work will promote a more transparent evaluation protocol for image\ncaptioning and its automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1\">Lavinia Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1\">Jacob Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand. (arXiv:2112.04139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04139","description":"<p>Natural language processing researchers have identified limitations of\nevaluation methodology for generation tasks, with new questions raised about\nthe validity of automatic metrics and of crowdworker judgments. Meanwhile,\nefforts to improve generation models tend to depend on simple n-gram overlap\nmetrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics\nshould each more directly benefit and inform the other. We therefore propose a\ngeneralization of leaderboards, bidimensional leaderboards (Billboards), that\nsimultaneously tracks progress in language generation models and metrics for\ntheir evaluation. Unlike conventional unidimensional leaderboards that sort\nsubmitted systems by predetermined metrics, a Billboard accepts both generators\nand evaluation metrics as competing entries. A Billboard automatically creates\nan ensemble metric that selects and linearly combines a few metrics based on a\nglobal analysis across generators. Further, metrics are ranked based on their\ncorrelation with human judgments. We release four Billboards for machine\ntranslation, summarization, and image captioning. We demonstrate that a linear\nensemble of a few diverse metrics sometimes substantially outperforms existing\nmetrics in isolation. Our mixed-effects model analysis shows that most\nautomatic metrics, especially the reference-based ones, overrate machine over\nhuman generation, demonstrating the importance of updating metrics as\ngeneration models become stronger (and perhaps more similar to humans) in the\nfuture. Our project website is available at\nhttps://nlp.cs.washington.edu/billboard/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1\">Lavinia Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1\">Jacob Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech. (arXiv:2201.06723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06723","description":"<p>We introduce a generic, language-independent method to collect a large\npercentage of offensive and hate tweets regardless of their topics or genres.\nWe harness the extralinguistic information embedded in the emojis to collect a\nlarge number of offensive tweets. We apply the proposed method on Arabic tweets\nand compare it with English tweets - analysing key cultural differences. We\nobserved a constant usage of these emojis to represent offensiveness throughout\ndifferent timespans on Twitter. We manually annotate and publicly release the\nlargest Arabic dataset for offensive, fine-grained hate speech, vulgar and\nviolence content. Furthermore, we benchmark the dataset for detecting\noffensiveness and hate speech using different transformer architectures and\nperform in-depth linguistic analysis. We evaluate our models on external\ndatasets - a Twitter dataset collected using a completely different method, and\na multi-platform dataset containing comments from Twitter, YouTube and\nFacebook, for assessing generalization capability. Competitive results on these\ndatasets suggest that the data collected using our method captures universal\ncharacteristics of offensive language. Our findings also highlight the common\nwords used in offensive communications, common targets for hate speech,\nspecific patterns in violence tweets; and pinpoint common classification errors\nthat can be attributed to limitations of NLP models. We observe that even\nstate-of-the-art transformer models may fail to take into account culture,\nbackground and context or understand nuances present in real-world data such as\nsarcasm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07449","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) is\ncurrently one of the most important and state-of-the-art models for natural\nlanguage. However, it has also been shown that for domain-specific tasks it is\nhelpful to pretrain BERT on a domain-specific corpus. In this paper, we present\nTourBERT, a pretrained language model for tourism. We describe how TourBERT was\ndeveloped and evaluated. The evaluations show that TourBERT is outperforming\nBERT in all tourism-specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arefieva_V/0/1/0/all/0/1\">Veronika Arefieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_R/0/1/0/all/0/1\">Roman Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Extractive Opinion Summarization Using Sparse Coding. (arXiv:2203.07921v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07921","description":"<p>Opinion summarization is the task of automatically generating summaries that\nencapsulate information from multiple user reviews. We present Semantic\nAutoencoder (SemAE) to perform extractive opinion summarization in an\nunsupervised manner. SemAE uses dictionary learning to implicitly capture\nsemantic information from the review and learns a latent representation of each\nsentence over semantic units. A semantic unit is supposed to capture an\nabstract semantic concept. Our extractive summarization algorithm leverages the\nrepresentations to identify representative opinions among hundreds of reviews.\nSemAE is also able to perform controllable summarization to generate\naspect-specific summaries. We report strong performance on SPACE and AMAZON\ndatasets, and perform experiments to investigate the functioning of our model.\nOur code is publicly available at https://github.com/brcsomnath/SemAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again. (arXiv:2203.08410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08410","description":"<p>The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for application\ndomains such as biomedicine, which feature high and diverse demands of language\ntechnologies but also high data annotation costs. In this paper, we present the\nfirst systematic and comprehensive study to compare the few-shot performance of\nGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on\ntwo highly representative biomedical information extraction tasks, named entity\nrecognition and relation extraction. We follow the true few-shot setting to\navoid overestimating models' few-shot performance by model selection over a\nlarge validation set. We also optimize GPT-3's performance with known\ntechniques such as contextual calibration and dynamic in-context example\nretrieval. However, our results show that GPT-3 still significantly\nunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3\nin-context learning also yields smaller gains in accuracy when more training\ndata becomes available. Our in-depth analyses further reveal issues of the\nin-context learning setting that may be detrimental to information extraction\ntasks in general. Given the high cost of experimenting with GPT-3, we hope our\nstudy provides guidance for biomedical researchers and practitioners towards\nmore promising directions such as fine-tuning small PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_B/0/1/0/all/0/1\">Bernal Jim&#xe9;nez Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNeal_N/0/1/0/all/0/1\">Nikolas McNeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_C/0/1/0/all/0/1\">Clay Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based System for Personality and Interpersonal Reactivity Prediction. (arXiv:2203.12481v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12481","description":"<p>This paper describes our proposed method for the Workshop on Computational\nApproaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA) 2022\nshared task on Personality Prediction (PER) and Reactivity Index Prediction\n(IRI). In this paper, we adopt the prompt-based learning method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide knowledge of the extra personalized information for\nenhancing the pre-trained model. Data augmentation and model ensemble are\nadopted for obtaining better results. Moreover, we also provided the online\nsoftware demonstration and the codes of the software for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagonal State Spaces are as Effective as Structured State Spaces. (arXiv:2203.14343v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14343","description":"<p>Modeling long range dependencies in sequential data is a fundamental step\ntowards attaining human-level performance in many modalities such as text,\nvision, audio and video. While attention-based models are a popular and\neffective choice in modeling short-range interactions, their performance on\ntasks requiring long range reasoning has been largely inadequate. In an\nexciting result, Gu et al. (ICLR 2022) proposed the $\\textit{Structured State\nSpace}$ (S4) architecture delivering large gains over state-of-the-art models\non several long-range tasks across various modalities. The core proposition of\nS4 is the parameterization of state matrices via a diagonal plus low rank\nstructure, allowing efficient computation. In this work, we show that one can\nmatch the performance of S4 even without the low rank correction and thus\nassuming the state matrices to be diagonal. Our $\\textit{Diagonal State Space}$\n(DSS) model matches the performance of S4 on Long Range Arena tasks, speech\nclassification on Speech Commands dataset, while being conceptually simpler and\nstraightforward to implement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models learn from explanations in context?. (arXiv:2204.02329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02329","description":"<p>Large language models can perform new tasks by adapting to a few in-context\nexamples. For humans, rapid learning from examples can benefit from\nexplanations that connect examples to task principles. We therefore investigate\nwhether explanations of few-shot examples can allow language models to adapt\nmore effectively. We annotate a set of 40 challenging tasks from BIG-Bench with\nexplanations of answers to a small subset of questions, as well as a variety of\nmatched control explanations. We evaluate the effects of various zero-shot and\nfew-shot prompts that include different types of explanations, instructions,\nand controls on the performance of a range of large language models. We analyze\nthese results using statistical multilevel modeling techniques that account for\nthe nested dependencies among conditions, tasks, prompts, and models. We find\nthat explanations of examples can improve performance. Adding untuned\nexplanations to a few-shot prompt offers a modest improvement in performance;\nabout 1/3 the effect size of adding few-shot examples, but twice the effect\nsize of task instructions. We then show that explanations tuned for performance\non a small validation set offer substantially larger benefits; building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Hand-tuning explanations can\nsubstantially improve performance on challenging tasks. Furthermore, even\nuntuned explanations outperform carefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather\nthan lower-level features of the language used. However, only large models can\nbenefit from explanations. In summary, explanations can support the in-context\nlearning abilities of large language models on challenging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthewson_K/0/1/0/all/0/1\">Kory Matthewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriWOZ: Corpus for Exploiting Cross-Lingual Transferability for Generation of Dialogues in Low-Resource, African Languages. (arXiv:2204.08083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08083","description":"<p>Dialogue generation is an important NLP task fraught with many challenges.\nThe challenges become more daunting for low-resource African languages. To\nenable the creation of dialogue agents for African languages, we contribute the\nfirst high-quality dialogue datasets for 6 African languages: Swahili, Wolof,\nHausa, Nigerian Pidgin English, Kinyarwanda &amp; Yor\\`ub\\'a. These datasets\nconsist of 1,500 turns each, which we translate from a portion of the English\nmulti-domain MultiWOZ dataset. Subsequently, we investigate &amp; analyze the\neffectiveness of modelling through transfer learning by utilziing\nstate-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We\ncompare the models with a simple seq2seq baseline using perplexity. Besides\nthis, we conduct human evaluation of single-turn conversations by using\nmajority votes and measure inter-annotator agreement (IAA). We find that the\nhypothesis that deep monolingual models learn some abstractions that generalize\nacross languages holds. We observe human-like conversations, to different\ndegrees, in 5 out of the 6 languages. The language with the most transferable\nproperties is the Nigerian Pidgin English, with a human-likeness score of\n78.1%, of which 34.4% are unanimous. We freely provide the datasets and host\nthe model checkpoints/demos on the HuggingFace hub for public access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anuoluwapo_A/0/1/0/all/0/1\">Aremu Anuoluwapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_B/0/1/0/all/0/1\">Bukola Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_O/0/1/0/all/0/1\">Oyerinde Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rufai_A/0/1/0/all/0/1\">Amina Mardiyyah Rufai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajibade_B/0/1/0/all/0/1\">Benjamin Ajibade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajudeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traore_M/0/1/0/all/0/1\">Mory Moussou Koulibaly Traore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruwa_A/0/1/0/all/0/1\">Ahmed Baruwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owoicho_P/0/1/0/all/0/1\">Paul Owoicho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogunremi_T/0/1/0/all/0/1\">Tolulope Ogunremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngigi_P/0/1/0/all/0/1\">Phylis Ngigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_R/0/1/0/all/0/1\">Ruqayya Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration. (arXiv:2205.02517v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02517","description":"<p>The cross-entropy objective has proved to be an all-purpose training\nobjective for autoregressive language models (LMs). However, without\nconsidering the penalization of problematic tokens, LMs trained using\ncross-entropy exhibit text degeneration. To address this, unlikelihood training\nhas been proposed to reduce the probability of unlikely tokens predicted by\nLMs. But unlikelihood does not consider the relationship between the label\ntokens and unlikely token candidates, thus showing marginal improvements in\ndegeneration. We propose a new contrastive token learning objective that\ninherits the advantages of cross-entropy and unlikelihood training and avoids\ntheir limitations. The key idea is to teach a LM to generate high probabilities\nfor label tokens and low probabilities of negative candidates. Comprehensive\nexperiments on language modeling and open-domain dialogue generation tasks show\nthat the proposed contrastive token objective yields much less repetitive\ntexts, with a higher generation quality than baseline approaches, achieving the\nnew state-of-the-art performance on text degeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shaojie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language. (arXiv:2205.03026v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03026","description":"<p>This paper explains our work in developing new acoustic models for automated\nspeech recognition (ASR) at KBLab, the infrastructure for data-driven research\nat the National Library of Sweden (KB). We evaluate different approaches for a\nviable speech-to-text pipeline for audiovisual resources in Swedish, using the\nwav2vec 2.0 architecture in combination with speech corpuses created from KB's\ncollections. These approaches include pretraining an acoustic model for Swedish\nfrom the ground up, and fine-tuning existing monolingual and multilingual\nmodels. The collections-based corpuses we use have been sampled from millions\nof hours of speech, with a conscious attempt to balance regional dialects to\nproduce a more representative, and thus more democratic, model. The acoustic\nmodel this enabled, \"VoxRex\", outperforms existing models for Swedish ASR. We\nalso evaluate combining this model with various pretrained language models,\nwhich further enhanced performance. We conclude by highlighting the potential\nof such technology for cultural heritage institutions with vast collections of\npreviously unlabelled audiovisual data. Our models are released for further\nexploration and research here: https://huggingface.co/KBLab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malmsten_M/0/1/0/all/0/1\">Martin Malmsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffenden_C/0/1/0/all/0/1\">Chris Haffenden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borjeson_L/0/1/0/all/0/1\">Love B&#xf6;rjeson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05435","description":"<p>Where performance of text classification models drops over time due to\nchanges in data, development of models whose performance persists over time is\nimportant. An ability to predict a model's ability to persist over time can\nhelp design models that can be effectively used over a longer period of time.\nIn this paper, we look at this problem from a practical perspective by\nassessing the ability of a wide range of language models and classification\nalgorithms to persist over time, as well as how dataset characteristics can\nhelp predict the temporal stability of different models. We perform\nlongitudinal classification experiments on three datasets spanning between 6\nand 19 years, and involving diverse tasks and types of data. We find that one\ncan estimate how a model will retain its performance over time based on (i) how\nwell the model performs over a restricted time period and its extrapolation to\na longer time period, and (ii) the linguistic characteristics of the dataset,\nsuch as the familiarity score between subsets from different years. Findings\nfrom these experiments have important implications for the design of text\nclassification models with the aim of preserving performance over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition. (arXiv:2205.05832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05832","description":"<p>Recently, Flat-LAttice Transformer (FLAT) has achieved great success in\nChinese Named Entity Recognition (NER). FLAT performs lexical enhancement by\nconstructing flat lattices, which mitigates the difficulties posed by blurred\nword boundaries and the lack of word semantics. In FLAT, the positions of\nstarting and ending characters are used to connect a matching word. However,\nthis method is likely to match more words when dealing with long texts,\nresulting in long input sequences. Therefore, it significantly increases the\nmemory and computational costs of the self-attention module. To deal with this\nissue, we advocate a novel lexical enhancement method, InterFormer, that\neffectively reduces the amount of computational and memory costs by\nconstructing non-flat lattices. Furthermore, with InterFormer as the backbone,\nwe implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context\nfeature encoding. Compared with FLAT, it reduces unnecessary attention\ncalculations in \"word-character\" and \"word-word\". This reduces the memory usage\nby about 50% and can use more extensive lexicons or higher batches for network\ntraining. The experimental results obtained on several well-known benchmarks\ndemonstrate the superiority of the proposed method over the state-of-the-art\nhybrid (character-word) models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoning Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhenhua Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs. (arXiv:2205.05943v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05943","description":"<p>We propose a generative model for text generation, which exhibits\ndisentangled latent representations of syntax and semantics. Contrary to\nprevious work, this model does not need syntactic information such as\nconstituency parses, or semantic information such as paraphrase pairs. Our\nmodel relies solely on the inductive bias found in attention-based\narchitectures such as Transformers.\n</p>\n<p>In the attention of Transformers, keys handle information selection while\nvalues specify what information is conveyed. Our model, dubbed QKVAE, uses\nAttention in its decoder to read latent variables where one latent variable\ninfers keys while another infers values. We run experiments on latent\nrepresentations and experiments on syntax/semantics transfer which show that\nQKVAE displays clear signs of disentangled syntax and semantics. We also show\nthat our model displays competitive syntax transfer capabilities when compared\nto supervised models and that comparable supervised models need a fairly large\namount of data (more than 50K samples) to outperform it on both syntactic and\nsemantic transfer. The code for our experiments is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felhi_G/0/1/0/all/0/1\">Ghazi Felhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Joseph Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalist Agent. (arXiv:2205.06175v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.06175","description":"<p>Inspired by progress in large-scale language modeling, we apply a similar\napproach towards building a single generalist agent beyond the realm of text\noutputs. The agent, which we refer to as Gato, works as a multi-modal,\nmulti-task, multi-embodiment generalist policy. The same network with the same\nweights can play Atari, caption images, chat, stack blocks with a real robot\narm and much more, deciding based on its context whether to output text, joint\ntorques, button presses, or other tokens. In this report we describe the model\nand the data, and document the current capabilities of Gato.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reed_S/0/1/0/all/0/1\">Scott Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolna_K/0/1/0/all/0/1\">Konrad Zolna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1\">Emilio Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colmenarejo_S/0/1/0/all/0/1\">Sergio Gomez Colmenarejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikov_A/0/1/0/all/0/1\">Alexander Novikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1\">Gabriel Barth-Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_M/0/1/0/all/0/1\">Mai Gimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulsky_Y/0/1/0/all/0/1\">Yury Sulsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Jackie Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1\">Jost Tobias Springenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eccles_T/0/1/0/all/0/1\">Tom Eccles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1\">Jake Bruce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1\">Ali Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1\">Ashley Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordbar_M/0/1/0/all/0/1\">Mahyar Bordbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Budge programming language. (arXiv:2205.07979v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2205.07979","description":"<p>We present a simple, esoteric programming language based on G\\\"odel numbering\nand prime factorization, enhanced with explicit, scoped loops, allowing for\neasy program composition. We will show the syntax and semantics and then\nprovide a few example programs and their evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitnikovski_B/0/1/0/all/0/1\">Boro Sitnikovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"What makes a question inquisitive?\" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08056","description":"<p>We propose a type-controlled framework for inquisitive question generation.\nWe annotate an inquisitive question dataset with question types, train question\ntype classifiers, and finetune models for type-controlled question generation.\nEmpirical results demonstrate that we can generate a variety of questions that\nadhere to specific types while drawing from the source texts. We also\ninvestigate strategies for selecting a single question from a generated set,\nconsidering both an informative vs.~inquisitive question classifier and a\npairwise ranker trained from a small set of expert annotations. Question\nselection using the pairwise ranker yields strong results in automatic and\nmanual evaluation. Our human evaluation assesses multiple aspects of the\ngenerated questions, finding that the ranker chooses questions with the best\nsyntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,\neven rivaling the performance of human-written questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Induction for Efficient Semi-Supervised Learning. (arXiv:2205.09067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09067","description":"<p>Semi-supervised learning has shown promise in allowing NLP models to\ngeneralize from small amounts of labeled data. Meanwhile, pretrained\ntransformer models act as black-box correlation engines that are difficult to\nexplain and sometimes behave unreliably. In this paper, we propose tackling\nboth of these challenges via Automatic Rule Induction (ARI), a simple and\ngeneral-purpose framework for the automatic discovery and integration of\nsymbolic rules into pretrained transformer models. First, we extract weak\nsymbolic rules from low-capacity machine learning models trained on small\namounts of labeled data. Next, we use an attention mechanism to integrate these\nrules into high-capacity pretrained transformer models. Last, the\nrule-augmented system becomes part of a self-training framework to boost\nsupervision signal on unlabeled data. These steps can be layered beneath a\nvariety of existing weak supervision and semi-supervised NLP algorithms in\norder to improve performance and interpretability. Experiments across nine\nsequence classification and relation extraction tasks suggest that ARI can\nimprove state-of-the-art methods with no manual effort and minimal\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Dark Solitons in Bose-Einstein Condensates: A Dataset for Many-body Physics Research. (arXiv:2205.09114v1 [cond-mat.quant-gas])","link":"http://arxiv.org/abs/2205.09114","description":"<p>We establish a dataset of over $1.6\\times10^4$ experimental images of\nBose-Einstein condensates containing solitonic excitations to enable machine\nlearning (ML) for many-body physics research. About 33 % of this dataset has\nmanually assigned and carefully curated labels. The remainder is automatically\nlabeled using SolDet -- an implementation of a physics-informed ML data\nanalysis framework -- consisting of a convolutional-neural-network-based\nclassifier and object detector as well as a statistically motivated\nphysics-informed classifier and a quality metric. This technical note\nconstitutes the definitive reference of the dataset, providing an opportunity\nfor the data science community to develop more sophisticated analysis tools, to\nfurther understand nonlinear many-body physics, and even advance cold atom\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1\">Amilson R. Fritsch</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1\">Shangjie Guo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Koh_S/0/1/0/all/0/1\">Sophia M. Koh</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1\">I. B. Spielman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Adjugate Matrix Approach to Quaternion Pose Extraction. (arXiv:2205.09116v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09116","description":"<p>Quaternions are important for a wide variety of rotation-related problems in\ncomputer graphics, machine vision, and robotics. We study the nontrivial\ngeometry of the relationship between quaternions and rotation matrices by\nexploiting the adjugate matrix of the characteristic equation of a related\neigenvalue problem to obtain the manifold of the space of a quaternion\neigenvector. We argue that quaternions parameterized by their corresponding\nrotation matrices cannot be expressed, for example, in machine learning tasks,\nas single-valued functions: the quaternion solution must instead be treated as\na manifold, with different algebraic solutions for each of several\nsingle-valued sectors represented by the adjugate matrix. We conclude with\nnovel constructions exploiting the quaternion adjugate variables to revisit\nseveral classic pose estimation applications: 2D point-cloud matching, 2D\npoint-cloud-to-projection matching, 3D point-cloud matching, 3D orthographic\npoint-cloud-to-projection matching, and 3D perspective\npoint-cloud-to-projection matching. We find an exact solution to the 3D\northographic least squares pose extraction problem, and apply it successfully\nalso to the perspective pose extraction problem with results that improve on\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hanson_A/0/1/0/all/0/1\">Andrew J. Hanson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanson_S/0/1/0/all/0/1\">Sonya M. Hanson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeRaC: Learning Rate Curriculum. (arXiv:2205.09180v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09180","description":"<p>Most curriculum learning methods require an approach to sort the data samples\nby difficulty, which is often cumbersome to perform. In this work, we propose a\nnovel curriculum learning approach termed Learning Rate Curriculum (LeRaC),\nwhich leverages the use of a different learning rate for each layer of a neural\nnetwork to create a data-free curriculum during the initial training epochs.\nMore specifically, LeRaC assigns higher learning rates to neural layers closer\nto the input, gradually decreasing the learning rates as the layers are placed\nfarther away from the input. The learning rates increase at various paces\nduring the first training iterations, until they all reach the same value. From\nthis point on, the neural model is trained as usual. This creates a model-level\ncurriculum learning strategy that does not require sorting the examples by\ndifficulty and is compatible with any neural network, generating higher\nperformance levels regardless of the architecture. We conduct comprehensive\nexperiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100,\nTiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D)\ndomains, considering various convolutional (ResNet-18, Wide-ResNet-50,\nDenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr)\narchitectures, comparing our approach with the conventional training regime.\nMoreover, we also compare with Curriculum by Smoothing (CBS), a\nstate-of-the-art data-free curriculum learning approach. Unlike CBS, our\nperformance improvements over the standard training regime are consistent\nacross all datasets and models. Furthermore, we significantly surpass CBS in\nterms of training time (there is no additional cost over the standard training\nregime for LeRaC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing the ensemble spread from deterministic weather predictions using conditional generative adversarial networks. (arXiv:2205.09182v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09182","description":"<p>Ensemble prediction systems are an invaluable tool for weather forecasting.\nPractically, ensemble predictions are obtained by running several perturbations\nof the deterministic control forecast. However, ensemble prediction is\nassociated with a high computational cost and often involves statistical\npost-processing steps to improve its quality. Here we propose to use\ndeep-learning-based algorithms to learn the statistical properties of an\nensemble prediction system, the ensemble spread, given only the deterministic\ncontrol forecast. Thus, once trained, the costly ensemble prediction system\nwill not be needed anymore to obtain future ensemble forecasts, and the\nstatistical properties of the ensemble can be derived from a single\ndeterministic forecast. We adapt the classical pix2pix architecture to a\nthree-dimensional model and also experiment with a shared latent space\nencoder-decoder model, and train them against several years of operational\n(ensemble) weather forecasts for the 500 hPa geopotential height. The results\ndemonstrate that the trained models indeed allow obtaining a highly accurate\nensemble spread from the control forecast only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brecht_R/0/1/0/all/0/1\">R&#xfc;diger Brecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bihlo_A/0/1/0/all/0/1\">Alex Bihlo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Multi-view Clustering with Graph Filtering. (arXiv:2205.09228v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09228","description":"<p>With the explosive growth of multi-source data, multi-view clustering has\nattracted great attention in recent years. Most existing multi-view methods\noperate in raw feature space and heavily depend on the quality of original\nfeature representation. Moreover, they are often designed for feature data and\nignore the rich topology structure information. Accordingly, in this paper, we\npropose a generic framework to cluster both attribute and graph data with\nheterogeneous features. It is capable of exploring the interplay between\nfeature and structure. Specifically, we first adopt graph filtering technique\nto eliminate high-frequency noise to achieve a clustering-friendly smooth\nrepresentation. To handle the scalability challenge, we develop a novel\nsampling strategy to improve the quality of anchors. Extensive experiments on\nattribute and graph benchmarks demonstrate the superiority of our approach with\nrespect to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yonggang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sanchu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MESH2IR: Neural Acoustic Impulse Response Generator for Complex 3D Scenes. (arXiv:2205.09248v1 [cs.SD])","link":"http://arxiv.org/abs/2205.09248","description":"<p>We propose a mesh-based neural network (MESH2IR) to generate acoustic impulse\nresponses (IRs) for indoor 3D scenes represented using a mesh. The IRs are used\nto create a high-quality sound experience in interactive applications and audio\nprocessing. Our method can handle input triangular meshes with arbitrary\ntopologies (2K - 3M triangles). We present a novel training technique to train\nMESH2IR using energy decay relief and highlight its benefits. We also show that\ntraining MESH2IR on IRs preprocessed using our proposed technique significantly\nimproves the accuracy of IR generation. We reduce the non-linearity in the mesh\nspace by transforming 3D scene meshes to latent space using a graph convolution\nnetwork. Our MESH2IR is more than 200 times faster than a geometric acoustic\nalgorithm on a CPU and can generate more than 10,000 IRs per second on an\nNVIDIA GeForce RTX 2080 Ti GPU for a given furnished indoor 3D scene. The\nacoustic metrics are used to characterize the acoustic environment. We show\nthat the acoustic metrics of the IRs predicted from our MESH2IR match the\nground truth with less than 10% error. We also highlight the benefits of\nMESH2IR on audio and speech processing applications such as speech\ndereverberation and speech separation. To the best of our knowledge, ours is\nthe first neural-network-based approach to predict IRs from a given 3D scene\nmesh in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1\">Anton Ratnarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatti_R/0/1/0/all/0/1\">Rohith Chandrashekar Aralikatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets. (arXiv:2205.09249v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09249","description":"<p>Natural language guided embodied task completion is a challenging problem\nsince it requires understanding natural language instructions, aligning them\nwith egocentric visual observations, and choosing appropriate actions to\nexecute in the environment to produce desired changes. We experiment with\naugmenting a transformer model for this task with modules that effectively\nutilize a wider field of view and learn to choose whether the next step\nrequires a navigation or manipulation action. We observed that the proposed\nmodules resulted in improved, and in fact state-of-the-art performance on an\nunseen validation set of a popular benchmark dataset, ALFRED. However, our best\nmodel selected using the unseen validation set underperforms on the unseen test\nsplit of ALFRED, indicating that performance on the unseen validation set may\nnot in itself be a sufficient indicator of whether model improvements\ngeneralize to unseen test sets. We highlight this result as we believe it may\nbe a wider phenomenon in machine learning tasks but primarily noticeable only\nin benchmarks that limit evaluations on test splits, and highlights the need to\nmodify benchmark design to better account for variance in model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Convolutional Neural Networks for Limited Data Hyperspectral Remote Sensing Image Classification. (arXiv:2205.09250v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09250","description":"<p>Employing deep neural networks for Hyper-spectral remote sensing (HSRS) image\nclassification is a challenging task. HSRS images have high dimensionality and\na large number of channels with substantial redundancy between channels. In\naddition, the training data for classifying HSRS images is limited and the\namount of available training data is much smaller compared to other\nclassification tasks. These factors complicate the training process of deep\nneural networks with many parameters and cause them to not perform well even\ncompared to conventional models. Moreover, convolutional neural networks\nproduce over-confident predictions, which is highly undesirable considering the\naforementioned problem.\n</p>\n<p>In this work, we use a special class of deep neural networks, namely Bayesian\nneural network, to classify HSRS images. To the extent of our knowledge, this\nis the first time that this class of neural networks has been used in HSRS\nimage classification. Bayesian neural networks provide an inherent tool for\nmeasuring uncertainty. We show that a Bayesian network can outperform a\nsimilarly-constructed non-Bayesian convolutional neural network (CNN) and an\noff-the-shelf Random Forest (RF). Moreover, experimental results for the Pavia\nCentre, Salinas, and Botswana datasets show that the Bayesian network is more\nstable and robust to model pruning. Furthermore, we analyze the prediction\nuncertainty of the Bayesian model and show that the prediction uncertainty\nmetric can provide information about the model predictions and has a positive\ncorrelation with the prediction error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshaghani_M/0/1/0/all/0/1\">Mohammad Joshaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davari_A/0/1/0/all/0/1\">Amirabbas Davari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatamian_F/0/1/0/all/0/1\">Faezeh Nejati Hatamian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Vision-Language Transformers from Captions Alone. (arXiv:2205.09256v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09256","description":"<p>We show that Vision-Language Transformers can be learned without human labels\n(e.g. class labels, bounding boxes, etc). Existing work, whether explicitly\nutilizing bounding boxes or patches, assumes that the visual backbone must\nfirst be trained on ImageNet class prediction before being integrated into a\nmultimodal linguistic pipeline. We show that this is not necessary and\nintroduce a new model Vision-Language from Captions (VLC) built on top of\nMasked Auto-Encoders that does not require this supervision. In fact, in a\nhead-to-head comparison between ViLT, the current state-of-the-art patch-based\nvision-language transformer which is pretrained with supervised object\nclassification, and our model, VLC, we find that our approach 1. outperforms\nViLT on standard benchmarks, 2. provides more interpretable and intuitive patch\nvisualizations, and 3. is competitive with many larger models that utilize ROIs\ntrained on annotated bounding-boxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Liangke Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions. (arXiv:2205.09292v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09292","description":"<p>Self-supervised learning has witnessed great progress in vision and NLP;\nrecently, it also attracted much attention to various medical imaging\nmodalities such as X-ray, CT, and MRI. Existing methods mostly focus on\nbuilding new pretext self-supervision tasks such as reconstruction,\norientation, and masking identification according to the properties of medical\nimages. However, the publicly available self-supervision models are not fully\nexploited. In this paper, we present a powerful yet efficient self-supervision\nframework for surgical video understanding. Our key insight is to distill\nknowledge from publicly available models trained on large generic datasets4 to\nfacilitate the self-supervised learning of surgical videos. To this end, we\nfirst introduce a semantic-preserving training scheme to obtain our teacher\nmodel, which not only contains semantics from the publicly available models,\nbut also can produce accurate knowledge for surgical data. Besides training\nwith only contrastive learning, we also introduce a distillation objective to\ntransfer the rich learned information from the teacher model to self-supervised\nlearning on surgical data. Extensive experiments on two surgical phase\nrecognition benchmarks show that our framework can significantly improve the\nperformance of existing self-supervised learning methods. Notably, our\nframework demonstrates a compelling advantage under a low-data regime. Our code\nis available at https://github.com/xmed-lab/DistillingSelf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinpeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DConvCaps: 3DUnet with Convolutional Capsule Encoder for Medical Image Segmentation. (arXiv:2205.09299v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09299","description":"<p>Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan T.H. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Support-set based Multi-modal Representation Enhancement for Video Captioning. (arXiv:2205.09307v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09307","description":"<p>Video captioning is a challenging task that necessitates a thorough\ncomprehension of visual scenes. Existing methods follow a typical one-to-one\nmapping, which concentrates on a limited sample space while ignoring the\nintrinsic semantic associations between samples, resulting in rigid and\nuninformative expressions. To address this issue, we propose a novel and\nflexible framework, namely Support-set based Multi-modal Representation\nEnhancement (SMRE) model, to mine rich information in a semantic subspace\nshared between samples. Specifically, we propose a Support-set Construction\n(SC) module to construct a support-set to learn underlying connections between\nsamples and obtain semantic-related visual elements. During this process, we\ndesign a Semantic Space Transformation (SST) module to constrain relative\ndistance and administrate multi-modal interactions in a self-supervised way.\nExtensive experiments on MSVD and MSR-VTT datasets demonstrate that our SMRE\nachieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1\">Pengpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sub-pixel Accurate Quantification of Joint Space Narrowing Progression in Rheumatoid Arthritis. (arXiv:2205.09315v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09315","description":"<p>Rheumatoid arthritis (RA) is a chronic autoimmune disease that primarily\naffects peripheral synovial joints, like fingers, wrist and feet. Radiology\nplays a critical role in the diagnosis and monitoring of RA. Limited by the\ncurrent spatial resolution of radiographic imaging, joint space narrowing (JSN)\nprogression of RA with the same reason above can be less than one pixel per\nyear with universal spatial resolution. Insensitive monitoring of JSN can\nhinder the radiologist/rheumatologist from making a proper and timely clinical\njudgment. In this paper, we propose a novel and sensitive method that we call\npartial image phase-only correlation which aims to automatically quantify JSN\nprogression in the early stages of RA. The majority of the current literature\nutilizes the mean error, root-mean-square deviation and standard deviation to\nreport the accuracy at pixel level. Our work measures JSN progression between a\nbaseline and its follow-up finger joint images by using the phase spectrum in\nthe frequency domain. Using this study, the mean error can be reduced to\n0.0130mm when applied to phantom radiographs with ground truth, and 0.0519mm\nstandard deviation for clinical radiography. With its sub-pixel accuracy far\nbeyond manual measurement, we are optimistic that our work is promising for\nautomatically quantifying JSN progression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ou_Y/0/1/0/all/0/1\">Yafei Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ambalathankandy_P/0/1/0/all/0/1\">Prasoon Ambalathankandy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Furuya_R/0/1/0/all/0/1\">Ryunosuke Furuya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawada_S/0/1/0/all/0/1\">Seiya Kawada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_T/0/1/0/all/0/1\">Tianyu Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_Y/0/1/0/all/0/1\">Yujie An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamishima_T/0/1/0/all/0/1\">Tamotsu Kamishima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamura_K/0/1/0/all/0/1\">Kenichi Tamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikebe_M/0/1/0/all/0/1\">Masayuki Ikebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Demographic Bias in Fingerprint Recognition. (arXiv:2205.09318v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09318","description":"<p>Fingerprint recognition systems have been deployed globally in numerous\napplications including personal devices, forensics, law enforcement, banking,\nand national identity systems. For these systems to be socially acceptable and\ntrustworthy, it is critical that they perform equally well across different\ndemographic groups. In this work, we propose a formal statistical framework to\ntest for the existence of bias (demographic differentials) in fingerprint\nrecognition across four major demographic groups (white male, white female,\nblack male, and black female) for two state-of-the-art (SOTA) fingerprint\nmatchers operating in verification and identification modes. Experiments on two\ndifferent fingerprint databases (with 15,468 and 1,014 subjects) show that\ndemographic differentials in SOTA fingerprint recognition systems decrease as\nthe matcher accuracy increases and any small bias that may be evident is likely\ndue to certain outlier, low-quality fingerprint images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Akash Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Talk! Striking Up Conversations via Conversational Visual Question Generation. (arXiv:2205.09327v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09327","description":"<p>An engaging and provocative question can open up a great conversation. In\nthis work, we explore a novel scenario: a conversation agent views a set of the\nuser's photos (for example, from social media platforms) and asks an engaging\nquestion to initiate a conversation with the user. The existing\nvision-to-question models mostly generate tedious and obvious questions, which\nmight not be ideals conversation starters. This paper introduces a two-phase\nframework that first generates a visual story for the photo set and then uses\nthe story to produce an interesting question. The human evaluation shows that\nour framework generates more response-provoking questions for starting\nconversations than other vision-to-question baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Shih-Han Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsai-Lun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yun-Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chi-Yang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Y/0/1/0/all/0/1\">Yu-Shian Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically-Based Editing of Indoor Scene Lighting from a Single Image. (arXiv:2205.09343v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09343","description":"<p>We present a method to edit complex indoor lighting from a single image with\nits predicted depth and light source segmentation masks. This is an extremely\nchallenging problem that requires modeling complex light transport, and\ndisentangling HDR lighting from material and geometry with only a partial LDR\nobservation of the scene. We tackle this problem using two novel components: 1)\na holistic scene reconstruction method that estimates scene reflectance and\nparametric 3D lighting, and 2) a neural rendering framework that re-renders the\nscene from our predictions. We use physically-based indoor light\nrepresentations that allow for intuitive editing, and infer both visible and\ninvisible light sources. Our neural rendering framework combines\nphysically-based direct illumination and shadow rendering with deep networks to\napproximate global illumination. It can capture challenging lighting effects,\nsuch as soft shadows, directional lighting, specular materials, and\ninterreflections. Previous single image inverse rendering methods usually\nentangle scene lighting and geometry and only support applications like object\ninsertion. Instead, by combining parametric 3D lighting estimation with neural\nscene rendering, we demonstrate the first automatic method to achieve full\nscene relighting, including light source insertion, removal, and replacement,\nfrom a single image. All source code and data will be publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields. (arXiv:2205.09351v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09351","description":"<p>Neural scene representations, such as neural radiance fields (NeRF), are\nbased on training a multilayer perceptron (MLP) using a set of color images\nwith known poses. An increasing number of devices now produce RGB-D\ninformation, which has been shown to be very important for a wide range of\ntasks. Therefore, the aim of this paper is to investigate what improvements can\nbe made to these promising implicit representations by incorporating depth\ninformation with the color images. In particular, the recently proposed\nMip-NeRF approach, which uses conical frustums instead of rays for volume\nrendering, allows one to account for the varying area of a pixel with distance\nfrom the camera center. The proposed method additionally models depth\nuncertainty. This allows to address major limitations of NeRF-based approaches\nincluding improving the accuracy of geometry, reduced artifacts, faster\ntraining time, and shortened prediction time. Experiments are performed on\nwell-known benchmark scenes, and comparisons show improved accuracy in scene\ngeometry and photometric reconstruction, while reducing the training time by 3\n- 5 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arnab Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmine_Y/0/1/0/all/0/1\">Yassine Ahmine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew I. Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plane Geometry Diagram Parsing. (arXiv:2205.09363v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09363","description":"<p>Geometry diagram parsing plays a key role in geometry problem solving,\nwherein the primitive extraction and relation parsing remain challenging due to\nthe complex layout and between-primitive relationship. In this paper, we\npropose a powerful diagram parser based on deep learning and graph reasoning.\nSpecifically, a modified instance segmentation method is proposed to extract\ngeometric primitives, and the graph neural network (GNN) is leveraged to\nrealize relation parsing and primitive classification incorporating geometric\nfeatures and prior knowledge. All the modules are integrated into an end-to-end\nmodel called PGDPNet to perform all the sub-tasks simultaneously. In addition,\nwe build a new large-scale geometry diagram dataset named PGDP5K with primitive\nlevel annotations. Experiments on PGDP5K and an existing dataset IMP-Geometry3K\nshow that our model outperforms state-of-the-art methods in four sub-tasks\nremarkably. Our code, dataset and appendix material are available at\nhttps://github.com/mingliangzhang2018/PGDP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming-Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yi-Han Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-Lin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection. (arXiv:2205.09373v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09373","description":"<p>As an inherently ill-posed problem, depth estimation from single images is\nthe most challenging part of monocular 3D object detection (M3OD). Many\nexisting methods rely on preconceived assumptions to bridge the missing spatial\ninformation in monocular images, and predict a sole depth value for every\nobject of interest. However, these assumptions do not always hold in practical\napplications. To tackle this problem, we propose a depth solving system that\nfully explores the visual clues from the subtasks in M3OD and generates\nmultiple estimations for the depth of each target. Since the depth estimations\nrely on different assumptions in essence, they present diverse distributions.\nEven if some assumptions collapse, the estimations established on the remaining\nassumptions are still reliable. In addition, we develop a depth selection and\ncombination strategy. This strategy is able to remove abnormal estimations\ncaused by collapsed assumptions, and adaptively combine the remaining\nestimations into a single one. In this way, our depth solving system becomes\nmore precise and robust. Exploiting the clues from multiple subtasks of M3OD\nand without introducing any extra information, our method surpasses the current\nbest method by more than 20% relatively on the Moderate level of test split in\nthe KITTI 3D object detection benchmark, while still maintaining real-time\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lihui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video. (arXiv:2205.09382v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09382","description":"<p>Predicting fetal weight at birth is an important aspect of perinatal care,\nparticularly in the context of antenatal management, which includes the planned\ntiming and the mode of delivery. Accurate prediction of weight using prenatal\nultrasound is challenging as it requires images of specific fetal body parts\nduring advanced pregnancy which is difficult to capture due to poor quality of\nimages caused by the lack of amniotic fluid. As a consequence, predictions\nwhich rely on standard methods often suffer from significant errors. In this\npaper we propose the Residual Transformer Module which extends a 3D\nResNet-based network for analysis of 2D+t spatio-temporal ultrasound video\nscans. Our end-to-end method, called BabyNet, automatically predicts fetal\nbirth weight based on fetal ultrasound video scans. We evaluate BabyNet using a\ndedicated clinical set comprising 225 2D fetal ultrasound videos of pregnancies\nfrom 75 patients performed one day prior to delivery. Experimental results show\nthat BabyNet outperforms several state-of-the-art methods and estimates the\nweight at birth with accuracy comparable to human experts. Furthermore,\ncombining estimates provided by human experts with those computed by BabyNet\nyields the best results, outperforming either of other methods by a significant\nmargin. The source code of BabyNet is available at\nhttps://github.com/SanoScience/BabyNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzeszczyk_M/0/1/0/all/0/1\">Micha&#x142; K. Grzeszczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brawura_Biskupski_Samaha_R/0/1/0/all/0/1\">Robert Brawura-Biskupski-Samaha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutaj_P/0/1/0/all/0/1\">Pawe&#x142; Gutaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unconventional Visual Sensors for Autonomous Vehicles. (arXiv:2205.09383v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09383","description":"<p>Autonomous vehicles rely on perception systems to understand their\nsurroundings for further navigation missions. Cameras are essential for\nperception systems due to the advantages of object detection and recognition\nprovided by modern computer vision algorithms, comparing to other sensors, such\nas LiDARs and radars. However, limited by its inherent imaging principle, a\nstandard RGB camera may perform poorly in a variety of adverse scenarios,\nincluding but not limited to: low illumination, high contrast, bad weather such\nas fog/rain/snow, etc. Meanwhile, estimating the 3D information from the 2D\nimage detection is generally more difficult when compared to LiDARs or radars.\nSeveral new sensing technologies have emerged in recent years to address the\nlimitations of conventional RGB cameras. In this paper, we review the\nprinciples of four novel image sensors: infrared cameras, range-gated cameras,\npolarization cameras, and event cameras. Their comparative advantages, existing\nor potential applications, and corresponding data processing algorithms are all\npresented in a systematic manner. We expect that this study will assist\npractitioners in the autonomous driving society with new perspectives and\ninsights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">You Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibanez_Guzman_J/0/1/0/all/0/1\">Javier Ibanez-Guzman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIF: An Objective Quality Assessment for Underwater Image Enhancement. (arXiv:2205.09392v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09392","description":"<p>Due to complex and volatile lighting environment, underwater imaging can be\nreadily impaired by light scattering, warping, and noises. To improve the\nvisual quality, Underwater Image Enhancement (UIE) techniques have been widely\nstudied. Recent efforts have also been contributed to evaluate and compare the\nUIE performances with subjective and objective methods. However, the subjective\nevaluation is time-consuming and uneconomic for all images, while existing\nobjective methods have limited capabilities for the newly-developed UIE\napproaches based on deep learning. To fill this gap, we propose an Underwater\nImage Fidelity (UIF) metric for objective evaluation of enhanced underwater\nimages. By exploiting the statistical features of these images, we present to\nextract naturalness-related, sharpness-related, and structure-related features.\nAmong them, the naturalness-related and sharpness-related features evaluate\nvisual improvement of enhanced images; the structure-related feature indicates\nstructural similarity between images before and after UIE. Then, we employ\nsupport vector regression to fuse the above three features into a final UIF\nmetric. In addition, we have also established a large-scale UIE database with\nsubjective scores, namely Underwater Image Enhancement Database (UIED), which\nis utilized as a benchmark to compare all objective metrics. Experimental\nresults confirm that the proposed UIF outperforms a variety of underwater and\ngeneral-purpose image quality metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yannan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongfu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oracle-MNIST: a Realistic Image Dataset for Benchmarking Machine Learning Algorithms. (arXiv:2205.09442v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09442","description":"<p>We introduce the Oracle-MNIST dataset, comprising of 28$\\times $28 grayscale\nimages of 30,222 ancient characters from 10 categories, for benchmarking\npattern classification, with particular challenges on image noise and\ndistortion. The training set totally consists of 27,222 images, and the test\nset contains 300 images per class. Oracle-MNIST shares the same data format\nwith the original MNIST dataset, allowing for direct compatibility with all\nexisting classifiers and systems, but it constitutes a more challenging\nclassification task than MNIST. The images of ancient characters suffer from 1)\nextremely serious and unique noises caused by three-thousand years of burial\nand aging and 2) dramatically variant writing styles by ancient Chinese, which\nall make them realistic for machine learning research. The dataset is freely\navailable at https://github.com/wm-bupt/oracle-mnist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PYSKL: Towards Good Practices for Skeleton Action Recognition. (arXiv:2205.09443v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09443","description":"<p>We present PYSKL: an open-source toolbox for skeleton-based action\nrecognition based on PyTorch. The toolbox supports a wide variety of skeleton\naction recognition algorithms, including approaches based on GCN and CNN. In\ncontrast to existing open-source skeleton action recognition projects that\ninclude only one or two algorithms, PYSKL implements six different algorithms\nunder a unified framework with both the latest and original good practices to\nease the comparison of efficacy and efficiency. We also provide an original\nGCN-based skeleton action recognition model named ST-GCN++, which achieves\ncompetitive recognition performance without any complicated attention schemes,\nserving as a strong baseline. Meanwhile, PYSKL supports the training and\ntesting of nine skeleton-based action recognition benchmarks and achieves\nstate-of-the-art recognition performance on eight of them. To facilitate future\nresearch on skeleton action recognition, we also provide a large number of\ntrained models and detailed benchmark results to give some insights. PYSKL is\nreleased at https://github.com/kennymckormick/pyskl and is actively maintained.\nWe will update this report when we add new features or benchmarks. The current\nversion corresponds to PYSKL v0.2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Enhancement Transformer for Action Segmentation. (arXiv:2205.09445v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09445","description":"<p>Temporal convolutions have been the paradigm of choice in action\nsegmentation, which enhances long-term receptive fields by increasing\nconvolution layers. However, high layers cause the loss of local information\nnecessary for frame recognition. To solve the above problem, a novel\nencoder-decoder structure is proposed in this paper, called Cross-Enhancement\nTransformer. Our approach can be effective learning of temporal structure\nrepresentation with interactive self-attention mechanism. Concatenated each\nlayer convolutional feature maps in encoder with a set of features in decoder\nproduced via self-attention. Therefore, local and global information are used\nin a series of frame actions simultaneously. In addition, a new loss function\nis proposed to enhance the training process that penalizes over-segmentation\nerrors. Experiments show that our framework performs state-of-the-art on three\nchallenging datasets: 50Salads, Georgia Tech Egocentric Activities and the\nBreakfast dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shanna Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Augmentation Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes. (arXiv:2205.09448v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09448","description":"<p>Many scenes in real life can be abstracted to the sparse reward visual\nscenes, where it is difficult for an agent to tackle the task under the\ncondition of only accepting images and sparse rewards. We propose to decompose\nthis problem into two sub-problems: the visual representation and the sparse\nreward. To address them, a novel framework IAMMIR combining the self-supervised\nrepresentation learning with the intrinsic motivation is presented. For visual\nrepresentation, a representation driven by a combination of the imageaugmented\nforward dynamics and the reward is acquired. For sparse rewards, a new type of\nintrinsic reward is designed, the Momentum Memory Intrinsic Reward (MMIR). It\nutilizes the difference of the outputs from the current model (online network)\nand the historical model (target network) to present the agent's state\nfamiliarity. Our method is evaluated on the visual navigation task with sparse\nrewards in Vizdoom. Experiments demonstrate that our method achieves the state\nof the art performance in sample efficiency, at least 2 times faster than the\nexisting methods reaching 100% success rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Biao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Feature Fusion for Unsupervised Domain Adaptive Person Re-identification. (arXiv:2205.09495v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09495","description":"<p>Unsupervised domain adaptive (UDA) person re-identification (ReID) has gained\nincreasing attention for its effectiveness on the target domain without manual\nannotations. Most fine-tuning based UDA person ReID methods focus on encoding\nglobal features for pseudo labels generation, neglecting the local feature that\ncan provide for the fine-grained information. To handle this issue, we propose\na Learning Feature Fusion (LF2) framework for adaptively learning to fuse\nglobal and local features to obtain a more comprehensive fusion feature\nrepresentation. Specifically, we first pre-train our model within a source\ndomain, then fine-tune the model on unlabeled target domain based on the\nteacher-student training strategy. The average weighting teacher network is\ndesigned to encode global features, while the student network updating at each\niteration is responsible for fine-grained local features. By fusing these\nmulti-view features, multi-level clustering is adopted to generate diverse\npseudo labels. In particular, a learnable Fusion Module (FM) for giving\nprominence to fine-grained local information within the global feature is also\nproposed to avoid obscure learning of multiple pseudo labels. Experiments show\nthat our proposed LF2 framework outperforms the state-of-the-art with 73.5% mAP\nand 83.7% Rank1 on Market1501 to DukeMTMC-ReID, and achieves 83.2% mAP and\n92.8% Rank1 on DukeMTMC-ReID to Market1501.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xue Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Transferability of Adversarial Examples via a Few Queries. (arXiv:2205.09518v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09518","description":"<p>Due to the vulnerability of deep neural networks, the black-box attack has\ndrawn great attention from the community. Though transferable priors decrease\nthe query number of the black-box query attacks in recent efforts, the average\nnumber of queries is still larger than 100, which is easily affected by the\nnumber of queries limit policy. In this work, we propose a novel method called\nquery prior-based method to enhance the family of fast gradient sign methods\nand improve their attack transferability by using a few queries. Specifically,\nfor the untargeted attack, we find that the successful attacked adversarial\nexamples prefer to be classified as the wrong categories with higher\nprobability by the victim model. Therefore, the weighted augmented\ncross-entropy loss is proposed to reduce the gradient angle between the\nsurrogate model and the victim model for enhancing the transferability of the\nadversarial examples. Theoretical analysis and extensive experiments\ndemonstrate that our method could significantly improve the transferability of\ngradient-based adversarial attacks on CIFAR10/100 and ImageNet and outperform\nthe black-box query attack with the same few queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the ultrasound attenuation coefficient using convolutional neural networks -- a feasibility study. (arXiv:2205.09533v1 [physics.med-ph])","link":"http://arxiv.org/abs/2205.09533","description":"<p>Attenuation coefficient (AC) is a fundamental measure of tissue acoustical\nproperties, which can be used in medical diagnostics. In this work, we\ninvestigate the feasibility of using convolutional neural networks (CNNs) to\ndirectly estimate AC from radio-frequency (RF) ultrasound signals. To develop\nthe CNNs we used RF signals collected from tissue mimicking numerical phantoms\nfor the AC values in a range from 0.1 to 1.5 dB/(MHz*cm). The models were\ntrained based on 1-D patches of RF data. We obtained mean absolute AC\nestimation errors of 0.08, 0.12, 0.20, 0.25 for the patch lengths: 10 mm, 5 mm,\n2 mm and 1 mm, respectively. We explain the performance of the model by\nvisualizing the frequency content associated with convolutional filters. Our\nstudy presents that the AC can be calculated using deep learning, and the\nweights of the CNNs can have physical interpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Jarosik_P/0/1/0/all/0/1\">Piotr Jarosik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Byra_M/0/1/0/all/0/1\">Michal Byra</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lewandowski_M/0/1/0/all/0/1\">Marcin Lewandowski</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klimonda_Z/0/1/0/all/0/1\">Ziemowit Klimonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning. (arXiv:2205.09542v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09542","description":"<p>In this work, we tackle the challenging problem of arbitrary image style\ntransfer using a novel style feature representation learning method. A suitable\nstyle representation, as a key component in image stylization tasks, is\nessential to achieve satisfactory results. Existing deep neural network based\napproaches achieve reasonable results with the guidance from second-order\nstatistics such as Gram matrix of content features. However, they do not\nleverage sufficient style information, which results in artifacts such as local\ndistortions and style inconsistency. To address these issues, we propose to\nlearn style representation directly from image features instead of their\nsecond-order statistics, by analyzing the similarities and differences between\nmultiple styles and considering the style distribution. Specifically, we\npresent Contrastive Arbitrary Style Transfer (CAST), which is a new style\nrepresentation learning and style transfer method via contrastive learning. Our\nframework consists of three key components, i.e., a multi-layer style projector\nfor style code encoding, a domain enhancement module for effective learning of\nstyle distribution, and a generative network for image style transfer. We\nconduct qualitative and quantitative evaluations comprehensively to demonstrate\nthat our approach achieves significantly better results compared to those\nobtained via state-of-the-art methods. Code and models are available at\nhttps://github.com/zyxElsa/CAST_pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong-Yee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Dynamic Functional Brain Networks via Spatial and Channel-wise Attention. (arXiv:2205.09576v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09576","description":"<p>Using deep learning models to recognize functional brain networks (FBNs) in\nfunctional magnetic resonance imaging (fMRI) has been attracting increasing\ninterest recently. However, most existing work focuses on detecting static FBNs\nfrom entire fMRI signals, such as correlation-based functional connectivity.\nSliding-window is a widely used strategy to capture the dynamics of FBNs, but\nit is still limited in representing intrinsic functional interactive dynamics\nat each time step. And the number of FBNs usually need to be set manually. More\nover, due to the complexity of dynamic interactions in brain, traditional\nlinear and shallow models are insufficient in identifying complex and spatially\noverlapped FBNs across each time step. In this paper, we propose a novel\nSpatial and Channel-wise Attention Autoencoder (SCAAE) for discovering FBNs\ndynamically. The core idea of SCAAE is to apply attention mechanism to FBNs\nconstruction. Specifically, we designed two attention modules: 1) spatial-wise\nattention (SA) module to discover FBNs in the spatial domain and 2) a\nchannel-wise attention (CA) module to weigh the channels for selecting the FBNs\nautomatically. We evaluated our approach on ADHD200 dataset and our results\nindicate that the proposed SCAAE method can effectively recover the dynamic\nchanges of the FBNs at each fMRI time step, without using sliding windows. More\nimportantly, our proposed hybrid attention modules (SA and CA) do not enforce\nassumptions of linearity and independence as previous methods, and thus provide\na novel approach to better understanding dynamic functional brain networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_E/0/1/0/all/0/1\">Enjie Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRT-ViT: TensorRT-oriented Vision Transformer. (arXiv:2205.09579v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09579","description":"<p>We revisit the existing excellent Transformers from the perspective of\npractical application. Most of them are not even as efficient as the basic\nResNets series and deviate from the realistic deployment scenario. It may be\ndue to the current criterion to measure computation efficiency, such as FLOPs\nor parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this\npaper directly treats the TensorRT latency on the specific hardware as an\nefficiency metric, which provides more comprehensive feedback involving\ncomputational capacity, memory cost, and bandwidth. Based on a series of\ncontrolled experiments, this work derives four practical guidelines for\nTensorRT-oriented and deployment-friendly network design, e.g., early CNN and\nlate Transformer at stage-level, early Transformer and late CNN at block-level.\nAccordingly, a family of TensortRT-oriented Transformers is presented,\nabbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT\nsignificantly outperforms existing ConvNets and vision Transformers with\nrespect to the latency/accuracy trade-off across diverse visual tasks, e.g.,\nimage classification, object detection and semantic segmentation. For example,\nat 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin\nand 2.0$\\times$ faster than Twins. On the MS-COCO object detection task,\nTRT-ViT achieves comparable performance with Twins, while the inference speed\nis increased by 2.8$\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Trace of PGD-Like Adversarial Attacks. (arXiv:2205.09586v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09586","description":"<p>Adversarial attacks pose safety and security concerns for deep learning\napplications. Yet largely imperceptible, a strong PGD-like attack may leave\nstrong trace in the adversarial example. Since attack triggers the local\nlinearity of a network, we speculate network behaves in different extents of\nlinearity for benign examples and adversarial examples. Thus, we construct\nAdversarial Response Characteristics (ARC) features to reflect the model's\ngradient consistency around the input to indicate the extent of linearity.\nUnder certain conditions, it shows a gradually varying pattern from benign\nexample to adversarial example, as the later leads to Sequel Attack Effect\n(SAE). ARC feature can be used for informed attack detection (perturbation\nmagnitude is known) with binary classifier, or uninformed attack detection\n(perturbation magnitude is unknown) with ordinal regression. Due to the\nuniqueness of SAE to PGD-like attacks, ARC is also capable of inferring other\nattack details such as loss function, or the ground-truth label as a\npost-processing defense. Qualitative and quantitative evaluations manifest the\neffectiveness of ARC feature on CIFAR-10 w/ ResNet-18 and ImageNet w/\nResNet-152 and SwinT-B-IN1K with considerable generalization among PGD-like\nattacks despite domain shift. Our method is intuitive, light-weighted,\nnon-intrusive, and data-undemanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable Physical Attack against Object Detection with Separable Attention. (arXiv:2205.09592v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09592","description":"<p>Transferable adversarial attack is always in the spotlight since deep\nlearning models have been demonstrated to be vulnerable to adversarial samples.\nHowever, existing physical attack methods do not pay enough attention on\ntransferability to unseen models, thus leading to the poor performance of\nblack-box attack.In this paper, we put forward a novel method of generating\nphysically realizable adversarial camouflage to achieve transferable attack\nagainst detection models. More specifically, we first introduce multi-scale\nattention maps based on detection models to capture features of objects with\nvarious resolutions. Meanwhile, we adopt a sequence of composite\ntransformations to obtain the averaged attention maps, which could curb\nmodel-specific noise in the attention and thus further boost transferability.\nUnlike the general visualization interpretation methods where model attention\nshould be put on the foreground object as much as possible, we carry out attack\non separable attention from the opposite perspective, i.e. suppressing\nattention of the foreground and enhancing that of the background. Consequently,\ntransferable adversarial camouflage could be yielded efficiently with our novel\nattention-based loss function. Extensive comparison experiments verify the\nsuperiority of our method to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YongQian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_K/0/1/0/all/0/1\">Kangcheng Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiahao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1\">Ping Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Feature Expansion Unit for 3D Point Cloud Upsampling. (arXiv:2205.09594v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09594","description":"<p>Recently, deep learning methods have shown great success in 3D point cloud\nupsampling. Among these methods, many feature expansion units were proposed to\ncomplete point expansion at the end. In this paper, we compare various feature\nexpansion units by both theoretical analysis and quantitative experiments. We\nshow that most of the existing feature expansion units process each point\nfeature independently, while ignoring the feature interaction among different\npoints. Further, inspired by upsampling module of image super-resolution and\nrecent success of dynamic graph CNN on point clouds, we propose a novel feature\nexpansion units named ProEdgeShuffle. Experiments show that our proposed method\ncan achieve considerable improvement over previous feature expansion units.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORPS: Cost-free Rigorous Pseudo-labeling based on Similarity-ranking for Brain MRI Segmentation. (arXiv:2205.09601v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09601","description":"<p>Segmentation of brain magnetic resonance images (MRI) is crucial for the\nanalysis of the human brain and diagnosis of various brain disorders. The\ndrawbacks of time-consuming and error-prone manual delineation procedures are\naimed to be alleviated by atlas-based and supervised machine learning methods\nwhere the former methods are computationally intense and the latter methods\nlack a sufficiently large number of labeled data. With this motivation, we\npropose CORPS, a semi-supervised segmentation framework built upon a novel\natlas-based pseudo-labeling method and a 3D deep convolutional neural network\n(DCNN) for 3D brain MRI segmentation. In this work, we propose to generate\nexpert-level pseudo-labels for unlabeled set of images in an order based on a\nlocal intensity-based similarity score to existing labeled set of images and\nusing a novel atlas-based label fusion method. Then, we propose to train a 3D\nDCNN on the combination of expert and pseudo labeled images for binary\nsegmentation of each anatomical structure. The binary segmentation approach is\nproposed to avoid the poor performance of multi-class segmentation methods on\nlimited and imbalanced data. This also allows to employ a lightweight and\nefficient 3D DCNN in terms of the number of filters and reserve memory\nresources for training the binary networks on full-scale and full-resolution 3D\nMRI volumes instead of 2D/3D patches or 2D slices. Thus, the proposed framework\ncan encapsulate the spatial contiguity in each dimension and enhance\ncontext-awareness. The experimental results demonstrate the superiority of the\nproposed framework over the baseline method both qualitatively and\nquantitatively without additional labeling cost for manual labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sari_C/0/1/0/all/0/1\">Can Taylan Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1\">Simon K. Warfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network. (arXiv:2205.09612v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09612","description":"<p>In this paper, we propose a Classification Confidence Network (CLCNet) that\ncan determine whether the classification model classifies input samples\ncorrectly. It can take a classification result in the form of vector in any\ndimension, and return a confidence score as output, which represents the\nprobability of an instance being classified correctly. We can utilize CLCNet in\na simple cascade structure system consisting of several SOTA (state-of-the-art)\nclassification models, and our experiments show that the system can achieve the\nfollowing advantages: 1. The system can customize the average computation\nrequirement (FLOPs) per image while inference. 2. Under the same computation\nrequirement, the performance of the system can exceed any model that has\nidentical structure with the model in the system, but different in size. In\nfact, this is a new type of ensemble modeling. Like general ensemble modeling,\nit can achieve higher performance than single classification model, yet our\nsystem requires much less computation than general ensemble modeling. We have\nuploaded our code to a github repository:\nhttps://github.com/yaoching0/CLCNet-Rethinking-of-Ensemble-Modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao-Ching Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Shi-Jinn Horng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integral Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection. (arXiv:2205.09613v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09613","description":"<p>Modern object detectors have taken the advantages of pre-trained vision\ntransformers by using them as backbone networks. However, except for the\nbackbone networks, other detector components, such as the detector head and the\nfeature pyramid network, remain randomly initialized, which hinders the\nconsistency between detectors and pre-trained models. In this study, we propose\nto integrally migrate the pre-trained transformer encoder-decoders (imTED) for\nobject detection, constructing a feature extraction-operation path that is not\nonly \"fully pre-trained\" but also consistent with pre-trained models. The\nessential improvements of imTED over existing transformer-based detectors are\ntwofold: (1) it embeds the pre-trained transformer decoder to the detector\nhead; and (2) it removes the feature pyramid network from the feature\nextraction path. Such improvements significantly reduce the proportion of\nrandomly initialized parameters and enhance the generation capability of\ndetectors. Experiments on MS COCO dataset demonstrate that imTED consistently\noutperforms its counterparts by ~2.8% AP. Without bells and whistles, imTED\nimproves the state-of-the-art of few-shot object detection by up to 7.6% AP,\ndemonstrating significantly higher generalization capability. Code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaosong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zonghao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXACT: How to Train Your Accuracy. (arXiv:2205.09615v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09615","description":"<p>Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, Hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on image classification show that the proposed optimization method\nis a powerful alternative to widely used classification losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_I/0/1/0/all/0/1\">Ivan Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dereka_S/0/1/0/all/0/1\">Stanislav Dereka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1\">Sergey Kolesnikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Image Modeling with Denoising Contrast. (arXiv:2205.09616v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09616","description":"<p>Since the development of self-supervised visual representation learning from\ncontrastive learning to masked image modeling, there is no significant\ndifference in essence, that is, how to design proper pretext tasks for vision\ndictionary look-up. Masked image modeling recently dominates this line of\nresearch with state-of-the-art performance on vision Transformers, where the\ncore is to enhance the patch-level visual context capturing of the network via\ndenoising auto-encoding mechanism. Rather than tailoring image tokenizers with\nextra training stages as in previous works, we unleash the great potential of\ncontrastive learning on denoising auto-encoding and introduce a new\npre-training method, ConMIM, to produce simple intra-image inter-patch\ncontrastive constraints as the learning objectives for masked patch prediction.\nWe further strengthen the denoising mechanism with asymmetric designs,\nincluding image perturbations and model progress rates, to improve the network\npre-training. ConMIM-pretrained vision Transformers with various scales achieve\npromising results on downstream image classification, semantic segmentation,\nobject detection, and instance segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topological Approach for Semi-Supervised Learning. (arXiv:2205.09617v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09617","description":"<p>Nowadays, Machine Learning and Deep Learning methods have become the\nstate-of-the-art approach to solve data classification tasks. In order to use\nthose methods, it is necessary to acquire and label a considerable amount of\ndata; however, this is not straightforward in some fields, since data\nannotation is time consuming and might require expert knowledge. This challenge\ncan be tackled by means of semi-supervised learning methods that take advantage\nof both labelled and unlabelled data. In this work, we present new\nsemi-supervised learning methods based on techniques from Topological Data\nAnalysis (TDA), a field that is gaining importance for analysing large amounts\nof data with high variety and dimensionality. In particular, we have created\ntwo semi-supervised learning methods following two different topological\napproaches. In the former, we have used a homological approach that consists in\nstudying the persistence diagrams associated with the data using the Bottleneck\nand Wasserstein distances. In the latter, we have taken into account the\nconnectivity of the data. In addition, we have carried out a thorough analysis\nof the developed methods using 3 synthetic datasets, 5 structured datasets, and\n2 datasets of images. The results show that the semi-supervised methods\ndeveloped in this work outperform both the results obtained with models trained\nwith only manually labelled data, and those obtained with classical\nsemi-supervised learning methods, reaching improvements of up to a 16%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ines_A/0/1/0/all/0/1\">Adri&#xe1;n In&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_C/0/1/0/all/0/1\">C&#xe9;sar Dom&#xed;nguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heras_J/0/1/0/all/0/1\">J&#xf3;nathan Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_G/0/1/0/all/0/1\">Gadea Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubio_J/0/1/0/all/0/1\">Julio Rubio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Adversarial Attacks. (arXiv:2205.09624v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09624","description":"<p>Recent advances in machine learning show that neural models are vulnerable to\nminimally perturbed inputs, or adversarial examples. Adversarial algorithms are\noptimization problems that minimize the accuracy of ML models by perturbing\ninputs, often using a model's loss function to craft such perturbations.\nState-of-the-art object detection models are characterized by very large output\nmanifolds due to the number of possible locations and sizes of objects in an\nimage. This leads to their outputs being sparse and optimization problems that\nuse them incur a lot of unnecessary computation.\n</p>\n<p>We propose to use a very limited subset of a model's learned manifold to\ncompute adversarial examples. Our \\textit{Focused Adversarial Attacks} (FA)\nalgorithm identifies a small subset of sensitive regions to perform\ngradient-based adversarial attacks. FA is significantly faster than other\ngradient-based attacks when a model's manifold is sparsely activated. Also, its\nperturbations are more efficient than other methods under the same perturbation\nconstraints. We evaluate FA on the COCO 2017 and Pascal VOC 2007 detection\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1\">Thomas Cilloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1\">Charles Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1\">Charles Fleming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A graph-transformer for whole slide image classification. (arXiv:2205.09671v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09671","description":"<p>Deep learning is a powerful tool for whole slide image (WSI) analysis.\nTypically, when performing supervised deep learning, a WSI is divided into\nsmall patches, trained and the outcomes are aggregated to estimate disease\ngrade. However, patch-based methods introduce label noise during training by\nassuming that each patch is independent with the same label as the WSI and\nneglect overall WSI-level information that is significant in disease grading.\nHere we present a Graph-Transformer (GT) that fuses a graph-based\nrepresentation of an WSI and a vision transformer for processing pathology\nimages, called GTP, to predict disease grade. We selected $4,818$ WSIs from the\nClinical Proteomic Tumor Analysis Consortium (CPTAC), the National Lung\nScreening Trial (NLST), and The Cancer Genome Atlas (TCGA), and used GTP to\ndistinguish adenocarcinoma (LUAD) and squamous cell carcinoma (LSCC) from\nadjacent non-cancerous tissue (normal). First, using NLST data, we developed a\ncontrastive learning framework to generate a feature extractor. This allowed us\nto compute feature vectors of individual WSI patches, which were used to\nrepresent the nodes of the graph followed by construction of the GTP framework.\nOur model trained on the CPTAC data achieved consistently high performance on\nthree-label classification (normal versus LUAD versus LSCC: mean accuracy$=\n91.2$ $\\pm$ $2.5\\%$) based on five-fold cross-validation, and mean accuracy $=\n82.3$ $\\pm$ $1.0\\%$ on external test data (TCGA). We also introduced a\ngraph-based saliency mapping technique, called GraphCAM, that can identify\nregions that are highly associated with the class label. Our findings\ndemonstrate GTP as an interpretable and effective deep learning framework for\nWSI-level classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gindra_R/0/1/0/all/0/1\">Rushin H. Gindra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_E/0/1/0/all/0/1\">Emily J. Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burks_E/0/1/0/all/0/1\">Eric J. Burks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betke_M/0/1/0/all/0/1\">Margrit Betke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_J/0/1/0/all/0/1\">Jennifer E. Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Greedy Search: Tracking by Multi-Agent Reinforcement Learning-based Beam Search. (arXiv:2205.09676v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09676","description":"<p>Existing trackers usually select a location or proposal with the maximum\nscore as tracking result for each frame. However, such greedy search scheme\nmaybe not the optimal choice, especially when encountering challenging tracking\nscenarios like heavy occlusions and fast motion. Since the accumulated errors\nwould make response scores not reliable anymore. In this paper, we propose a\nnovel multi-agent reinforcement learning based beam search strategy (termed\nBeamTracking) to address this issue. Specifically, we formulate the tracking as\na sample selection problem fulfilled by multiple parallel decision-making\nprocesses, each of which aims at picking out one sample as their tracking\nresult in each frame. We take the target feature, proposal feature, and its\nresponse score as state, and also consider actions predicted by nearby agent,\nto train multi-agents to select their actions. When all the frames are\nprocessed, we select the trajectory with the maximum accumulated score as the\ntracking result. Extensive experiments on seven popular tracking benchmark\ndatasets validated the effectiveness of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning for Image Classification using Compact Networks in the BioMedical Context. (arXiv:2205.09678v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09678","description":"<p>The development of mobile and on the edge applications that embed deep\nconvolutional neural models has the potential to revolutionise biomedicine.\nHowever, most deep learning models require computational resources that are not\navailable in smartphones or edge devices; an issue that can be faced by means\nof compact models. The problem with such models is that they are, at least\nusually, less accurate than bigger models. In this work, we study how this\nlimitation can be addressed with the application of semi-supervised learning\ntechniques. We conduct several statistical analyses to compare performance of\ndeep compact architectures when trained using semi-supervised learning methods\nfor tackling image classification tasks in the biomedical context. In\nparticular, we explore three families of compact networks, and two families of\nsemi-supervised learning techniques for 10 biomedical tasks. By combining\nsemi-supervised learning methods with compact networks, it is possible to\nobtain a similar performance to standard size networks. In general, the best\nresults are obtained when combining data distillation with MixNet, and plain\ndistillation with ResNet-18. Also, in general, NAS networks obtain better\nresults than manually designed networks and quantized networks. The work\npresented in this paper shows the benefits of apply semi-supervised methods to\ncompact networks; this allow us to create compact models that are not only as\naccurate as standard size models, but also faster and lighter. Finally, we have\ndeveloped a library that simplifies the construction of compact models using\nsemi-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ines_A/0/1/0/all/0/1\">Adri&#xe1;n In&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Pinto_A/0/1/0/all/0/1\">Andr&#xe9;s D&#xed;az-Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_C/0/1/0/all/0/1\">C&#xe9;sar Dom&#xed;nguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heras_J/0/1/0/all/0/1\">J&#xf3;nathan Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_E/0/1/0/all/0/1\">Eloy Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_V/0/1/0/all/0/1\">Vico Pascual</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VNT-Net: Rotational Invariant Vector Neuron Transformers. (arXiv:2205.09690v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09690","description":"<p>Learning 3D point sets with rotational invariance is an important and\nchallenging problem in machine learning. Through rotational invariant\narchitectures, 3D point cloud neural networks are relieved from requiring a\ncanonical global pose and from exhaustive data augmentation with all possible\nrotations. In this work, we introduce a rotational invariant neural network by\ncombining recently introduced vector neurons with self-attention layers to\nbuild a point cloud vector neuron transformer network (VNT-Net). Vector neurons\nare known for their simplicity and versatility in representing SO(3) actions\nand are thereby incorporated in common neural operations. Similarly,\nTransformer architectures have gained popularity and recently were shown\nsuccessful for images by applying directly on sequences of image patches and\nachieving superior performance and convergence. In order to benefit from both\nworlds, we combine the two structures by mainly showing how to adapt the\nmulti-headed attention layers to comply with vector neurons operations. Through\nthis adaptation attention layers become SO(3) and the overall network becomes\nrotational invariant. Experiments demonstrate that our network efficiently\nhandles 3D point cloud objects in arbitrary poses. We also show that our\nnetwork achieves higher accuracy when compared to related state-of-the-art\nmethods and requires less training due to a smaller number of hyperparameters\nin common classification and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zisling_H/0/1/0/all/0/1\">Hedi Zisling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharf_A/0/1/0/all/0/1\">Andrei Sharf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"k-strip: A novel segmentation algorithm in k-space for the application of skull stripping. (arXiv:2205.09706v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09706","description":"<p>Objectives: Present a novel deep learning-based skull stripping algorithm for\nmagnetic resonance imaging (MRI) that works directly in the information rich\nk-space.\n</p>\n<p>Materials and Methods: Using two datasets from different institutions with a\ntotal of 36,900 MRI slices, we trained a deep learning-based model to work\ndirectly with the complex raw k-space data. Skull stripping performed by HD-BET\n(Brain Extraction Tool) in the image domain were used as the ground truth.\n</p>\n<p>Results: Both datasets were very similar to the ground truth (DICE scores of\n92\\%-98\\% and Hausdorff distances of under 5.5 mm). Results on slices above the\neye-region reach DICE scores of up to 99\\%, while the accuracy drops in regions\naround the eyes and below, with partially blurred output. The output of k-strip\noften smoothed edges at the demarcation to the skull. Binary masks are created\nwith an appropriate threshold.\n</p>\n<p>Conclusion: With this proof-of-concept study, we were able to show the\nfeasibility of working in the k-space frequency domain, preserving phase\ninformation, with consistent results. Future research should be dedicated to\ndiscovering additional ways the k-space can be used for innovative image\nanalysis and further workflows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rempe_M/0/1/0/all/0/1\">Moritz Rempe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mentzel_F/0/1/0/all/0/1\">Florian Mentzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pomykala_K/0/1/0/all/0/1\">Kelsey L. Pomykala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haubold_J/0/1/0/all/0/1\">Johannes Haubold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nensa_F/0/1/0/all/0/1\">Felix Nensa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kroninger_K/0/1/0/all/0/1\">Kevin Kr&#xf6;ninger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-LSTM Scoring Based Similarity Measurement with Agglomerative Hierarchical Clustering (AHC) for Speaker Diarization. (arXiv:2205.09709v1 [eess.AS])","link":"http://arxiv.org/abs/2205.09709","description":"<p>Majority of speech signals across different scenarios are never available\nwith well-defined audio segments containing only a single speaker. A typical\nconversation between two speakers consists of segments where their voices\noverlap, interrupt each other or halt their speech in between multiple\nsentences. Recent advancements in diarization technology leverage neural\nnetwork-based approaches to improvise multiple subsystems of speaker\ndiarization system comprising of extracting segment-wise embedding features and\ndetecting changes in the speaker during conversation. However, to identify\nspeaker through clustering, models depend on methodologies like PLDA to\ngenerate similarity measure between two extracted segments from a given\nconversational audio. Since these algorithms ignore the temporal structure of\nconversations, they tend to achieve a higher Diarization Error Rate (DER), thus\nleading to misdetections both in terms of speaker and change identification.\nTherefore, to compare similarity of two speech segments both independently and\nsequentially, we propose a Bi-directional Long Short-term Memory network for\nestimating the elements present in the similarity matrix. Once the similarity\nmatrix is generated, Agglomerative Hierarchical Clustering (AHC) is applied to\nfurther identify speaker segments based on thresholding. To evaluate the\nperformance, Diarization Error Rate (DER%) metric is used. The proposed model\nachieves a low DER of 34.80% on a test set of audio samples derived from ICSI\nMeeting Corpus as compared to traditional PLDA based similarity measurement\nmechanism which achieved a DER of 39.90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nijhawan_S/0/1/0/all/0/1\">Siddharth S. Nijhawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beigi_H/0/1/0/all/0/1\">Homayoon Beigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-informed Language Grounding. (arXiv:2205.09710v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09710","description":"<p>Natural language applied to natural 2D images describes a fundamentally 3D\nworld. We present the Voxel-informed Language Grounder (VLG), a language\ngrounding model that leverages 3D geometric information in the form of voxel\nmaps derived from the visual input using a volumetric reconstruction model. We\nshow that VLG significantly improves grounding accuracy on SNARE, an object\nreference game task. At the time of writing, VLG holds the top place on the\nSNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corona_R/0/1/0/all/0/1\">Rodolfo Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shizhan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light In The Black: An Evaluation of Data Augmentation Techniques for COVID-19 CT's Semantic Segmentation. (arXiv:2205.09722v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09722","description":"<p>With the COVID-19 global pandemic, computer-assisted diagnoses of medical\nimages have gained much attention, and robust methods of Semantic Segmentation\nof Computed Tomography (CT) became highly desirable. Semantic Segmentation of\nCT is one of many research fields of automatic detection of COVID-19 and has\nbeen widely explored since the COVID-19 outbreak. In this work, we propose an\nextensive analysis of how different data augmentation techniques improve the\ntraining of encoder-decoder neural networks on this problem. Twenty different\ndata augmentation techniques were evaluated on five different datasets. Each\ndataset was validated through a five-fold cross-validation strategy, thus\nresulting in over 3,000 experiments. Our findings show that spatial level\ntransformations are the most promising to improve the learning of neural\nnetworks on this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krinski_B/0/1/0/all/0/1\">Bruno A. Krinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1\">Daniel V. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todt_E/0/1/0/all/0/1\">Eduardo Todt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Efficient Medical Imaging with Self-Supervision. (arXiv:2205.09723v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09723","description":"<p>Recent progress in Medical Artificial Intelligence (AI) has delivered systems\nthat can reach clinical expert level performance. However, such systems tend to\ndemonstrate sub-optimal \"out-of-distribution\" performance when evaluated in\nclinical settings different from the training environment. A common mitigation\nstrategy is to develop separate systems for each clinical setting using\nsite-specific data [1]. However, this quickly becomes impractical as medical\ndata is time-consuming to acquire and expensive to annotate [2]. Thus, the\nproblem of \"data-efficient generalization\" presents an ongoing difficulty for\nMedical AI development. Although progress in representation learning shows\npromise, their benefits have not been rigorously studied, specifically for\nout-of-distribution settings. To meet these challenges, we present REMEDIS, a\nunified representation learning strategy to improve robustness and\ndata-efficiency of medical imaging AI. REMEDIS uses a generic combination of\nlarge-scale supervised transfer learning with self-supervised learning and\nrequires little task-specific customization. We study a diverse range of\nmedical imaging tasks and simulate three realistic application scenarios using\nretrospective data. REMEDIS exhibits significantly improved in-distribution\nperformance with up to 11.5% relative improvement in diagnostic accuracy over a\nstrong supervised baseline. More importantly, our strategy leads to strong\ndata-efficient generalization of medical imaging AI, matching strong supervised\nbaselines using between 1% to 33% of retraining data across tasks. These\nresults suggest that REMEDIS can significantly accelerate the life-cycle of\nmedical imaging AI development thereby presenting an important step forward for\nmedical imaging AI to deliver broad impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1\">Laura Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1\">Jan Freyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1\">Sebastien Baur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacWilliams_P/0/1/0/all/0/1\">Patricia MacWilliams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1\">S. Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulczyn_E/0/1/0/all/0/1\">Ellery Wulczyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_B/0/1/0/all/0/1\">Boris Babenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Megan Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_A/0/1/0/all/0/1\">Aaron Loh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Hsuan Cameron Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavishi_P/0/1/0/all/0/1\">Pinal Bavishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkens_J/0/1/0/all/0/1\">Jim Winkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhijit Guha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_Z/0/1/0/all/0/1\">Zach Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1\">Fiona Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krogue_J/0/1/0/all/0/1\">Justin Krogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telang_U/0/1/0/all/0/1\">Umesh Telang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1\">Dale R. Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unified Keyframe Propagation Models. (arXiv:2205.09731v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09731","description":"<p>Many video editing tasks such as rotoscoping or object removal require the\npropagation of context across frames. While transformers and other\nattention-based approaches that aggregate features globally have demonstrated\ngreat success at propagating object masks from keyframes to the whole video,\nthey struggle to propagate high-frequency details such as textures faithfully.\nWe hypothesize that this is due to an inherent bias of global attention towards\nlow-frequency features. To overcome this limitation, we present a two-stream\napproach, where high-frequency features interact locally and low-frequency\nfeatures interact globally. The global interaction stream remains robust in\ndifficult situations such as large camera motions, where explicit alignment\nfails. The local interaction stream propagates high-frequency details through\ndeformable feature aggregation and, informed by the global interaction stream,\nlearns to detect and correct errors of the deformation field. We evaluate our\ntwo-stream approach for inpainting tasks, where experiments show that it\nimproves both the propagation of features within a single frame as required for\nimage inpainting, as well as their propagation from keyframes to target frames.\nApplied to video inpainting, our approach leads to 44% and 26% improvements in\nFID and LPIPS scores. Code at https://github.com/runwayml/guided-inpainting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_P/0/1/0/all/0/1\">Peter Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Soumyadip Sengupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Weight Averaging for Out-of-Distribution Generalization. (arXiv:2205.09739v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09739","description":"<p>Standard neural networks struggle to generalize under distribution shifts.\nFor out-of-distribution generalization in computer vision, the best current\napproach averages the weights along a training run. In this paper, we propose\nDiverse Weight Averaging (DiWA) that makes a simple change to this strategy:\nDiWA averages the weights obtained from several independent training runs\nrather than from a single run. Perhaps surprisingly, averaging these weights\nperforms well under soft constraints despite the network's nonlinearities. The\nmain motivation behind DiWA is to increase the functional diversity across\naveraged models. Indeed, models obtained from different runs are more diverse\nthan those collected along a single run thanks to differences in\nhyperparameters and training procedures. We motivate the need for diversity by\na new bias-variance-covariance-locality decomposition of the expected error,\nexploiting similarities between DiWA and standard functional ensembling.\nMoreover, this decomposition highlights that DiWA succeeds when the variance\nterm dominates, which we show happens when the marginal distribution changes at\ntest time. Experimentally, DiWA consistently improves the state of the art on\nthe competitive DomainBed benchmark without inference overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchmeyer_M/0/1/0/all/0/1\">Matthieu Kirchmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahier_T/0/1/0/all/0/1\">Thibaud Rahier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1\">Alain Rakotomamonjy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving. (arXiv:2205.09743v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09743","description":"<p>In this paper, we present BEVerse, a unified framework for 3D perception and\nprediction based on multi-camera systems. Unlike existing studies focusing on\nthe improvement of single-task approaches, BEVerse features in producing\nspatio-temporal Birds-Eye-View (BEV) representations from multi-camera videos\nand jointly reasoning about multiple tasks for vision-centric autonomous\ndriving. Specifically, BEVerse first performs shared feature extraction and\nlifting to generate 4D BEV representations from multi-timestamp and multi-view\nimages. After the ego-motion alignment, the spatio-temporal encoder is utilized\nfor further feature extraction in BEV. Finally, multiple task decoders are\nattached for joint reasoning and prediction. Within the decoders, we propose\nthe grid sampler to generate BEV features with different ranges and\ngranularities for different tasks. Also, we design the method of iterative flow\nfor memory-efficient future prediction. We show that the temporal information\nimproves 3D object detection and semantic map construction, while the\nmulti-task learning can implicitly benefit motion prediction. With extensive\nexperiments on the nuScenes dataset, we show that the multi-task BEVerse\noutperforms existing single-task methods on 3D object detection, semantic map\nconstruction, and motion prediction. Compared with the sequential paradigm,\nBEVerse also favors in significantly improved efficiency. The code and trained\nmodels will be released at https://github.com/zhangyp15/BEVerse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HandoverSim: A Simulation Framework and Benchmark for Human-to-Robot Object Handovers. (arXiv:2205.09747v1 [cs.RO])","link":"http://arxiv.org/abs/2205.09747","description":"<p>We introduce a new simulation benchmark \"HandoverSim\" for human-to-robot\nobject handovers. To simulate the giver's motion, we leverage a recent motion\ncapture dataset of hand grasping of objects. We create training and evaluation\nenvironments for the receiver with standardized protocols and metrics. We\nanalyze the performance of a set of baselines and show a correlation with a\nreal-world evaluation. Code is open sourced at https://handover-sim.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1\">Yu-Wei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaralingam_B/0/1/0/all/0/1\">Balakumar Sundaralingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1\">Adithyavairavan Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakmak_M/0/1/0/all/0/1\">Maya Cakmak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Generalized Zero-Shot Learning Methods. (arXiv:2011.08641v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08641","description":"<p>Generalized zero-shot learning (GZSL) aims to train a model for classifying\ndata samples under the condition that some output classes are unknown during\nsupervised learning. To address this challenging task, GZSL leverages semantic\ninformation of the seen (source) and unseen (target) classes to bridge the gap\nbetween both seen and unseen classes. Since its introduction, many GZSL models\nhave been formulated. In this review paper, we present a comprehensive review\non GZSL. Firstly, we provide an overview of GZSL including the problems and\nchallenges. Then, we introduce a hierarchical categorization for the GZSL\nmethods and discuss the representative methods in each category. In addition,\nwe discuss the available benchmark data sets and applications of GZSL, along\nwith a discussion on the research gaps and directions for future\ninvestigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourpanah_F/0/1/0/all/0/1\">Farhad Pourpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinlei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1\">Chee Peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi-Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12815","description":"<p>Convolutional neural networks (CNNs) have been tremendously successful in\nsolving imaging inverse problems. To understand their success, an effective\nstrategy is to construct simpler and mathematically more tractable\nconvolutional sparse coding (CSC) models that share essential ingredients with\nCNNs. Existing CSC methods, however, underperform leading CNNs in challenging\ninverse problems. We hypothesize that the performance gap may be attributed in\npart to how they process images at different spatial scales: While many CNNs\nuse multiscale feature representations, existing CSC models mostly rely on\nsingle-scale dictionaries. To close the performance gap, we thus propose a\nmultiscale convolutional dictionary structure. The proposed dictionary\nstructure is derived from the U-Net, arguably the most versatile and widely\nused CNN for image-to-image learning problems. We show that incorporating the\nproposed multiscale dictionary in an otherwise standard CSC framework yields\nperformance competitive with state-of-the-art CNNs across a range of\nchallenging inverse problems including CT and MRI reconstruction. Our work thus\ndemonstrates the effectiveness and scalability of the multiscale CSC approach\nin solving challenging inverse problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1\">David Belius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and Localization. (arXiv:2012.01821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01821","description":"<p>Recently, many detection methods based on convolutional neural networks\n(CNNs) have been proposed for image splicing forgery detection. Most of these\ndetection methods focus on the local patches or local objects. In fact, image\nsplicing forgery detection is a global binary classification task that\ndistinguishes the tampered and non-tampered regions by image fingerprints.\nHowever, some specific image contents are hardly retained by CNN-based\ndetection networks, but if included, would improve the detection accuracy of\nthe networks. To resolve these issues, we propose a novel network called\ndual-encoder U-Net (D-Unet) for image splicing forgery detection, which employs\nan unfixed encoder and a fixed encoder. The unfixed encoder autonomously learns\nthe image fingerprints that differentiate between the tampered and non-tampered\nregions, whereas the fixed encoder intentionally provides the direction\ninformation that assists the learning and detection of the network. This\ndual-encoder is followed by a spatial pyramid global-feature extraction module\nthat expands the global insight of D-Unet for classifying the tampered and\nnon-tampered regions more accurately. In an experimental comparison study of\nD-Unet and state-of-the-art methods, D-Unet outperformed the other methods in\nimage-level and pixel-level detection, without requiring pre-training or\ntraining on a large number of forgery images. Moreover, it was stably robust to\ndifferent attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiuli Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ranglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Domain Generalization in Underwater Object Detection by Domain Mixup and Contrastive Learning. (arXiv:2104.02230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02230","description":"<p>The performance of existing underwater object detection methods degrades\nseriously when facing domain shift caused by complicated underwater\nenvironments. Due to the limitation of the number of domains in the dataset,\ndeep detectors easily memorize a few seen domains, which leads to low\ngeneralization ability. There are two common ideas to improve the domain\ngeneralization performance. First, it can be inferred that the detector trained\non as many domains as possible is domain-invariant. Second, for the images with\nthe same semantic content in different domains, their hidden features should be\nequivalent. This paper further excavates these two ideas and proposes a domain\ngeneralization framework (named DMC) that learns how to generalize across\ndomains from Domain Mixup and Contrastive Learning. First, based on the\nformation of underwater images, an image in an underwater environment is the\nlinear transformation of another underwater environment. Thus, a style transfer\nmodel, which outputs a linear transformation matrix instead of the whole image,\nis proposed to transform images from one source domain to another, enriching\nthe domain diversity of the training data. Second, mixup operation interpolates\ndifferent domains on the feature level, sampling new domains on the domain\nmanifold. Third, contrastive loss is selectively applied to features from\ndifferent domains to force the model to learn domain invariant features but\nretain the discriminative capacity. With our method, detectors will be robust\nto domain shift. Also, a domain generalization benchmark S-UODAC2020 for\ndetection is set up to measure the performance of our method. Comprehensive\nexperiments on S-UODAC2020 and two object recognition benchmarks (PACS and\nVLCS) demonstrate that the proposed method is able to learn domain-invariant\nrepresentations, and outperforms other domain generalization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Linhui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1\">Peipei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Noisy Neural Networks: A Bayesian Approach with Compensation. (arXiv:2105.10699v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.10699","description":"<p>Deep neural networks (DNNs) with noisy weights, which we refer to as noisy\nneural networks (NoisyNNs), arise from the training and inference of DNNs in\nthe presence of noise. NoisyNNs emerge in many new applications, including the\nwireless transmission of DNNs, the efficient deployment or storage of DNNs in\nanalog devices, and the truncation or quantization of DNN weights. This paper\nstudies a fundamental problem of NoisyNNs: how to reconstruct the DNN weights\nfrom their noisy manifestations. While all prior works relied on the maximum\nlikelihood (ML) estimation, this paper puts forth a denoising approach to\nreconstruct DNNs with the aim of maximizing the inference accuracy of the\nreconstructed models. The superiority of our denoiser is rigorously proven in\ntwo small-scale problems, wherein we consider a quadratic neural network\nfunction and a shallow feedforward neural network, respectively. When applied\nto advanced learning tasks with modern DNN architectures, our denoiser exhibits\nsignificantly better performance than the ML estimator. Consider the average\ntest accuracy of the denoised DNN model versus the weight variance to noise\npower ratio (WNR) performance. When denoising a noisy ResNet34 model arising\nfrom noisy inference, our denoiser outperforms ML estimation by up to 4.1 dB to\nachieve a test accuracy of 60%.When denoising a noisy ResNet18 model arising\nfrom noisy training, our denoiser outperforms ML estimation by 13.4 dB and 8.3\ndB to achieve test accuracies of 60% and 80%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Soung Chang Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BR-NPA: A Non-Parametric High-Resolution Attention Model to improve the Interpretability of Attention. (arXiv:2106.02566v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02566","description":"<p>The prevalence of employing attention mechanisms has brought along concerns\non the interpretability of attention distributions. Although it provides\ninsights about how a model is operating, utilizing attention as the explanation\nof model predictions is still highly dubious. The community is still seeking\nmore interpretable strategies for better identifying local active regions that\ncontribute the most to the final decision. To improve the interpretability of\nexisting attention models, we propose a novel Bilinear Representative\nNon-Parametric Attention (BR-NPA) strategy that captures the task-relevant\nhuman-interpretable information. The target model is first distilled to have\nhigher-resolution intermediate feature maps. From which, representative\nfeatures are then grouped based on local pairwise feature similarity, to\nproduce finer-grained, more precise attention maps highlighting task-relevant\nparts of the input. The obtained attention maps are ranked according to the\nactivity level of the compound feature, which provides information regarding\nthe important level of the highlighted regions. The proposed model can be\neasily adapted in a wide variety of modern deep models, where classification is\ninvolved. Extensive quantitative and qualitative experiments showcase more\ncomprehensive and accurate visual explanations compared to state-of-the-art\nattention models and visualizations methods across multiple tasks including\nfine-grained image classification, few-shot classification, and person\nre-identification, without compromising the classification accuracy. The\nproposed visualization model sheds imperative light on how neural networks `pay\ntheir attention' differently in different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1\">Tristan Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Suiyi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1\">Thomas Fr&#xe9;our</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1\">Harold Mouch&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration. (arXiv:2108.03443v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03443","description":"<p>Deformable image registration (DIR), aiming to find spatial correspondence\nbetween images, is one of the most critical problems in the domain of medical\nimage analysis. In this paper, we present a novel, generic, and accurate\ndiffeomorphic image registration framework that utilizes neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Our method\nleverages deep neural networks for their expressive power in modeling dynamical\nsystems, and simultaneously optimizes for a dynamical system between the image\npairs and the corresponding transformation. Our formulation allows various\nconstraints to be imposed along the transformation to maintain desired\nregularities. Our experiment results show that our method outperforms the\nbenchmarks under various metrics. Additionally, we demonstrate the feasibility\nto expand our framework to register multiple image sets using a unified form of\ntransformation,which could possibly serve a wider range of applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A. Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C. Gee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Monitoring System using Social Distancing and Face Mask Detection on Surveillance video datasets. (arXiv:2110.03905v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03905","description":"<p>In the current times, the fear and danger of COVID-19 virus still stands\nlarge. Manual monitoring of social distancing norms is impractical with a large\npopulation moving about and with insufficient task force and resources to\nadminister them. There is a need for a lightweight, robust and 24X7\nvideo-monitoring system that automates this process. This paper proposes a\ncomprehensive and effective solution to perform person detection, social\ndistancing violation detection, face detection and face mask classification\nusing object detection, clustering and Convolution Neural Network (CNN) based\nbinary classifier. For this, YOLOv3, Density-based spatial clustering of\napplications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and\nMobileNetV2 based binary classifier have been employed on surveillance video\ndatasets. This paper also provides a comparative study of different face\ndetection and face mask classification models. Finally, a video dataset\nlabelling method is proposed along with the labelled video dataset to\ncompensate for the lack of dataset in the community and is used for evaluation\nof the system. The system performance is evaluated in terms of accuracy, F1\nscore as well as the prediction time, which has to be low for practical\napplicability. The system performs with an accuracy of 91.2% and F1 score of\n90.79% on the labelled video dataset and has an average prediction time of 7.12\nseconds for 78 frames of a video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sahana Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1\">Rujula Singh R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biradar_R/0/1/0/all/0/1\">Ruchita Biradar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nikhil Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05706","description":"<p>This paper unifies the multi-focus images fusion (MFIF) and blind super\nresolution (SR) problems as the multi-focus image super resolution fusion\n(MFISRF) task, and proposes a novel unified dataset-free unsupervised framework\nnamed deep fusion prior (DFP) to address such MFISRF task. DFP consists of\nSKIPnet network, DoubleReblur focus measurement tactic, decision embedding\nmodule and loss functions. In particular, DFP can obtain MFISRF only from two\nlow-resolution inputs without any extent dataset; SKIPnet implementing\nunsupervised learning via deep image prior is an end-to-end generated network\nacting as the engine of DFP; DoubleReblur is used to determine the primary\ndecision map without learning but based on estimated PSF and Gaussian kernels\nconvolution; decision embedding module optimizes the decision map via learning;\nand DFP losses composed of content loss, joint gradient loss and gradient limit\nloss can obtain high-quality MFISRF results robustly. Experiments have proved\nthat our proposed DFP approaches and even outperforms those state-of-art MFIF\nand SR method combinations. Additionally, DFP is a general framework, thus its\nnetworks and focus measurement tactics can be continuously updated to further\nimprove the MFISRF performance. DFP codes are open source and will be available\nsoon at <a href=\"http://github.com/GuYuanjie/DeepFusionPrior.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBSegment: Fast and robust segmentation of deep brain structures -- Evaluation of transportability across acquisition domains. (arXiv:2110.09473v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09473","description":"<p>Segmenting deep brain structures from magnetic resonance images is important\nfor patient diagnosis, surgical planning, and research. Most current\nstate-of-the-art solutions follow a segmentation-by-registration approach,\nwhere subject MRIs are mapped to a template with well-defined segmentations.\nHowever, registration-based pipelines are time-consuming, thus, limiting their\nclinical use. This paper uses deep learning to provide a robust and efficient\ndeep brain segmentation solution. The method consists of a pre-processing step\nto conform all MRI images to the same orientation, followed by a convolutional\nneural network using the nnU-Net framework. We use a total of 14 datasets from\nboth research and clinical collections. Of these, seven were used for training\nand validation and seven were retained for independent testing. We trained the\nnetwork to segment 30 deep brain structures, as well as a brain mask, using\nlabels generated from a registration-based approach. We evaluated the\ngeneralizability of the network by performing a leave-one-dataset-out\ncross-validation, and extensive testing on external datasets. Furthermore, we\nassessed cross-domain transportability by evaluating the results separately on\ndifferent domains. We achieved an average DSC of 0.89 $\\pm$ 0.04 on the\nindependent testing datasets when compared to the registration-based gold\nstandard. On our test system, the computation time decreased from 42 minutes\nfor a reference registration-based pipeline to 1 minute. Our proposed method is\nfast, robust, and generalizes with high reliability. It can be extended to the\nsegmentation of other brain structures. The method is publicly available on\nGitHub, as well as a pip package for convenient usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baniasadi_M/0/1/0/all/0/1\">Mehri Baniasadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petersen_M/0/1/0/all/0/1\">Mikkel V. Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goncalves_J/0/1/0/all/0/1\">Jorge Goncalves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horn_A/0/1/0/all/0/1\">Andreas Horn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vlasov_V/0/1/0/all/0/1\">Vanja Vlasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hertel_F/0/1/0/all/0/1\">Frank Hertel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Husch_A/0/1/0/all/0/1\">Andreas Husch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Human Evaluation for Image Captioning. (arXiv:2111.08940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08940","description":"<p>We establish THumB, a rubric-based human evaluation protocol for image\ncaptioning models. Our scoring rubrics and their definitions are carefully\ndeveloped based on machine- and human-generated captions on the MSCOCO dataset.\nEach caption is evaluated along two main dimensions in a tradeoff (precision\nand recall) as well as other aspects that measure the text quality (fluency,\nconciseness, and inclusive language). Our evaluations demonstrate several\ncritical problems of the current evaluation practice. Human-generated captions\nshow substantially higher quality than machine-generated ones, especially in\ncoverage of salient information (i.e., recall), while most automatic metrics\nsay the opposite. Our rubric-based results reveal that CLIPScore, a recent\nmetric that uses image features, better correlates with human judgments than\nconventional text-only metrics because it is more sensitive to recall. We hope\nthat this work will promote a more transparent evaluation protocol for image\ncaptioning and its automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1\">Lavinia Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1\">Jacob Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-pass Object-adaptive Data Undersampling and Reconstruction for MRI. (arXiv:2111.09212v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.09212","description":"<p>There is much recent interest in techniques to accelerate the data\nacquisition process in MRI by acquiring limited measurements. Often\nsophisticated reconstruction algorithms are deployed to maintain high image\nquality in such settings. In this work, we propose a data-driven sampler using\na convolutional neural network, MNet, to provide object-specific sampling\npatterns adaptive to each scanned object. The network observes very limited\nlow-frequency k-space data for each object and rapidly predicts the desired\nundersampling pattern in one go that achieves high image reconstruction\nquality. We propose an accompanying alternating-type training framework with a\nmask-backward procedure that efficiently generates training labels for the\nsampler network and jointly trains an image reconstruction network.\nExperimental results on the fastMRI knee dataset demonstrate the ability of the\nproposed learned undersampling network to generate object-specific masks at\nfourfold and eightfold acceleration that achieve superior image reconstruction\nperformance than several existing schemes. The source code for the proposed\njoint sampling and reconstruction learning framework is available at\nhttps://github.com/zhishenhuang/mri.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhishen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1\">Saiprasad Ravishankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning. (arXiv:2112.04564v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04564","description":"<p>In this paper, we propose a novel co-learning framework (CoSSL) with\ndecoupled representation learning and classifier learning for imbalanced SSL.\nTo handle the data imbalance, we devise Tail-class Feature Enhancement (TFE)\nfor classifier learning. Furthermore, the current evaluation protocol for\nimbalanced SSL focuses only on balanced test sets, which has limited\npracticality in real-world scenarios. Therefore, we further conduct a\ncomprehensive evaluation under various shifted test distributions. In\nexperiments, we show that our approach outperforms other methods over a large\nrange of shifted distributions, achieving state-of-the-art performance on\nbenchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1\">Anna Kukleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Skeleton-Based Human Activity Discovery Using Particle Swarm Optimization with Gaussian Mutation. (arXiv:2201.05314v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05314","description":"<p>Human activity discovery aims to cluster the activities performed by humans,\nwithout any prior information of what defines each activity. Most methods\npresented in human activity recognition are supervised, where there are labeled\ninputs to train the system. In reality, it is difficult to label activities\ndata because of its huge volume and the variety of activities performed by\nhumans. In this paper, an unsupervised approach is proposed to perform human\nactivity discovery in 3D skeleton sequences. First, important frames are\nselected based on kinetic energy. Next, the displacement of joints, statistical\ndisplacements, angles, and orientation features are extracted to represent the\nactivities information. Since not all extracted features have useful\ninformation, the dimension of features is reduced using PCA. Most human\nactivity discovery proposed are not fully unsupervised. They use pre-segmented\nvideos before categorizing activities. To deal with this, we have used sliding\ntime window to segment the time series of activities with some overlapping.\nThen, activities are discovered by a hybrid particle swarm optimization with\nGaussian mutation algorithm to provide diverse solutions. Finally, k-means is\napplied to the outcome centroids from each iteration of the PSO to overcome the\nslow convergence rate of PSO. Experiments on three datasets have been presented\nand the results show the proposed method has superior performance in\ndiscovering activities in compared to the other state-of-the-art methods and\nhas increased accuracy of at least 4 % on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hadikhani_P/0/1/0/all/0/1\">Parham Hadikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_D/0/1/0/all/0/1\">Daphne Teck Ching Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1\">Wee-Hong Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10737","description":"<p>Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: (1) existing\nmethods fail to capture the important features of the images due to the naive\ntokenization scheme; (2) the models suffer from information loss because they\nonly consider single-scale feature representations; and (3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCASTformer, a novel type of generative adversarial transformers, for 2D medical\nimage segmentation. First, we take advantage of the pyramid structure to\nconstruct multi-scale representations and handle multi-scale variations. We\nthen design a novel class-aware transformer module to better learn the\ndiscriminative regions of objects with semantic structures. Lastly, we utilize\nan adversarial training strategy that boosts segmentation accuracy and\ncorrespondingly allows a transformer-based discriminator to capture high-level\nsemantically correlated contents and low-level anatomical features. Our\nexperiments demonstrate that CASTformer dramatically outperforms previous\nstate-of-the-art transformer-based approaches on three benchmarks, obtaining\n2.54%-5.88% absolute improvements in Dice over previous models. Further\nqualitative experiments provide a more detailed picture of the model's inner\nworkings, shed light on the challenges in improved transparency, and\ndemonstrate that transfer learning can greatly improve performance and reduce\nthe size of medical image datasets in training, making CASTformer a strong\nstarting point for downstream medical image analysis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinchali_S/0/1/0/all/0/1\">Sandeep Chinchali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04533","description":"<p>Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zesong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingwenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Real-time Image Reconstruction with Neural Networks for MRI-guided Radiotherapy. (arXiv:2202.05267v2 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2202.05267","description":"<p>MRI-guidance techniques that dynamically adapt radiation beams to follow\ntumor motion in real-time will lead to more accurate cancer treatments and\nreduced collateral healthy tissue damage. The gold-standard for reconstruction\nof undersampled MR data is compressed sensing (CS) which is computationally\nslow and limits the rate that images can be available for real-time adaptation.\nHere, we demonstrate the use of automated transform by manifold approximation\n(AUTOMAP), a generalized framework that maps raw MR signal to the target image\ndomain, to rapidly reconstruct images from undersampled radial k-space data.\nThe AUTOMAP neural network was trained to reconstruct images from a\ngolden-angle radial acquisition, a benchmark for motion-sensitive imaging, on\nlung cancer patient data and generic images from ImageNet. Model training was\nsubsequently augmented with motion-encoded k-space data derived from videos in\nthe YouTube-8M dataset to encourage motion robust reconstruction. We find that\nAUTOMAP-reconstructed radial k-space has equivalent accuracy to CS but with\nmuch shorter processing times after initial fine-tuning on retrospectively\nacquired lung cancer patient data. Validation of motion-trained models with a\nvirtual dynamic lung tumor phantom showed that the generalized motion\nproperties learned from YouTube lead to improved target tracking accuracy. Our\nwork shows that AUTOMAP can achieve real-time, accurate reconstruction of\nradial data. These findings imply that neural-network-based reconstruction is\npotentially superior to existing approaches for real-time image guidance\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Waddington_D/0/1/0/all/0/1\">David E. J. Waddington</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hindley_N/0/1/0/all/0/1\">Nicholas Hindley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koonjoo_N/0/1/0/all/0/1\">Neha Koonjoo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chiu_C/0/1/0/all/0/1\">Christopher Chiu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reynolds_T/0/1/0/all/0/1\">Tess Reynolds</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_P/0/1/0/all/0/1\">Paul Z. Y. Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhu_B/0/1/0/all/0/1\">Bo Zhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhutto_D/0/1/0/all/0/1\">Danyal Bhutto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paganelli_C/0/1/0/all/0/1\">Chiara Paganelli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Keall_P/0/1/0/all/0/1\">Paul J. Keall</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rosen_M/0/1/0/all/0/1\">Matthew S. Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11094","description":"<p>Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 52.3\\% mIoU on the PASCAL VOC 2012 and 22.4\\% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. We\nopen-source our code at\n\\href{https://github.com/NVlabs/GroupViT}{https://github.com/NVlabs/GroupViT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09581","description":"<p>Following the successful application of vision transformers in multiple\ncomputer vision tasks, these models have drawn the attention of the signal\nprocessing community. This is because signals are often represented as\nspectrograms (e.g. through Discrete Fourier Transform) which can be directly\nprovided as input to vision transformers. However, naively applying\ntransformers to spectrograms is suboptimal. Since the axes represent distinct\ndimensions, i.e. frequency and time, we argue that a better approach is to\nseparate the attention dedicated to each axis. To this end, we propose the\nSeparable Transformer (SepTr), an architecture that employs two transformer\nblocks in a sequential manner, the first attending to tokens within the same\nfrequency bin, and the second attending to tokens within the same time\ninterval. We conduct experiments on three benchmark data sets, showing that our\nseparable architecture outperforms conventional vision transformers and other\nstate-of-the-art methods. Unlike standard transformers, SepTr linearly scales\nthe number of trainable parameters with the input size, thus having a lower\nmemory footprint. Our code is available as open source at\nhttps://github.com/ristea/septr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2203.17066","description":"<p>Gesture recognition is one of the most intuitive ways of interaction and has\ngathered particular attention for human computer interaction. Radar sensors\npossess multiple intrinsic properties, such as their ability to work in low\nillumination, harsh weather conditions, and being low-cost and compact, making\nthem highly preferable for a gesture recognition solution. However, most\nliterature work focuses on solutions with a limited range that is lower than a\nmeter. We propose a novel architecture for a long-range (1m - 2m) gesture\nrecognition solution that leverages a point cloud-based cross-learning approach\nfrom camera point cloud to 60-GHz FMCW radar point cloud, which allows learning\nbetter representations while suppressing noise. We use a variant of Dynamic\nGraph CNN (DGCNN) for the cross-learning, enabling us to model relationships\nbetween the points at a local and global level and to model the temporal\ndynamics a Bi-LSTM network is employed. In the experimental results section, we\ndemonstrate our model's overall accuracy of 98.4% for five gestures and its\ngeneralization capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hazra_S/0/1/0/all/0/1\">Souvik Hazra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiprit_G/0/1/0/all/0/1\">Gamze Naz Kiprit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stephan_M/0/1/0/all/0/1\">Michael Stephan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Servadei_L/0/1/0/all/0/1\">Lorenzo Servadei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wille_R/0/1/0/all/0/1\">Robert Wille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weigel_R/0/1/0/all/0/1\">Robert Weigel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santra_A/0/1/0/all/0/1\">Avik Santra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. (arXiv:2204.02964v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02964","description":"<p>We present an approach to efficiently and effectively adapt a masked image\nmodeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object\ndetection, which is based on our two novel observations: (i) A MIM pre-trained\nvanilla ViT encoder can work surprisingly well in the challenging object-level\nrecognition scenario even with randomly sampled partial observations, e.g.,\nonly 25% $\\sim$ 50% of the input embeddings. (ii) In order to construct\nmulti-scale representations for object detection from single-scale ViT, a\nrandomly initialized compact convolutional stem supplants the pre-trained large\nkernel patchify stem, and its intermediate features can naturally serve as the\nhigher resolution inputs of a feature pyramid network without further\nupsampling or other manipulations. While the pre-trained ViT is only regarded\nas the 3$^{rd}$-stage of our detector's backbone instead of the whole feature\nextractor. This results in a ConvNet-ViT hybrid feature extractor. The proposed\ndetector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform\nhierarchical Swin Transformer by 2.5 box AP and 2.6 mask AP on COCO, and\nachieves better results compared with the previous best adapted vanilla ViT\ndetector using a more modest fine-tuning recipe while converging 2.8$\\times$\nfaster. Code and pre-trained models are available at\nhttps://github.com/hustvl/MIMDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Signatures. (arXiv:2204.07953v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07953","description":"<p>In this work we investigate the use of the Signature Transform in the context\nof Learning. Under this assumption, we advance a supervised framework that\npotentially provides state-of-the-art classification accuracy with the use of\nfew labels without the need of credit assignment and with minimal or no\noverfitting. We leverage tools from harmonic analysis by the use of the\nsignature and log-signature, and use as a score function RMSE and MAE Signature\nand log-signature. We develop a closed-form equation to compute probably good\noptimal scale factors, as well as the formulation to obtain them by\noptimization. Techniques of Signal Processing are addressed to further\ncharacterize the problem. Classification is performed at the CPU level orders\nof magnitude faster than other methods. We report results on AFHQ, MNIST and\nCIFAR10, achieving 100% accuracy on all tasks assuming we can determine at test\ntime which probably good optimal scale factor to use for each category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1\">J. de Curt&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1\">I. de Zarz&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calafate_C/0/1/0/all/0/1\">Carlos T. Calafate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary latent space search for driving human portrait generation. (arXiv:2204.11887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11887","description":"<p>This article presents an evolutionary approach for synthetic human portraits\ngeneration based on the latent space exploration of a generative adversarial\nnetwork. The idea is to produce different human face images very similar to a\ngiven target portrait. The approach applies StyleGAN2 for portrait generation\nand FaceNet for face similarity evaluation. The evolutionary search is based on\nexploring the real-coded latent space of StyleGAN2. The main results over both\nsynthetic and real images indicate that the proposed approach generates\naccurate and diverse solutions, which represent realistic human portraits. The\nproposed research can contribute to improving the security of face recognition\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machin_B/0/1/0/all/0/1\">Benjam&#xed;n Mach&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesmachnow_S/0/1/0/all/0/1\">Sergio Nesmachnow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (arXiv:2205.00199v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2205.00199","description":"<p>Recently, how to protect the Intellectual Property (IP) of deep neural\nnetworks (DNN) becomes a major concern for the AI industry. To combat potential\nmodel piracy, recent works explore various watermarking strategies to embed\nsecret identity messages into the prediction behaviors or the internals (e.g.,\nweights and neuron activation) of the target model. Sacrificing less\nfunctionality and involving more knowledge about the target model, the latter\nbranch of watermarking schemes (i.e., white-box model watermarking) is claimed\nto be accurate, credible and secure against most known watermark removal\nattacks, with emerging research efforts and applications in the industry.\n</p>\n<p>In this paper, we present the first effective removal attack which cracks\nalmost all the existing white-box watermarking schemes with provably no\nperformance overhead and no required prior knowledge. By analyzing these IP\nprotection mechanisms at the granularity of neurons, we for the first time\ndiscover their common dependence on a set of fragile features of a local neuron\ngroup, all of which can be arbitrarily tampered by our proposed chain of\ninvariant neuron transforms. On $9$ state-of-the-art white-box watermarking\nschemes and a broad set of industry-level DNN architectures, our attack for the\nfirst time reduces the embedded identity message in the protected models to be\nalmost random. Meanwhile, unlike known removal attacks, our attack requires no\nprior knowledge on the training data distribution or the adopted watermark\nalgorithms, and leaves model functionality intact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yifan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xudong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01920","description":"<p>Multi-modal aerial view object classification (MAVOC) in Automatic target\nrecognition (ATR), although an important and challenging problem, has been\nunder studied. This paper firstly finds that fine-grained data, class imbalance\nand various shooting conditions preclude the representational ability of\ngeneral image classification. Moreover, the MAVOC dataset has scene aggregation\ncharacteristics. By exploiting these properties, we propose Scene Clustering\nBased Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to\nemploy in post-processing. The SCP-Label brings greater accuracy by assigning\nthe same label to objects within the same scene while also mitigating bias and\nconfusion with model ensembles. Its performance surpasses the official baseline\nby a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on\nTrack 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.\nFinally, we win the championship both on Track1 and Track2 in the CVPR 2022\nPerception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our\ncode is available at https://github.com/HowieChangchn/SCP-Label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shenshen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.02152","description":"<p>Recent studies indicate that detecting radiographic patterns on CT scans can\nyield high sensitivity and specificity for Covid-19 localization. In this\npaper, we investigate the appropriateness of deep learning models\ntransferability, for semantic segmentation of pneumonia-infected areas in CT\nimages. Transfer learning allows for the fast initialization/reutilization of\ndetection models, given that large volumes of training data are not available.\nOur work explores the efficacy of using pre-trained U-Net architectures, on a\nspecific CT data set, for identifying Covid-19 side-effects over images from\ndifferent datasets. Experimental results indicate improvement in the\nsegmentation accuracy of identifying Covid-19 infected regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maganaris_C/0/1/0/all/0/1\">Constantine Maganaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakalos_N/0/1/0/all/0/1\">Nikolaos Bakalos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalogeras_D/0/1/0/all/0/1\">Dimitris Kalogeras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angeli_A/0/1/0/all/0/1\">Aikaterini Angeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnrealNAS: Can We Search Neural Architectures with Unreal Data?. (arXiv:2205.02162v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02162","description":"<p>Neural architecture search (NAS) has shown great success in the automatic\ndesign of deep neural networks (DNNs). However, the best way to use data to\nsearch network architectures is still unclear and under exploration. Previous\nwork has analyzed the necessity of having ground-truth labels in NAS and\ninspired broad interest. In this work, we take a further step to question\nwhether real data is necessary for NAS to be effective. The answer to this\nquestion is important for applications with limited amount of accessible data,\nand can help people improve NAS by leveraging the extra flexibility of data\ngeneration. To explore if NAS needs real data, we construct three types of\nunreal datasets using: 1) randomly labeled real images; 2) generated images and\nlabels; and 3) generated Gaussian noise with random labels. These datasets\nfacilitate to analyze the generalization and expressivity of the searched\narchitectures. We study the performance of architectures searched on these\nconstructed datasets using popular differentiable NAS methods. Extensive\nexperiments on CIFAR, ImageNet and CheXpert show that the searched\narchitectures can achieve promising results compared with those derived from\nthe conventional NAS pipeline with real labeled data, suggesting the\nfeasibility of performing NAS with unreal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaicheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03146","description":"<p>The unabated mystique of large-scale neural networks, such as the CLIP dual\nimage-and-text encoder, popularized automatically generated art. Increasingly\nmore sophisticated generators enhanced the artworks' realism and visual\nappearance, and creative prompt engineering enabled stylistic expression.\nGuided by an artist-in-the-loop ideal, we design a gradient-based generator to\nproduce collages. It requires the human artist to curate libraries of image\npatches and to describe (with prompts) the whole image composition, with the\noption to manually adjust the patches' positions during generation, thereby\nallowing humans to reclaim some control of the process and achieve greater\ncreative freedom. We explore the aesthetic potentials of high-resolution\ncollages, and provide an open-source Google Colab as an artistic tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1\">Piotr Mirowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banarse_D/0/1/0/all/0/1\">Dylan Banarse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_C/0/1/0/all/0/1\">Chrisantha Fernando</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvMAE: Masked Convolution Meets Masked Autoencoders. (arXiv:2205.03892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03892","description":"<p>Vision Transformers (ViT) become widely-adopted architectures for various\nvision tasks. Masked auto-encoding for feature pretraining and multi-scale\nhybrid convolution-transformer architectures can further unleash the potentials\nof ViT, leading to state-of-the-art performances on image classification,\ndetection and semantic segmentation. In this paper, our ConvMAE framework\ndemonstrates that multi-scale hybrid convolution-transformer can learn more\ndiscriminative representations via the mask auto-encoding scheme. However,\ndirectly using the original masking strategy leads to the heavy computational\ncost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the\nmasked convolution to prevent information leakage in the convolution blocks. A\nsimple block-wise masking strategy is proposed to ensure computational\nefficiency. We also propose to more directly supervise the multi-scale features\nof the encoder to boost multi-scale features. Based on our pretrained ConvMAE\nmodels, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared\nwith MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs\nsurpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP\nrespectively. Code and pretrained models are available at\nhttps://github.com/Alpha-VL/ConvMAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning. (arXiv:2205.04042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04042","description":"<p>Incremental few-shot object detection aims at detecting novel classes without\nforgetting knowledge of the base classes with only a few labeled training data\nfrom the novel classes. Most related prior works are on incremental object\ndetection that rely on the availability of abundant training samples per novel\nclass that substantially limits the scalability to real-world setting where\nnovel data can be scarce. In this paper, we propose the Incremental-DETR that\ndoes incremental few-shot object detection via fine-tuning and self-supervised\nlearning on the DETR object detector. To alleviate severe over-fitting with few\nnovel class data, we first fine-tune the class-specific components of DETR with\nself-supervision from additional object proposals generated using Selective\nSearch as pseudo labels. We further introduce a incremental few-shot\nfine-tuning strategy with knowledge distillation on the class-specific\ncomponents of DETR to encourage the network in detecting novel classes without\ncatastrophic forgetting. Extensive experiments conducted on standard\nincremental object detection and incremental few-shot object detection settings\nshow that our approach significantly outperforms state-of-the-art methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Na Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attracting and Dispersing: A Simple Approach for Source-free Domain Adaptation. (arXiv:2205.04183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04183","description":"<p>We propose a simple but effective source-free domain adaptation (SFDA)\nmethod. Treating SFDA as an unsupervised clustering problem and following the\nintuition that local neighbors in feature space should have more similar\npredictions than other features, we propose to optimize an objective of\nprediction consistency. This objective encourages local neighborhood features\nin feature space to have similar predictions while features farther away in\nfeature space have dissimilar predictions, leading to efficient feature\nclustering and cluster assignment simultaneously. For efficient training, we\nseek to optimize an upper-bound of the objective resulting in two simple terms.\nFurthermore, we relate popular existing methods in domain adaptation,\nsource-free domain adaptation and contrastive learning via the perspective of\ndiscriminability and diversity. The experimental results prove the superiority\nof our method, and our method can be adopted as a simple but strong baseline\nfor future research in SFDA. Our method can be also adapted to source-free\nopen-set and partial-set DA which further shows the generalization ability of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinction Maximization Loss: Efficiently Improving Classification Accuracy, Uncertainty Estimation, and Out-of-Distribution Detection Simply Replacing the Loss and Calibrating. (arXiv:2205.05874v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.05874","description":"<p>Building robust deterministic neural networks remains a challenge. On the one\nhand, some approaches improve out-of-distribution detection at the cost of\nreducing classification accuracy in some situations. On the other hand, some\nmethods simultaneously increase classification accuracy, uncertainty\nestimation, and out-of-distribution detection at the expense of reducing the\ninference efficiency and requiring training the same model many times to tune\nhyperparameters. In this paper, we propose training deterministic neural\nnetworks using our DisMax loss, which works as a drop-in replacement for the\nusual SoftMax loss (i.e., the combination of the linear output layer, the\nSoftMax activation, and the cross-entropy loss). Starting from the IsoMax+\nloss, we create each logit based on the distances to all prototypes rather than\njust the one associated with the correct class. We also introduce a mechanism\nto combine images to construct what we call fractional probability\nregularization. Moreover, we present a fast way to calibrate the network after\ntraining. Finally, we propose a composite score to perform out-of-distribution\ndetection. Our experiments show that DisMax usually outperforms current\napproaches simultaneously in classification accuracy, uncertainty estimation,\nand out-of-distribution detection while maintaining deterministic neural\nnetwork inference efficiency and avoiding training the same model repetitively\nfor hyperparameter tuning. The code to reproduce the results is available at\nhttps://github.com/dlmacedo/distinction-maximization-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Fourier Style Calibration for Domain Generalization. (arXiv:2205.06427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06427","description":"<p>The topic of generalizing machine learning models learned on a collection of\nsource domains to unknown target domains is challenging. While many domain\ngeneralization (DG) methods have achieved promising results, they primarily\nrely on the source domains at train-time without manipulating the target\ndomains at test-time. Thus, it is still possible that those methods can overfit\nto source domains and perform poorly on target domains. Driven by the\nobservation that domains are strongly related to styles, we argue that reducing\nthe gap between source and target styles can boost models' generalizability. To\nsolve the dilemma of having no access to the target domain during training, we\nintroduce Test-time Fourier Style Calibration (TF-Cal) for calibrating the\ntarget domain style on the fly during testing. To access styles, we utilize\nFourier transformation to decompose features into amplitude (style) features\nand phase (semantic) features. Furthermore, we present an effective technique\nto Augment Amplitude Features (AAF) to complement TF-Cal. Extensive experiments\non several popular DG benchmarks and a segmentation dataset for medical images\ndemonstrate that our method outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07403","description":"<p>Real-time and high-performance 3D object detection is of critical importance\nfor autonomous driving. Recent top-performing 3D object detectors mainly rely\non point-based or 3D voxel-based convolutions, which are both computationally\ninefficient for onboard deployment. While recent researches focus on\npoint-based or 3D voxel-based convolutions for higher performance, these\nmethods fail to meet latency and power efficiency requirements especially for\ndeployment on embedded devices. In contrast, pillar-based methods use merely 2D\nconvolutions, which consume less computation resources, but they lag far behind\ntheir voxel-based counterparts in detection accuracy. However, the superiority\nof such 3D voxel-based methods over pillar-based methods is still broadly\nattributed to the effectiveness of 3D convolution neural network (CNN). In this\npaper, by examining the primary performance gap between pillar- and voxel-based\ndetectors, we develop a real-time and high-performance pillar-based detector,\ndubbed PillarNet. The proposed PillarNet consists of a powerful encoder network\nfor effective pillar feature learning, a neck network for spatial-semantic\nfeature fusion and the commonly used detect head. Using only 2D convolutions,\nPillarNet is flexible to an optional pillar size and compatible with classical\n2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits\nfrom our designed orientation-decoupled IoU regression loss along with the\nIoU-aware prediction branch. Extensive experimental results on large-scale\nnuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet\nperforms well over the state-of-the-art 3D detectors in terms of effectiveness\nand efficiency. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Detection & Recognition in the Wild for Robot Localization. (arXiv:2205.08565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08565","description":"<p>Signage is everywhere and a robot should be able to take advantage of signs\nto help it localize (including Visual Place Recognition (VPR)) and map. Robust\ntext detection &amp; recognition in the wild is challenging due to such factors as\npose, irregular text, illumination, and occlusion. We propose an end-to-end\nscene text spotting model that simultaneously outputs the text string and\nbounding boxes. This model is more suitable for VPR. Our central contribution\nis introducing utilizing an end-to-end scene text spotting framework to\nadequately capture the irregular and occluded text regions in different\nchallenging places. To evaluate our proposed architecture's performance for\nVPR, we conducted several experiments on the challenging Self-Collected Text\nPlace (SCTP) benchmark dataset. The initial experimental results show that the\nproposed method outperforms the SOTA methods in terms of precision and recall\nwhen tested on this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raisi_Z/0/1/0/all/0/1\">Zobeir Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemiCurv: Semi-Supervised Curvilinear Structure Segmentation. (arXiv:2205.08706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08706","description":"<p>Recent work on curvilinear structure segmentation has mostly focused on\nbackbone network design and loss engineering. The challenge of collecting\nlabelled data, an expensive and labor intensive process, has been overlooked.\nWhile labelled data is expensive to obtain, unlabelled data is often readily\navailable. In this work, we propose SemiCurv, a semi-supervised learning (SSL)\nframework for curvilinear structure segmentation that is able to utilize such\nunlabelled data to reduce the labelling burden. Our framework addresses two key\nchallenges in formulating curvilinear segmentation in a semi-supervised manner.\nFirst, to fully exploit the power of consistency based SSL, we introduce a\ngeometric transformation as strong data augmentation and then align\nsegmentation predictions via a differentiable inverse transformation to enable\nthe computation of pixel-wise consistency. Second, the traditional mean square\nerror (MSE) on unlabelled data is prone to collapsed predictions and this issue\nexacerbates with severe class imbalance (significantly more background pixels).\nWe propose a N-pair consistency loss to avoid trivial predictions on unlabelled\ndata. We evaluate SemiCurv on six curvilinear segmentation datasets, and find\nthat with no more than 5% of the labelled data, it achieves close to 95% of the\nperformance relative to its fully supervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Manh Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazici_Y/0/1/0/all/0/1\">Yasin Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kangkang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_H/0/1/0/all/0/1\">Hlaing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan-Sheng Foo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Financial Time Series Data Augmentation with Generative Adversarial Networks and Extended Intertemporal Return Plots. (arXiv:2205.08924v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08924","description":"<p>Data augmentation is a key regularization method to support the forecast and\nclassification performance of highly parameterized models in computer vision.\nIn the time series domain however, regularization in terms of augmentation is\nnot equally common even though these methods have proven to mitigate effects\nfrom small sample size or non-stationarity. In this paper we apply state-of-the\nart image-based generative models for the task of data augmentation and\nintroduce the extended intertemporal return plot (XIRP), a new image\nrepresentation for time series. Multiple tests are conducted to assess the\nquality of the augmentation technique regarding its ability to synthesize time\nseries effectively and improve forecast results on a subset of the M4\ncompetition. We further investigate the relationship between data set\ncharacteristics and sampling results via Shapley values for feature attribution\non the performance metrics and the optimal ratio of augmented data. Over all\ndata sets, our approach proves to be effective in reducing the return forecast\nerror by 7% on 79% of the financial data sets with varying statistical\nproperties and frequencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hellermann_J/0/1/0/all/0/1\">Justin Hellermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qinzhuan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}