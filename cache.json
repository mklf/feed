{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Recent, rapid advancement in visual question answering architecture. (arXiv:2203.01322v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01445","description":"<p>The volume of available data has grown dramatically in recent years in many\napplications. Furthermore, the age of networks that used multiple modalities\nseparately has practically ended. Therefore, enabling bidirectional\ncross-modality data retrieval capable of processing has become a requirement\nfor many domains and disciplines of research. This is especially true in the\nmedical field, as data comes in a multitude of types, including various types\nof images and reports as well as molecular data. Most contemporary works apply\ncross attention to highlight the essential elements of an image or text in\nrelation to the other modalities and try to match them together. However,\nregardless of their importance in their own modality, these approaches usually\nconsider features of each modality equally. In this study, self-attention as an\nadditional loss term will be proposed to enrich the internal representation\nprovided into the cross attention module. This work suggests a novel\narchitecture with a new loss term to help represent images and texts in the\njoint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO\nand ARCH, show the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maleki_D/0/1/0/all/0/1\">Danial Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding. (arXiv:2203.01515v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01515","description":"<p>Automatic ICD coding is defined as assigning disease codes to electronic\nmedical records (EMRs). Existing methods usually apply label attention with\ncode representations to match related text snippets. Unlike these works that\nmodel the label with the code hierarchy or description, we argue that the code\nsynonyms can provide more comprehensive knowledge based on the observation that\nthe code expressions in EMRs vary from their descriptions in ICD. By aligning\ncodes to concepts in UMLS, we collect synonyms of every code. Then, we propose\na multiple synonyms matching network to leverage synonyms for better code\nrepresentation learning, and finally help the code classification. Experiments\non the MIMIC-III dataset show that our proposed method outperforms previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition. (arXiv:2203.01543v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01543","description":"<p>Recently, prompt-based learning for pre-trained language models has succeeded\nin few-shot Named Entity Recognition (NER) by exploiting prompts as task\nguidance to increase label efficiency. However, previous prompt-based methods\nfor few-shot NER have limitations such as a higher computational complexity,\npoor zero-shot ability, requiring manual prompt engineering, or lack of prompt\nrobustness. In this work, we address these shortcomings by proposing a new\nprompt-based learning NER method with Question Answering (QA), called QaNER.\nOur approach includes 1) a refined strategy for converting NER problems into\nthe QA formulation; 2) NER prompt generation for QA models; 3) prompt-based\ntuning with QA models on a few annotated NER examples; 4) zero-shot NER by\nprompting the QA model. Comparing the proposed approach with previous methods,\nQaNER is faster at inference, insensitive to the prompt quality, and robust to\nhyper-parameters, as well as demonstrating significantly better low-resource\nperformance and zero-shot capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking. (arXiv:2203.01552v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01552","description":"<p>Annotating task-oriented dialogues is notorious for the expensive and\ndifficult data collection process. Few-shot dialogue state tracking (DST) is a\nrealistic solution to this problem. In this paper, we hypothesize that dialogue\nsummaries are essentially unstructured dialogue states; hence, we propose to\nreformulate dialogue state tracking as a dialogue summarization problem. To\nelaborate, we train a text-to-text language model with synthetic template-based\ndialogue summaries, generated by a set of rules from the dialogue states. Then,\nthe dialogue states can be recovered by inversely applying the summary\ngeneration rules. We empirically show that our method DS2 outperforms previous\nworks on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and\nmulti-domain settings. Our method also exhibits vast speedup during both\ntraining and inference as it can generate all states at once. Finally, based on\nour analysis, we discover that the naturalness of the summary templates plays a\nkey role for successful training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeongdon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Juneyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism. (arXiv:2203.01594v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01594","description":"<p>Image captioning is a fast-growing research field of computer vision and\nnatural language processing that involves creating text explanations for\nimages. This study aims to develop a system that uses a pre-trained\nconvolutional neural network (CNN) to extract features from an image,\nintegrates the features with an attention mechanism, and creates captions using\na recurrent neural network (RNN). To encode an image into a feature vector as\ngraphical attributes, we employed multiple pre-trained convolutional neural\nnetworks. Following that, a language model known as GRU is chosen as the\ndecoder to construct the descriptive sentence. In order to increase\nperformance, we merge the Bahdanau attention model with GRU to allow learning\nto be focused on a specific portion of the image. On the MSCOCO dataset, the\nexperimental results achieve competitive performance against state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rashid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">M Shujah Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_K/0/1/0/all/0/1\">Khadija Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1\">Mansoor Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Imran Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhongfu Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDAAN - Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01644","description":"<p>We introduce UDAAN, an open-source post-editing tool that can reduce manual\nediting efforts to quickly produce publishable-standard documents in different\nlanguages. UDAAN has an end-to-end Machine Translation (MT) plus post-editing\npipeline wherein users can upload a document to obtain raw MT output. Further,\nusers can edit the raw translations using our tool. UDAAN offers several\nadvantages: a) Domain-aware, vocabulary-based lexical constrained MT. b)\nsource-target and target-target lexicon suggestions for users. Replacements are\nbased on the source and target texts lexicon alignment. c) Suggestions for\ntranslations are based on logs created during user interaction. d)\nSource-target sentence alignment visualisation that reduces the cognitive load\nof users during editing. e) Translated outputs from our tool are available in\nmultiple formats: docs, latex, and PDF. Although we limit our experiments to\nEnglish-to-Hindi translation for the current study, our tool is independent of\nthe source and target languages. Experimental results based on the usage of the\ntools and users feedback show that our tool speeds up the translation time\napproximately by a factor of three compared to the baseline method of\ntranslating documents from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_A/0/1/0/all/0/1\">Ajay Ravindran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Venkatapathy Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalan_A/0/1/0/all/0/1\">Akshay Jalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation. (arXiv:2203.01670v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01670","description":"<p>Early exiting allows instances to exit at different layers according to the\nestimation of difficulty. Previous works usually adopt heuristic metrics such\nas the entropy of internal outputs to measure instance difficulty, which\nsuffers from generalization and threshold-tuning. In contrast, learning to\nexit, or learning to predict instance difficulty is a more appealing way.\nThough some effort has been devoted to employing such \"learn-to-exit\" modules,\nit is still unknown whether and how well the instance difficulty can be\nlearned. As a response, we first conduct experiments on the learnability of\ninstance difficulty, which demonstrates that modern neural models perform\npoorly on predicting instance difficulty. Based on this observation, we propose\na simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces\nthe learn-to-exit modules with hash functions to assign each token to a fixed\nexiting layer. Different from previous methods, HashEE requires no internal\nclassifiers nor extra parameters, and therefore is more efficient. Experimental\nresults on classification, regression, and generation tasks demonstrate that\nHashEE can achieve higher performance with fewer FLOPs and inference time\ncompared with previous state-of-the-art early exiting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingling Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yilong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation. (arXiv:2203.01677v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01677","description":"<p>Word-level adversarial attacks have shown success in NLP models, drastically\ndecreasing the performance of transformer-based models in recent years. As a\ncountermeasure, adversarial defense has been explored, but relatively few\nefforts have been made to detect adversarial examples. However, detecting\nadversarial examples may be crucial for automated tasks (e.g. review sentiment\nanalysis) that wish to amass information about a certain population and\nadditionally be a step towards a robust defense system. To this end, we release\na dataset for four popular attack methods on four datasets and four models to\nencourage further research in this field. Along with it, we propose a\ncompetitive baseline based on density estimation that has the highest AUC on 29\nout of 30 dataset-attack-model combinations. Source code is available in\nhttps://github.com/anoymous92874838/text-adv-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jangho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jiho Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeerSum: A Peer Review Dataset for Abstractive Multi-document Summarization. (arXiv:2203.01769v1 [cs.IR])","link":"http://arxiv.org/abs/2203.01769","description":"<p>We present PeerSum, a new MDS dataset using peer reviews of scientific\npublications. Our dataset differs from the existing MDS datasets in that our\nsummaries (i.e., the meta-reviews) are highly abstractive and they are real\nsummaries of the source documents (i.e., the reviews) and it also features\ndisagreements among source documents. We found that current state-of-the-art\nMDS models struggle to generate high-quality summaries for PeerSum, offering\nnew research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Enhanced Short Text Matching using Clickthrough Data. (arXiv:2203.01849v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01849","description":"<p>The short text matching task employs a model to determine whether two short\ntexts have the same semantic meaning or intent. Existing short text matching\nmodels usually rely on the content of short texts which are lack information or\nmissing some key clues. Therefore, the short texts need external knowledge to\ncomplete their semantic meaning. To address this issue, we propose a new short\ntext matching framework for introducing external knowledge to enhance the short\ntext contextual representation. In detail, we apply a self-attention mechanism\nto enrich short text representation with external contexts. Experiments on two\nChinese datasets and one English dataset demonstrate that our framework\noutperforms the state-of-the-art short text matching models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mao Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Intelligence: Tasks, Representation Learning, and Large Models. (arXiv:2203.01922v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01922","description":"<p>This paper presents a comprehensive survey of vision-language (VL)\nintelligence from the perspective of time. This survey is inspired by the\nremarkable progress in both computer vision and natural language processing,\nand recent trends shifting from single modality processing to multiple modality\ncomprehension. We summarize the development in this field into three time\nperiods, namely task-specific methods, vision-language pre-training (VLP)\nmethods, and larger models empowered by large-scale weakly-labeled data. We\nfirst take some common VL tasks as examples to introduce the development of\ntask-specific methods. Then we focus on VLP methods and comprehensively review\nkey components of the model structures and training methods. After that, we\nshow how recent work utilizes large-scale raw image-text data to learn\nlanguage-aligned visual representations that generalize better on zero or few\nshot learning tasks. Finally, we discuss some potential future trends towards\nmodality cooperation, unified representation, and knowledge incorporation. We\nbelieve that this review will be of help for researchers and practitioners of\nAI and ML, especially those interested in computer vision and natural language\nprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">PengChuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning. (arXiv:2203.01927v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01927","description":"<p>Omission and addition of content is a typical issue in neural machine\ntranslation. We propose a method for detecting such phenomena with\noff-the-shelf translation models. Using contrastive conditioning, we compare\nthe likelihood of a full sequence under a translation model to the likelihood\nof its parts, given the corresponding source or target sequence. This allows to\npinpoint superfluous words in the translation and untranslated words in the\nsource even in the absence of a reference translation. The accuracy of our\nmethod is comparable to a supervised method that requires a custom quality\nestimation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Robustness of Visual Question Answering Models. (arXiv:1912.01452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.01452","description":"<p>Deep neural networks have been playing an essential role in the task of\nVisual Question Answering (VQA). Until recently, their accuracy has been the\nmain focus of research. Now there is a trend toward assessing the robustness of\nthese models against adversarial attacks by evaluating the accuracy of these\nmodels under increasing levels of noisiness in the inputs of VQA models. In\nVQA, the attack can target the image and/or the proposed query question, dubbed\nmain question, and yet there is a lack of proper analysis of this aspect of\nVQA. In this work, we propose a new method that uses semantically related\nquestions, dubbed basic questions, acting as noise to evaluate the robustness\nof VQA models. We hypothesize that as the similarity of a basic question to the\nmain question decreases, the level of noise increases. To generate a reasonable\nnoise level for a given main question, we rank a pool of basic questions based\non their similarity with this main question. We cast this ranking problem as a\nLASSO optimization problem. We also propose a novel robustness measure Rscore\nand two large-scale basic question datasets in order to standardize robustness\nanalysis of VQA models. The experimental results demonstrate that the proposed\nevaluation method is able to effectively analyze the robustness of VQA models.\nTo foster the VQA research, we will publish our proposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfadly_M/0/1/0/all/0/1\">Modar Alfadly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. (arXiv:2104.08786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08786","description":"<p>When primed with only a handful of training samples, very large, pretrained\nlanguage models such as GPT-3 have shown competitive results when compared to\nfully-supervised, fine-tuned, large, pretrained language models. We demonstrate\nthat the order in which the samples are provided can make the difference\nbetween near state-of-the-art and random guess performance: essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the true few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of language\nmodels to construct an artificial development set and based on entropy\nstatistics of the candidate permutations on this set, we identify performant\nprompts. Our method yields a 13% relative improvement for GPT-family models\nacross eleven different established text classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1\">Alastair Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training ELECTRA Augmented with Multi-word Selection. (arXiv:2106.00139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00139","description":"<p>Pre-trained text encoders such as BERT and its variants have recently\nachieved state-of-the-art performances on many NLP tasks. While being\neffective, these pre-training methods typically demand massive computation\nresources. To accelerate pre-training, ELECTRA trains a discriminator that\npredicts whether each input token is replaced by a generator. However, this new\ntask, as a binary classification, is less semantically informative. In this\nstudy, we present a new text encoder pre-training method that improves ELECTRA\nbased on multi-task learning. Specifically, we train the discriminator to\nsimultaneously detect replaced tokens and select original tokens from candidate\nsets. We further develop two techniques to effectively combine all pre-training\ntasks: (1) using attention-based networks for task-specific heads, and (2)\nsharing bottom layers of the generator and the discriminator. Extensive\nexperiments on GLUE and SQuAD datasets demonstrate both the effectiveness and\nthe efficiency of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04570","description":"<p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00967","description":"<p>Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00578","description":"<p>Neural models command state-of-the-art performance across NLP tasks,\nincluding ones involving \"reasoning\". Models claiming to reason about the\nevidence presented to them should attend to the correct parts of the input\navoiding spurious patterns therein, be self-consistent in their predictions\nacross inputs, and be immune to biases derived from their pre-training in a\nnuanced, context-sensitive fashion. {\\em Do the prevalent *BERT-family of\nmodels do so?} In this paper, we study this question using the problem of\nreasoning on tabular data. Tabular inputs are especially well-suited for the\nstudy -- they admit systematic probes targeting the properties listed above.\nOur experiments demonstrate that a RoBERTa-based model, representative of the\ncurrent state-of-the-art, fails at reasoning on the following counts: it (a)\nignores relevant parts of the evidence, (b) is over-sensitive to annotation\nartifacts, and (c) relies on the knowledge encoded in the pre-trained language\nmodel rather than the evidence presented in its tabular inputs. Finally,\nthrough inoculation experiments, we show that fine-tuning the model on\nperturbed data does not help it overcome the above challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz A. Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Atreya Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.04378","description":"<p>Several studies have reported the inability of Transformer models to\ngeneralize compositionally, a key type of generalization in many NLP tasks such\nas semantic parsing. In this paper we explore the design space of Transformer\nmodels showing that the inductive biases given to the model by several design\ndecisions significantly impact compositional generalization. Through this\nexploration, we identified Transformer configurations that generalize\ncompositionally significantly better than previously reported in the literature\nin a diverse set of compositional tasks, and that achieve state-of-the-art\nresults in a semantic parsing compositional generalization benchmark (COGS),\nand a string edit operation composition benchmark (PCFG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1\">Vaclav Cvicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zachary Fisher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Latent Semantic Analysis and Correspondence Analysis of Document-Term Matrices. (arXiv:2108.06197v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.06197","description":"<p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional and dense\nvectors that capture relationships among documents and terms. In this article,\nwe present a theoretical analysis and comparison of the two techniques in the\ncontext of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins arising\nfrom differing document-lengths and term-frequencies are effectively\neliminated, so that the CA solution is optimally suited to focus on\nrelationships among documents and terms. A unifying framework is proposed that\nincludes both CA and LSA as special cases. We empirically compare CA to various\nLSA based methods on two tasks, a document classification task in English and\nan authorship attribution task on historical Dutch texts, and find that CA\nperforms significantly better. We also apply CA to a long-standing question\nregarding the authorship of the Dutch national anthem Wilhelmus and provide\nfurther support that it can be attributed to the author Datheen, amongst\nseveral contenders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deoskar_T/0/1/0/all/0/1\">Tejaswini Deoskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Relation Modeling: Learning to Define Relations between Entities. (arXiv:2108.09241v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09241","description":"<p>Relations between entities can be represented by different instances, e.g., a\nsentence containing both entities or a fact in a Knowledge Graph (KG). However,\nthese instances may not well capture the general relations between entities,\nmay be difficult to understand by humans, even may not be found due to the\nincompleteness of the knowledge source. In this paper, we introduce the Open\nRelation Modeling problem - given two entities, generate a coherent sentence\ndescribing the relation between them. To solve this problem, we propose to\nteach machines to generate definition-like relation descriptions by letting\nthem learn from defining entities. Specifically, we fine-tune Pre-trained\nLanguage Models (PLMs) to produce definitions conditioned on extracted entity\npairs. To help PLMs reason between entities and provide additional relational\nknowledge to PLMs for open relation modeling, we incorporate reasoning paths in\nKGs and include a reasoning path selection mechanism. Experimental results show\nthat our model can generate concise but informative relation descriptions that\ncapture the representative characteristics of entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08615","description":"<p>A morphological analyzer, which is a significant component of many natural\nlanguage processing applications especially for morphologically rich languages,\ndivides an input word into all its composing morphemes and identifies their\nmorphological roles. In this paper, we introduce a comprehensive morphological\nanalyzer for Central Kurdish (CK), a low-resourced language with a rich\nmorphology. Building upon the limited existing literature, we first assembled\nand systematically categorized a comprehensive collection of the morphological\nand morphophonological rules of the language. Additionally, we collected and\nmanually labeled a generative lexicon containing nearly 10,000 verb, noun and\nadjective stems, named entities, and other types of word stems. We used these\nrule sets and resources to implement CKMorph Analyzer based on finite-state\ntransducers. In order to provide a benchmark for future research, we collected,\nmanually labeled, and publicly shared test sets for evaluating accuracy and\ncoverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the\naccuracy test set, containing 1,000 CK words morphologically analyzed according\nto the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M\nCK tokens of the coverage test set. The demonstration of the application and\nresources including CK verb database and test sets are openly accessible at\nhttps://github.com/CKMorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naserzade_M/0/1/0/all/0/1\">Morteza Naserzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmudi_A/0/1/0/all/0/1\">Aso Mahmudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hawre Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MohammadAmini_M/0/1/0/all/0/1\">Mohammad MohammadAmini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Molecular Representation Learning via Contrastive Pre-training. (arXiv:2109.08830v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08830","description":"<p>Molecular representation learning plays an essential role in cheminformatics.\nRecently, language model-based approaches have gained popularity as an\nalternative to traditional expert-designed features to encode molecules.\nHowever, these approaches only utilize a single molecular language for\nrepresentation learning. Motivated by the fact that a given molecule can be\ndescribed using different languages such as Simplified Molecular Line Entry\nSystem (SMILES), The International Union of Pure and Applied Chemistry (IUPAC),\nand The IUPAC International Chemical Identifier (InChI), we propose a\nmultilingual molecular embedding generation approach called MM-Deacon\n(multilingual molecular domain embedding analysis via contrastive learning).\nMM-Deacon is pre-trained using SMILES and IUPAC as two different languages on\nlarge-scale molecules. We evaluated the robustness of our method on seven\nmolecular property prediction tasks from MoleculeNet benchmark, zero-shot\ncross-lingual retrieval, and a drug-drug interaction prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pramod Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Andy Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet. (arXiv:2111.14706v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14706","description":"<p>As Automatic Speech Processing (ASR) systems are getting better, there is an\nincreasing interest of using the ASR output to do downstream Natural Language\nProcessing (NLP) tasks. However, there are few open source toolkits that can be\nused to generate reproducible results on different Spoken Language\nUnderstanding (SLU) benchmarks. Hence, there is a need to build an open source\nstandard that can be used to have a faster start into SLU research. We present\nESPnet-SLU, which is designed for quick development of spoken language\nunderstanding in a single framework. ESPnet-SLU is a project inside end-to-end\nspeech processing toolkit, ESPnet, which is a widely used open-source standard\nfor various speech processing tasks like ASR, Text to Speech (TTS) and Speech\nTranslation (ST). We enhance the toolkit to provide implementations for various\nSLU benchmarks that enable researchers to seamlessly mix-and-match different\nASR and NLU models. We also provide pretrained models with intensively tuned\nhyper-parameters that can match or even outperform the current state-of-the-art\nperformances. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_Y/0/1/0/all/0/1\">Yushi Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuekai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sujay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_K/0/1/0/all/0/1\">Karthik Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking. (arXiv:2112.01206v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.01206","description":"<p>The goal of local citation recommendation is to recommend a missing reference\nfrom the local citation context and optionally also from the global context. To\nbalance the tradeoff between speed and accuracy of citation recommendation in\nthe context of a large-scale paper database, a viable approach is to first\nprefetch a limited number of relevant documents using efficient ranking methods\nand then to perform a fine-grained reranking using more sophisticated models.\nIn that vein, BM25 has been found to be a tough-to-beat approach to\nprefetching, which is why recent work has focused mainly on the reranking step.\nEven so, we explore prefetching with nearest neighbor search among text\nembeddings constructed by a hierarchical attention network. When coupled with a\nSciBERT reranker fine-tuned on local citation recommendation tasks, our\nhierarchical Attention encoder (HAtten) achieves high prefetch recall for a\ngiven number of candidates to be reranked. Consequently, our reranker requires\nfewer prefetch candidates to rerank, yet still achieves state-of-the-art\nperformance on various local citation recommendation datasets such as ACL-200,\nFullTextPeerRead, RefSeer, and arXiv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08687","description":"<p>In previous work we have proposed an efficient pattern matching algorithm\nbased on the notion of set automaton. In this article we investigate how set\nautomata can be exploited to implement efficient term rewriting procedures.\nThese procedures interleave pattern matching steps and rewriting steps and thus\nsmoothly integrate redex discovery and subterm replacement. Concretely, we\npropose an optimised algorithm for outermost rewriting of left-linear term\nrewriting systems, prove its correctness, and present the results of some\nimplementation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouwman_M/0/1/0/all/0/1\">Mark Bouwman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erkens_R/0/1/0/all/0/1\">Rick Erkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How reparametrization trick broke differentially-private text representation learning. (arXiv:2202.12138v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12138","description":"<p>As privacy gains traction in the NLP community, researchers have started\nadopting various approaches to privacy-preserving methods. One of the favorite\nprivacy frameworks, differential privacy (DP), is perhaps the most compelling\nthanks to its fundamental theoretical guarantees. Despite the apparent\nsimplicity of the general concept of differential privacy, it seems non-trivial\nto get it right when applying it to NLP. In this short paper, we formally\nanalyze several recent NLP papers proposing text representation learning using\nDPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and\nreveal their false claims of being differentially private. Furthermore, we also\nshow a simple yet general empirical sanity check to determine whether a given\nimplementation of a DP mechanism almost certainly violates the privacy loss\nguarantees. Our main goal is to raise awareness and help the community\nunderstand potential pitfalls of applying differential privacy to text\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Colon Nuclei Instance Segmentation using a Probabilistic Two-Stage Detector. (arXiv:2203.01321v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01321","description":"<p>Cancer is one of the leading causes of death in the developed world. Cancer\ndiagnosis is performed through the microscopic analysis of a sample of\nsuspicious tissue. This process is time consuming and error prone, but Deep\nLearning models could be helpful for pathologists during cancer diagnosis. We\npropose to change the CenterNet2 object detection model to also perform\ninstance segmentation, which we call SegCenterNet2. We train SegCenterNet2 in\nthe CoNIC challenge dataset and show that it performs better than Mask R-CNN in\nthe competition metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Costa_P/0/1/0/all/0/1\">Pedro Costa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yongpan Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nunes_J/0/1/0/all/0/1\">Jo&#xe3;o Nunes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campilho_A/0/1/0/all/0/1\">Aur&#xe9;lio Campilho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent, rapid advancement in visual question answering architecture. (arXiv:2203.01322v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01322","description":"<p>Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1\">Venkat Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2203.01323v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01323","description":"<p>Accuracies of deep learning (DL) classifiers are often unstable in that they\nmay change significantly when retested on adversarial images, imperfect images,\nor perturbed images. This paper adds to the fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. To measure\nrobust DL classifiers, previous research reported on single-factor corruption.\nWe created comprehensive 69 benchmarking image sets, including a clean set,\nsets with single factor perturbations, and sets with two-factor perturbation\nconditions. The state-of-the-art two-factor perturbation includes (a) two\ndigital perturbations (salt &amp; pepper noise and Gaussian noise) applied in both\nsequences, and (b) one digital perturbation (salt &amp; pepper noise) and a\ngeometric perturbation (rotation) applied in both sequences. Previous research\nevaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a\nnew two-dimensional, statistical matrix to evaluating robustness of DL\nclassifiers. Also, we introduce a new visualization tool, including minimum\naccuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV),\nfor benchmarking robustness of DL classifiers. Comparing with single factor\ncorruption, we first report that using two-factor perturbed images improves\nboth robustness and accuracy of DL classifiers. All source codes and related\nimage sets are shared on the Website at <a href=\"http://cslinux.semo.edu/david/data\">this http URL</a> to\nsupport future academic research and industry projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation. (arXiv:2203.01324v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01324","description":"<p>Semi-supervised segmentation remains challenging in medical imaging since the\namount of annotated medical data is often limited and there are many blurred\npixels near the adhesive edges or low-contrast regions. To address the issues,\nwe advocate to firstly constrain the consistency of samples with and without\nstrong perturbations to apply sufficient smoothness regularization and further\nencourage the class-level separation to exploit the unlabeled ambiguous pixels\nfor the model training. Particularly, in this paper, we propose the SS-Net for\nsemi-supervised medical image segmentation tasks, via exploring the pixel-level\nSmoothness and inter-class Separation at the same time. The pixel-level\nsmoothness forces the model to generate invariant results under adversarial\nperturbations. Meanwhile, the inter-class separation constrains individual\nclass features should approach their corresponding high-quality prototypes, in\norder to make each class distribution compact and separate different classes.\nWe evaluated our SS-Net against five recent methods on the public LA and ACDC\ndatasets. The experimental results under two semi-supervised settings\ndemonstrate the superiority of our proposed SS-Net, achieving new\nstate-of-the-art (SOTA) performance on both datasets. The codes will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations. (arXiv:2203.01325v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01325","description":"<p>In this paper, we consider two challenging issues in reference-based\nsuper-resolution (RefSR), (i) how to choose a proper reference image, and (ii)\nhow to learn real-world RefSR in a self-supervised manner. Particularly, we\npresent a novel self-supervised learning approach for real-world image SR from\nobservations at dual camera zooms (SelfDZSR). For the first issue, the more\nzoomed (telephoto) image can be naturally leveraged as the reference to guide\nthe SR of the lesser zoomed (short-focus) image. For the second issue, SelfDZSR\nlearns a deep network to obtain the SR result of short-focal image and with the\nsame resolution as the telephoto image. For this purpose, we take the telephoto\nimage instead of an additional high-resolution image as the supervision\ninformation and select a patch from it as the reference to super-resolve the\ncorresponding short-focus image patch. To mitigate the effect of various\nmisalignment between the short-focus low-resolution (LR) image and telephoto\nground-truth (GT) image, we design a degradation model and map the GT to a\npseudo-LR image aligned with GT. Then the pseudo-LR and LR image can be fed\ninto the proposed adaptive spatial transformer networks (AdaSTN) to deform the\nLR features. During testing, SelfDZSR can be directly deployed to super-solve\nthe whole short-focus image with the reference of telephoto image. Experiments\nshow that our method achieves better quantitative and qualitative performance\nagainst state-of-the-arts. The code and pre-trained models will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ruohao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunjin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01327","description":"<p>Hyperspectral pixel intensities result from a mixing of reflectances from\nseveral materials. This paper develops a method of hyperspectral pixel {\\it\nunmixing} that aims to recover the \"pure\" spectral signal of each material\n(hereafter referred to as {\\it endmembers}) together with the mixing ratios\n({\\it abundances}) given the spectrum of a single pixel. The unmixing problem\nis particularly relevant in the case of low-resolution hyperspectral images\ncaptured in a remote sensing setting, where individual pixels can cover large\nregions of the scene. Under the assumptions that (1) a multivariate Normal\ndistribution can represent the spectra of an endmember and (2) a Dirichlet\ndistribution can encode abundances of different endmembers, we develop a Latent\nDirichlet Variational Autoencoder for hyperspectral pixel unmixing. Our\napproach achieves state-of-the-art results on standard benchmarks and on\nsynthetic data generated using United States Geological Survey spectral\nlibrary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mantripragada_K/0/1/0/all/0/1\">Kiran Mantripragada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qureshi_F/0/1/0/all/0/1\">Faisal Z. Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Reconstruction for Open-set Semantic Segmentation. (arXiv:2203.01368v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01368","description":"<p>Open set segmentation is a relatively new and unexploredtask, with just a\nhandful of methods proposed to model suchtasks.We propose a novel method called\nCoReSeg thattackles the issue using class conditional reconstruction ofthe\ninput images according to their pixelwise mask. Ourmethod conditions each input\npixel to all known classes,expecting higher errors for pixels of unknown\nclasses. Itwas observed that the proposed method produces better se-mantic\nconsistency in its predictions, resulting in cleanersegmentation maps that\nbetter fit object boundaries. CoRe-Seg outperforms state-of-the-art methods on\nthe Vaihin-gen and Potsdam ISPRS datasets, while also being com-petitive on the\nHouston 2018 IEEE GRSS Data Fusiondataset. Official implementation for CoReSeg\nis availableat:https://github.com/iannunes/CoReSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_I/0/1/0/all/0/1\">Ian Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1\">Matheus B. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Marcus Poggi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01386","description":"<p>The main question we address in this paper is how to scale up visual\nrecognition of unseen classes, also known as zero-shot learning, to tens of\nthousands of categories as in the ImageNet-21K benchmark. At this scale,\nespecially with many fine-grained categories included in ImageNet-21K, it is\ncritical to learn quality visual semantic representations that are\ndiscriminative enough to recognize unseen classes and distinguish them from\nseen ones. We propose a Hierarchical Graphical knowledge Representation\nframework for the confidence-based classification method, dubbed as HGR-Net.\nOur experimental results demonstrate that HGR-Net can grasp class inheritance\nrelations by utilizing hierarchical conceptual knowledge. Our method\nsignificantly outperformed all existing techniques, boosting the performance 7%\ncompared to the runner-up approach on the ImageNet-21K benchmark. We show that\nHGR-Net is learning-efficient in few-shot scenarios. We also analyzed our\nmethod on smaller datasets like ImageNet-21K-P, 2-hops and 3-hops,\ndemonstrating its generalization ability. Our benchmark and code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoqian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yunhao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iMVS: Improving MVS Networks by Learning Depth Discontinuities. (arXiv:2203.01391v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01391","description":"<p>Existing learning-based multi-view stereo (MVS) techniques are effective in\nterms of completeness in reconstruction. We further improve these techniques by\nlearning depth continuities. Our idea is to jointly estimate the depth and\nboundary maps. To this end, we introduce learning-based MVS strategies to\nimprove the quality of depth maps via mixture density and depth discontinuity\nlearning. We validate our idea and demonstrate that our strategies can be\neasily integrated into existing learning-based MVS pipelines where the\nreconstruction depends on high-quality depth map estimation. We also introduce\na bimodal depth representation and a novel spatial regularization approach to\nthe MVS networks. Extensive experiments on various datasets show that our\nmethod sets a new state of the art in terms of completeness and overall\nreconstruction quality. Experiments also demonstrate that the presented model\nand strategies have good generalization capabilities. The source code will be\navailable soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimli_N/0/1/0/all/0/1\">Nail Ibrahimli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1\">Julian Kooij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Timing Error: A Case Study of Navigation Camera. (arXiv:2203.01412v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01412","description":"<p>We focus on the problem of timing errors in navigation camera as a case study\nin a broader problem of the effect of a timing error in cyber-physical systems.\nThese systems rely on the requirement that certain things happen at the same\ntime or certain things happen periodically at some period $T$. However, as\nthese systems get more complex, timing errors can occur between the components\nthereby violating the assumption about events being simultaneous (or periodic).\n</p>\n<p>We consider the problem of a surgical navigation system where optical markers\ndetected in the 2D pictures taken by two cameras are used to localize the\nmarkers in 3D space. A predefined array of such markers, known as a reference\nelement, is used to navigate the corresponding CAD model of a surgical\ninstrument on patient's images. The cameras rely on the assumption that the\npictures from both cameras are taken exactly at the same time. If a timing\nerror occurs then the instrument may have moved between the pictures. We find\nthat, depending upon the location of the instrument, this can lead to a\nsubstantial error in the localization of the instrument. Specifically, we find\nthat if the actual movement is $\\delta$ then the observed movement may be as\nhigh as $5\\delta$ in the operating range of the camera. Furthermore, we also\nidentify potential issues that could affect the error in case there are changes\nto the camera system or to the operating range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Sandeep S. Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sanjay M. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUAD: Multiple Uncertainties for Autonomous Driving benchmark for multiple uncertainty types and tasks. (arXiv:2203.01437v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01437","description":"<p>Predictive uncertainty estimation is essential for deploying Deep Neural\nNetworks in real-world autonomous systems. However, disentangling the different\ntypes and sources of uncertainty is non trivial in most datasets, especially\nsince there is no ground truth for uncertainty. In addition, different degrees\nof weather conditions can disrupt neural networks, resulting in inconsistent\ntraining data quality. Thus, we introduce the MUAD dataset (Multiple\nUncertainties for Autonomous Driving), consisting of 8,500 realistic synthetic\nimages with diverse adverse weather conditions (night, fog, rain, snow),\nout-of-distribution objects and annotations for semantic segmentation, depth\nestimation, object and instance detection. MUAD allows to better assess the\nimpact of different sources of uncertainty on model performance. We propose a\nstudy that shows the importance of having reliable Deep Neural Networks (DNNs)\nin multiple experiments, and will release our dataset to allow researchers to\nbenchmark their algorithm methodically in ad-verse conditions. More information\nand the download link for MUAD are available at https://muad-dataset.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmierczak_R/0/1/0/all/0/1\">R&#xe9;mi Kazmierczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubuisson_S/0/1/0/all/0/1\">S&#xe9;verine Dubuisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Adversarial Robustness for Deep Metric Learning. (arXiv:2203.01439v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01439","description":"<p>Owing to security implications of adversarial vulnerability, adversarial\nrobustness of deep metric learning models has to be improved. In order to avoid\nmodel collapse due to excessively hard examples, the existing defenses dismiss\nthe min-max adversarial training, but instead learn from a weak adversary\ninefficiently. Conversely, we propose Hardness Manipulation to efficiently\nperturb the training triplet till a specified level of hardness for adversarial\ntraining, according to a harder benign triplet or a pseudo-hardness function.\nIt is flexible since regular training and min-max adversarial training are its\nboundary cases. Besides, Gradual Adversary, a family of pseudo-hardness\nfunctions is proposed to gradually increase the specified hardness level during\ntraining for a better balance between performance and robustness. Additionally,\nan Intra-Class Structure loss term among benign and adversarial examples\nfurther improves model robustness and efficiency. Comprehensive experimental\nresults suggest that the proposed method, although simple in its form,\noverwhelmingly outperforms the state-of-the-art defenses in terms of\nrobustness, training efficiency, as well as performance on benign examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01441","description":"<p>We introduce a set of image transformations that can be used as `corruptions'\nto evaluate the robustness of models as well as `data augmentation' mechanisms\nfor training neural networks. The primary distinction of the proposed\ntransformations is that, unlike existing approaches such as Common Corruptions,\nthe geometry of the scene is incorporated in the transformations -- thus\nleading to corruptions that are more likely to occur in the real world. We show\nthese transformations are `efficient' (can be computed on-the-fly),\n`extendable' (can be applied on most datasets of real images), expose\nvulnerability of existing models, and can effectively make models more robust\nwhen employed as `3D data augmentation' mechanisms. Our evaluations performed\non several tasks and datasets suggest incorporating 3D information into\nrobustness benchmarking and training opens up a promising direction for\nrobustness research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1\">O&#x11f;uzhan Fatih Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_T/0/1/0/all/0/1\">Teresa Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01445","description":"<p>The volume of available data has grown dramatically in recent years in many\napplications. Furthermore, the age of networks that used multiple modalities\nseparately has practically ended. Therefore, enabling bidirectional\ncross-modality data retrieval capable of processing has become a requirement\nfor many domains and disciplines of research. This is especially true in the\nmedical field, as data comes in a multitude of types, including various types\nof images and reports as well as molecular data. Most contemporary works apply\ncross attention to highlight the essential elements of an image or text in\nrelation to the other modalities and try to match them together. However,\nregardless of their importance in their own modality, these approaches usually\nconsider features of each modality equally. In this study, self-attention as an\nadditional loss term will be proposed to enrich the internal representation\nprovided into the cross attention module. This work suggests a novel\narchitecture with a new loss term to help represent images and texts in the\njoint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO\nand ARCH, show the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maleki_D/0/1/0/all/0/1\">Danial Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Pose Estimation using Mid-level Visual Representations. (arXiv:2203.01449v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01449","description":"<p>This work proposes a novel pose estimation model for object categories that\ncan be effectively transferred to previously unseen environments. The deep\nconvolutional network models (CNN) for pose estimation are typically trained\nand evaluated on datasets specifically curated for object detection, pose\nestimation, or 3D reconstruction, which requires large amounts of training\ndata. In this work, we propose a model for pose estimation that can be trained\nwith small amount of data and is built on the top of generic mid-level\nrepresentations \\cite{taskonomy2018} (e.g. surface normal estimation and\nre-shading). These representations are trained on a large dataset without\nrequiring pose and object annotations. Later on, the predictions are refined\nwith a small CNN neural network that exploits object masks and silhouette\nretrieval. The presented approach achieves superior performance on the Pix3D\ndataset \\cite{pix3d} and shows nearly 35\\% improvement over the existing models\nwhen only 25\\% of the training data is available. We show that the approach is\nfavorable when it comes to generalization and transfer to novel environments.\nTowards this end, we introduce a new pose estimation benchmark for commonly\nencountered furniture categories on challenging Active Vision Dataset\n\\cite{Ammirato2017ADF} and evaluated the models trained on the Pix3D dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nejatishahidin_N/0/1/0/all/0/1\">Negar Nejatishahidin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyazsanavi_P/0/1/0/all/0/1\">Pooya Fayyazsanavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1\">Jana Kosecka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation. (arXiv:2203.01452v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01452","description":"<p>Panoramic images with their 360-degree directional view encompass exhaustive\ninformation about the surrounding space, providing a rich foundation for scene\nunderstanding. To unfold this potential in the form of robust panoramic\nsegmentation models, large quantities of expensive, pixel-wise annotations are\ncrucial for success. Such annotations are available, but predominantly for\nnarrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal\nresources for training panoramic models. Distortions and the distinct\nimage-feature distribution in 360-degree panoramas impede the transfer from the\nannotation-rich pinhole domain and therefore come with a big dent in\nperformance. To get around this domain difference and bring together semantic\nannotations from pinhole- and 360-degree surround-visuals, we propose to learn\nobject deformations and panoramic image distortions in the Deformable Patch\nEmbedding (DPE) and Deformable MLP (DMLP) components which blend into our\nTransformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we\ntie together shared semantics in pinhole- and panoramic feature embeddings by\ngenerating multi-scale prototype features and aligning them in our Mutual\nPrototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor\nStanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance\nto fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled\npanoramas. On the outdoor DensePASS dataset, we break state-of-the-art by\n14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available\nat https://github.com/jamycheung/Trans4PASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chaoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiss_S/0/1/0/all/0/1\">Simon Rei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01474","description":"<p>Predicting future motion based on historical motion sequence is a fundamental\nproblem in computer vision, and it has wide applications in autonomous driving\nand robotics. Some recent works have shown that Graph Convolutional\nNetworks(GCN) are instrumental in modeling the relationship between different\njoints. However, considering the variants and diverse action types in human\nmotion data, the cross-dependency of the spatial-temporal relationships will be\ndifficult to depict due to the decoupled modeling strategy, which may also\nexacerbate the problem of insufficient generalization. Therefore, we propose\nthe Spatial-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex\nspatial-temporal dependencies over diverse action types. Specifically, we adopt\ngating networks to enhance the generalization of GCN via the trainable adaptive\nadjacency matrix obtained by blending the candidate spatial-temporal adjacency\nmatrices. Moreover, GAGCN addresses the cross-dependency of space and time by\nbalancing the weights of spatial-temporal modeling and fusing the decoupled\nspatial-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW\ndemonstrate that GAGCN achieves state-of-the-art performance in both short-term\nand long-term predictions. Our code will be released in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chongyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yongjing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shihong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. (arXiv:2203.01475v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01475","description":"<p>Curating a large set of fully annotated training data can be costly,\nespecially for the tasks of medical image segmentation. Scribble, a weaker form\nof annotation, is more obtainable in practice, but training segmentation models\nfrom limited supervision of scribbles is still challenging. To address the\ndifficulties, we propose a new framework for scribble learning-based medical\nimage segmentation, which is composed of mix augmentation and cycle consistency\nand thus is referred to as CycleMix. For augmentation of supervision, CycleMix\nadopts the mixup strategy with a dedicated design of random occlusion, to\nperform increments and decrements of scribbles. For regularization of\nsupervision, CycleMix intensifies the training objective with consistency\nlosses to penalize inconsistent segmentation, which results in significant\nimprovement of segmentation performance. Results on two open datasets, i.e.,\nACDC and MSCMRseg, showed that the proposed method achieved exhilarating\nperformance, demonstrating comparable or even better accuracy than the\nfully-supervised methods. The code and expert-made scribble annotations for\nMSCMRseg will be released once this article is accepted for publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaDT: Meta Decision Tree for Interpretable Few-Shot Learning. (arXiv:2203.01482v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01482","description":"<p>Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel\nclasses with few examples. Recently, lots of methods have been proposed from\nthe perspective of meta-learning and representation learning for improving FSL\nperformance. However, few works focus on the interpretability of FSL decision\nprocess. In this paper, we take a step towards the interpretable FSL by\nproposing a novel decision tree-based meta-learning framework, namely, MetaDT.\nOur insight is replacing the last black-box FSL classifier of the existing\nrepresentation learning methods by an interpretable decision tree with\nmeta-learning. The key challenge is how to effectively learn the decision tree\n(i.e., the tree structure and the parameters of each node) in the FSL setting.\nTo address the challenge, we introduce a tree-like class hierarchy as our\nprior: 1) the hierarchy is directly employed as the tree structure; 2) by\nregarding the class hierarchy as an undirected graph, a graph convolution-based\ndecision tree inference network is designed as our meta-learner to learn to\ninfer the parameters of each node. At last, a two-loop optimization mechanism\nis incorporated into our framework for a fast adaptation of the decision tree\nwith few examples. Extensive experiments on performance comparison and\ninterpretability analysis show the effectiveness and superiority of our MetaDT.\nOur code will be publicly available upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shanshan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rui Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PetsGAN: Rethinking Priors for Single Image Generation. (arXiv:2203.01488v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01488","description":"<p>Single image generation (SIG), described as generating diverse samples that\nhave similar visual content with the given single image, is first introduced by\nSinGAN which builds a pyramid of GANs to progressively learn the internal patch\ndistribution of the single image. It also shows great potentials in a wide\nrange of image manipulation tasks. However, the paradigm of SinGAN has\nlimitations in terms of generation quality and training time. Firstly, due to\nthe lack of high-level information, SinGAN cannot handle the object images well\nas it does on the scene and texture images. Secondly, the separate progressive\ntraining scheme is time-consuming and easy to cause artifact accumulation. To\ntackle these problems, in this paper, we dig into the SIG problem and improve\nSinGAN by fully-utilization of internal and external priors. The main\ncontributions of this paper include: 1) We introduce to SIG a regularized\nlatent variable model. To the best of our knowledge, it is the first time to\ngive a clear formulation and optimization goal of SIG, and all the existing\nmethods for SIG can be regarded as special cases of this model. 2) We design a\nnovel Prior-based end-to-end training GAN (PetsGAN) to overcome the problems of\nSinGAN. Our method gets rid of the time-consuming progressive training scheme\nand can be trained end-to-end. 3) We construct abundant qualitative and\nquantitative experiments to show the superiority of our method on both\ngenerated image quality, diversity, and the training speed. Moreover, we apply\nour method to other image manipulation tasks (e.g., style transfer,\nharmonization), and the results further prove the effectiveness and efficiency\nof our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Congying Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tiande Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation. (arXiv:2203.01502v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01502","description":"<p>Estimating the accurate depth from a single image is challenging since it is\ninherently ambiguous and ill-posed. While recent works design increasingly\ncomplicated and powerful networks to directly regress the depth map, we take\nthe path of CRFs optimization. Due to the expensive computation, CRFs are\nusually performed between neighborhoods rather than the whole graph. To\nleverage the potential of fully-connected CRFs, we split the input into windows\nand perform the FC-CRFs optimization within each window, which reduces the\ncomputation complexity and makes FC-CRFs feasible. To better capture the\nrelationships between nodes in the graph, we exploit the multi-head attention\nmechanism to compute a multi-head potential function, which is fed to the\nnetworks to output an optimized depth map. Then we build a bottom-up-top-down\nstructure, where this neural window FC-CRFs module serves as the decoder, and a\nvision transformer serves as the encoder. The experiments demonstrate that our\nmethod significantly improves the performance across all metrics on both the\nKITTI and NYUv2 datasets, compared to previous methods. Furthermore, the\nproposed method can be directly applied to panorama images and outperforms all\nprevious panorama methods on the MatterPort3D dataset. The source code of our\nmethod will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zuozhuo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftGroup for 3D Instance Segmentation on Point Clouds. (arXiv:2203.01509v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01509","description":"<p>Existing state-of-the-art 3D instance segmentation methods perform semantic\nsegmentation followed by grouping. The hard predictions are made when\nperforming semantic segmentation such that each point is associated with a\nsingle class. However, the errors stemming from hard decision propagate into\ngrouping that results in (1) low overlaps between the predicted instance with\nthe ground truth and (2) substantial false positives. To address the\naforementioned problems, this paper proposes a 3D instance segmentation method\nreferred to as SoftGroup by performing bottom-up soft grouping followed by\ntop-down refinement. SoftGroup allows each point to be associated with multiple\nclasses to mitigate the problems stemming from semantic prediction errors and\nsuppresses false positive instances by learning to categorize them as\nbackground. Experimental results on different datasets and multiple evaluation\nmetrics demonstrate the efficacy of SoftGroup. Its performance surpasses the\nstrongest prior method by a significant margin of +6.2% on the ScanNet v2\nhidden test set and +6.8% on S3DIS Area 5 in terms of AP_50. SoftGroup is also\nfast, running at 345ms per scan with a single Titan X on ScanNet v2 dataset.\nThe source code and trained models for both datasets are available at\n\\url{https://github.com/thangvubk/SoftGroup.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kookhoi Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1\">Tung M. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking. (arXiv:2203.01516v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01516","description":"<p>Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related\napplications, which leads to a highly demanding requirement on the robustness\nof UAV trackers. However, adding imperceptible perturbations can easily fool\nthe tracker and cause tracking failures. This risk is often overlooked and\nrarely researched at present. Therefore, to help increase awareness of the\npotential risk and the robustness of UAV tracking, this work proposes a novel\nadaptive adversarial attack approach, i.e., Ad$^2$Attack, against UAV object\ntracking. Specifically, adversarial examples are generated online during the\nresampling of the search patch image, which leads trackers to lose the target\nin the following frames. Ad$^2$Attack is composed of a direct downsampling\nmodule and a super-resolution upsampling module with adaptive stages. A novel\noptimization function is proposed for balancing the imperceptibility and\nefficiency of the attack. Comprehensive experiments on several well-known\nbenchmarks and real-world conditions show the effectiveness of our attack\nmethod, which dramatically reduces the performance of the most advanced Siamese\ntrackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xinnan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01522","description":"<p>Despite the success of deep neural networks, there are still many challenges\nin deep representation learning due to the data scarcity issues such as data\nimbalance, unseen distribution, and domain shift. To address the\nabove-mentioned issues, a variety of methods have been devised to explore the\nsample relationships in a vanilla way (i.e., from the perspectives of either\nthe input or the loss function), failing to explore the internal structure of\ndeep neural networks for learning with sample relationships. Inspired by this,\nwe propose to enable deep neural networks themselves with the ability to learn\nthe sample relationships from each mini-batch. Specifically, we introduce a\nbatch transformer module or BatchFormer, which is then applied into the batch\ndimension of each mini-batch to implicitly explore sample relationships during\ntraining. By doing this, the proposed method enables the collaboration of\ndifferent samples, e.g., the head-class samples can also contribute to the\nlearning of the tail classes for long-tailed recognition. Furthermore, to\nmitigate the gap between training and testing, we share the classifier between\nwith or without the BatchFormer during training, which can thus be removed\nduring testing. We perform extensive experiments on over ten datasets and the\nproposed method achieves significant improvements on different data scarcity\napplications without any bells and whistles, including the tasks of long-tailed\nrecognition, compositional zero-shot learning, domain generalization, and\ncontrastive learning. Code will be made publicly available at\n\\url{https://github.com/zhihou7/BatchFormer}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAFE: Learning to Condense Dataset by Aligning Features. (arXiv:2203.01531v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01531","description":"<p>Dataset condensation aims at reducing the network training effort through\ncondensing a cumbersome training set into a compact synthetic one.\nState-of-the-art approaches largely rely on learning the synthetic data by\nmatching the gradients between the real and synthetic data batches. Despite the\nintuitive motivation and promising results, such gradient-based methods, by\nnature, easily overfit to a biased set of samples that produce dominant\ngradients, and thus lack global supervision of data distribution. In this\npaper, we propose a novel scheme to Condense dataset by Aligning FEatures\n(CAFE), which explicitly attempts to preserve the real-feature distribution as\nwell as the discriminant power of the resulting synthetic set, lending itself\nto strong generalization capability to various architectures. At the heart of\nour approach is an effective strategy to align features from the real and\nsynthetic data across various scales, while accounting for the classification\nof real samples. Our scheme is further backed up by a novel dynamic bi-level\noptimization, which adaptively adjusts parameter updates to prevent\nover-/under-fitting. We validate the proposed CAFE across various datasets, and\ndemonstrate that it generally outperforms the state of the art: on the SVHN\ndataset, for example, the performance gain is up to 11%. Extensive experiments\nand analyses verify the effectiveness and necessity of proposed designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks. (arXiv:2203.01532v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01532","description":"<p>Recently, contrastive learning-based image translation methods have been\nproposed, which contrasts different spatial locations to enhance the spatial\ncorrespondence. However, the methods often ignore the diverse semantic relation\nwithin the images. To address this, here we propose a novel semantic relation\nconsistency (SRC) regularization along with the decoupled contrastive learning,\nwhich utilize the diverse semantics by focusing on the heterogeneous semantics\nbetween the image patches of a single image. To further improve the\nperformance, we present a hard negative mining by exploiting the semantic\nrelation. We verified our method for three tasks: single-modal and multi-modal\nimage translations, and GAN compression task for image translation.\nExperimental results confirmed the state-of-art performance of our method in\nall the three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Chanyong Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01536","description":"<p>Vision Transformers (ViTs) are becoming more popular and dominating technique\nfor various vision tasks, compare to Convolutional Neural Networks (CNNs). As a\ndemanding technique in computer vision, ViTs have been successfully solved\nvarious vision problems while focusing on long-range relationships. In this\npaper, we begin by introducing the fundamental concepts and background of the\nself-attention mechanism. Next, we provide a comprehensive overview of recent\ntop-performing ViT methods describing in terms of strength and weakness,\ncomputational cost as well as training and testing dataset. We thoroughly\ncompare the performance of various ViT algorithms and most representative CNN\nmethods on popular benchmark datasets. Finally, we explore some limitations\nwith insightful observations and provide further research direction. The\nproject page along with the collections of papers are available at\nhttps://github.com/khawar512/ViT-Survey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1\">Khawar Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Transparent Liquid Segmentation for Robotic Pouring. (arXiv:2203.01538v1 [cs.RO])","link":"http://arxiv.org/abs/2203.01538","description":"<p>Liquid state estimation is important for robotics tasks such as pouring;\nhowever, estimating the state of transparent liquids is a challenging problem.\nWe propose a novel segmentation pipeline that can segment transparent liquids\nsuch as water from a static, RGB image without requiring any manual annotations\nor heating of the liquid for training. Instead, we use a generative model that\nis capable of translating images of colored liquids into synthetically\ngenerated transparent liquid images, trained only on an unpaired dataset of\ncolored and transparent liquid images. Segmentation labels of colored liquids\nare obtained automatically using background subtraction. Our experiments show\nthat we are able to accurately predict a segmentation mask for transparent\nliquids without requiring any manual annotations. We demonstrate the utility of\ntransparent liquid segmentation in a robotic pouring task that controls pouring\nby perceiving the liquid height in a transparent cup. Accompanying video and\nsupplementary materials can be found\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_G/0/1/0/all/0/1\">Gautham Narayan Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1\">Ben Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum-style Local-to-global Adaptation for Cross-domain Remote Sensing Image Segmentation. (arXiv:2203.01539v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01539","description":"<p>Although domain adaptation has been extensively studied in natural\nimage-based segmentation task, the research on cross-domain segmentation for\nvery high resolution (VHR) remote sensing images (RSIs) still remains\nunderexplored. The VHR RSIs-based cross-domain segmentation mainly faces two\ncritical challenges: 1) Large area land covers with many diverse object\ncategories bring severe local patch-level data distribution deviations, thus\nyielding different adaptation difficulties for different local patches; 2)\nDifferent VHR sensor types or dynamically changing modes cause the VHR images\nto go through intensive data distribution differences even for the same\ngeographical location, resulting in different global feature-level domain gap.\nTo address these challenges, we propose a curriculum-style local-to-global\ncross-domain adaptation framework for the segmentation of VHR RSIs. The\nproposed curriculum-style adaptation performs the adaptation process in an\neasy-to-hard way according to the adaptation difficulties that can be obtained\nusing an entropy-based score for each patch of the target domain, and thus well\naligns the local patches in a domain image. The proposed local-to-global\nadaptation performs the feature alignment process from the locally semantic to\nglobally structural feature discrepancies, and consists of a semantic-level\ndomain classifier and an entropy-level domain classifier that can reduce the\nabove cross-domain feature discrepancies. Extensive experiments have been\nconducted in various cross-domain scenarios, including geographic location\nvariations and imaging mode variations, and the experimental results\ndemonstrate that the proposed method can significantly boost the domain\nadaptability of segmentation networks for VHR RSIs. Our code is available at:\nhttps://github.com/BOBrown/CCDA_LGFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegTAD: Precise Temporal Action Detection via Semantic Segmentation. (arXiv:2203.01542v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01542","description":"<p>Temporal action detection (TAD) is an important yet challenging task in video\nanalysis. Most existing works draw inspiration from image object detection and\ntend to reformulate it as a proposal generation - classification problem.\nHowever, there are two caveats with this paradigm. First, proposals are not\nequipped with annotated labels, which have to be empirically compiled, thus the\ninformation in the annotations is not necessarily precisely employed in the\nmodel training process. Second, there are large variations in the temporal\nscale of actions, and neglecting this fact may lead to deficient representation\nin the video features. To address these issues and precisely model temporal\naction detection, we formulate the task of temporal action detection in a novel\nperspective of semantic segmentation. Owing to the 1-dimensional property of\nTAD, we are able to convert the coarse-grained detection annotations to\nfine-grained semantic segmentation annotations for free. We take advantage of\nthem to provide precise supervision so as to mitigate the impact induced by the\nimprecise proposal labels. We propose an end-to-end framework SegTAD composed\nof a 1D semantic segmentation network (1D-SSN) and a proposal detection network\n(PDN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramazanova_M/0/1/0/all/0/1\">Merey Ramazanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the role of normalization and residual blocks for spiking neural networks. (arXiv:2203.01544v1 [cs.NE])","link":"http://arxiv.org/abs/2203.01544","description":"<p>Biologically inspired spiking neural networks (SNNs) are widely used to\nrealize ultralow-power energy consumption. However, deep SNNs are not easy to\ntrain due to the excessive firing of spiking neurons in the hidden layers. To\ntackle this problem, we propose a novel but simple normalization technique\ncalled postsynaptic potential normalization. This normalization removes the\nsubtraction term from the standard normalization and uses the second raw moment\ninstead of the variance as the division term. The spike firing can be\ncontrolled, enabling the training to proceed appropriating, by conducting this\nsimple normalization to the postsynaptic potential. The experimental results\nshow that SNNs with our normalization outperformed other models using other\nnormalizations. Furthermore, through the pre-activation residual blocks, the\nproposed model can train with more than 100 layers without other special\ntechniques dedicated to SNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ikegawa_S/0/1/0/all/0/1\">Shin-ichi Ikegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saiin_R/0/1/0/all/0/1\">Ryuji Saiin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_Y/0/1/0/all/0/1\">Yoshihide Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natori_N/0/1/0/all/0/1\">Naotake Natori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing the Shape-Radiance Ambiguity in View-Dependent Radiance Fields. (arXiv:2203.01553v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01553","description":"<p>We present a method for handling view-dependent information in radiance\nfields to help with convergence and quality of 3D reconstruction. Radiance\nfields with view-dependence suffers from the so called shape-radiance\nambiguity, which can lead to incorrect geometry given a high angular resolution\nof view-dependent colors. We propose the addition of a difference plane in\nfront of each camera, with the purpose of separating view-dependent and\nLambertian components during training. We also propose an additional step where\nwe train, but do not store, a low-resolution view-dependent function that helps\nto isolate the surface if such a separation is proven difficult. These\nadditions have a small impact on performance and memory usage but enables\nreconstruction of scenes with highly specular components without any other\nexplicit handling of view-dependence such as Spherical Harmonics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasmuson_S/0/1/0/all/0/1\">Sverker Rasmuson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sintorn_E/0/1/0/all/0/1\">Erik Sintorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assarsson_U/0/1/0/all/0/1\">Ulf Assarsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Ego-Motion Estimation Based on Multi-Layer Fusion of RGB and Inferred Depth. (arXiv:2203.01557v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01557","description":"<p>In existing self-supervised depth and ego-motion estimation methods,\nego-motion estimation is usually limited to only leveraging RGB information.\nRecently, several methods have been proposed to further improve the accuracy of\nself-supervised ego-motion estimation by fusing information from other\nmodalities, e.g., depth, acceleration, and angular velocity. However, they\nrarely focus on how different fusion strategies affect performance. In this\npaper, we investigate the effect of different fusion strategies for ego-motion\nestimation and propose a new framework for self-supervised learning of depth\nand ego-motion estimation, which performs ego-motion estimation by leveraging\nRGB and inferred depth information in a Multi-Layer Fusion manner. As a result,\nwe have achieved state-of-the-art performance among learning-based methods on\nthe KITTI odometry benchmark. Detailed studies on the design choices of\nleveraging inferred depth information and fusion strategies have also been\ncarried out, which clearly demonstrate the advantages of our proposed\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zijie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taira_H/0/1/0/all/0/1\">Hajime Taira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyashita_N/0/1/0/all/0/1\">Naoyuki Miyashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection. (arXiv:2203.01562v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01562","description":"<p>Face Presentation Attack Detection (PAD) is an important measure to prevent\nspoof attacks for face biometric systems. Many works based on Convolution\nNeural Networks (CNNs) for face PAD formulate the problem as an image-level\nbinary classification task without considering the context. Alternatively,\nVision Transformers (ViT) using self-attention to attend the context of an\nimage become the mainstreams in face PAD. Inspired by ViT, we propose a\nVideo-based Transformer for face PAD (ViTransPAD) with short/long-range\nspatio-temporal attention which can not only focus on local details with short\nattention within a frame but also capture long-range dependencies over frames.\nInstead of using coarse image patches with single-scale as in ViT, we propose\nthe Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate\nmulti-scale patch partitions of Q, K, V feature maps to the heads of\ntransformer in a coarse-to-fine manner, which enables to learn a fine-grained\nrepresentation to perform pixel-level discrimination for face PAD. Due to lack\ninductive biases of convolutions in pure transformers, we also introduce\nconvolutions to the proposed ViTransPAD to integrate the desirable properties\nof CNNs by using convolution patch embedding and convolution projection. The\nextensive experiments show the effectiveness of our proposed ViTransPAD with a\npreferable accuracy-computation balance, which can serve as a new backbone for\nface PAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ghadi_M/0/1/0/all/0/1\">Musab Al-Ghadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visani_M/0/1/0/all/0/1\">Muriel Visani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MuzzamilLuqman_M/0/1/0/all/0/1\">Muhammad MuzzamilLuqman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burie_J/0/1/0/all/0/1\">Jean-Christophe Burie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-Aware Cost Constructor for Light Field Depth Estimation. (arXiv:2203.01576v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01576","description":"<p>Matching cost construction is a key step in light field (LF) depth\nestimation, but was rarely studied in the deep learning era. Recent deep\nlearning-based LF depth estimation methods construct matching cost by\nsequentially shifting each sub-aperture image (SAI) with a series of predefined\noffsets, which is complex and time-consuming. In this paper, we propose a\nsimple and fast cost constructor to construct matching cost for LF depth\nestimation. Our cost constructor is composed by a series of convolutions with\nspecifically designed dilation rates. By applying our cost constructor to SAI\narrays, pixels under predefined disparities can be integrated and matching cost\ncan be constructed without using any shifting operation. More importantly, the\nproposed cost constructor is occlusion-aware and can handle occlusions by\ndynamically modulating pixels from different views. Based on the proposed cost\nconstructor, we develop a deep network for LF depth estimation. Our network\nranks first on the commonly used 4D LF benchmark in terms of the mean square\nerror (MSE), and achieves a faster running time than other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01577","description":"<p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,\nto catalyze the research of category-level human-object interaction. HOI4D\nconsists of 3M RGB-D egocentric video frames over 5000 sequences collected by 9\nparticipants interacting with 1000 different object instances from 20\ncategories over 610 different indoor rooms. Frame-wise annotations for panoptic\nsegmentation, motion segmentation, 3D hand pose, category-level object pose and\nhand action have also been provided, together with reconstructed object meshes\nand scene point clouds. With HOI4D, we establish three benchmarking tasks to\npromote category-level HOI from 4D visual signals including semantic\nsegmentation of 4D dynamic point cloud sequences, category-level object pose\ntracking, and egocentric action segmentation with diverse interaction targets.\nIn-depth analysis shows HOI4D poses great challenges to existing methods and\nproduces great research opportunities. We will release the dataset soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Che Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoujie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kangbo Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weikang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Boqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal Backward-Compatible Representation Learning. (arXiv:2203.01583v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01583","description":"<p>Conventional model upgrades for visual search systems require offline refresh\nof gallery features by feeding gallery images into new models (dubbed as\n\"backfill\"), which is time-consuming and expensive, especially in large-scale\napplications. The task of backward-compatible representation learning is\ntherefore introduced to support backfill-free model upgrades, where the new\nquery features are interoperable with the old gallery features. Despite the\nsuccess, previous works only investigated a close-set training scenario (i.e.,\nthe new training set shares the same classes as the old one), and are limited\nby more realistic and challenging open-set scenarios. To this end, we first\nintroduce a new problem of universal backward-compatible representation\nlearning, covering all possible data split in model upgrades. We further\npropose a simple yet effective method, dubbed as Universal Backward-Compatible\nTraining (UniBCT) with a novel structural prototype refinement algorithm, to\nlearn compatible representations in all kinds of model upgrading benchmarks in\na unified manner. Comprehensive experiments on the large-scale face recognition\ndatasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yantao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shupeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tailed Vision Transformer for Efficient Inference. (arXiv:2203.01587v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01587","description":"<p>Recently, Vision Transformer (ViT) has achieved promising performance in\nimage recognition and gradually serves as a powerful backbone in various vision\ntasks. To satisfy the sequential input of Transformer, the tail of ViT first\nsplits each image into a sequence of visual tokens with a fixed length. Then\nthe following self-attention layers constructs the global relationship between\ntokens to produce useful representation for the downstream tasks. Empirically,\nrepresenting the image with more tokens leads to better performance, yet the\nquadratic computational complexity of self-attention layer to the number of\ntokens could seriously influence the efficiency of ViT's inference. For\ncomputational reduction, a few pruning methods progressively prune\nuninformative tokens in the Transformer encoder, while leaving the number of\ntokens before the Transformer untouched. In fact, fewer tokens as the input for\nthe Transformer encoder can directly reduce the following computational cost.\nIn this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the\npaper. MT-ViT adopts multiple tails to produce visual sequences of different\nlengths for the following Transformer encoder. A tail predictor is introduced\nto decide which tail is the most efficient for the image to produce accurate\nprediction. Both modules are optimized in an end-to-end fashion, with the\nGumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can\nachieve a significant reduction on FLOPs with no degradation of the accuracy\nand outperform other compared methods in both accuracy and FLOPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Motion Prediction: A Survey. (arXiv:2203.01593v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01593","description":"<p>3D human motion prediction, predicting future poses from a given sequence, is\nan issue of great significance and challenge in computer vision and machine\nintelligence, which can help machines in understanding human behaviors. Due to\nthe increasing development and understanding of Deep Neural Networks (DNNs) and\nthe availability of large-scale human motion datasets, the human motion\nprediction has been remarkably advanced with a surge of interest among academia\nand industrial community. In this context, a comprehensive survey on 3D human\nmotion prediction is conducted for the purpose of retrospecting and analyzing\nrelevant works from existing released literature. In addition, a pertinent\ntaxonomy is constructed to categorize these existing approaches for 3D human\nmotion prediction. In this survey, relevant methods are categorized into three\ncategories: human pose representation, network structure design, and\n\\textit{prediction target}. We systematically review all relevant journal and\nconference papers in the field of human motion prediction since 2015, which are\npresented in detail based on proposed categorizations in this survey.\nFurthermore, the outline for the public benchmark datasets, evaluation\ncriteria, and performance comparisons are respectively presented in this paper.\nThe limitations of the state-of-the-art methods are discussed as well, hoping\nfor paving the way for future explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kedi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism. (arXiv:2203.01594v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01594","description":"<p>Image captioning is a fast-growing research field of computer vision and\nnatural language processing that involves creating text explanations for\nimages. This study aims to develop a system that uses a pre-trained\nconvolutional neural network (CNN) to extract features from an image,\nintegrates the features with an attention mechanism, and creates captions using\na recurrent neural network (RNN). To encode an image into a feature vector as\ngraphical attributes, we employed multiple pre-trained convolutional neural\nnetworks. Following that, a language model known as GRU is chosen as the\ndecoder to construct the descriptive sentence. In order to increase\nperformance, we merge the Bahdanau attention model with GRU to allow learning\nto be focused on a specific portion of the image. On the MSCOCO dataset, the\nexperimental results achieve competitive performance against state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rashid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">M Shujah Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_K/0/1/0/all/0/1\">Khadija Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1\">Mansoor Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Imran Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhongfu Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01601","description":"<p>Handwritten mathematical expression recognition (HMER) is a challenging task\nthat has many potential applications. Recent methods for HMER have achieved\noutstanding performance with an encoder-decoder architecture. However, these\nmethods adhere to the paradigm that the prediction is made \"from one character\nto another\", which inevitably yields prediction errors due to the complicated\nstructures of mathematical expressions or crabbed handwritings. In this paper,\nwe propose a simple and efficient method for HMER, which is the first to\nincorporate syntax information into an encoder-decoder network. Specifically,\nwe present a set of grammar rules for converting the LaTeX markup sequence of\neach expression into a parsing tree; then, we model the markup sequence\nprediction as a tree traverse process with a deep neural network. In this way,\nthe proposed method can effectively describe the syntax context of expressions,\navoiding the structure prediction errors of HMER. Experiments on two benchmark\ndatasets demonstrate that our method achieves significantly better recognition\nperformance than prior arts. To further validate the effectiveness of our\nmethod, we create a large-scale dataset consisting of 100k handwritten\nmathematical expression images acquired from ten thousand writers. The source\ncode, new dataset, and pre-trained models of this work will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1\">Wondimu Dikubab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Path Planning for UAVs for Multi-Resolution Semantic Segmentation. (arXiv:2203.01642v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01642","description":"<p>Efficient data collection methods play a major role in helping us better\nunderstand the Earth and its ecosystems. In many applications, the usage of\nunmanned aerial vehicles (UAVs) for monitoring and remote sensing is rapidly\ngaining momentum due to their high mobility, low cost, and flexible deployment.\nA key challenge is planning missions to maximize the value of acquired data in\nlarge environments given flight time limitations. This is, for example,\nrelevant for monitoring agricultural fields. This paper addresses the problem\nof adaptive path planning for accurate semantic segmentation of using UAVs. We\npropose an online planning algorithm which adapts the UAV paths to obtain\nhigh-resolution semantic segmentations necessary in areas with fine details as\nthey are detected in incoming images. This enables us to perform close\ninspections at low altitudes only where required, without wasting energy on\nexhaustive mapping at maximum image resolution. A key feature of our approach\nis a new accuracy model for deep learning-based architectures that captures the\nrelationship between UAV altitude and semantic segmentation accuracy. We\nevaluate our approach on different domains using real-world data, proving the\nefficacy and generability of our solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stache_F/0/1/0/all/0/1\">Felix Stache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westheider_J/0/1/0/all/0/1\">Jonas Westheider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magistri_F/0/1/0/all/0/1\">Federico Magistri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1\">Marija Popovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Residual M-Net for Real Image Denoising. (arXiv:2203.01645v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01645","description":"<p>Image restoration is a low-level vision task which is to restore degraded\nimages to noise-free images. With the success of deep neural networks, the\nconvolutional neural networks surpass the traditional restoration methods and\nbecome the mainstream in the computer vision area. To advance the performanceof\ndenoising algorithms, we propose a blind real image denoising network (SRMNet)\nby employing a hierarchical architecture improved from U-Net. Specifically, we\nuse a selective kernel with residual block on the hierarchical structure called\nM-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet\nhas competitive performance results on two synthetic and two real-world noisy\ndatasets in terms of quantitative metrics and visual quality. The source code\nand pretrained model are available at\nhttps://github.com/TentativeGitHub/SRMNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_C/0/1/0/all/0/1\">Chi-Mao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tsung-Jung Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kuan-Hsien Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correlation-Aware Deep Tracking. (arXiv:2203.01666v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01666","description":"<p>Robustness and discrimination power are two fundamental requirements in\nvisual object tracking. In most tracking paradigms, we find that the features\nextracted by the popular Siamese-like networks cannot fully discriminatively\nmodel the tracked targets and distractor objects, hindering them from\nsimultaneously meeting these two requirements. While most methods focus on\ndesigning robust correlation operations, we propose a novel target-dependent\nfeature network inspired by the self-/cross-attention scheme. In contrast to\nthe Siamese-like feature extraction, our network deeply embeds cross-image\nfeature correlation in multiple layers of the feature network. By extensively\nmatching the features of the two images through multiple layers, it is able to\nsuppress non-target features, resulting in instance-varying feature extraction.\nThe output features of the search image can be directly used for predicting\ntarget locations without extra correlation step. Moreover, our model can be\nflexibly pre-trained on abundant unpaired images, leading to notably faster\nconvergence than the existing methods. Extensive experiments show our method\nachieves the state-of-the-art results while running at real-time. Our feature\nnetworks also can be applied to existing tracking pipelines seamlessly to raise\nthe tracking performance. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translational Lung Imaging Analysis Through Disentangled Representations. (arXiv:2203.01668v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01668","description":"<p>The development of new treatments often requires clinical trials with\ntranslational animal models using (pre)-clinical imaging to characterize\ninter-species pathological processes. Deep Learning (DL) models are commonly\nused to automate retrieving relevant information from the images. Nevertheless,\nthey typically suffer from low generability and explainability as a product of\ntheir entangled design, resulting in a specific DL model per animal model.\nConsequently, it is not possible to take advantage of the high capacity of DL\nto discover statistical relationships from inter-species images.\n</p>\n<p>To alleviate this problem, in this work, we present a model capable of\nextracting disentangled information from images of different animal models and\nthe mechanisms that generate the images. Our method is located at the\nintersection between deep generative models, disentanglement and causal\nrepresentation learning. It is optimized from images of pathological lung\ninfected by Tuberculosis and is able: a) from an input slice, infer its\nposition in a volume, the animal model to which it belongs, the damage present\nand even more, generate a mask covering the whole lung (similar overlap\nmeasures to the nnU-Net), b) generate realistic lung images by setting the\nabove variables and c) generate counterfactual images, namely, healthy versions\nof a damaged input slice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gordaliza_P/0/1/0/all/0/1\">Pedro M. Gordaliza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaquero_J/0/1/0/all/0/1\">Juan Jos&#xe9; Vaquero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained unsupervised anomaly segmentation. (arXiv:2203.01671v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01671","description":"<p>Current unsupervised anomaly localization approaches rely on generative\nmodels to learn the distribution of normal images, which is later used to\nidentify potential anomalous regions derived from errors on the reconstructed\nimages. However, a main limitation of nearly all prior literature is the need\nof employing anomalous images to set a class-specific threshold to locate the\nanomalies. This limits their usability in realistic scenarios, where only\nnormal data is typically accessible. Despite this major drawback, only a\nhandful of works have addressed this limitation, by integrating supervision on\nattention maps during training. In this work, we propose a novel formulation\nthat does not require accessing images with abnormalities to define the\nthreshold. Furthermore, and in contrast to very recent work, the proposed\nconstraint is formulated in a more principled manner, leveraging well-known\nknowledge in constrained optimization. In particular, the equality constraint\non the attention maps in prior work is replaced by an inequality constraint,\nwhich allows more flexibility. In addition, to address the limitations of\npenalty-based functions we employ an extension of the popular log-barrier\nmethods to handle the constraint. Last, we propose an alternative\nregularization term that maximizes the Shannon entropy of the attention maps,\nreducing the amount of hyperparameters of the proposed model. Comprehensive\nexperiments on two publicly available datasets on brain lesion segmentation\ndemonstrate that the proposed approach substantially outperforms relevant\nliterature, establishing new state-of-the-art results for unsupervised lesion\nsegmentation, and without the need to access anomalous images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Earth Mover's Distance for Visible Thermal Person Re-Identification. (arXiv:2203.01675v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01675","description":"<p>Visible thermal person re-identification (VT-ReID) suffers from the\ninter-modality discrepancy and intra-identity variations. Distribution\nalignment is a popular solution for VT-ReID, which, however, is usually\nrestricted to the influence of the intra-identity variations. In this paper, we\npropose the Cross-Modality Earth Mover's Distance (CM-EMD) that can alleviate\nthe impact of the intra-identity variations during modality alignment. CM-EMD\nselects an optimal transport strategy and assigns high weights to pairs that\nhave a smaller intra-identity variation. In this manner, the model will focus\non reducing the inter-modality discrepancy while paying less attention to\nintra-identity variations, leading to a more effective modality alignment.\nMoreover, we introduce two techniques to improve the advantage of CM-EMD.\nFirst, the Cross-Modality Discrimination Learning (CM-DL) is designed to\novercome the discrimination degradation problem caused by modality alignment.\nBy reducing the ratio between intra-identity and inter-identity variances,\nCM-DL leads the model to learn more discriminative representations. Second, we\nconstruct the Multi-Granularity Structure (MGS), enabling us to align\nmodalities from both coarse- and fine-grained levels with the proposed CM-EMD.\nExtensive experiments show the benefits of the proposed CM-EMD and its\nauxiliary techniques (CM-DL and MGS). Our method achieves state-of-the-art\nperformance on two VT-ReID benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yongguo Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Donglin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yaojin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaozi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Source-to-target Gap for Cross-domain Person Re-Identification with Intermediate Domains. (arXiv:2203.01682v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01682","description":"<p>Cross-domain person re-identification (re-ID), such as unsupervised domain\nadaptive (UDA) re-ID, aims to transfer the identity-discriminative knowledge\nfrom the source to the target domain. Existing methods commonly consider the\nsource and target domains are isolated from each other, i.e., no intermediate\nstatus is modeled between both domains. Directly transferring the knowledge\nbetween two isolated domains can be very difficult, especially when the domain\ngap is large. From a novel perspective, we assume these two domains are not\ncompletely isolated, but can be connected through intermediate domains. Instead\nof directly aligning the source and target domains against each other, we\npropose to align the source and target domains against their intermediate\ndomains for a smooth knowledge transfer. To discover and utilize these\nintermediate domains, we propose an Intermediate Domain Module (IDM) and a\nMirrors Generation Module (MGM). IDM has two functions: 1) it generates\nmultiple intermediate domains by mixing the hidden-layer features from source\nand target domains and 2) it dynamically reduces the domain gap between the\nsource / target domain features and the intermediate domain features. While IDM\nachieves good domain alignment, it introduces a side effect, i.e., the mix-up\noperation may mix the identities into a new identity and lose the original\nidentities. To compensate this, MGM is introduced by mapping the features into\nthe IDM-generated intermediate domains without changing their original\nidentity. It allows to focus on minimizing domain variations to promote the\nalignment between the source / target domain and intermediate domains, which\nreinforces IDM into IDM++. We extensively evaluate our method under both the\nUDA and domain generalization (DG) scenarios and observe that IDM++ yields\nconsistent performance improvement for cross-domain re-ID, achieving new state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative distance matters for one-shot landmark detection. (arXiv:2203.01687v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01687","description":"<p>Contrastive learning based methods such as cascade comparing to detect (CC2D)\nhave shown great potential for one-shot medical landmark detection. However,\nthe important cue of relative distance between landmarks is ignored in CC2D. In\nthis paper, we upgrade CC2D to version II by incorporating a\nsimple-yet-effective relative distance bias in the training stage, which is\ntheoretically proved to encourage the encoder to project the relatively distant\nlandmarks to the embeddings with low similarities. As consequence, CC2Dv2 is\nless possible to detect a wrong point far from the correct landmark.\nFurthermore, we present an open-source, landmark-labeled dataset for the\nmeasurement of biomechanical parameters of the lower extremity to alleviate the\nburden of orthopedic surgeons. The effectiveness of CC2Dv2 is evaluated on the\npublic dataset from the ISBI 2015 Grand-Challenge of cephalometric radiographs\nand our new dataset, which greatly outperforms the state-of-the-art one-shot\nlandmark detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yihua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction. (arXiv:2203.01703v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01703","description":"<p>Reconstructing 3D objects from 2D images is both challenging for our brains\nand machine learning algorithms. To support this spatial reasoning task,\ncontextual information about the overall shape of an object is critical.\nHowever, such information is not captured by established loss terms (e.g. Dice\nloss). We propose to complement geometrical shape information by including\nmulti-scale topological features, such as connected components, cycles, and\nvoids, in the reconstruction loss. Our method calculates topological features\nfrom 3D volumetric data based on cubical complexes and uses an optimal\ntransport distance to guide the reconstruction process. This topology-aware\nloss is fully differentiable, computationally efficient, and can be added to\nany neural network. We demonstrate the utility of our loss by incorporating it\ninto SHAPR, a model for predicting the 3D cell shape of individual cells based\non 2D microscopy images. Using a hybrid loss that leverages both geometrical\nand topological information of single objects to assess their shape, we find\nthat topological information substantially improves the quality of\nreconstructions, thus highlighting its ability to extract more relevant\nfeatures from image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waibel_D/0/1/0/all/0/1\">Dominik J. E. Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_S/0/1/0/all/0/1\">Scott Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_M/0/1/0/all/0/1\">Matthias Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01714","description":"<p>Weakly supervised object localization (WSOL) focuses on localizing objects\nonly with the supervision of image-level classification masks. Most previous\nWSOL methods follow the classification activation map (CAM) that localizes\nobjects based on the classification structure with the multi-instance learning\n(MIL) mechanism. However, the MIL mechanism makes CAM only activate\ndiscriminative object parts rather than the whole object, weakening its\nperformance for localizing objects. To avoid this problem, this work provides a\nnovel perspective that models WSOL as a domain adaption (DA) task, where the\nscore estimator trained on the source/image domain is tested on the\ntarget/pixel domain to locate objects. Under this perspective, a DA-WSOL\npipeline is designed to better engage DA approaches into WSOL to enhance\nlocalization performance. It utilizes a proposed target sampling strategy to\nselect different types of target samples. Based on these types of target\nsamples, domain adaption localization (DAL) loss is elaborated. It aligns the\nfeature distribution between the two domains by DA and makes the estimator\nperceive target domain cues by Universum regularization. Experiments show that\nour pipeline outperforms SOTA methods on multi benchmarks. Code are released at\n\\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yunfei You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting High-Quality GAN-Generated Face Images using Neural Networks. (arXiv:2203.01716v1 [cs.CR])","link":"http://arxiv.org/abs/2203.01716","description":"<p>In the past decades, the excessive use of the last-generation GAN (Generative\nAdversarial Networks) models in computer vision has enabled the creation of\nartificial face images that are visually indistinguishable from genuine ones.\nThese images are particularly used in adversarial settings to create fake\nsocial media accounts and other fake online profiles. Such malicious activities\ncan negatively impact the trustworthiness of users identities. On the other\nhand, the recent development of GAN models may create high-quality face images\nwithout evidence of spatial artifacts. Therefore, reassembling uniform color\nchannel correlations is a challenging research problem. To face these\nchallenges, we need to develop efficient tools able to differentiate between\nfake and authentic face images. In this chapter, we propose a new strategy to\ndifferentiate GAN-generated images from authentic images by leveraging spectral\nband discrepancies, focusing on artificial face image synthesis. In particular,\nwe enable the digital preservation of face images using the Cross-band\nco-occurrence matrix and spatial co-occurrence matrix. Then, we implement these\ntechniques and feed them to a Convolutional Neural Networks (CNN) architecture\nto identify the real from artificial faces. Additionally, we show that the\nperformance boost is particularly significant and achieves more than 92% in\ndifferent post-processing environments. Finally, we provide several research\nobservations demonstrating that this strategy improves a comparable detection\nmethod based only on intra-band spatial co-occurrences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1\">Yassine Mekdad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-Identification. (arXiv:2203.01723v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01723","description":"<p>Generalizable person re-identification aims to learn a model with only\nseveral labeled source domains that can perform well on unseen domains. Without\naccess to the unseen domain, the feature statistics of the batch normalization\n(BN) layer learned from a limited number of source domains is doubtlessly\nbiased for unseen domain. This would mislead the feature representation\nlearning for unseen domain and deteriorate the generalizaiton ability of the\nmodel. In this paper, we propose a novel Debiased Batch Normalization via\nGaussian Process approach (GDNorm) for generalizable person re-identification,\nwhich models the feature statistic estimation from BN layers as a dynamically\nself-refining Gaussian process to alleviate the bias to unseen domain for\nimproving the generalization. Specifically, we establish a lightweight model\nwith multiple set of domain-specific BN layers to capture the discriminability\nof individual source domain, and learn the corresponding parameters of the\ndomain-specific BN layers. These parameters of different source domains are\nemployed to deduce a Gaussian process. We randomly sample several paths from\nthis Gaussian process served as the BN estimations of potential new domains\noutside of existing source domains, which can further optimize these learned\nparameters from source domains, and estimate more accurate Gaussian process by\nthem in return, tending to real data distribution. Even without a large number\nof source domains, GDNorm can still provide debiased BN estimation by using the\nmean path of the Gaussian process, while maintaining low computational cost\nduring testing. Extensive experiments demonstrate that our GDNorm effectively\nimproves the generalization ability of the model on unseen domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology. (arXiv:2203.01726v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01726","description":"<p>Monitoring biodiversity is paramount to manage and protect natural resources,\nparticularly in times of global change. Collecting images of organisms over\nlarge temporal or spatial scales is a promising practice to monitor and study\nbiodiversity change of natural ecosystems, providing large amounts of data with\nminimal interference with the environment. Deep learning models are currently\nused to automate classification of organisms into taxonomic units. However,\nimprecision in these classifiers introduce a measurement noise that is\ndifficult to control and can significantly hinder the analysis and\ninterpretation of data. In our study, we show that this limitation can be\novercome by ensembles of Data-efficient image Transformers (DeiTs), which\nsignificantly outperform the previous state of the art (SOTA). We validate our\nresults on a large number of ecological imaging datasets of diverse origin, and\norganisms of study ranging from plankton to insects, birds, dog breeds, animals\nin the wild, and corals. On all the data sets we test, we achieve a new SOTA,\nwith a reduction of the error with respect to the previous SOTA ranging from\n18.48% to 87.50%, depending on the data set, and often achieving performances\nvery close to perfect classification. The main reason why ensembles of DeiTs\nperform better is not due to the single-model performance of DeiTs, but rather\nto the fact that predictions by independent models have a smaller overlap, and\nthis maximizes the profit gained by ensembling. This positions DeiT ensembles\nas the best candidate for image classification in biodiversity monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kyathanahally_S/0/1/0/all/0/1\">S. Kyathanahally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardeman_T/0/1/0/all/0/1\">T. Hardeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">M. Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merz_E/0/1/0/all/0/1\">E. Merz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulas_T/0/1/0/all/0/1\">T. Bulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomati_F/0/1/0/all/0/1\">F. Pomati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baity_Jesi_M/0/1/0/all/0/1\">M. Baity-Jesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds. (arXiv:2203.01730v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01730","description":"<p>3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role\nin autonomous driving. Current approaches all follow the Siamese paradigm based\non appearance matching. However, LiDAR point clouds are usually textureless and\nincomplete, which hinders effective appearance matching. Besides, previous\nmethods greatly overlook the critical motion clues among targets. In this work,\nbeyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle 3D\nSOT from a new perspective. Following this paradigm, we propose a matching-free\ntwo-stage tracker M^2-Track. At the 1^st-stage, M^2-Track localizes the target\nwithin successive frames via motion transformation. Then it refines the target\nbox through motion-assisted shape completion at the 2^nd-stage. Extensive\nexperiments confirm that M^2-Track significantly outperforms previous\nstate-of-the-arts on three large-scale datasets while running at 57FPS (~8%,\n~17%, and ~22%) precision gains on KITTI, NuScenes, and Waymo Open Dataset\nrespectively). Further analysis verifies each component's effectiveness and\nshows the motion-centric paradigm's promising potential when combined with\nappearance matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification. (arXiv:2203.01735v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01735","description":"<p>RGB-infrared person re-identification is an emerging cross-modality\nre-identification task, which is very challenging due to significant modality\ndiscrepancy between RGB and infrared images. In this work, we propose a novel\nmodality-adaptive mixup and invariant decomposition (MID) approach for\nRGB-infrared person re-identification towards learning modality-invariant and\ndiscriminative representations. MID designs a modality-adaptive mixup scheme to\ngenerate suitable mixed modality images between RGB and infrared images for\nmitigating the inherent modality discrepancy at the pixel-level. It formulates\nmodality mixup procedure as Markov decision process, where an actor-critic\nagent learns dynamical and local linear interpolation policy between different\nregions of cross-modality images under a deep reinforcement learning framework.\nSuch policy guarantees modality-invariance in a more continuous latent space\nand avoids manifold intrusion by the corrupted mixed modality samples.\nMoreover, to further counter modality discrepancy and enforce invariant visual\nsemantics at the feature-level, MID employs modality-adaptive convolution\ndecomposition to disassemble a regular convolution layer into modality-specific\nbasis layers and a modality-shared coefficient layer. Extensive experimental\nresults on two challenging benchmarks demonstrate superior performance of MID\nover state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence. (arXiv:2203.01754v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01754","description":"<p>We present a novel method to learn Personalized Implicit Neural Avatars\n(PINA) from a short RGB-D sequence. This allows non-expert users to create a\ndetailed and personalized virtual copy of themselves, which can be animated\nwith realistic clothing deformations. PINA does not require complete scans, nor\ndoes it require a prior learned from large datasets of clothed humans. Learning\na complete avatar in this setting is challenging, since only few depth\nobservations are available, which are noisy and incomplete (i.e.only partial\nvisibility of the body per frame). We propose a method to learn the shape and\nnon-rigid deformations via a pose-conditioned implicit surface and a\ndeformation field, defined in canonical space. This allows us to fuse all\npartial observations into a single consistent canonical representation. Fusion\nis formulated as a global optimization problem over the pose, shape and\nskinning parameters. The method can learn neural avatars from real noisy RGB-D\nsequences for a diverse set of people and clothing styles and these avatars can\nbe animated given unseen motion sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zijian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields. (arXiv:2203.01762v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01762","description":"<p>Deep learning has shown great potential for modeling the physical dynamics of\ncomplex particle systems such as fluids (in Lagrangian descriptions). Existing\napproaches, however, require the supervision of consecutive particle\nproperties, including positions and velocities. In this paper, we consider a\npartially observable scenario known as fluid dynamics grounding, that is,\ninferring the state transitions and interactions within the fluid particle\nsystems from sequential visual observations of the fluid surface. We propose a\ndifferentiable two-stage network named NeuroFluid. Our approach consists of (i)\na particle-driven neural renderer, which involves fluid physical properties\ninto the volume rendering function, and (ii) a particle transition model\noptimized to reduce the differences between the rendered and the observed\nimages. NeuroFluid provides the first solution to unsupervised learning of\nparticle-based fluid dynamics by training these two models jointly. It is shown\nto reasonably estimate the underlying physics of fluids with different initial\nshapes, viscosity, and densities. It is a potential alternative approach to\nunderstanding complex fluid mechanics, such as turbulence, that are difficult\nto model using traditional methods of mathematical physics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shanyan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huayu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Quantum Neural Networks (RQNN) for Noisy Image Recognition. (arXiv:2203.01764v1 [quant-ph])","link":"http://arxiv.org/abs/2203.01764","description":"<p>Classical Random Neural Networks (RNNs) have demonstrated effective\napplications in decision making, signal processing, and image recognition\ntasks. However, their implementation has been limited to deterministic digital\nsystems that output probability distributions in lieu of stochastic behaviors\nof random spiking signals. We introduce the novel class of supervised Random\nQuantum Neural Networks (RQNNs) with a robust training strategy to better\nexploit the random nature of the spiking RNN. The proposed RQNN employs hybrid\nclassical-quantum algorithms with superposition state and amplitude encoding\nfeatures, inspired by quantum information theory and the brain's\nspatial-temporal stochastic spiking property of neuron information encoding. We\nhave extensively validated our proposed RQNN model, relying on hybrid\nclassical-quantum algorithms via the PennyLane Quantum simulator with a limited\nnumber of \\emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST\ndatasets demonstrate that the proposed RQNN model achieves an average\nclassification accuracy of $94.9\\%$. Additionally, the experimental findings\nillustrate the proposed RQNN's effectiveness and resilience in noisy settings,\nwith enhanced image classification accuracy when compared to the classical\ncounterparts (RNNs), classical Spiking Neural Networks (SNNs), and the\nclassical convolutional neural network (AlexNet). Furthermore, the RQNN can\ndeal with noise, which is useful for various applications, including computer\nvision in NISQ devices. The PyTorch code (https://github.com/darthsimpus/RQN)\nis made available on GitHub to reproduce the results reported in this\nmanuscript.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Konar_D/0/1/0/all/0/1\">Debanjan Konar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gelenbe_E/0/1/0/all/0/1\">Erol Gelenbe</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bhandary_S/0/1/0/all/0/1\">Soham Bhandary</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sarma_A/0/1/0/all/0/1\">Aditya Das Sarma</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cangi_A/0/1/0/all/0/1\">Attila Cangi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Click-based Interactive Video Object Segmentation. (arXiv:2203.01784v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01784","description":"<p>While current methods for interactive Video Object Segmentation (iVOS) rely\non scribble-based interactions to generate precise object masks, we propose a\nClick-based interactive Video Object Segmentation (CiVOS) framework to simplify\nthe required user workload as much as possible. CiVOS builds on de-coupled\nmodules reflecting user interaction and mask propagation. The interaction\nmodule converts click-based interactions into an object mask, which is then\ninferred to the remaining frames by the propagation module. Additional user\ninteractions allow for a refinement of the object mask. The approach is\nextensively evaluated on the popular interactive~DAVIS dataset, but with an\ninevitable adaptation of scribble-based interactions with click-based\ncounterparts. We consider several strategies for generating clicks during our\nevaluation to reflect various user inputs and adjust the DAVIS performance\nmetric to perform a hardware-independent comparison. The presented CiVOS\npipeline achieves competitive results, although requiring a lower user\nworkload.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vujasinovic_S/0/1/0/all/0/1\">Stephane Vujasinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullinger_S/0/1/0/all/0/1\">Sebastian Bullinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_Negenborn_N/0/1/0/all/0/1\">Norbert Scherer-Negenborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01785","description":"<p>Deep neural networks are able to memorize noisy labels easily with a softmax\ncross-entropy (CE) loss. Previous studies attempted to address this issue focus\non incorporating a noise-robust loss function to the CE loss. However, the\nmemorization issue is alleviated but still remains due to the non-robust CE\nloss. To address this issue, we focus on learning robust contrastive\nrepresentations of data on which the classifier is hard to memorize the label\nnoise under the CE loss. We propose a novel contrastive regularization function\nto learn such representations over noisy data where label noise does not\ndominate the representation learning. By theoretically investigating the\nrepresentations induced by the proposed regularization function, we reveal that\nthe learned representations keep information related to true labels and discard\ninformation related to corrupted labels. Moreover, our theoretical results also\nindicate that the learned representations are robust to the label noise. The\neffectiveness of this method is demonstrated with experiments on benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLeod_A/0/1/0/all/0/1\">A. Ian McLeod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Local-Global Relational Network for Facial Action Units Recognition and Facial Paralysis Estimation. (arXiv:2203.01800v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01800","description":"<p>Facial action units (AUs) refer to a unique set of facial muscle movements at\ncertain facial locations defined by the Facial Action Coding System (FACS),\nwhich can be used for describing nearly any anatomically possible facial\nexpression. Many existing facial action units (AUs) recognition approaches\noften enhance the AU representation by combining local features from multiple\nindependent branches, each corresponding to a different AU, which usually\nneglect potential mutual assistance and exclusion relationship between AU\nbranches or simply employ a pre-defined and fixed knowledge-graph as a prior.\nIn addition, extracting features from pre-defined AU regions of regular shapes\nlimits the representation ability. In this paper, we propose a novel Adaptive\nLocal-Global Relational Network (ALGRNet) for facial AU recognition and apply\nit to facial paralysis estimation. ALGRNet mainly consists of three novel\nstructures, i.e., an adaptive region learning module which learns the adaptive\nmuscle regions based on the detected landmarks, a skip-BiLSTM module which\nmodels the latent mutual assistance and exclusion relationship among local AU\nfeatures, and a feature fusion\\&amp;refining module which explores the\ncomplementarity between local AUs and the whole face for the local AU\nrefinement. In order to evaluate our proposed method, we migrated ALGRNet to a\nfacial paralysis dataset which is collected and annotated by medical\nprofessionals. Experiments on the BP4D and DISFA AU datasets show that the\nproposed approach outperforms the state-of-the-art methods by a large margin.\nAdditionally, we also demonstrated the effectiveness of the proposed ALGRNet in\napplications to facial paralysis estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Joemon M. Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arunachalam Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intensity Image-based LiDAR Fiducial Marker System. (arXiv:2203.01816v1 [cs.RO])","link":"http://arxiv.org/abs/2203.01816","description":"<p>The fiducial marker system for LiDAR is crucial for the robotic application\nbut it is still rare to date. In this paper, an Intensity Image-based LiDAR\nFiducial Marker (IILFM) system is developed. This system only requires an\nunstructured point cloud with intensity as the input and it has no restriction\non marker placement and shape. A marker detection method that locates the\npredefined 3D fiducials in the point cloud through the intensity image is\nintroduced. Then, an approach that utilizes the detected 3D fiducials to\nestimate the LiDAR 6-DOF pose that describes the transmission from the world\ncoordinate system to the LiDAR coordinate system is developed. Moreover, all\nthese processes run in real-time (approx 40 Hz on Livox Mid-40 and approx 143\nHz on VLP-16). Qualitative and quantitative experiments are conducted to\ndemonstrate that the proposed system has similar convenience and accuracy as\nthe conventional visual fiducial marker system. The codes and results are\navailable at: https://github.com/York-SDCNLab/IILFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schofield_H/0/1/0/all/0/1\">Hunter Schofield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jinjun Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network. (arXiv:2203.01824v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01824","description":"<p>3D room layout estimation by a single panorama using deep neural networks has\nmade great progress. However, previous approaches can not obtain efficient\ngeometry awareness of room layout with the only latitude of boundaries or\nhorizon-depth. We present that using horizon-depth along with room height can\nobtain omnidirectional-geometry awareness of room layout in both horizontal and\nvertical directions. In addition, we propose a planar-geometry aware loss\nfunction with normals and gradients of normals to supervise the planeness of\nwalls and turning of corners. We propose an efficient network, LGT-Net, for\nroom layout estimation, which contains a novel Transformer architecture called\nSWG Transformer to model geometry relations. SWG Transformer consists of\n(Shifted) Window Blocks and Global Blocks to combine the local and global\ngeometry relations. Moreover, we design a novel relative position embedding of\nTransformer to enhance the spatial identification ability for the panorama.\nExperiments show that the proposed LGT-Net achieves better performance than\ncurrent state-of-the-arts (SOTA) on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhigang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhongzheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. (arXiv:2203.01825v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01825","description":"<p>Transfer learning is a standard technique to transfer knowledge from one\ndomain to another. For applications in medical imaging, transfer from ImageNet\nhas become the de-facto approach, despite differences in the tasks and image\ncharacteristics between the domains. However, it is unclear what factors\ndetermine whether - and to what extent - transfer learning to the medical\ndomain is useful. The long-standing assumption that features from the source\ndomain get reused has recently been called into question. Through a series of\nexperiments on several medical image benchmark datasets, we explore the\nrelationship between transfer learning, data size, the capacity and inductive\nbias of the model, as well as the distance between the source and target\ndomain. Our findings suggest that transfer learning is beneficial in most\ncases, and we characterize the important role feature reuse plays in its\nsuccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1\">Christos Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1\">Johan Fredin Haslum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkhei_M/0/1/0/all/0/1\">Moein Sorkhei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1\">Magnus S&#xf6;derberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STUN: Self-Teaching Uncertainty Estimation for Place Recognition. (arXiv:2203.01851v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01851","description":"<p>Place recognition is key to Simultaneous Localization and Mapping (SLAM) and\nspatial perception. However, a place recognition in the wild often suffers from\nerroneous predictions due to image variations, e.g., changing viewpoints and\nstreet appearance. Integrating uncertainty estimation into the life cycle of\nplace recognition is a promising method to mitigate the impact of variations on\nplace recognition performance. However, existing uncertainty estimation\napproaches in this vein are either computationally inefficient (e.g., Monte\nCarlo dropout) or at the cost of dropped accuracy. This paper proposes STUN, a\nself-teaching framework that learns to simultaneously predict the place and\nestimate the prediction uncertainty given an input image. To this end, we first\ntrain a teacher net using a standard metric learning pipeline to produce\nembedding priors. Then, supervised by the pretrained teacher net, a student net\nwith an additional variance branch is trained to finetune the embedding priors\nand estimate the uncertainty sample by sample. During the online inference\nphase, we only use the student net to generate a place prediction in\nconjunction with the uncertainty. When compared with place recognition systems\nthat are ignorant to the uncertainty, our framework features the uncertainty\nestimation for free without sacrificing any prediction accuracy. Our\nexperimental results on the large-scale Pittsburgh30k dataset demonstrate that\nSTUN outperforms the state-of-the-art methods in both recognition accuracy and\nthe quality of uncertainty estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1\">Kaiwen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Video Instance Segmentation via Tracklet Query and Proposal. (arXiv:2203.01853v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01853","description":"<p>Video Instance Segmentation (VIS) aims to simultaneously classify, segment,\nand track multiple object instances in videos. Recent clip-level VIS takes a\nshort video clip as input each time showing stronger performance than\nframe-level VIS (tracking-by-segmentation), as more temporal context from\nmultiple frames is utilized. Yet, most clip-level methods are neither\nend-to-end learnable nor real-time. These limitations are addressed by the\nrecent VIS transformer (VisTR) which performs VIS end-to-end within a clip.\nHowever, VisTR suffers from long training time due to its frame-wise dense\nattention. In addition, VisTR is not fully end-to-end learnable in multiple\nvideo clips as it requires a hand-crafted data association to link instance\ntracklets between successive clips. This paper proposes EfficientVIS, a fully\nend-to-end framework with efficient training and inference. At the core are\ntracklet query and tracklet proposal that associate and segment\nregions-of-interest (RoIs) across space and time by an iterative query-video\ninteraction. We further propose a correspondence learning that makes tracklets\nlinking between clips end-to-end learnable. Compared to VisTR, EfficientVIS\nrequires 15x fewer training epochs while achieving state-of-the-art accuracy on\nthe YouTube-VIS benchmark. Meanwhile, our method enables whole video instance\nsegmentation in a single end-to-end pass without data association at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarram_S/0/1/0/all/0/1\">Sudhir Yarram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eledath_J/0/1/0/all/0/1\">Jayan Eledath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study on the distribution of social biases in self-supervised learning visual models. (arXiv:2203.01854v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01854","description":"<p>Deep neural networks are efficient at learning the data distribution if it is\nsufficiently sampled. However, they can be strongly biased by non-relevant\nfactors implicitly incorporated in the training data. These include operational\nbiases, such as ineffective or uneven data sampling, but also ethical concerns,\nas the social biases are implicitly present\\textemdash even inadvertently, in\nthe training data or explicitly defined in unfair training schedules. In tasks\nhaving impact on human processes, the learning of social biases may produce\ndiscriminatory, unethical and untrustworthy consequences. It is often assumed\nthat social biases stem from supervised learning on labelled data, and thus,\nSelf-Supervised Learning (SSL) wrongly appears as an efficient and bias-free\nsolution, as it does not require labelled data. However, it was recently proven\nthat a popular SSL method also incorporates biases. In this paper, we study the\nbiases of a varied set of SSL visual models, trained using ImageNet data, using\na method and dataset designed by psychological experts to measure social\nbiases. We show that there is a correlation between the type of the SSL model\nand the number of biases that it incorporates. Furthermore, the results also\nsuggest that this number does not strictly depend on the model's accuracy and\nchanges throughout the network. Finally, we conclude that a careful SSL model\nselection process can reduce the number of social biases in the deployed model,\nwhilst keeping high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirotkin_K/0/1/0/all/0/1\">Kirill Sirotkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carballeira_P/0/1/0/all/0/1\">Pablo Carballeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Vinolo_M/0/1/0/all/0/1\">Marcos Escudero-Vi&#xf1;olo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness and Adaptation to Hidden Factors of Variation. (arXiv:2203.01864v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01864","description":"<p>We tackle here a specific, still not widely addressed aspect, of AI\nrobustness, which consists of seeking invariance / insensitivity of model\nperformance to hidden factors of variations in the data. Towards this end, we\nemploy a two step strategy that a) does unsupervised discovery, via generative\nmodels, of sensitive factors that cause models to under-perform, and b)\nintervenes models to make their performance invariant to these sensitive\nfactors' influence. We consider 3 separate interventions for robustness,\nincluding: data augmentation, semantic consistency, and adversarial alignment.\nWe evaluate our method using metrics that measure trade offs between invariance\n(insensitivity) and overall performance (utility) and show the benefits of our\nmethod for 3 settings (unsupervised, semi-supervised and generalization).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentFormer: Multi-Agent Transformer-Based Interaction Modeling and Trajectory Prediction. (arXiv:2203.01880v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01880","description":"<p>Multi-agent trajectory prediction is a fundamental problem in autonomous\ndriving. The key challenges in prediction are accurately anticipating the\nbehavior of surrounding agents and understanding the scene context. To address\nthese problems, we propose LatentFormer, a transformer-based model for\npredicting future vehicle trajectories. The proposed method leverages a novel\ntechnique for modeling interactions among dynamic objects in the scene.\nContrary to many existing approaches which model cross-agent interactions\nduring the observation time, our method additionally exploits the future states\nof the agents. This is accomplished using a hierarchical attention mechanism\nwhere the evolving states of the agents autoregressively control the\ncontributions of past trajectories and scene encodings in the final prediction.\nFurthermore, we propose a multi-resolution map encoding scheme that relies on a\nvision transformer module to effectively capture both local and global scene\ncontext to guide the generation of more admissible future trajectories. We\nevaluate the proposed method on the nuScenes benchmark dataset and show that\nour approach achieves state-of-the-art performance and improves upon trajectory\nmetrics by up to 40%. We further investigate the contributions of various\ncomponents of the proposed technique via extensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirloo_E/0/1/0/all/0/1\">Elmira Amirloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Amir Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakner_P/0/1/0/all/0/1\">Peter Lakner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohani_M/0/1/0/all/0/1\">Mohsen Rohani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Failure Modes of Self-Supervised Learning. (arXiv:2203.01881v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01881","description":"<p>Self-supervised learning methods have shown impressive results in downstream\nclassification tasks. However, there is limited work in understanding their\nfailure models and interpreting the learned representations of these models. In\nthis paper, we tackle these issues and study the representation space of\nself-supervised models by understanding the underlying reasons for\nmisclassifications in a downstream task. Over several state-of-the-art\nself-supervised models including SimCLR, SwaV, MoCo V2 and BYOL, we observe\nthat representations of correctly classified samples have few discriminative\nfeatures with highly deviated values compared to other features. This is in a\nclear contrast with representations of misclassified samples. We also observe\nthat noisy features in the representation space often correspond to spurious\nattributes in images making the models less interpretable. Building on these\nobservations, we propose a sample-wise Self-Supervised Representation Quality\nScore (or, Q-Score) that, without access to any label information, is able to\npredict if a given sample is likely to be misclassified in the downstream task,\nachieving an AUPRC of up to 0.90. Q-Score can also be used as a regularization\nto remedy low-quality representations leading to 3.26% relative improvement in\naccuracy of SimCLR on ImageNet-100. Moreover, we show that Q-Score\nregularization increases representation sparsity, thus reducing noise and\nimproving interpretability through gradient heatmaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1\">Neha Mukund Kalibhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_K/0/1/0/all/0/1\">Kanika Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy. (arXiv:2203.01882v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01882","description":"<p>To estimate the corneal endothelial parameters from specular microscopy\nimages depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new\ndeep learning methodology that includes a novel attention mechanism named\nfeedback non-local attention (fNLA). Our approach first infers the cell edges,\nthen selects the cells that are well detected, and finally applies a\npostprocessing method to correct mistakes and provide the binary segmentation\nfrom which the corneal parameters are estimated (cell density [ECD],\ncoefficient of variation [CV], and hexagonality [HEX]). In this study, we\nanalyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which\ncontained guttae. Manual segmentation was performed in all images. We compared\nthe results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and\nfound that DenseUNets with fNLA provided the best performance, with a mean\nabsolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%]\nin HEX, which was 3-6 times smaller than the error obtained by Topcon's\nbuilt-in software. Our approach handled the cells affected by guttae remarkably\nwell, detecting cell edges occluded by small guttae while discarding areas\ncovered by large guttae. fNLA made use of the local information, providing\nsharper edges in guttae areas and better results in the selection of\nwell-detected cells. Overall, the proposed method obtained reliable and\naccurate estimations in extremely challenging specular images with guttae,\nbeing the first method in the literature to solve this problem adequately. Code\nis available in our GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rooij_J/0/1/0/all/0/1\">Jeroen van Rooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dooren_B/0/1/0/all/0/1\">Bart T.H. van Dooren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemij_H/0/1/0/all/0/1\">Hans G. Lemij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islamaj_E/0/1/0/all/0/1\">Esma Islamaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vliet_L/0/1/0/all/0/1\">Lucas J. van Vliet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeer_K/0/1/0/all/0/1\">Koenraad A. Vermeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images. (arXiv:2203.01883v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01883","description":"<p>Optical coherence tomography (OCT) imaging is a well-known technology for\nvisualizing retinal layers and helps ophthalmologists to detect possible\ndiseases. Accurate and early diagnosis of common retinal diseases can prevent\nthe patients from suffering critical damages to their vision. Computer-aided\ndiagnosis (CAD) systems can significantly assist ophthalmologists in improving\ntheir examinations. This paper presents a new enhanced deep ensemble\nconvolutional neural network for detecting retinal diseases from OCT images.\nOur model generates rich and multi-resolution features by employing the\nlearning architectures of two robust convolutional models. Spatial resolution\nis a critical factor in medical images, especially the OCT images that contain\ntiny essential points. To empower our model, we apply a new post-architecture\nmodel to our ensemble model for enhancing spatial resolution learning without\nincreasing computational costs. The introduced post-architecture model can be\ndeployed to any feature extraction model to improve the utilization of the\nfeature map's spatial values. We have collected two open-source datasets for\nour experiments to make our models capable of detecting six crucial retinal\ndiseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy\n(CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic\nMacular Edema (DME), and Drusen alongside the normal cases. Our experiments on\ntwo datasets and comparing our model with some other well-known deep\nconvolutional neural networks have proven that our architecture can increase\nthe classification accuracy up to 5%. We hope that our proposed methods create\nthe next step of CAD systems development and help future researches. The code\nof this paper is shared at https://github.com/mr7495/OCT-classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rahimzadeh_M/0/1/0/all/0/1\">Mohammad Rahimzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_M/0/1/0/all/0/1\">Mahmoud Reza Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01885","description":"<p>Temporal contexts among consecutive frames are far from been fully utilized\nin existing visual trackers. In this work, we present TCTrack, a comprehensive\nframework to fully exploit temporal contexts for aerial tracking. The temporal\ncontexts are incorporated at \\textbf{two levels}: the extraction of\n\\textbf{features} and the refinement of \\textbf{similarity maps}. Specifically,\nfor feature extraction, an online temporally adaptive convolution is proposed\nto enhance the spatial features using temporal information, which is achieved\nby dynamically calibrating the convolution weights according to the previous\nframes. For similarity map refinement, we propose an adaptive temporal\ntransformer, which first effectively encodes temporal knowledge in a\nmemory-efficient way, before the temporal knowledge is decoded for accurate\nadjustment of the similarity map. TCTrack is effective and efficient:\nevaluation on four aerial tracking benchmarks shows its impressive performance;\nreal-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX\nXavier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Segmentation for Autonomous Log Grasping in Forestry Operations. (arXiv:2203.01902v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01902","description":"<p>Wood logs picking is a challenging task to automate. Indeed, logs usually\ncome in cluttered configurations, randomly orientated and overlapping. Recent\nwork on log picking automation usually assume that the logs' pose is known,\nwith little consideration given to the actual perception problem. In this\npaper, we squarely address the latter, using a data-driven approach. First, we\nintroduce a novel dataset, named TimberSeg 1.0, that is densely annotated,\ni.e., that includes both bounding boxes and pixel-level mask annotations for\nlogs. This dataset comprises 220 images with 2500 individually segmented logs.\nUsing our dataset, we then compare three neural network architectures on the\ntask of individual logs detection and segmentation; two region-based methods\nand one attention-based method. Unsurprisingly, our results show that\naxis-aligned proposals, failing to take into account the directional nature of\nlogs, underperform with 19.03 mAP. A rotation-aware proposal method\nsignificantly improve results to 31.83 mAP. More interestingly, a\nTransformer-based approach, without any inductive bias on rotations,\noutperformed the two others, achieving a mAP of 57.53 on our dataset. Our use\ncase demonstrates the limitations of region-based approaches for cluttered,\nelongated objects. It also highlights the potential of attention-based methods\non this specific task, as they work directly at the pixel-level. These\nencouraging results indicate that such a perception system could be used to\nassist the operators on the short-term, or to fully automate log picking\noperations in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fortin_J/0/1/0/all/0/1\">Jean-Michel Fortin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamache_O/0/1/0/all/0/1\">Olivier Gamache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grondin_V/0/1/0/all/0/1\">Vincent Grondin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerleau_F/0/1/0/all/0/1\">Fran&#xe7;ois Pomerleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giguere_P/0/1/0/all/0/1\">Philippe Gigu&#xe8;re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision Aided Blockage Prediction in Real-World Millimeter Wave Deployments. (arXiv:2203.01907v1 [eess.SP])","link":"http://arxiv.org/abs/2203.01907","description":"<p>This paper provides the first real-world evaluation of using visual (RGB\ncamera) data and machine learning for proactively predicting millimeter wave\n(mmWave) dynamic link blockages before they happen. Proactively predicting\nline-of-sight (LOS) link blockages enables mmWave/sub-THz networks to make\nproactive network management decisions, such as proactive beam switching and\nhand-off) before a link failure happens. This can significantly enhance the\nnetwork reliability and latency while efficiently utilizing the wireless\nresources. To evaluate this gain in reality, this paper (i) develops a computer\nvision based solution that processes the visual data captured by a camera\ninstalled at the infrastructure node and (ii) studies the feasibility of the\nproposed solution based on the large-scale real-world dataset, DeepSense 6G,\nthat comprises multi-modal sensing and communication data. Based on the adopted\nreal-world dataset, the developed solution achieves $\\approx 90\\%$ accuracy in\npredicting blockages happening within the future $0.1$s and $\\approx 80\\%$ for\nblockages happening within $1$s, which highlights a promising solution for\nmmWave/sub-THz communication networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Charan_G/0/1/0/all/0/1\">Gouranga Charan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alkhateeb_A/0/1/0/all/0/1\">Ahmed Alkhateeb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields. (arXiv:2203.01913v1 [cs.RO])","link":"http://arxiv.org/abs/2203.01913","description":"<p>Thin, reflective objects such as forks and whisks are common in our daily\nlives, but they are particularly challenging for robot perception because it is\nhard to reconstruct them using commodity RGB-D cameras or multi-view stereo\ntechniques. While traditional pipelines struggle with objects like these,\nNeural Radiance Fields (NeRFs) have recently been shown to be remarkably\neffective for performing view synthesis on objects with thin structures or\nreflective materials. In this paper we explore the use of NeRF as a new source\nof supervision for robust robot vision systems. In particular, we demonstrate\nthat a NeRF representation of a scene can be used to train dense object\ndescriptors. We use an optimized NeRF to extract dense correspondences between\nmultiple views of an object, and then use these correspondences as training\ndata for learning a view-invariant representation of the object. NeRF's usage\nof a density field allows us to reformulate the correspondence problem with a\nnovel distribution-of-depths formulation, as opposed to the conventional\napproach of using a depth map. Dense correspondence models supervised with our\nmethod significantly outperform off-the-shelf learned descriptors by 106%\n(PCK@3px metric, more than doubling performance) and outperform our baseline\nsupervised with multi-view stereo by 29%. Furthermore, we demonstrate the\nlearned dense descriptors enable robots to perform accurate 6-degree of freedom\n(6-DoF) pick and place of thin and reflective objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playable Environments: Video Manipulation in Space and Time. (arXiv:2203.01914v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01914","description":"<p>We present Playable Environments - a new representation for interactive video\ngeneration and manipulation in space and time. With a single image at inference\ntime, our novel framework allows the user to move objects in 3D while\ngenerating a video by providing a sequence of desired actions. The actions are\nlearnt in an unsupervised manner. The camera can be controlled to get the\ndesired viewpoint. Our method builds an environment state for each frame, which\ncan be manipulated by our proposed action module and decoded back to the image\nspace with volumetric rendering. To support diverse appearances of objects, we\nextend neural radiance fields with style-based modulation. Our method trains on\na collection of various monocular videos requiring only the estimated camera\nparameters and 2D object locations. To set a challenging benchmark, we\nintroduce two large scale video datasets with significant camera movements. As\nevidenced by our experiments, playable environments enable several creative\napplications not attainable by prior video synthesis works, including playable\n3D video generation, stylization and manipulation. Further details, code and\nexamples are available at\nhttps://willi-menapace.github.io/playable-environments-website\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menapace_W/0/1/0/all/0/1\">Willi Menapace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1\">Aliaksandr Siarohin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the limited performance of a deep-learning-based SPECT denoising approach: An observer-study-based characterization. (arXiv:2203.01918v1 [physics.med-ph])","link":"http://arxiv.org/abs/2203.01918","description":"<p>Multiple objective assessment of image-quality-based studies have reported\nthat several deep-learning-based denoising methods show limited performance on\nsignal-detection tasks. Our goal was to investigate the reasons for this\nlimited performance. To achieve this goal, we conducted a task-based\ncharacterization of a DL-based denoising approach for individual signal\nproperties. We conducted this study in the context of evaluating a DL-based\napproach for denoising SPECT images. The training data consisted of signals of\ndifferent sizes and shapes within a clustered-lumpy background, imaged with a\n2D parallel-hole-collimator SPECT system. The projections were generated at\nnormal and 20% low count level, both of which were reconstructed using an OSEM\nalgorithm. A CNN-based denoiser was trained to process the low-count images.\nThe performance of this CNN was characterized for five different signal sizes\nand four different SBR by designing each evaluation as an SKE/BKS\nsignal-detection task. Performance on this task was evaluated using an\nanthropomorphic CHO. As in previous studies, we observed that the DL-based\ndenoising method did not improve performance on signal-detection tasks.\nEvaluation using the idea of observer-study-based characterization demonstrated\nthat the DL-based denoising approach did not improve performance on the\nsignal-detection task for any of the signal types. Overall, these results\nprovide new insights on the performance of the DL-based denoising approach as a\nfunction of signal size and contrast. More generally, the observer study-based\ncharacterization provides a mechanism to evaluate the sensitivity of the method\nto specific object properties and may be explored as analogous to\ncharacterizations such as modulation transfer function for linear systems.\nFinally, this work underscores the need for objective task-based evaluation of\nDL-based denoising approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahman_M/0/1/0/all/0/1\">Md Ashequr Rahman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jha_A/0/1/0/all/0/1\">Abhinav K. Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification. (arXiv:2203.01921v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01921","description":"<p>Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue\nmicro-architecture, which can, in turn, be used to reconstruct tissue\nmicrostructure and white matter pathways. The accuracy of such tasks is\nhampered by the low signal-to-noise ratio in dMRI. Today, the noise is\ncharacterized mainly by visual inspection of residual maps and estimated\nstandard deviation. However, it is hard to estimate the impact of noise on\ndownstream tasks based only on such qualitative assessments. To address this\nissue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for\nquantitative image quality analysis in the absence of a ground truth reference\nimage. NUQ uses a recent Bayesian formulation of dMRI models to estimate the\nuncertainty of microstructural measures. Specifically, NUQ uses the maximum\nmean discrepancy metric to compute a pooled quality score by comparing samples\ndrawn from the posterior distribution of the microstructure measures. We show\nthat NUQ allows a fine-grained analysis of noise, capturing details that are\nvisually imperceptible. We perform qualitative and quantitative comparisons on\nreal datasets, showing that NUQ generates consistent scores across different\ndenoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics\nand controls, we quantify the substantial impact of denoising on group\ndifferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fadnavis_S/0/1/0/all/0/1\">Shreyas Fadnavis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sjolund_J/0/1/0/all/0/1\">Jens Sj&#xf6;lund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1\">Anders Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garyfallidis_E/0/1/0/all/0/1\">Eleftherios Garyfallidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Intelligence: Tasks, Representation Learning, and Large Models. (arXiv:2203.01922v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01922","description":"<p>This paper presents a comprehensive survey of vision-language (VL)\nintelligence from the perspective of time. This survey is inspired by the\nremarkable progress in both computer vision and natural language processing,\nand recent trends shifting from single modality processing to multiple modality\ncomprehension. We summarize the development in this field into three time\nperiods, namely task-specific methods, vision-language pre-training (VLP)\nmethods, and larger models empowered by large-scale weakly-labeled data. We\nfirst take some common VL tasks as examples to introduce the development of\ntask-specific methods. Then we focus on VLP methods and comprehensively review\nkey components of the model structures and training methods. After that, we\nshow how recent work utilizes large-scale raw image-text data to learn\nlanguage-aligned visual representations that generalize better on zero or few\nshot learning tasks. Finally, we discuss some potential future trends towards\nmodality cooperation, unified representation, and knowledge incorporation. We\nbelieve that this review will be of help for researchers and practitioners of\nAI and ML, especially those interested in computer vision and natural language\nprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">PengChuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering 3D Human Mesh from Monocular Images: A Survey. (arXiv:2203.01923v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01923","description":"<p>Estimating human pose and shape from monocular images is a long-standing\nproblem in computer vision. Since the release of statistical body models, 3D\nhuman mesh recovery has been drawing broader attention. With the same goal of\nobtaining well-aligned and physically plausible mesh results, two paradigms\nhave been developed to overcome challenges in the 2D-to-3D lifting process: i)\nan optimization-based paradigm, where different data terms and regularization\nterms are exploited as optimization objectives; and ii) a regression-based\nparadigm, where deep learning techniques are embraced to solve the problem in\nan end-to-end fashion. Meanwhile, continuous efforts are devoted to improving\nthe quality of 3D mesh labels for a wide range of datasets. Though remarkable\nprogress has been achieved in the past decade, the task is still challenging\ndue to flexible body motions, diverse appearances, complex environments, and\ninsufficient in-the-wild annotations. To the best of our knowledge, this is the\nfirst survey to focus on the task of monocular 3D human mesh recovery. We start\nwith the introduction of body models, and then introduce recovery frameworks\nand training objectives by providing in-depth analyses of their strengths and\nweaknesses. We also summarize datasets, evaluation metrics, and benchmark\nresults. Open issues and future directions are discussed in the end, hoping to\nmotivate researchers and facilitate their research in this area. A regularly\nupdated project page can be found at https://github.com/tinatiansjz/hmr-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Only Model Inversion Attacks via Boundary Repulsion. (arXiv:2203.01925v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01925","description":"<p>Recent studies show that the state-of-the-art deep neural networks are\nvulnerable to model inversion attacks, in which access to a model is abused to\nreconstruct private training data of any given target class. Existing attacks\nrely on having access to either the complete target model (whitebox) or the\nmodel's soft-labels (blackbox). However, no prior work has been done in the\nharder but more practical scenario, in which the attacker only has access to\nthe model's predicted label, without a confidence measure. In this paper, we\nintroduce an algorithm, Boundary-Repelling Model Inversion (BREP-MI), to invert\nprivate training data using only the target model's predicted labels. The key\nidea of our algorithm is to evaluate the model's predicted labels over a sphere\nand then estimate the direction to reach the target class's centroid. Using the\nexample of face recognition, we show that the images reconstructed by BREP-MI\nsuccessfully reproduce the semantics of the private training data for various\ndatasets and target model architectures. We compare BREP-MI with the\nstate-of-the-art whitebox and blackbox model inversion attacks and the results\nshow that despite assuming less knowledge about the target model, BREP-MI\noutperforms the blackbox attack and achieves comparable results to the whitebox\nattack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahla_M/0/1/0/all/0/1\">Mostafa Kahla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Just_H/0/1/0/all/0/1\">Hoang Anh Just</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation. (arXiv:2203.01929v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01929","description":"<p>This paper studies the complex task of simultaneous multi-object 3D\nreconstruction, 6D pose and size estimation from a single-view RGB-D\nobservation. In contrast to instance-level pose estimation, we focus on a more\nchallenging problem where CAD models are not available at inference time.\nExisting approaches mainly follow a complex multi-stage pipeline which first\nlocalizes and detects each object instance in the image and then regresses to\neither their 3D meshes or 6D poses. These approaches suffer from\nhigh-computational cost and low performance in complex multi-object scenarios,\nwhere occlusions can be present. Hence, we present a simple one-stage approach\nto predict both the 3D shape and estimate the 6D pose and size jointly in a\nbounding-box free manner. In particular, our method treats object instances as\nspatial centers where each center denotes the complete shape of an object along\nwith its 6D pose and size. Through this per-pixel representation, our approach\ncan reconstruct in real-time (40 FPS) multiple novel object instances and\npredict their 6D pose and sizes in a single-forward pass. Through extensive\nexperiments, we demonstrate that our approach significantly outperforms all\nshape completion and categorical 6D pose and size estimation baselines on\nmulti-object ShapeNet and NOCS datasets respectively with a 12.6% absolute\nimprovement in mAP for 6D pose for novel real-world object instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1\">Muhammad Zubair Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the mathematics of beauty: beautiful images. (arXiv:1705.08244v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1705.08244","description":"<p>In this paper, we will study the simplest kind of beauty which can be found\nin simple visual patterns. The proposed approach shows that aesthetically\nappealing patterns deliver higher amount of information over multiple levels in\ncomparison with less aesthetically appealing patterns when the same amount of\nenergy is used. The proposed approach is used to classify aesthetically\nappealing patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalili_A/0/1/0/all/0/1\">A. M. Khalili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Robustness of Visual Question Answering Models. (arXiv:1912.01452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.01452","description":"<p>Deep neural networks have been playing an essential role in the task of\nVisual Question Answering (VQA). Until recently, their accuracy has been the\nmain focus of research. Now there is a trend toward assessing the robustness of\nthese models against adversarial attacks by evaluating the accuracy of these\nmodels under increasing levels of noisiness in the inputs of VQA models. In\nVQA, the attack can target the image and/or the proposed query question, dubbed\nmain question, and yet there is a lack of proper analysis of this aspect of\nVQA. In this work, we propose a new method that uses semantically related\nquestions, dubbed basic questions, acting as noise to evaluate the robustness\nof VQA models. We hypothesize that as the similarity of a basic question to the\nmain question decreases, the level of noise increases. To generate a reasonable\nnoise level for a given main question, we rank a pool of basic questions based\non their similarity with this main question. We cast this ranking problem as a\nLASSO optimization problem. We also propose a novel robustness measure Rscore\nand two large-scale basic question datasets in order to standardize robustness\nanalysis of VQA models. The experimental results demonstrate that the proposed\nevaluation method is able to effectively analyze the robustness of VQA models.\nTo foster the VQA research, we will publish our proposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfadly_M/0/1/0/all/0/1\">Modar Alfadly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radon cumulative distribution transform subspace modeling for image classification. (arXiv:2004.03669v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.03669","description":"<p>We present a new supervised image classification method applicable to a broad\nclass of image deformation models. The method makes use of the previously\ndescribed Radon Cumulative Distribution Transform (R-CDT) for image data, whose\nmathematical properties are exploited to express the image data in a form that\nis more suitable for machine learning. While certain operations such as\ntranslation, scaling, and higher-order transformations are challenging to model\nin native image space, we show the R-CDT can capture some of these variations\nand thus render the associated image classification problems easier to solve.\nThe method -- utilizing a nearest-subspace algorithm in R-CDT space -- is\nsimple to implement, non-iterative, has no hyper-parameters to tune, is\ncomputationally efficient, label efficient, and provides competitive accuracies\nto state-of-the-art neural networks for many types of classification problems.\nIn addition to the test accuracy performances, we show improvements (with\nrespect to neural network-based methods) in terms of computational efficiency\n(it can be implemented without the use of GPUs), number of training samples\nneeded for training, as well as out-of-distribution generalization. The Python\ncode for reproducing our results is available at\nhttps://github.com/rohdelab/rcdt_ns_classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shifat_E_Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat-E-Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldroubi_A/0/1/0/all/0/1\">Akram Aldroubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichols_J/0/1/0/all/0/1\">Jonathan M. Nichols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthMorph: learning contrast-invariant registration without acquired images. (arXiv:2004.10282v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2004.10282","description":"<p>We introduce a strategy for learning image registration without acquired\nimaging data, producing powerful networks agnostic to contrast introduced by\nmagnetic resonance imaging (MRI). While classical registration methods\naccurately estimate the spatial correspondence between images, they solve an\noptimization problem for every new image pair. Learning-based techniques are\nfast at test time but limited to registering images with contrasts and\ngeometric content similar to those seen during training. We propose to remove\nthis dependency on training data by leveraging a generative strategy for\ndiverse synthetic label maps and images that exposes networks to a wide range\nof variability, forcing them to learn more invariant features. This approach\nresults in powerful networks that accurately generalize to a broad array of MRI\ncontrasts. We present extensive experiments with a focus on 3D neuroimaging,\nshowing that this strategy enables robust and accurate registration of\narbitrary MRI contrasts even if the target contrast is not seen by the networks\nduring training. We demonstrate registration accuracy surpassing the state of\nthe art both within and across contrasts, using a single model. Critically,\ntraining on arbitrary shapes synthesized from noise distributions results in\ncompetitive performance, removing the dependency on acquired data of any kind.\nAdditionally, since anatomical label maps are often available for the anatomy\nof interest, we show that synthesizing images from these dramatically boosts\nperformance, while still avoiding the need for real intensity images. Our code\nis available at https://w3id.org/synthmorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Localization Networks for Language-based Moment Localization. (arXiv:2102.01282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01282","description":"<p>This paper targets the task of language-based video moment localization. The\nlanguage-based setting of this task allows for an open set of target\nactivities, resulting in a large variation of the temporal lengths of video\nmoments. Most existing methods prefer to first sample sufficient candidate\nmoments with various temporal lengths, and then match them with the given query\nto determine the target moment. However, candidate moments generated with a\nfixed temporal granularity may be suboptimal to handle the large variation in\nmoment lengths. To this end, we propose a novel multi-stage Progressive\nLocalization Network (PLN) which progressively localizes the target moment in a\ncoarse-to-fine manner. Specifically, each stage of PLN has a localization\nbranch, and focuses on candidate moments that are generated with a specific\ntemporal granularity. The temporal granularities of candidate moments are\ndifferent across the stages. Moreover, we devise a conditional feature\nmanipulation module and an upsampling connection to bridge the multiple\nlocalization branches. In this fashion, the later stages are able to absorb the\npreviously learned information, thus facilitating the more fine-grained\nlocalization. Extensive experiments on three public datasets demonstrate the\neffectiveness of our proposed PLN for language-based moment localization,\nespecially for localizing short moments in long videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Baolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04053","description":"<p>Real-world large-scale medical image analysis (MIA) datasets havethree\nchallenges: 1) they contain noisy-labelled samples that affect training\ncon-vergence and generalisation, 2) they usually have an imbalanced\ndistribution ofsamples per class, and 3) they normally comprise a multi-label\nproblem, wheresamples can have multiple diagnoses. Current approaches are\ncommonly trainedto solve a subset of those problems, but we are unaware of\nmethods that ad-dress the three problems simultaneously. In this paper, we\npropose a new trainingmodule called Non-Volatile Unbiased Memory (NVUM), which\nnon-volatilitystores running average of model logits for a new regularization\nloss on noisymulti-label problem. We further unbias the classification\nprediction in NVUMupdate for imbalanced learning problem. We run extensive\nexperiments to eval-uate NVUM on new benchmarks proposed by this paper, where\ntraining is per-formed on noisy multi-label imbalanced chest X-ray (CXR)\ntraining sets, formedby Chest-Xray14 and CheXpert, and the testing is performed\non the clean multi-label CXR datasets OpenI and PadChest. Our method\noutperforms previous state-of-the-art CXR classifiers and previous methods that\ncan deal with noisy labelson all evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?. (arXiv:2104.09425v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.09425","description":"<p>While additional training data improves the robustness of deep neural\nnetworks against adversarial examples, it presents the challenge of curating a\nlarge number of specific real-world samples. We circumvent this challenge by\nusing additional data from proxy distributions learned by advanced generative\nmodels. We first seek to formally understand the transfer of robustness from\nclassifiers trained on proxy distributions to the real data distribution. We\nprove that the difference between the robustness of a classifier on the two\ndistributions is upper bounded by the conditional Wasserstein distance between\nthem. Next we use proxy distributions to significantly improve the performance\nof adversarial training on five different datasets. For example, we improve\nrobust accuracy by up to 7.5% and 6.7% in $\\ell_{\\infty}$ and $\\ell_2$ threat\nmodel over baselines that are not using proxy distributions on the CIFAR-10\ndataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10\ndataset. We further demonstrate that different generative models bring a\ndisparate improvement in the performance in robust training. We propose a\nrobust discrimination approach to characterize the impact of individual\ngenerative models and further provide a deeper understanding of why current\nstate-of-the-art in diffusion-based generative models are a better choice for\nproxy distribution than generative adversarial networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handina_T/0/1/0/all/0/1\">Tinashe Handina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Sihui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous-Agent Trajectory Forecasting Incorporating Class Uncertainty. (arXiv:2104.12446v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12446","description":"<p>Reasoning about the future behavior of other agents is critical to safe robot\nnavigation. The multiplicity of plausible futures is further amplified by the\nuncertainty inherent to agent state estimation from data, including positions,\nvelocities, and semantic class. Forecasting methods, however, typically neglect\nclass uncertainty, conditioning instead only on the agent's most likely class,\neven though perception models often return full class distributions. To exploit\nthis information, we present HAICU, a method for heterogeneous-agent trajectory\nforecasting that explicitly incorporates agents' class probabilities. We\nadditionally present PUP, a new challenging real-world autonomous driving\ndataset, to investigate the impact of Perceptual Uncertainty in Prediction. It\ncontains challenging crowded scenes with unfiltered agent class probabilities\nthat reflect the long-tail of current state-of-the-art perception systems. We\ndemonstrate that incorporating class probabilities in trajectory forecasting\nsignificantly improves performance in the face of uncertainty, and enables new\nforecasting capabilities such as counterfactual predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuan-Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1\">Pavel Tokmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulfe_B/0/1/0/all/0/1\">Blake Wulfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAllister_R/0/1/0/all/0/1\">Rowan McAllister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Partial Multi-view Learning. (arXiv:2105.02046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02046","description":"<p>It is often the case that data are with multiple views in real-world\napplications. Fully exploring the information of each view is significant for\nmaking data more representative. However, real data tend to suffer from\narbitrary view-missing due to various failures in data collection and\npre-processing. Besides, as obtaining large-scale labeled data is laborious and\nexpensive, the collected training samples in the target task may be scarce as\nwell. The co-existence of these two problems makes it more challenging to\nachieve the pattern classification task. Currently, to our best knowledge, few\nappropriate methods can well-handle these two problems simultaneously. To draw\nmore attention from the community to this challenge, we present a new task in\nthis paper called Few-shot Partial Multi-view Learning (FPML), aiming to\nalleviate the view-missing effects in the low-data regime. The challenges of\nthis task are twofold: (1) under the interference of the missing views, it is\ndifficult to overcome the negative impact brought by data scarcity; (2) the\nlimited number of data exacerbates information scarcity, thereby making it\nharder to address the view-missing problem. In this paper, we propose a novel\nmethod called Unified Gaussian Dense-anchoring (UGD) for this task. We propose\nto learn the unified dense Gaussian anchors for each training sample.\nTherefore, the incomplete multi-view data can be densely anchored into a\nunified representation space, where data scarcity and view missing are both\nrelieved by the unified dense anchor representations. In particular, we also\nextend our FPML to the multimodal and the cross-domain scenario, and validate\nour UGD method on them. The extensive experiments firmly demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shijie Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse to Dense Dynamic 3D Facial Expression Generation. (arXiv:2105.07463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07463","description":"<p>In this paper, we propose a solution to the task of generating dynamic 3D\nfacial expressions from a neutral 3D face and an expression label. This\ninvolves solving two sub-problems: (i)modeling the temporal dynamics of\nexpressions, and (ii) deforming the neutral mesh to obtain the expressive\ncounterpart. We represent the temporal evolution of expressions using the\nmotion of a sparse set of 3D landmarks that we learn to generate by training a\nmanifold-valued GAN (Motion3DGAN). To better encode the expression-induced\ndeformation and disentangle it from the identity information, the generated\nmotion is represented as per-frame displacement from a neutral configuration.\nTo generate the expressive meshes, we train a Sparse2Dense mesh Decoder\n(S2D-Dec) that maps the landmark displacements to a dense, per-vertex\ndisplacement. This allows us to learn how the motion of a sparse set of\nlandmarks influences the deformation of the overall face surface, independently\nfrom the identity. Experimental results on the CoMA and D3DFACS datasets show\nthat our solution brings significant improvements with respect to previous\nsolutions in terms of both dynamic expression generation and mesh\nreconstruction, while retaining good generalization to unseen data. The code\nand the pretrained model will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1\">Claudio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1\">Stefano Berretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10375","description":"<p>Face recognition has achieved significant progress in deep learning era due\nto the ultra-large-scale and welllabeled datasets. However, training on the\noutsize datasets is time-consuming and takes up a lot of hardware resource.\nTherefore, designing an efficient training approach is indispensable. The heavy\ncomputational and memory costs mainly result from the million-level\ndimensionality of thefully connected (FC) layer. To this end, we propose a\nnovel training approach, termed Faster Face Classification (F2C), to alleviate\ntime and cost without sacrificing the performance. This method adopts Dynamic\nClass Pool (DCP) for storing and updating the identities features dynamically,\nwhich could be regarded as a substitute for the FC layer. DCP is efficiently\ntime-saving and cost-saving, as its smaller size with the independence from the\nwhole face identities together. We further validate the proposed F2C method\nacross several face benchmarks and private datasets, and display comparable\nresults, meanwhile the speed is faster than state-of-the-art FC-based methods\nin terms of recognition accuracy and hardware costs. Moreover, our method is\nfurther improved by a well-designed dual data loader including indentity-based\nand instancebased loaders, which makes it more efficient for the updating DCP\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Panpan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch Tracking-based Online Tensor Ring Completion for Streaming Visual Data. (arXiv:2105.14620v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14620","description":"<p>Tensor completion is the problem of estimating the missing entries of a\npartially observed tensor by exploiting its low-rank structure. In streaming\napplications where the frames arrive sequentially such as video completion, the\nmissing entries of the tensor need to be dynamically recovered in an online\nfashion. Traditional online completion algorithms treat the entire visual data\nas a tensor, which may not work satisfactorily when there is a big change in\nthe tensor subspace along the temporal dimension, such as due to strong motion\nacross the video frames. In this paper, we develop a novel patch tracking-based\nonline tensor completion framework for streaming data. Each incoming tensor is\nextracted into small patches, and similar patches are tracked along the\ntemporal domain. We propose a new patch tracking strategy that can accurately\nand efficiently track the patches with missing data. Further, a new online\ntensor ring completion method is proposed which can efficiently and accurately\nupdate the latent core tensors and complete the missing entries of the tracked\npatches. Extensive experimental results demonstrate the superior performance of\nthe proposed algorithms compared with both online and offline state-of-the-art\ntensor completion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George K. Atia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04169","description":"<p>Vision transformers (ViTs) process input images as sequences of patches via\nself-attention; a radically different architecture than convolutional neural\nnetworks (CNNs). This makes it interesting to study the adversarial feature\nspace of ViT models and their transferability. In particular, we observe that\nadversarial patterns found via conventional adversarial attacks show very\n\\emph{low} black-box transferability even for large ViT models. We show that\nthis phenomenon is only due to the sub-optimal attack procedures that do not\nleverage the true representation potential of ViTs. A deep ViT is composed of\nmultiple blocks, with a consistent architecture comprising of self-attention\nand feed-forward layers, where each block is capable of independently producing\na class token. Formulating an attack using only the last class token\n(conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial\ntransferability of ViTs. Using the compositional nature of ViT models, we\nenhance transferability of existing attacks by introducing two novel strategies\nspecific to the architecture of ViT models. (i) Self-Ensemble: We propose a\nmethod to find multiple discriminative pathways by dissecting a single ViT\nmodel into an ensemble of networks. This allows explicitly utilizing\nclass-specific information at each ViT block. (ii) Token Refinement: We then\npropose to refine the tokens to further enhance the discriminative capacity at\neach block of ViT. Our token refinement systematically combines the class\ntokens with structural information preserved within the patch tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04570","description":"<p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST++: Make Self-training Work Better for Semi-supervised Semantic Segmentation. (arXiv:2106.05095v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05095","description":"<p>Self-training via pseudo labeling is a conventional, simple, and popular\npipeline to leverage unlabeled data. In this work, we first construct a strong\nbaseline of self-training (namely ST) for semi-supervised semantic segmentation\nvia injecting strong data augmentations (SDA) on unlabeled images to alleviate\noverfitting noisy labels as well as decouple similar predictions between the\nteacher and student. With this simple mechanism, our ST outperforms all\nexisting methods without any bells and whistles, e.g., iterative re-training.\nInspired by the impressive results, we thoroughly investigate the SDA and\nprovide some empirical analysis. Nevertheless, incorrect pseudo labels are\nstill prone to accumulate and degrade the performance. To this end, we further\npropose an advanced self-training framework (namely ST++), that performs\nselective re-training via prioritizing reliable unlabeled images based on\nholistic prediction-level stability. Concretely, several model checkpoints are\nsaved in the first stage supervised training, and the discrepancy of their\npredictions on the unlabeled image serves as a measurement for reliability. Our\nimage-level selection offers holistic contextual information for learning. We\ndemonstrate that it is more suitable for segmentation than common pixel-wise\nselection. As a result, ST++ further boosts the performance of our ST. Code is\navailable at https://github.com/LiheYoung/ST-PlusPlus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lihe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are conditional GANs explicitly conditional?. (arXiv:2106.15011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15011","description":"<p>This paper proposes two important contributions for conditional Generative\nAdversarial Networks (cGANs) to improve the wide variety of applications that\nexploit this architecture. The first main contribution is an analysis of cGANs\nto show that they are not explicitly conditional. In particular, it will be\nshown that the discriminator and subsequently the cGAN does not automatically\nlearn the conditionality between inputs. The second contribution is a new\nmethod, called a contrario cGAN, that explicitly models conditionality for both\nparts of the adversarial architecture via a novel a contrario loss that\ninvolves training the discriminator to learn unconditional (adverse) examples.\nThis leads to a novel type of data augmentation approach for GANs (a contrario\nlearning) which allows to restrict the search space of the generator to\nconditional outputs using adverse examples. Extensive experimentation is\ncarried out to evaluate the conditionality of the discriminator by proposing a\nprobability distribution analysis. Comparisons with the cGAN architecture for\ndifferent applications show significant improvements in performance on well\nknown datasets including, semantic image synthesis, image segmentation,\nmonocular depth prediction and \"single label\"-to-image using different metrics\nincluding Fr\\'echet Inception Distance (FID), mean Intersection over Union\n(mIoU), Root Mean Square Error log (RMSE log) and Number of\nstatistically-Different Bins (NDB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem eddine Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition. (arXiv:2106.15125v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15125","description":"<p>One essential problem in skeleton-based action recognition is how to extract\ndiscriminative features over all skeleton joints. However, the complexity of\nthe recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly\nsophisticated and over-parameterized. The low efficiency in model training and\ninference has increased the validation costs of model architectures in\nlarge-scale datasets. To address the above issue, recent advanced separable\nconvolutional layers are embedded into an early fused Multiple Input Branches\n(MIB) network, constructing an efficient Graph Convolutional Network (GCN)\nbaseline for skeleton-based action recognition. In addition, based on such the\nbaseline, we design a compound scaling strategy to expand the model's width and\ndepth synchronously, and eventually obtain a family of efficient GCN baselines\nwith high accuracies and small amounts of trainable parameters, termed\nEfficientGCN-Bx, where \"x\" denotes the scaling coefficient. On two large-scale\ndatasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4 baseline\noutperforms other SOTA methods, e.g., achieving 91.7% accuracy on the\ncross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x\nfaster than MS-G3D, which is one of the best SOTA methods. The source code in\nPyTorch version and the pretrained models are available at\nhttps://github.com/yfsong0709/EfficientGCNv1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Fan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mobile-Former: Bridging MobileNet and Transformer. (arXiv:2108.05895v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05895","description":"<p>We present Mobile-Former, a parallel design of MobileNet and transformer with\na two-way bridge in between. This structure leverages the advantages of\nMobileNet at local processing and transformer at global interaction. And the\nbridge enables bidirectional fusion of local and global features. Different\nfrom recent works on vision transformer, the transformer in Mobile-Former\ncontains very few tokens (e.g. 6 or fewer tokens) that are randomly initialized\nto learn global priors, resulting in low computational cost. Combining with the\nproposed light-weight cross attention to model the bridge, Mobile-Former is not\nonly computationally efficient, but also has more representation power. It\noutperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet\nclassification. For instance, Mobile-Former achieves 77.9\\% top-1 accuracy at\n294M FLOPs, gaining 1.3\\% over MobileNetV3 but saving 17\\% of computations.\nWhen transferring to object detection, Mobile-Former outperforms MobileNetV3 by\n8.6 AP in RetinaNet framework. Furthermore, we build an efficient end-to-end\ndetector by replacing backbone, encoder and decoder in DETR with Mobile-Former,\nwhich outperforms DETR by 1.1 AP but saves 52\\% of computational cost and 36\\%\nof parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.02173","description":"<p>Autonomous vehicles must reason about spatial occlusions in urban\nenvironments to ensure safety without being overly cautious. Prior work\nexplored occlusion inference from observed social behaviors of road agents,\nhence treating people as sensors. Inferring occupancy from agent behaviors is\nan inherently multimodal problem; a driver may behave similarly for different\noccupancy patterns ahead of them (e.g., a driver may move at constant speed in\ntraffic or on an open road). Past work, however, does not account for this\nmultimodality, thus neglecting to model this source of aleatoric uncertainty in\nthe relationship between driver behaviors and their environment. We propose an\nocclusion inference method that characterizes observed behaviors of human\nagents as sensor measurements, and fuses them with those from a standard sensor\nsuite. To capture the aleatoric uncertainty, we train a conditional variational\nautoencoder with a discrete latent space to learn a multimodal mapping from\nobserved driver trajectories to an occupancy grid representation of the view\nahead of the driver. Our method handles multi-agent scenarios, combining\nmeasurements from multiple observed drivers using evidential theory to solve\nthe sensor fusion problem. Our approach is validated on a cluttered, real-world\nintersection, outperforming baselines and demonstrating real-time capable\nperformance. Our code is available at\nhttps://github.com/sisl/MultiAgentVariationalOcclusionInference .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_Y/0/1/0/all/0/1\">Ye-Ji Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03223","description":"<p>Out of all existing frameworks for surgical workflow analysis in endoscopic\nvideos, action triplet recognition stands out as the only one aiming to provide\ntruly fine-grained and comprehensive information on surgical activities. This\ninformation, presented as &lt;instrument, verb, target&gt; combinations, is highly\nchallenging to be accurately identified. Triplet components can be difficult to\nrecognize individually; in this task, it requires not only performing\nrecognition simultaneously for all three triplet components, but also correctly\nestablishing the data association between them. To achieve this task, we\nintroduce our new model, the Rendezvous (RDV), which recognizes triplets\ndirectly from surgical videos by leveraging attention at two different levels.\nWe first introduce a new form of spatial attention to capture individual action\ntriplet components in a scene; called Class Activation Guided Attention\nMechanism (CAGAM). This technique focuses on the recognition of verbs and\ntargets using activations resulting from instruments. To solve the association\nproblem, our RDV model adds a new form of semantic attention inspired by\nTransformer networks; called Multi-Head of Mixed Attention (MHMA). This\ntechnique uses several cross and self attentions to effectively capture\nrelationships between instruments, verbs, and targets. We also introduce\nCholecT50 - a dataset of 50 endoscopic videos in which every frame has been\nannotated with labels from 100 triplet classes. Our proposed RDV model\nsignificantly improves the triplet prediction mean AP by over 9% compared to\nthe state-of-the-art methods on this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristians Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_B/0/1/0/all/0/1\">Barbara Seeliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1\">Didier Mutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marescaux_J/0/1/0/all/0/1\">Jacques Marescaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04275","description":"<p>Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07644","description":"<p>Employing Vehicle-to-Vehicle communication to enhance perception performance\nin self-driving technology has attracted considerable attention recently;\nhowever, the absence of a suitable open dataset for benchmarking algorithms has\nmade it difficult to develop and assess cooperative perception technologies. To\nthis end, we present the first large-scale open simulated dataset for\nVehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464\nframes, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns\nin CARLA and a digital town of Culver City, Los Angeles. We then construct a\ncomprehensive benchmark with a total of 16 implemented models to evaluate\nseveral information fusion strategies~(i.e. early, late, and intermediate\nfusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose\na new Attentive Intermediate Fusion pipeline to aggregate information from\nmultiple connected vehicles. Our experiments show that the proposed pipeline\ncan be easily integrated with existing 3D LiDAR detectors and achieve\noutstanding performance even with large compression rates. To encourage more\nresearchers to investigate Vehicle-to-Vehicle perception, we will release the\ndataset, benchmark methods, and all related codes in\nhttps://mobility-lab.seas.ucla.edu/opv2v/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Horizon Detection Algorithm for Maritime Surveillance. (arXiv:2110.13694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13694","description":"<p>The horizon line is a valuable feature in the maritime environment as it has\na high persistence when compared to other features (e.g., shore corners,\nwaves). It is used in several applications, especially in maritime\nsurveillance. The task of horizon detection may be easy for humans, but it is\nhard on computers due to the high change of color and texture on maritime\nscenes. Moreover, the computational complexity is an important constraint to\ntake into account while developing the algorithm. In this paper, we propose a\nnew method that we expect to enhance the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zardoua_Y/0/1/0/all/0/1\">Yassir Zardoua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Astito Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_B/0/1/0/all/0/1\">Boulaala Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction. (arXiv:2111.06636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06636","description":"<p>This work proposes a new computational framework for learning a structured\ngenerative model for real-world datasets. In particular, we propose to learn a\nclosed-loop transcription between a multi-class multi-dimensional data\ndistribution and a linear discriminative representation (LDR) in the feature\nspace that consists of multiple independent multi-dimensional linear subspaces.\nIn particular, we argue that the optimal encoding and decoding mappings sought\ncan be formulated as the equilibrium point of a two-player minimax game between\nthe encoder and decoder. A natural utility function for this game is the\nso-called rate reduction, a simple information-theoretic measure for distances\nbetween mixtures of subspace-like Gaussians in the feature space. Our\nformulation draws inspiration from closed-loop error feedback from control\nsystems and avoids expensive evaluating and minimizing approximated distances\nbetween arbitrary distributions in either the data space or the feature space.\nTo a large extent, this new formulation unifies the concepts and benefits of\nAuto-Encoding and GAN and naturally extends them to the settings of learning a\nboth discriminative and generative representation for multi-class and\nmulti-dimensional real-world data. Our extensive experiments on many benchmark\nimagery datasets demonstrate tremendous potential of this new closed-loop\nformulation: under fair comparison, visual quality of the learned decoder and\nclassification performance of the encoder is competitive and often better than\nexisting methods based on GAN, VAE, or a combination of both. Unlike existing\ngenerative models, the so learned features of the multiple classes are\nstructured: different classes are explicitly mapped onto corresponding\nindependent principal subspaces in the feature space. Source code can be found\nat https://github.com/Delay-Xili/LDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xili Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psenka_M/0/1/0/all/0/1\">Michael Psenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Pengyuan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung Yeung Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09099","description":"<p>Anomaly detection is commonly pursued as a one-class classification problem,\nwhere models can only learn from normal training samples, while being evaluated\non both normal and abnormal test samples. Among the successful approaches for\nanomaly detection, a distinguished category of methods relies on predicting\nmasked information (e.g. patches, future frames, etc.) and leveraging the\nreconstruction error with respect to the masked information as an abnormality\nscore. Different from related methods, we propose to integrate the\nreconstruction-based functionality into a novel self-supervised predictive\narchitectural building block. The proposed self-supervised block is generic and\ncan easily be incorporated into various state-of-the-art anomaly detection\nmethods. Our block starts with a convolutional layer with dilated filters,\nwhere the center area of the receptive field is masked. The resulting\nactivation maps are passed through a channel attention module. Our block is\nequipped with a loss that minimizes the reconstruction error with respect to\nthe masked area in the receptive field. We demonstrate the generality of our\nblock by integrating it into several state-of-the-art frameworks for anomaly\ndetection on image and video, providing empirical evidence that shows\nconsiderable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We\nrelease our code as open source at https://github.com/ristea/sspcab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1\">Neelu Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Aegis: Robust adversarial protectors for medical images. (arXiv:2111.10969v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10969","description":"<p>Deep neural network based medical image systems are vulnerable to adversarial\nexamples. Many defense mechanisms have been proposed in the literature,\nhowever, the existing defenses assume a passive attacker who knows little about\nthe defense system and does not change the attack strategy according to the\ndefense. Recent works have shown that a strong adaptive attack, where an\nattacker is assumed to have full knowledge about the defense system, can easily\nbypass the existing defenses. In this paper, we propose a novel adversarial\nexample defense system called Medical Aegis. To the best of our knowledge,\nMedical Aegis is the first defense in the literature that successfully\naddresses the strong adaptive adversarial example attacks to medical images.\nMedical Aegis boasts two-tier protectors: The first tier of Cushion weakens the\nadversarial manipulation capability of an attack by removing its high-frequency\ncomponents, yet posing a minimal effect on classification performance of the\noriginal image; the second tier of Shield learns a set of per-class DNN models\nto predict the logits of the protected model. Deviation from the Shield's\nprediction indicates adversarial examples. Shield is inspired by the\nobservations in our stress tests that there exist robust trails in the shallow\nlayers of a DNN model, which the adaptive attacks can hardly destruct.\nExperimental results show that the proposed defense accurately detects adaptive\nattacks, with negligible overhead for model inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zecheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Object Detection via Adaptive Self-Training. (arXiv:2111.13216v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13216","description":"<p>We tackle the problem of domain adaptation in object detection, where there\nis a significant domain shift between a source (a domain with supervision) and\na target domain (a domain of interest without supervision). As a widely adopted\ndomain adaptation method, the self-training teacher-student framework (a\nstudent model learns from pseudo labels generated from a teacher model) has\nyielded remarkable accuracy gain on the target domain. However, it still\nsuffers from the large amount of low-quality pseudo labels (e.g., false\npositives) generated from the teacher due to its bias toward the source domain.\nTo address this issue, we propose a self-training framework called Adaptive\nUnbiased Teacher (AUT) leveraging adversarial learning and weak-strong data\naugmentation during mutual learning to address domain shift. Specifically, we\nemploy feature-level adversarial training in the student model, ensuring\nfeatures extracted from the source and target domains share similar statistics.\nThis enables the student model to capture domain-invariant features.\nFurthermore, we apply weak-strong augmentation and mutual learning between the\nteacher model on the target domain and the student model on both domains. This\nenables the teacher model to gradually benefit from the student model without\nsuffering domain shift. We show that AUT demonstrates superiority over all\nexisting approaches and even Oracle (fully supervised) models by a large\nmargin. For example, we achieve 50.9% (49.3%) mAP on Foggy Cityscape\n(Clipart1K), which is 9.2% (5.2%) and 8.2% (11.0%) higher than previous\nstate-of-the-art and Oracle, respectively\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaoliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chih-Yao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zijian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Frame Interpolation Transformer. (arXiv:2111.13817v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13817","description":"<p>Existing methods for video interpolation heavily rely on deep convolution\nneural networks, and thus suffer from their intrinsic limitations, such as\ncontent-agnostic kernel weights and restricted receptive field. To address\nthese issues, we propose a Transformer-based video interpolation framework that\nallows content-aware aggregation weights and considers long-range dependencies\nwith the self-attention operations. To avoid the high computational cost of\nglobal self-attention, we introduce the concept of local attention into video\ninterpolation and extend it to the spatial-temporal domain. Furthermore, we\npropose a space-time separation strategy to save memory usage, which also\nimproves performance. In addition, we develop a multi-scale frame synthesis\nscheme to fully realize the potential of Transformers. Extensive experiments\ndemonstrate the proposed model performs favorably against the state-of-the-art\nmethods both quantitatively and qualitatively on a variety of benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis. (arXiv:2111.14822v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14822","description":"<p>We present the vector quantized diffusion (VQ-Diffusion) model for\ntext-to-image generation. This method is based on a vector quantized\nvariational autoencoder (VQ-VAE) whose latent space is modeled by a conditional\nvariant of the recently developed Denoising Diffusion Probabilistic Model\n(DDPM). We find that this latent-space method is well-suited for text-to-image\ngeneration tasks because it not only eliminates the unidirectional bias with\nexisting methods but also allows us to incorporate a mask-and-replace diffusion\nstrategy to avoid the accumulation of errors, which is a serious problem with\nexisting methods. Our experiments show that the VQ-Diffusion produces\nsignificantly better text-to-image generation results when compared with\nconventional autoregressive (AR) models with similar numbers of parameters.\nCompared with previous GAN-based text-to-image methods, our VQ-Diffusion can\nhandle more complex scenes and improve the synthesized image quality by a large\nmargin. Finally, we show that the image generation computation in our method\ncan be made highly efficient by reparameterization. With traditional AR\nmethods, the text-to-image generation time increases linearly with the output\nimage resolution and hence is quite time consuming even for normal size images.\nThe VQ-Diffusion allows us to achieve a better trade-off between quality and\nspeed. Our experiments indicate that the VQ-Diffusion model with the\nreparameterization is fifteen times faster than traditional AR methods while\nachieving a better image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15430","description":"<p>In spite of the dominant performances of deep neural networks, recent works\nhave shown that they are poorly calibrated, resulting in over-confident\npredictions. Miscalibration can be exacerbated by overfitting due to the\nminimization of the cross-entropy during training, as it promotes the predicted\nsoftmax probabilities to match the one-hot label assignments. This yields a\npre-softmax activation of the correct class that is significantly larger than\nthe remaining activations. Recent evidence from the literature suggests that\nloss functions that embed implicit or explicit maximization of the entropy of\npredictions yield state-of-the-art calibration performances. We provide a\nunifying constrained-optimization perspective of current state-of-the-art\ncalibration losses. Specifically, these losses could be viewed as\napproximations of a linear penalty (or a Lagrangian) imposing equality\nconstraints on logit distances. This points to an important limitation of such\nunderlying equality constraints, whose ensuing gradients constantly push\ntowards a non-informative solution, which might prevent from reaching the best\ncompromise between the discriminative performance and calibration of the model\nduring gradient-based optimization. Following our observations, we propose a\nsimple and flexible generalization based on inequality constraints, which\nimposes a controllable margin on logit distances. Comprehensive experiments on\na variety of image classification, semantic segmentation and NLP benchmarks\ndemonstrate that our method sets novel state-of-the-art results on these tasks\nin terms of network calibration, without affecting the discriminative\nperformance. The code is available at https://github.com/by-liu/MbLS .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic travel pattern extraction from visa page stamps using CNN models. (arXiv:2112.00348v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00348","description":"<p>Manual travel pattern inference from visa page stamps is a time consuming\nactivity and constitutes an important bottleneck in the efficiency of traveler\ninspection at border crossings. Despite efforts to digitize and record the\nborder crossing information into databases, travel pattern inference from\nstamps will remain a problem until every country in the world is incorporated\ninto such a unified system. This could take decades. We propose an automated\ndocument analysis system that processes scanned visa pages and automatically\nextracts the travel pattern from detected stamps. The system processes the page\nvia the following pipeline: stamp detection in the visa page; general stamp\ncountry and entry/exit recognition; Schengen area stamp country and entry/exit\nrecognition; Schengen area stamp date extraction. For each stage of the\nproposed pipeline we construct neural network models and train then on a\nmixture of real and synthetic data. We integrated Schengen area stamp detection\nand date, country, entry/exit recognition models together with a graphical user\ninterface into a prototype of an automatic travel pattern extraction tool. We\nfind that by combining simple neural network models into our proposed pipeline\na useful tool can be created which can speed up the travel pattern extraction\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ledinauskas_E/0/1/0/all/0/1\">Eimantas Ledinauskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruseckas_J/0/1/0/all/0/1\">Julius Ruseckas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marozas_J/0/1/0/all/0/1\">Julius Marozas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlauskas_K/0/1/0/all/0/1\">Kasparas Karlauskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terentjevas_J/0/1/0/all/0/1\">Justas Terentjevas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macijauskas_A/0/1/0/all/0/1\">Augustas Ma&#x10d;ijauskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jursenas_A/0/1/0/all/0/1\">Alfonsas Jur&#x161;&#x117;nas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVT: BERT Pretraining of Video Transformers. (arXiv:2112.01529v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01529","description":"<p>This paper studies the BERT pretraining of video transformers. It is a\nstraightforward but worth-studying extension given the recent success from BERT\npretraining of image transformers. We introduce BEVT which decouples video\nrepresentation learning into spatial representation learning and temporal\ndynamics learning. In particular, BEVT first performs masked image modeling on\nimage data, and then conducts masked image modeling jointly with masked video\nmodeling on video data. This design is motivated by two observations: 1)\ntransformers learned on image datasets provide decent spatial priors that can\nease the learning of video transformers, which are often times\ncomputationally-intensive if trained from scratch; 2) discriminative clues,\ni.e., spatial and temporal information, needed to make correct predictions vary\namong different videos due to large intra-class and inter-class variations. We\nconduct extensive experiments on three challenging video benchmarks where BEVT\nachieves very promising results. On Kinetics 400, for which recognition mostly\nrelies on discriminative spatial representations, BEVT achieves comparable\nresults to strong supervised baselines. On Something-Something-V2 and Diving\n48, which contain videos relying on temporal dynamics, BEVT outperforms by\nclear margins all alternative baselines and achieves state-of-the-art\nperformance with a 71.4\\% and 87.2\\% Top-1 accuracy respectively. Code will be\nmade available at \\url{https://github.com/xyzforever/BEVT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Next Day Wildfire Spread: A Machine Learning Data Set to Predict Wildfire Spreading from Remote-Sensing Data. (arXiv:2112.02447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02447","description":"<p>Predicting wildfire spread is critical for land management and disaster\npreparedness. To this end, we present `Next Day Wildfire Spread,' a curated,\nlarge-scale, multivariate data set of historical wildfires aggregating nearly a\ndecade of remote-sensing data across the United States. In contrast to existing\nfire data sets based on Earth observation satellites, our data set combines 2D\nfire data with multiple explanatory variables (e.g., topography, vegetation,\nweather, drought index, population density) aligned over 2D regions, providing\na feature-rich data set for machine learning. To demonstrate the usefulness of\nthis data set, we implement a neural network that takes advantage of the\nspatial information of this data to predict wildfire spread. We compare the\nperformance of the neural network with other machine learning models: logistic\nregression and random forest. This data set can be used as a benchmark for\ndeveloping wildfire propagation models based on remote sensing data for a lead\ntime of one day.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1\">Fantine Huot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">R. Lily Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Nita Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_T/0/1/0/all/0/1\">Tharun Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihme_M/0/1/0/all/0/1\">Matthias Ihme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Fan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04108","description":"<p>Recent non-local self-attention methods have proven to be effective in\ncapturing long-range dependencies for semantic segmentation. These methods\nusually form a similarity map of RC*C (by compressing spatial dimensions) or\nRHW*HW (by compressing channels) to describe the feature relations along either\nchannel or spatial dimensions, where C is the number of channels, H and W are\nthe spatial dimensions of the input feature map. However, such practices tend\nto condense feature dependencies along the other dimensions,hence causing\nattention missing, which might lead to inferior results for small/thin\ncategories or inconsistent segmentation inside large objects. To address this\nproblem, we propose anew approach, namely Fully Attentional Network (FLANet),to\nencode both spatial and channel attentions in a single similarity map while\nmaintaining high computational efficiency. Specifically, for each channel map,\nour FLANet can harvest feature responses from all other channel maps, and the\nassociated spatial positions as well, through a novel fully attentional module.\nOur new method has achieved state-of-the-art performance on three challenging\nsemantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes\ntest set,the ADE20K validation set, and the PASCAL VOC test set,respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs. (arXiv:2112.04222v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04222","description":"<p>Today's VidSGG models are all proposal-based methods, i.e., they first\ngenerate numerous paired subject-object snippets as proposals, and then conduct\npredicate classification for each proposal. In this paper, we argue that this\nprevalent proposal-based framework has three inherent drawbacks: 1) The\nground-truth predicate labels for proposals are partially correct. 2) They\nbreak the high-order relations among different predicate instances of a same\nsubject-object pair. 3) VidSGG performance is upper-bounded by the quality of\nthe proposals. To this end, we propose a new classification-then-grounding\nframework for VidSGG, which can avoid all the three overlooked drawbacks.\nMeanwhile, under this framework, we reformulate the video scene graphs as\ntemporal bipartite graphs, where the entities and predicates are two types of\nnodes with time slots, and the edges denote different semantic roles between\nthese nodes. This formulation takes full advantage of our new framework.\nAccordingly, we further propose a novel BIpartite Graph based SGG model: BIG.\nSpecifically, BIG consists of two parts: a classification stage and a grounding\nstage, where the former aims to classify the categories of all the nodes and\nthe edges, and the latter tries to localize the temporal location of each\nrelation instance. Extensive ablations on two VidSGG datasets have attested to\nthe effectiveness of our framework and BIG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a memory-efficient transformer that combines a Swin\ntransformer with a lightweight multilayer perceptron (MLP) decoder. With the\nefficient Swin transformer blocks for hierarchical self-attention and the\nsimple MLP decoder for aggregating both local and global attention, iSegFormer\nlearns powerful representations while achieving high computational\nefficiencies. Specifically, we apply iSegFormer to interactive 3D medical image\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yining Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.01367","description":"<p>Increasing the performance of tactile sensing in robots enables versatile,\nin-hand manipulation. Vision-based tactile sensors have been widely used as\nrich tactile feedback has been shown to be correlated with increased\nperformance in manipulation tasks. Existing tactile sensor solutions with high\nresolution have limitations that include low accuracy, expensive components, or\nlack of scalability. In this paper, an inexpensive, scalable, and compact\ntactile sensor with high-resolution surface deformation modeling for surface\nreconstruction of the 3D sensor surface is proposed. By measuring the image\nfrom the fisheye camera, it is shown that the sensor can successfully estimate\nthe surface deformation in real-time (1.8ms) by using deep convolutional neural\nnetworks. This sensor in its design and sensing abilities represents a\nsignificant step toward better object in-hand localization, classification, and\nsurface estimation all enabled by high-resolution shape reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_W/0/1/0/all/0/1\">Won Kyung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_M/0/1/0/all/0/1\">Monroe Kennedy III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Recognition System using TensorFlow Object Detection API. (arXiv:2201.01486v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01486","description":"<p>Communication is defined as the act of sharing or exchanging information,\nideas or feelings. To establish communication between two people, both of them\nare required to have knowledge and understanding of a common language. But in\nthe case of deaf and dumb people, the means of communication are different.\nDeaf is the inability to hear and dumb is the inability to speak. They\ncommunicate using sign language among themselves and with normal people but\nnormal people do not take seriously the importance of sign language. Not\neveryone possesses the knowledge and understanding of sign language which makes\ncommunication difficult between a normal person and a deaf and dumb person. To\novercome this barrier, one can build a model based on machine learning. A model\ncan be trained to recognize different gestures of sign language and translate\nthem into English. This will help a lot of people in communicating and\nconversing with deaf and dumb people. The existing Indian Sing Language\nRecognition systems are designed using machine learning algorithms with single\nand double-handed gestures but they are not real-time. In this paper, we\npropose a method to create an Indian Sign Language dataset using a webcam and\nthen using transfer learning, train a TensorFlow model to create a real-time\nSign Language Recognition system. The system achieves a good level of accuracy\neven with a limited size dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sharvani Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1\">Amisha Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Richa Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sudhakar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02001","description":"<p>Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruotong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Weiliang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09168","description":"<p>This paper aims for the task of text-to-video retrieval, where given a query\nin the form of a natural-language sentence, it is asked to retrieve videos\nwhich are semantically relevant to the given query, from a great number of\nunlabeled videos. The success of this task depends on cross-modal\nrepresentation learning that projects both videos and sentences into common\nspaces for semantic similarity computation. In this work, we concentrate on\nvideo representation learning, an essential component for text-to-video\nretrieval. Inspired by the reading strategy of humans, we propose a\nReading-strategy Inspired Visual Representation Learning (RIVRL) to represent\nvideos, which consists of two branches: a previewing branch and an\nintensive-reading branch. The previewing branch is designed to briefly capture\nthe overview information of videos, while the intensive-reading branch is\ndesigned to obtain more in-depth information. Moreover, the intensive-reading\nbranch is aware of the video overview captured by the previewing branch. Such\nholistic information is found to be useful for the intensive-reading branch to\nextract more fine-grained features. Extensive experiments on three datasets are\nconducted, where our model RIVRL achieves a new state-of-the-art on TGIF and\nVATEX. Moreover, on MSR-VTT, our model using two video features shows\ncomparable performance to the state-of-the-art using seven video features and\neven outperforms models pre-trained on the large-scale HowTo100M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00455","description":"<p>Hierarchical semantic structures naturally exist in an image dataset, in\nwhich several semantically relevant image clusters can be further integrated\ninto a larger cluster with coarser-grained semantics. Capturing such structures\nwith image representations can greatly benefit the semantic understanding on\nvarious downstream tasks. Existing contrastive representation learning methods\nlack such an important model capability. In addition, the negative pairs used\nin these methods are not guaranteed to be semantically distinct, which could\nfurther hamper the structural correctness of learned image representations. To\ntackle these limitations, we propose a novel contrastive learning framework\ncalled Hierarchical Contrastive Selective Coding (HCSC). In this framework, a\nset of hierarchical prototypes are constructed and also dynamically updated to\nrepresent the hierarchical semantic structures underlying the data in the\nlatent space. To make image representations better fit such semantic\nstructures, we employ and further improve conventional instance-wise and\nprototypical contrastive learning via an elaborate pair selection scheme. This\nscheme seeks to select more diverse positive pairs with similar semantics and\nmore precise negative pairs with truly distinct semantics. On extensive\ndownstream tasks, we verify the superior performance of HCSC over\nstate-of-the-art contrastive methods, and the effectiveness of major model\ncomponents is proved by plentiful analytical studies. We build a comprehensive\nmodel zoo in Sec. D. Our source code and model weights are available at\nhttps://github.com/gyfastas/HCSC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Network. (arXiv:2202.09741v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09741","description":"<p>While originally designed for natural language processing (NLP) tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel large kernel attention\n(LKA) module to enable self-adaptive and long-range correlations in\nself-attention while avoiding the above issues. We further introduce a novel\nneural network based on LKA, namely Visual Attention Network (VAN). While\nextremely simple and efficient, VAN outperforms the state-of-the-art vision\ntransformers and convolutional neural networks with a large margin in extensive\nexperiments, including image classification, object detection, semantic\nsegmentation, instance segmentation, etc. Code is available at\nhttps://github.com/Visual-Attention-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Deterministic Face Mask Removal Based On 3D Priors. (arXiv:2202.09856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09856","description":"<p>This paper presents a novel image inpainting framework for face mask removal.\nAlthough current methods have demonstrated their impressive ability in\nrecovering damaged face images, they suffer from two main problems: the\ndependence on manually labeled missing regions and the deterministic result\ncorresponding to each input. The proposed approach tackles these problems by\nintegrating a multi-task 3D face reconstruction module with a face inpainting\nmodule. Given a masked face image, the former predicts a 3DMM-based\nreconstructed face together with a binary occlusion map, providing dense\ngeometrical and textural priors that greatly facilitate the inpainting task of\nthe latter. By gradually controlling the 3D shape parameters, our method\ngenerates high-quality dynamic inpainting results with different expressions\nand mouth movements. Qualitative and quantitative experiments verify the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangnan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10401","description":"<p>Vision-language representation learning largely benefits from image-text\nalignment through contrastive losses (e.g., InfoNCE loss). The success of this\nalignment strategy is attributed to its capability in maximizing the mutual\ninformation (MI) between an image and its matched text. However, simply\nperforming cross-modal alignment (CMA) ignores data potential within each\nmodality, which may result in degraded representations. For instance, although\nCMA-based models are able to map image-text pairs close together in the\nembedding space, they fail to ensure that similar inputs from the same modality\nstay close by. This problem can get even worse when the pre-training data is\nnoisy. In this paper, we propose triple contrastive learning (TCL) for\nvision-language pre-training by leveraging both cross-modal and intra-modal\nself-supervision. Besides CMA, TCL introduces an intra-modal contrastive\nobjective to provide complementary benefits in representation learning. To take\nadvantage of localized and structural information from image and text input,\nTCL further maximizes the average MI between local regions of image/text and\ntheir global summary. To the best of our knowledge, ours is the first work that\ntakes into account local structure information for multi-modality\nrepresentation learning. Experimental evaluations show that our approach is\ncompetitive and achieve the new state of the art on various common down-stream\nvision-language tasks such as image-text retrieval and visual question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sampath Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation. (arXiv:2202.13377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13377","description":"<p>LiDAR sensor is essential to the perception system in autonomous vehicles and\nintelligent robots. To fulfill the real-time requirements in real-world\napplications, it is necessary to efficiently segment the LiDAR scans. Most of\nprevious approaches directly project 3D point cloud onto the 2D spherical range\nimage so that they can make use of the efficient 2D convolutional operations\nfor image segmentation. Although having achieved the encouraging results, the\nneighborhood information is not well-preserved in the spherical projection.\nMoreover, the temporal information is not taken into consideration in the\nsingle scan segmentation task. To tackle these problems, we propose a novel\napproach to semantic segmentation for LiDAR sequences named Meta-RangeSeg,\nwhere a novel range residual image representation is introduced to capture the\nspatial-temporal information. Specifically, Meta-Kernel is employed to extract\nthe meta features, which reduces the inconsistency between the 2D range image\ncoordinates input and Cartesian coordinates output. An efficient U-Net backbone\nis used to obtain the multi-scale features. Furthermore, Feature Aggregation\nModule (FAM) aggregates the meta features and multi-scale features, which tends\nto strengthen the role of range channel. We have conducted extensive\nexperiments for performance evaluation on SemanticKITTI, which is the de-facto\ndataset for LiDAR semantic segmentation. The promising results show that our\nproposed Meta-RangeSeg method is more efficient and effective than the existing\napproaches. Our full implementation is publicly available at\nhttps://github.com/songw-zju/Meta-RangeSeg .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00048","description":"<p>Aligning signals from different modalities is an important step in\nvision-language representation learning as it affects the performance of later\nstages such as cross-modality fusion. Since image and text typically reside in\ndifferent regions of the feature space, directly aligning them at instance\nlevel is challenging especially when features are still evolving during\ntraining. In this paper, we propose to align at a higher and more stable level\nusing cluster representation. Specifically, we treat image and text as two\n\"views\" of the same entity, and encode them into a joint vision-language coding\nspace spanned by a dictionary of cluster centers (codebook). We contrast\npositive and negative samples via their cluster assignments while\nsimultaneously optimizing the cluster centers. To further smooth out the\nlearning process, we adopt a teacher-student distillation paradigm, where the\nmomentum teacher of one view guides the student learning of the other. We\nevaluated our approach on common vision language benchmarks and obtain new SoTA\non zero-shot cross modality retrieval while being competitive on various other\ntransfer tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks. (arXiv:2203.00131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00131","description":"<p>Transformers have emerged to be successful in a number of natural language\nprocessing and vision tasks, but their potential applications to medical\nimaging remain largely unexplored due to the unique difficulties of this field.\nIn this study, we present UTNetV2, a simple yet powerful backbone model that\ncombines the strengths of the convolutional neural network and Transformer for\nenhancing performance and efficiency in medical image segmentation. The\ncritical design of UTNetV2 includes three innovations: (1) We used a hybrid\nhierarchical architecture by introducing depthwise separable convolution to\nprojection and feed-forward network in the Transformer block, which brings\nlocal relationship modeling and desirable properties of CNNs (translation\ninvariance) to Transformer, thus eliminate the requirement of large-scale\npre-training. (2) We proposed efficient bidirectional attention (B-MHA) that\nreduces the quadratic computation complexity of self-attention to linear by\nintroducing an adaptively updated semantic map. The efficient attention makes\nit possible to capture long-range relationship and correct the fine-grained\nerrors in high-resolution token maps. (3) The semantic maps in the B-MHA allow\nus to perform semantically and spatially global multi-scale feature fusion\nwithout introducing much computational overhead. Furthermore, we provide a fair\ncomparison codebase of CNN-based and Transformer-based on various medical image\nsegmentation tasks to evaluate the merits and defects of both architectures.\nUTNetV2 demonstrated state-of-the-art performance across various settings,\nincluding large-scale datasets, small-scale datasets, 2D and 3D settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00523","description":"<p>Deep generative models, such as Variational Autoencoders (VAEs) and\nGenerative Adversarial Networks (GANs), have been employed widely in\ncomputational creativity research. However, such models discourage\nout-of-distribution generation to avoid spurious sample generation, thereby\nlimiting their creativity. Thus, incorporating research on human creativity\ninto generative deep learning techniques presents an opportunity to make their\noutputs more compelling and human-like. As we see the emergence of generative\nmodels directed toward creativity research, a need for machine learning-based\nsurrogate metrics to characterize creative output from these models is\nimperative. We propose group-based subset scanning to identify, quantify, and\ncharacterize creative processes by detecting a subset of anomalous\nnode-activations in the hidden layers of the generative models. Our experiments\non the standard image benchmarks, and their \"creatively generated\" variants,\nreveal that the proposed subset scores distribution is more useful for\ndetecting creative processes in the activation space rather than the pixel\nspace. Further, we found that creative samples generate larger subsets of\nanomalies than normal or non-creative samples across datasets. The node\nactivations highlighted during the creative decoding process are different from\nthose responsible for the normal sample generation. Lastly, we assess if the\nimages from the subsets selected by our method were also found creative by\nhuman evaluators, presenting a link between creativity perception in humans and\nnode activations within deep neural nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cintas_C/0/1/0/all/0/1\">Celia Cintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1\">Brian Quanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadesse_G/0/1/0/all/0/1\">Girmaw Abebe Tadesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speakman_S/0/1/0/all/0/1\">Skyler Speakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00859","description":"<p>Recent transformer-based solutions have been introduced to estimate 3D human\npose from 2D keypoint sequence by considering body joints among all frames\nglobally to learn spatio-temporal correlation. We observe that the motions of\ndifferent joints differ significantly. However, the previous methods cannot\nefficiently model the solid inter-frame correspondence of each joint, leading\nto insufficient learning of spatial-temporal correlation. We propose MixSTE\n(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to\nseparately model the temporal motion of each joint and a spatial transformer\nblock to learn inter-joint spatial correlation. These two blocks are utilized\nalternately to obtain better spatio-temporal feature encoding. In addition, the\nnetwork output is extended from the central frame to entire frames of the input\nvideo, thereby improving the coherence between the input and output sequences.\nExtensive experiments are conducted on three benchmarks (i.e. Human3.6M,\nMPI-INF-3DHP, and HumanEva) to evaluate the proposed method. The results show\nthat our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and\n7.6% MPJPE on the Human3.6M dataset. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01216","description":"<p>Equivariance to permutations and rigid motions is an important inductive bias\nfor various 3D learning problems. Recently it has been shown that the\nequivariant Tensor Field Network architecture is universal -- it can\napproximate any equivariant function. In this paper we suggest a much simpler\narchitecture, prove that it enjoys the same universality guarantees and\nevaluate its performance on Modelnet40. The code to reproduce our experiments\nis available at \\url{https://github.com/simpleinvariance/UniversalNetwork}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelshtein_B/0/1/0/all/0/1\">Ben Finkelshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01252","description":"<p>3D point cloud understanding is an important component in autonomous driving\nand robotics. In this paper, we present a novel Embedding-Querying paradigm\n(EQ-Paradigm) for 3D understanding tasks including detection, segmentation and\nclassification. EQ-Paradigm is a unified paradigm that enables the combination\nof any existing 3D backbone architectures with different task heads. Under the\nEQ-Paradigm, the input is firstly encoded in the embedding stage with an\narbitrary feature extraction architecture, which is independent of tasks and\nheads. Then, the querying stage enables the encoded features to be applicable\nfor diverse task heads. This is achieved by introducing an intermediate\nrepresentation, i.e., Q-representation, in the querying stage to serve as a\nbridge between the embedding stage and task heads. We design a novel Q-Net as\nthe querying stage network. Extensive experimental results on various 3D tasks\nincluding semantic segmentation, object detection and shape classification show\nthat EQ-Paradigm in tandem with Q-Net is a general and effective pipeline,\nwhich enables a flexible collaboration of backbones and heads, and further\nboosts the performance of the state-of-the-art methods. All codes and models\nwill be published soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zetong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Celebrities with Identity Consistency Transformer. (arXiv:2203.01318v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01318","description":"<p>In this work we propose Identity Consistency Transformer, a novel face\nforgery detection method that focuses on high-level semantics, specifically\nidentity information, and detecting a suspect face by finding identity\ninconsistency in inner and outer face regions. The Identity Consistency\nTransformer incorporates a consistency loss for identity consistency\ndetermination. We show that Identity Consistency Transformer exhibits superior\ngeneralization ability not only across different datasets but also across\nvarious types of image degradation forms found in real-world applications\nincluding deepfake videos. The Identity Consistency Transformer can be easily\nenhanced with additional identity information when such information is\navailable, and for this reason it is especially well-suited for detecting face\nforgeries involving celebrities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}