{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Convex Polytope Modelling for Unsupervised Derivation of Semantic Structure for Data-efficient Natural Language Understanding. (arXiv:2201.10588v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10588","description":"<p>Popular approaches for Natural Language Understanding (NLU) usually rely on a\nhuge amount of annotated data or handcrafted rules, which is laborious and not\nadaptive to domain extension. We recently proposed a\nConvex-Polytopic-Model-based framework that shows great potential in\nautomatically extracting semantic patterns by exploiting the raw dialog corpus.\nThe extracted semantic patterns can be used to generate semantic frames, which\nis essential in assisting NLU tasks. This paper further studies the CPM model\nin depth and visualizes its high interpretability and transparency at various\nlevels. We show that this framework can exploit\n</p>\n<p>semantic-frame-related features in the corpus, reveal the underlying semantic\nstructure of the utterances, and boost the performance of the state-of-the-art\nNLU model with minimal supervision. We conduct our experiments on the ATIS (Air\nTravel Information System) corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaohan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">King Keung Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOM-LM: Learning Generalizable Representations for HTML Documents. (arXiv:2201.10608v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10608","description":"<p>HTML documents are an important medium for disseminating information on the\nWeb for human consumption. An HTML document presents information in multiple\ntext formats including unstructured text, structured key-value pairs, and\ntables. Effective representation of these documents is essential for machine\nunderstanding to enable a wide range of applications, such as Question\nAnswering, Web Search, and Personalization. Existing work has either\nrepresented these documents using visual features extracted by rendering them\nin a browser, which is typically computationally expensive, or has simply\ntreated them as plain text documents, thereby failing to capture useful\ninformation presented in their HTML structure. We argue that the text and HTML\nstructure together convey important semantics of the content and therefore\nwarrant a special treatment for their representation learning. In this paper,\nwe introduce a novel representation learning approach for web pages, dubbed\nDOM-LM, which addresses the limitations of existing approaches by encoding both\ntext and DOM tree structure with a transformer-based encoder and learning\ngeneralizable representations for HTML documents via self-supervised\npre-training. We evaluate DOM-LM on a variety of webpage understanding tasks,\nincluding Attribute Extraction, Open Information Extraction, and Question\nAnswering. Our extensive experiments show that DOM-LM consistently outperforms\nall baselines designed for these tasks. In particular, DOM-LM demonstrates\nbetter generalization performance both in few-shot and zero-shot settings,\nmaking it attractive for making it suitable for real-world application settings\nwith limited labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiralkar_P/0/1/0/all/0/1\">Prashant Shiralkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lockard_C/0/1/0/all/0/1\">Colin Lockard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Binxuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ABBE Corpus: Animate Beings Being Emotional. (arXiv:2201.10618v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10618","description":"<p>Emotion detection is an established NLP task of demonstrated utility for text\nunderstanding. However, basic emotion detection leaves out key information,\nnamely, who is experiencing the emotion in question. For example, it may be the\nauthor, the narrator, or a character; or the emotion may correspond to\nsomething the audience is supposed to feel, or even be unattributable to a\nspecific being, e.g., when emotions are being discussed per se. We provide the\nABBE corpus -- Animate Beings Being Emotional -- a new double-annotated corpus\nof texts that captures this key information for one class of emotion\nexperiencer, namely, animate beings in the world described by the text. Such a\ncorpus is useful for developing systems that seek to model or understand this\nspecific type of expressed emotion. Our corpus contains 30 chapters, comprising\n134,513 words, drawn from the Corpus of English Novels, and contains 2,010\nunique emotion expressions attributable to 2,227 animate beings. The emotion\nexpressions are categorized according to Plutchik's 8-category emotion model,\nand the overall inter-annotator agreement for the annotations was 0.83 Cohen's\nKappa, indicating excellent agreement. We describe in detail our annotation\nscheme and procedure, and also release the corpus for use by other researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zad_S/0/1/0/all/0/1\">Samira Zad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_J/0/1/0/all/0/1\">Joshuan Jimenez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Mark A. Finlayson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model. (arXiv:2201.10707v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10707","description":"<p>Synthetic data construction of Grammatical Error Correction (GEC) for\nnon-English languages relies heavily on human-designed and language-specific\nrules, which produce limited error-corrected patterns. In this paper, we\npropose a generic and language-independent strategy for multilingual GEC, which\ncan train a GEC system effectively for a new non-English language with only two\neasy-to-access resources: 1) a pretrained cross-lingual language model (PXLM)\nand 2) parallel translation data between English and the language. Our approach\ncreates diverse parallel GEC data without any language-specific operations by\ntaking the non-autoregressive translation generated by PXLM and the gold\ntranslation as error-corrected sentence pairs. Then, we reuse PXLM to\ninitialize the GEC model and pretrain it with the synthetic data generated by\nitself, which yields further improvement. We evaluate our approach on three\npublic benchmarks of GEC in different languages. It achieves the\nstate-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains\ncompetitive performance on Falko-Merlin (German) and RULEC-GEC (Russian).\nFurther analysis demonstrates that our data construction method is\ncomplementary to rule-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models. (arXiv:2201.10716v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10716","description":"<p>Neural network models have achieved state-of-the-art performance on\ngrapheme-to-phoneme (G2P) conversion. However, their performance relies on\nlarge-scale pronunciation dictionaries, which may not be available for a lot of\nlanguages. Inspired by the success of the pre-trained language model BERT, this\npaper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which\nis built by self-supervised training on a large, language-specific word list\nwith only grapheme information. Furthermore, two approaches are developed to\nincorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,\nfine-tuning GBERT or fusing GBERT into the Transformer model by attention.\nExperimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean\ndatasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our\nGBERT-based G2P models under both medium-resource and low-resource data\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Lu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhi-Qiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Ya-Jun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Decoder Transformer For end-to-end Mandarin Chinese Speech Recognition with Pinyin and Character. (arXiv:2201.10792v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10792","description":"<p>End-to-end automatic speech recognition (ASR) has achieved promising results.\nHowever, most existing end-to-end ASR methods neglect the use of specific\nlanguage characteristics. For Mandarin Chinese ASR tasks, pinyin and character\nas writing and spelling systems respectively are mutual promotion in the\nMandarin Chinese language. Based on the above intuition, we investigate types\nof related models that are suitable but not for joint pinyin-character ASR and\npropose a novel Mandarin Chinese ASR model with dual-decoder Transformer\naccording to the characteristics of the pinyin transcripts and character\ntranscripts. Specifically, the joint pinyin-character layer-wise linear\ninteractive (LWLI) module and phonetic posteriorgrams adapter (PPGA) are\nproposed to achieve inter-layer multi-level interaction by adaptively fusing\npinyin and character information. Furthermore, a two-stage training strategy is\nproposed to make training more stable and faster convergence. The results on\nthe test sets of AISHELL-1 dataset show that the proposed\nSpeech-Pinyin-Character-Interaction (SPCI) model without a language model\nachieves 9.85% character error rate (CER) on the test set, which is 17.71%\nrelative reduction compared to baseline models based on Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1\">Wei Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jizhong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Question-Answering Framework Based on Evolution Algorithm. (arXiv:2201.10797v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10797","description":"<p>Building a deep learning model for a Question-Answering (QA) task requires a\nlot of human effort, it may need several months to carefully tune various model\narchitectures and find a best one. It's even harder to find different excellent\nmodels for multiple datasets. Recent works show that the best model structure\nis related to the dataset used, and one single model cannot adapt to all tasks.\nIn this paper, we propose an automated Question-Answering framework, which\ncould automatically adjust network architecture for multiple datasets. Our\nframework is based on an innovative evolution algorithm, which is stable and\nsuitable for multiple dataset scenario. The evolution algorithm for search\ncombine prior knowledge into initial population and use a performance estimator\nto avoid inefficient mutation by predicting the performance of candidate model\narchitecture. The prior knowledge used in initial population could improve the\nfinal result of the evolution algorithm. The performance estimator could\nquickly filter out models with bad performance in population as the number of\ntrials increases, to speed up the convergence. Our framework achieves 78.9 EM\nand 86.1 F1 on SQuAD 1.1, 69.9 EM and 72.5 F1 on SQuAD 2.0. On NewsQA dataset,\nthe found model achieves 47.0 EM and 62.9 F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sinan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qiyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Both the validity of the cultural tightness index and the association with creativity and order are spurious -- a comment on Jackson et al. (arXiv:2201.10812v1 [stat.AP])","link":"http://arxiv.org/abs/2201.10812","description":"<p>It was recently suggested in a study published in Nature Human Behaviour that\nthe historical loosening of American culture was associated with a trade-off\nbetween higher creativity and lower order. To this end, Jackson et al. generate\na linguistic index of cultural tightness based on the Google Books Ngram corpus\nand use this index to show that American norms loosened between 1800 and 2000.\nWhile we remain agnostic toward a potential loosening of American culture and a\nstatistical association with creativity/order, we show here that the methods\nused by Jackson et al. are neither suitable for testing the validity of the\nindex nor for establishing possible relationships with creativity/order.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfer_S/0/1/0/all/0/1\">Sascha Wolfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning. (arXiv:2201.10866v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10866","description":"<p>In this paper, we propose the CodeRetriever model, which combines the\nunimodal and bimodal contrastive learning to train function-level code semantic\nrepresentations, specifically for the code search task. For unimodal\ncontrastive learning, we design a semantic-guided method to build positive code\npairs based on the documentation and function name. For bimodal contrastive\nlearning, we leverage the documentation and in-line comments of code to build\ntext-code pairs. Both contrastive objectives can fully leverage the large-scale\ncode corpus for pre-training. Experimental results on several public\nbenchmarks, (i.e., CodeSearch, CoSQA, etc.) demonstrate the effectiveness of\nCodeRetriever in the zero-shot setting. By fine-tuning with domain/language\nspecified downstream data, CodeRetriever achieves the new state-of-the-art\nperformance with significant improvement over existing code pre-trained models.\nWe will make the code, model checkpoint, and constructed datasets publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Norwegian Parliamentary Speech Corpus. (arXiv:2201.10881v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10881","description":"<p>The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with\nrecordings of meetings from Stortinget, the Norwegian parliament. It is the\nfirst, publicly available dataset containing unscripted, Norwegian speech\ndesigned for training of automatic speech recognition (ASR) systems. The\nrecordings are manually transcribed and annotated with language codes and\nspeakers, and there are detailed metadata about the speakers. The\ntranscriptions exist in both normalized and non-normalized form, and\nnon-standardized words are explicitly marked and annotated with standardized\nequivalents. To test the usefulness of this dataset, we have compared an ASR\nsystem trained on the NPSC with a baseline system trained on only\nmanuscript-read speech. These systems were tested on an independent dataset\ncontaining spontaneous, dialectal speech. The NPSC-trained system performed\nsignificantly better, with a 22.9% relative improvement in word error rate\n(WER). Moreover, training on the NPSC is shown to have a \"democratizing\" effect\nin terms of dialects, as improvements are generally larger for dialects with\nhigher WER from the baseline system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solberg_P/0/1/0/all/0/1\">Per Erik Solberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_P/0/1/0/all/0/1\">Pablo Ortiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair-Level Supervised Contrastive Learning for Natural Language Inference. (arXiv:2201.10927v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10927","description":"<p>Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer the relationship\nbetween the sentence pair (premise and hypothesis). Many recent works have used\ncontrastive learning by incorporating the relationship of the sentence pair\nfrom NLI datasets to learn sentence representation. However, these methods only\nfocus on comparisons with sentence-level representations. In this paper, we\npropose a Pair-level Supervised Contrastive Learning approach (PairSCL). We\nadopt a cross attention module to learn the joint representations of the\nsentence pairs. A contrastive learning objective is designed to distinguish the\nvaried classes of sentence pairs by pulling those in one class together and\npushing apart the pairs in other classes. We evaluate PairSCL on two public\ndatasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%\non average. Furthermore, our method outperforms the previous state-of-the-art\nmethod on seven transfer tasks of text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Food Review and Recommendation. (arXiv:2201.10978v1 [cs.IR])","link":"http://arxiv.org/abs/2201.10978","description":"<p>Food reviews and recommendations have always been important for online food\nservice websites. However, reviewing and recommending food is not simple as it\nis likely to be overwhelmed by disparate contexts and meanings. In this paper,\nwe use different deep learning approaches to address the problems of sentiment\nanalysis, automatic review tag generation, and retrieval of food reviews. We\npropose to develop a web-based food review system at Nanyang Technological\nUniversity (NTU) named NTU Food Hunter, which incorporates different deep\nlearning approaches that help users with food selection. First, we implement\nthe BERT and LSTM deep learning models into the system for sentiment analysis\nof food reviews. Then, we develop a Part-of-Speech (POS) algorithm to\nautomatically identify and extract adjective-noun pairs from the review content\nfor review tag generation based on POS tagging and dependency parsing. Finally,\nwe also train a RankNet model for the re-ranking of the retrieval results to\nimprove the accuracy in our Solr-based food reviews search system. The\nexperimental results show that our proposed deep learning approaches are\npromising for the applications of real-world problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tan Khang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1\">Siu Cheung Hui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data. (arXiv:2201.10986v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10986","description":"<p>Twitter data have become essential to Natural Language Processing (NLP) and\nsocial science research, driving various scientific discoveries in recent\nyears. However, the textual data alone are often not enough to conduct studies:\nespecially social scientists need more variables to perform their analysis and\ncontrol for various factors. How we augment this information, such as users'\nlocation, age, or tweet sentiment, has ramifications for anonymity and\nreproducibility, and requires dedicated effort. This paper describes\nTwitter-Demographer, a simple, flow-based tool to enrich Twitter data with\nadditional information about tweets and users. Twitter-Demographer is aimed at\nNLP practitioners and (computational) social scientists who want to enrich\ntheir datasets with aggregated information, facilitating reproducibility, and\nproviding algorithmic privacy-by-design measures for pseudo-anonymity. We\ndiscuss our design choices, inspired by the flow-based programming paradigm, to\nuse black-box components that can easily be chained together and extended. We\nalso analyze the ethical issues related to the use of this tool, and the\nbuilt-in measures to facilitate pseudo-anonymity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutrona_V/0/1/0/all/0/1\">Vincenzo Cutrona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11014","description":"<p>Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interferences similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1\">Masataka Sawayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphe_M/0/1/0/all/0/1\">Maxime Adolphe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCAI-QReCC Shared Task on Conversational Question Answering. (arXiv:2201.11094v1 [cs.IR])","link":"http://arxiv.org/abs/2201.11094","description":"<p>Search-Oriented Conversational AI (SCAI) is an established venue that\nregularly puts a spotlight upon the recent work advancing the field of\nconversational search. SCAI'21 was organised as an independent on-line event\nand featured a shared task on conversational question answering. Since all of\nthe participant teams experimented with answer generation models for this task,\nwe identified evaluation of answer correctness in this settings as the major\nchallenge and a current research gap. Alongside the automatic evaluation, we\nconducted two crowdsourcing experiments to collect annotations for answer\nplausibility and faithfulness. As a result of this shared task, the original\nconversational QA dataset used for evaluation was further extended with\nalternative correct answers produced by the participant systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11114","description":"<p>Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to protected categories like race and gender in models\ntrained on datasets intended to obscure these features. Finally, we use MILAN\nfor editing, improving robustness in an image classifier by deleting neurons\nsensitive to text features spuriously correlated with class labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagashvili_T/0/1/0/all/0/1\">Teona Bagashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CsFEVER and CTKFacts: Czech Datasets for Fact Verification. (arXiv:2201.11115v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11115","description":"<p>In this paper we present two Czech datasets aimed for training automated\nfact-checking machine learning models. Specifically we deal with the task of\nassessment of a textual claim veracity w.r.t. to a (presumably) verified\ncorpus. The output of the system is the claim classification SUPPORTS or\nREFUTES complemented with evidence documents or NEI (Not Enough Info) alone. In\nthe first place we publish CsFEVER of approximately 112k claims which is an\nautomatically generated Czech version of the well-known Wikipedia-based FEVER\ndataset. We took a hybrid approach of machine translation and language\nalignment, where the same method (and tools we provide) can be easily applied\nto other languages. The second dataset CTKFacts of 3,097 claims is built on the\ncorpus of approximately two million Czech News Agency news reports. We present\nan extended methodology based on the FEVER approach. Most notably, we describe\na method to automatically generate wider claim contexts (dictionaries) for\nnon-hyperlinked corpora. The datasets are analyzed for spurious cues, which are\nannotation patterns leading to model overfitting. CTKFacts is further examined\nfor inter-annotator agreement, and a typology of common annotator errors is\nextracted. Finally, we provide baseline models for all stages of the\nfact-checking pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drchal_J/0/1/0/all/0/1\">Jan Drchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_H/0/1/0/all/0/1\">Herbert Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rypar_M/0/1/0/all/0/1\">Martin R&#xfd;par</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincourova_H/0/1/0/all/0/1\">Hana Vincourov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Representations for Modeling Variation in Speech. (arXiv:2011.12649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12649","description":"<p>Variation in speech is often quantified by comparing phonetic transcriptions\nof the same utterance. However, manually transcribing speech is time-consuming\nand error prone. As an alternative, therefore, we investigate the extraction of\nacoustic embeddings from several self-supervised neural models. We use these\nrepresentations to compute word-based pronunciation differences between\nnon-native and native speakers of English, and between Norwegian dialect\nspeakers. For comparison with several earlier studies, we evaluate how well\nthese differences match human perception by comparing them with available human\njudgements of similarity. We show that speech representations extracted from a\nspecific type of neural model (i.e. Transformers) lead to a better match with\nhuman perception than two earlier approaches on the basis of phonetic\ntranscriptions and MFCC-based acoustic features. We furthermore find that\nfeatures from the neural models can generally best be extracted from one of the\nmiddle hidden layers than from the final layer. We also demonstrate that neural\nspeech representations not only capture segmental differences, but also\nintonational and durational differences that cannot adequately be represented\nby a set of discrete symbols used in phonetic transcriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1\">Wietse de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanal_F/0/1/0/all/0/1\">Faraz Sanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1\">Caitlin Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberman_M/0/1/0/all/0/1\">Mark Liberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.09966","description":"<p>Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence. However, in the\nclassic persuasion games setting, the messages sent from the expert to the\ndecision-maker (DM) are abstract or well-structured signals rather than natural\nlanguage messages. This paper addresses the use of natural language in\npersuasion games. For this purpose, we conduct an online repeated interaction\nexperiment. At each trial of the interaction, an informed expert aims to sell\nan uninformed decision-maker a vacation in a hotel, by sending her a review\nthat describes the hotel. While the expert is exposed to several scored\nreviews, the decision-maker observes only the single review sent by the expert,\nand her payoff in case she chooses to take the hotel is a random draw from the\nreview score distribution available to the expert only. We also compare the\nbehavioral patterns in this experiment to the equivalent patterns in similar\nexperiments where the communication is based on the numerical values of the\nreviews rather than the reviews' text, and observe substantial differences\nwhich can be explained through an equilibrium analysis of the game. We consider\na number of modeling approaches for our verbal communication setup, differing\nfrom each other in the model type (deep neural network vs. linear classifier),\nthe type of features used by the model (textual, behavioral or both) and the\nsource of the textual features (DNN-based vs. hand-crafted). Our results\ndemonstrate that given a prefix of the interaction sequence, our models can\npredict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.\nFurther analysis of the hand-crafted textual features allows us to make initial\nobservations about the aspects of text that drive decision making in our setup\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erev_I/0/1/0/all/0/1\">Ido Erev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition. (arXiv:2110.04934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04934","description":"<p>The goal of self-supervised learning (SSL) for automatic speech recognition\n(ASR) is to learn good speech representations from a large amount of unlabeled\nspeech for the downstream ASR task. However, most SSL frameworks do not\nconsider noise robustness which is crucial for real-world applications. In this\npaper we propose wav2vec-Switch, a method to encode noise robustness into\ncontextualized representations of speech via contrastive learning.\nSpecifically, we feed original-noisy speech pairs simultaneously into the\nwav2vec 2.0 network. In addition to the existing contrastive learning task, we\nswitch the quantized representations of the original and noisy speech as\nadditional prediction targets of each other. By doing this, it enforces the\nnetwork to have consistent predictions for the original and noisy speech, thus\nallows to learn contextualized representation with noise robustness. Our\nexperiments on synthesized and real noisy data show the effectiveness of our\nmethod: it achieves 2.9--4.9% relative word error rate (WER) reduction on the\nsynthesized noisy LibriSpeech data without deterioration on the original data,\nand 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation\nbaseline even with a strong language model for decoding. Our results on CHiME-4\ncan match or even surpass those with well-designed speech enhancement\ncomponents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. (arXiv:2110.06800v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06800","description":"<p>Zero/few-shot transfer to unseen services is a critical challenge in\ntask-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset\nintroduced a paradigm for enabling models to support an unlimited number of\nservices without additional data collection or re-training through the use of\nschemas. Schemas describe APIs in natural language, which models consume to\nunderstand the services they need to support. However, the impact of the choice\nof language in these schemas on model performance remains unexplored. We\naddress this by releasing SGD-X, a benchmark for measuring the robustness of\ndialogue systems to linguistic variations in schemas. SGD-X extends the SGD\ndataset with crowdsourced variants for every schema, where variants are\nsemantically similar yet stylistically diverse. We evaluate two top-performing\ndialogue state tracking models on SGD-X and observe that neither generalizes\nwell across schema variants, measured by joint goal accuracy and a novel metric\nfor measuring schema sensitivity. Finally, we present a simple model-agnostic\ndata augmentation method to improve schema robustness and zero-shot\ngeneralization to unseen services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Smart Healthcare. (arXiv:2110.15803v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15803","description":"<p>Smart healthcare has achieved significant progress in recent years. Emerging\nartificial intelligence (AI) technologies enable various smart applications\nacross various healthcare scenarios. As an essential technology powered by AI,\nnatural language processing (NLP) plays a key role in smart healthcare due to\nits capability of analysing and understanding human language. In this work, we\nreview existing studies that concern NLP for smart healthcare from the\nperspectives of technique and application. We focus on feature extraction and\nmodelling for various NLP tasks encountered in smart healthcare from a\ntechnical point of view. In the context of smart healthcare applications\nemploying NLP techniques, the elaboration largely attends to representative\nsmart healthcare scenarios, including clinical practice, hospital management,\npersonal care, public health, and drug development. We further discuss the\nlimitations of current works and identify the directions for future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Binggui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanghua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaodan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01140","description":"<p>The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention. (arXiv:2201.10375v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2201.10375","description":"<p>With recent advancements in voice cloning, the performance of speech\nsynthesis for a target speaker has been rendered similar to the human level.\nHowever, autoregressive voice cloning systems still suffer from text alignment\nfailures, resulting in an inability to synthesize long sentences. In this work,\nwe propose a variant of attention-based text-to-speech system that can\nreproduce a target voice from a few seconds of reference speech and generalize\nto very long utterances as well. The proposed system is based on three\nindependently trained components: a speaker encoder, synthesizer and universal\nvocoder. Generalization to long utterances is realized using an energy-based\nattention mechanism known as Dynamic Convolution Attention, in combination with\na set of modifications proposed for the synthesizer based on Tacotron 2.\nMoreover, effective zero-shot speaker adaptation is achieved by conditioning\nboth the synthesizer and vocoder on a speaker encoder that has been pretrained\non a large corpus of diverse data. We compare several implementations of voice\ncloning systems in terms of speech naturalness, speaker similarity, alignment\nconsistency and ability to synthesize long utterances, and conclude that the\nproposed model can produce intelligible synthetic speech for extremely long\nutterances, while preserving a high extent of naturalness and similarity for\nshort texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gorodetskii_A/0/1/0/all/0/1\">Artem Gorodetskii</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozhiganov_I/0/1/0/all/0/1\">Ivan Ozhiganov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. (arXiv:2201.10474v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10474","description":"<p>Language models increasingly rely on massive web dumps for diverse text data.\nHowever, these sources are rife with undesirable content. As such, resources\nlike Wikipedia, books, and newswire often serve as anchors for automatically\nselecting web text most suitable for language modeling, a process typically\nreferred to as quality filtering. Using a new dataset of U.S. high school\nnewspaper articles -- written by students from across the country -- we\ninvestigate whose language is preferred by the quality filter used for GPT-3.\nWe find that newspapers from larger schools, located in wealthier, educated,\nand urban ZIP codes are more likely to be classified as high quality. We then\ndemonstrate that the filter's measurement of quality is unaligned with other\nsensible metrics, such as factuality or literary acclaim. We argue that\nprivileging any corpus as high quality entails a language ideology, and more\ncare is needed to construct training corpora for language models, with better\ntransparency and justification for the inclusion or exclusion of various texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreier_S/0/1/0/all/0/1\">Sarah K. Dreier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_E/0/1/0/all/0/1\">Emily K. Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leroy Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models. (arXiv:2201.08471v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2201.08471","description":"<p>The advent of transformer-based models such as BERT has led to the rise of\nneural ranking models. These models have improved the effectiveness of\nretrieval systems well beyond that of lexical term matching models such as\nBM25. While monolingual retrieval tasks have benefited from large-scale\ntraining collections such as MS MARCO and advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. This\npaper introduces ColBERT-X, a generalization of the ColBERT\nmulti-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)\nencoder to support cross-language information retrieval (CLIR). ColBERT-X can\nbe trained in two ways. In zero-shot training, the system is trained on the\nEnglish MS MARCO collection, relying on the XLM-R encoder for cross-language\nmappings. In translate-train, the system is trained on the MS MARCO English\nqueries coupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages demonstrate\nsubstantial and statistically significant improvements of these trained dense\nretrieval models over traditional lexical CLIR baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNamee_P/0/1/0/all/0/1\">Paul McNamee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1\">James Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas W. Oard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10602","description":"<p>In this paper we propose a method that estimates the $SE(3)$ continuous\ntrajectories (orientation and translation) of the dynamic rigid objects present\nin a scene, from multiple RGB-D views. Specifically, we fit the object\ntrajectories to cumulative B-Splines curves, which allow us to interpolate, at\nany intermediate time stamp, not only their poses but also their linear and\nangular velocities and accelerations. Additionally, we derive in this work the\nanalytical $SE(3)$ Jacobians needed by the optimization, being applicable to\nany other approach that uses this type of curves. To the best of our knowledge\nthis is the first work that proposes 6-DoF continuous-time object tracking,\nwhich we endorse with significant computational cost reduction thanks to our\nanalytical derivations. We evaluate our proposal in synthetic data and in a\npublic benchmark, showing competitive results in localization and significant\nimprovements in velocity estimation in comparison to discrete-time approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirado_J/0/1/0/all/0/1\">Javier Tirado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation via Semi-supervised Learning and Label Fusion. (arXiv:2201.10647v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10647","description":"<p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the\ncochlea from magnetic resonance imaging (MRI) are critical to VS treatment\nplanning. Although supervised methods have achieved satisfactory performance in\nVS segmentation, they require full annotations by experts, which is laborious\nand time-consuming. In this work, we aim to tackle the VS and cochlea\nsegmentation problem in an unsupervised domain adaptation setting. Our proposed\nmethod leverages both the image-level domain alignment to minimize the domain\ndivergence and semi-supervised training to further boost the performance.\nFurthermore, we propose to fuse the labels predicted from multiple models via\nnoisy label correction. In the MICCAI 2021 crossMoDA challenge, our results on\nthe final evaluation leaderboard showed that our proposed method has achieved\npromising segmentation performance with mean dice score of 79.9% and 82.5% and\nASSD of 1.29 mm and 0.18 mm for VS tumor and cochlea, respectively. The cochlea\nASSD achieved by our method has outperformed all other competing methods as\nwell as the supervised nnU-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNeil_A/0/1/0/all/0/1\">Andrew McNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M. Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Task Interaction Network for Multi-Task Learning. (arXiv:2201.10649v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10649","description":"<p>Multitask learning (MTL) has recently gained a lot of popularity as a\nlearning paradigm that can lead to improved per-task performance while also\nusing fewer per-task model parameters compared to single task learning. One of\nthe biggest challenges regarding MTL networks involves how to share features\nacross tasks. To address this challenge, we propose the Attentive Task\nInteraction Network (ATI-Net). ATI-Net employs knowledge distillation of the\nlatent features for each task, then combines the feature maps to provide\nimproved contextualized information to the decoder. This novel approach to\nintroducing knowledge distillation into an attention based multitask network\noutperforms state of the art MTL baselines such as the standalone MTAN and\nPAD-Net, with roughly the same number of model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinodinos_D/0/1/0/all/0/1\">Dimitrios Sinodinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1\">Narges Armanfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data. (arXiv:2201.10650v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10650","description":"<p>kin cancer is considered one of the most common type of cancer in several\ncountries. Due to the difficulty and subjectivity in the clinical diagnosis of\nskin lesions, Computer-Aided Diagnosis systems are being developed for assist\nexperts to perform more reliable diagnosis. The clinical analysis and diagnosis\nof skin lesions relies not only on the visual information but also on the\ncontext information provided by the patient. This work addresses the problem of\npigmented skin lesions detection from smartphones captured images. In addition\nto the features extracted from images, patient context information was\ncollected to provide a more accurate diagnosis. The experiments showed that the\ncombination of visual features with context information improved final results.\nExperimental results are very promising and comparable to experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esgario_J/0/1/0/all/0/1\">Jos&#xe9; G. M. Esgario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krohling_R/0/1/0/all/0/1\">Renato A. Krohling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering. (arXiv:2201.10654v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10654","description":"<p>Visual Question Answering (VQA) attracts much attention from both industry\nand academia. As a multi-modality task, it is challenging since it requires not\nonly visual and textual understanding, but also the ability to align\ncross-modality representations. Previous approaches extensively employ\nentity-level alignments, such as the correlations between the visual regions\nand their semantic labels, or the interactions across question words and object\nfeatures. These attempts aim to improve the cross-modality representations,\nwhile ignoring their internal relations. Instead, we propose to apply\nstructured alignments, which work with graph representation of visual and\ntextual content, aiming to capture the deep connections between the visual and\ntextual modalities. Nevertheless, it is nontrivial to represent and integrate\ngraphs for structured alignments. In this work, we attempt to solve this issue\nby first converting different modality entities into sequential nodes and the\nadjacency graph, then incorporating them for structured alignments. As\ndemonstrated in our experimental results, such a structured alignment improves\nreasoning performance. In addition, our model also exhibits better\ninterpretability for each generated answer. The proposed model, without any\npretraining, outperforms the state-of-the-art methods on GQA dataset, and beats\nthe non-pretrained state-of-the-art methods on VQA-v2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Peixi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGA-VQA: Multi-Granularity Alignment for Visual Question Answering. (arXiv:2201.10656v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10656","description":"<p>Learning to answer visual questions is a challenging task since the\nmulti-modal inputs are within two feature spaces. Moreover, reasoning in visual\nquestion answering requires the model to understand both image and question,\nand align them in the same space, rather than simply memorize statistics about\nthe question-answer pairs. Thus, it is essential to find component connections\nbetween different modalities and within each modality to achieve better\nattention. Previous works learned attention weights directly on the features.\nHowever, the improvement is limited since these two modality features are in\ntwo domains: image features are highly diverse, lacking structure and\ngrammatical rules as language, and natural language features have a higher\nprobability of missing detailed information. To better learn the attention\nbetween visual and text, we focus on how to construct input stratification and\nembed structural information to improve the alignment between different level\ncomponents. We propose Multi-Granularity Alignment architecture for Visual\nQuestion Answering task (MGA-VQA), which learns intra- and inter-modality\ncorrelations by multi-granularity alignment, and outputs the final result by\nthe decision fusion module. In contrast to previous works, our model splits\nalignment into different levels to achieve learning better correlations without\nneeding additional data and annotations. The experiments on the VQA-v2 and GQA\ndatasets demonstrate that our model significantly outperforms non-pretrained\nstate-of-the-art methods on both datasets without extra pretraining data and\nannotations. Moreover, it even achieves better results over the pre-trained\nmethods on GQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1\">Peixi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Neural Networks for Segmentation Understand Insideness?. (arXiv:2201.10664v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10664","description":"<p>The insideness problem is an aspect of image segmentation that consists of\ndetermining which pixels are inside and outside a region. Deep Neural Networks\n(DNNs) excel in segmentation benchmarks, but it is unclear if they have the\nability to solve the insideness problem as it requires evaluating long-range\nspatial dependencies. In this paper, the insideness problem is analysed in\nisolation, without texture or semantic cues, such that other aspects of\nsegmentation do not interfere in the analysis. We demonstrate that DNNs for\nsegmentation with few units have sufficient complexity to solve insideness for\nany curve. Yet, such DNNs have severe problems with learning general solutions.\nOnly recurrent networks trained with small images learn solutions that\ngeneralize well to almost any curve. Recurrent networks can decompose the\nevaluation of long-range dependencies into a sequence of local operations, and\nlearning with small images alleviates the common difficulties of training\nrecurrent networks with a large number of unrolling steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villalobos_K/0/1/0/all/0/1\">Kimberly Villalobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stih_V/0/1/0/all/0/1\">Vilim &#x160;tih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadinejad_A/0/1/0/all/0/1\">Amineh Ahmadinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Shobhita Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francl_A/0/1/0/all/0/1\">Andrew Francl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_F/0/1/0/all/0/1\">Frederico Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writer Recognition Using Off-line Handwritten Single Block Characters. (arXiv:2201.10665v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10665","description":"<p>Block characters are often used when filling paper forms for a variety of\npurposes. We investigate if there is biometric information contained within\nindividual digits of handwritten text. In particular, we use personal identity\nnumbers consisting of the six digits of the date of birth, DoB. We evaluate two\nrecognition approaches, one based on handcrafted features that compute contour\ndirectional measurements, and another based on deep features from a ResNet50\nmodel. We use a self-captured database of 317 individuals and 4920 written DoBs\nin total. Results show the presence of identity-related information in a piece\nof handwritten information as small as six digits with the DoB. We also analyze\nthe impact of the amount of enrolment samples, varying its number between one\nand ten. Results with such small amount of data are promising. With ten\nenrolment samples, the Top-1 accuracy with deep features is around 94%, and\nreaches nearly 100% by Top-10. The verification accuracy is more modest, with\nEER&gt;20%with any given feature and enrolment set size, showing that there is\nstill room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_A/0/1/0/all/0/1\">Adrian Leo Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanikzai_R/0/1/0/all/0/1\">Rustam Stanikzai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (arXiv:2201.10675v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10675","description":"<p>This study aims to develop a novel computer-aided diagnosis (CAD) scheme for\nmammographic breast mass classification using semi-supervised learning.\nAlthough supervised deep learning has achieved huge success across various\nmedical image analysis tasks, its success relies on large amounts of\nhigh-quality annotations, which can be challenging to acquire in practice. To\novercome this limitation, we propose employing a semi-supervised method, i.e.,\nvirtual adversarial training (VAT), to leverage and learn useful information\nunderlying in unlabeled data for better classification of breast masses.\nAccordingly, our VAT-based models have two types of losses, namely supervised\nand virtual adversarial losses. The former loss acts as in supervised\nclassification, while the latter loss aims at enhancing model robustness\nagainst virtual adversarial perturbation, thus improving model\ngeneralizability. To evaluate the performance of our VAT-based CAD scheme, we\nretrospectively assembled a total of 1024 breast mass images, with equal number\nof benign and malignant masses. A large CNN and a small CNN were used in this\ninvestigation, and both were trained with and without the adversarial loss.\nWhen the labeled ratios were 40% and 80%, VAT-based CNNs delivered the highest\nclassification accuracy of 0.740 and 0.760, respectively. The experimental\nresults suggest that the VAT-based CAD scheme can effectively utilize\nmeaningful knowledge from unlabeled data to better classify mammographic breast\nmass images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_K/0/1/0/all/0/1\">Kar-Ming Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_T/0/1/0/all/0/1\">Theresa C. Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1\">Kathleen Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannel_R/0/1/0/all/0/1\">Robert S. Mannel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuchen Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Spectral Biophysical Skin Properties from Captured RGB Albedo. (arXiv:2201.10695v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10695","description":"<p>We present a new method to reconstruct and manipulate the spectral properties\nof human skin from simple RGB albedo captures. To this end, we leverage Monte\nCarlo light simulation over an accurate biophysical human skin layering model\nparameterized by its most important components, thereby covering a plausible\nrange of colors. The practical complexity of the model allows us to learn the\ninverse mapping from any albedo to its most probable associated skin\nproperties. Our technique can faithfully reproduce any skin type, being\nexpressive enough to automatically handle more challenging areas like the lips\nor imperfections in the face. Thanks to the smoothness of the skin parameters\nmaps recovered, the albedo can be robustly edited through meaningful\nbiophysical properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aliaga_C/0/1/0/all/0/1\">Carlos Aliaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hery_C/0/1/0/all/0/1\">Christophe Hery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengqi Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image Deblurring: A Survey. (arXiv:2201.10700v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10700","description":"<p>Image deblurring is a classic problem in low-level computer vision, which\naims to recover a sharp image from a blurred input image. Recent advances in\ndeep learning have led to significant progress in solving this problem, and a\nlarge number of deblurring networks have been proposed. This paper presents a\ncomprehensive and timely survey of recently published deep-learning based image\ndeblurring approaches, aiming to serve the community as a useful literature\nreview. We start by discussing common causes of image blur, introduce benchmark\ndatasets and performance metrics, and summarize different problem formulations.\nNext we present a taxonomy of methods using convolutional neural networks (CNN)\nbased on architecture, loss function, and application, offering a detailed\nreview and comparison. In addition, we discuss some domain-specific deblurring\napplications including face images, text, and stereo image pairs. We conclude\nby discussing key challenges and future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenger_B/0/1/0/all/0/1\">Bjorn Stenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection via Reverse Distillation from One-Class Embedding. (arXiv:2201.10703v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10703","description":"<p>Knowledge distillation (KD) achieves promising results on the challenging\nproblem of unsupervised anomaly detection (AD).The representation discrepancy\nof anomalies in the teacher-student (T-S) model provides essential evidence for\nAD. However, using similar or identical architectures to build the teacher and\nstudent models in previous studies hinders the diversity of anomalous\nrepresentations. To tackle this problem, we propose a novel T-S model\nconsisting of a teacher encoder and a student decoder and introduce a simple\nyet effective \"reverse distillation\" paradigm accordingly. Instead of receiving\nraw images directly, the student network takes teacher model's one-class\nembedding as input and targets to restore the teacher's multiscale\nrepresentations. Inherently, knowledge distillation in this study starts from\nabstract, high-level presentations to low-level features. In addition, we\nintroduce a trainable one-class bottleneck embedding (OCBE) module in our T-S\nmodel. The obtained compact embedding effectively preserves essential\ninformation on normal patterns, but abandons anomaly perturbations. Extensive\nexperimentation on AD and one-class novelty detection benchmarks shows that our\nmethod surpasses SOTA performance, demonstrating our proposed approach's\neffectiveness and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanqiu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Data-Driven STAP Radar. (arXiv:2201.10712v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10712","description":"<p>Using an amalgamation of techniques from classical radar, computer vision,\nand deep learning, we characterize our ongoing data-driven approach to\nspace-time adaptive processing (STAP) radar. We generate a rich example dataset\nof received radar signals by randomly placing targets of variable strengths in\na predetermined region using RFView, a site-specific radio frequency modeling\nand simulation tool developed by ISL Inc. For each data sample within this\nregion, we generate heatmap tensors in range, azimuth, and elevation of the\noutput power of a minimum variance distortionless response (MVDR) beamformer,\nwhich can be replaced with a desired test statistic. These heatmap tensors can\nbe thought of as stacked images, and in an airborne scenario, the moving radar\ncreates a sequence of these time-indexed image stacks, resembling a video. Our\ngoal is to use these images and videos to detect targets and estimate their\nlocations, a procedure reminiscent of computer vision algorithms for object\ndetection$-$namely, the Faster Region-Based Convolutional Neural Network\n(Faster R-CNN). The Faster R-CNN consists of a proposal generating network for\ndetermining regions of interest (ROI), a regression network for positioning\nanchor boxes around targets, and an object classification algorithm; it is\ndeveloped and optimized for natural images. Our ongoing research will develop\nanalogous tools for heatmap images of radar data. In this regard, we will\ngenerate a large, representative adaptive radar signal processing database for\ntraining and testing, analogous in spirit to the COCO dataset for natural\nimages. As a preliminary example, we present a regression network in this paper\nfor estimating target locations to demonstrate the feasibility of and\nsignificant improvements provided by our data-driven approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1\">Shyam Venkatasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongkamthong_C/0/1/0/all/0/1\">Chayut Wongkamthong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bosung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogineni_S/0/1/0/all/0/1\">Sandeep Gogineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezeshki_A/0/1/0/all/0/1\">Ali Pezeshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangaswamy_M/0/1/0/all/0/1\">Muralidhar Rangaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Generation with Self Pixel-wise Normalization. (arXiv:2201.10725v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10725","description":"<p>Region-adaptive normalization (RAN) methods have been widely used in the\ngenerative adversarial network (GAN)-based image-to-image translation\ntechnique. However, since these approaches need a mask image to infer the\npixel-wise affine transformation parameters, they cannot be applied to the\ngeneral image generation models having no paired mask images. To resolve this\nproblem, this paper presents a novel normalization method, called self\npixel-wise normalization (SPN), which effectively boosts the generative\nperformance by performing the pixel-adaptive affine transformation without the\nmask image. In our method, the transforming parameters are derived from a\nself-latent mask that divides the feature map into the foreground and\nbackground regions. The visualization of the self-latent masks shows that SPN\neffectively captures a single object to be generated as the foreground. Since\nthe proposed method produces the self-latent mask without external data, it is\neasily applicable in the existing generative models. Extensive experiments on\nvarious datasets reveal that the proposed method significantly improves the\nperformance of image generation technique in terms of Frechet inception\ndistance (FID) and Inception score (IS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yong-Goo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Vision Transformers with Only 2040 Images. (arXiv:2201.10728v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10728","description":"<p>Vision Transformers (ViTs) is emerging as an alternative to convolutional\nneural networks (CNNs) for visual recognition. They achieve competitive results\nwith CNNs but the lack of the typical convolutional inductive bias makes them\nmore data-hungry than common CNNs. They are often pretrained on JFT-300M or at\nleast ImageNet and few works study training ViTs with limited data. In this\npaper, we investigate how to train ViTs with limited data (e.g., 2040 images).\nWe give theoretical analyses that our method (based on parametric instance\ndiscrimination) is superior to other methods in that it can capture both\nfeature alignment and instance similarities. We achieve state-of-the-art\nresults when training from scratch on 7 small datasets under various ViT\nbackbones. We also investigate the transferring ability of small datasets and\nfind that representations learned from small datasets can even improve\nlarge-scale ImageNet training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun-Hao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating the Mutual Error Amplification for Semi-Supervised Object Detection. (arXiv:2201.10734v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10734","description":"<p>Semi-supervised object detection (SSOD) has achieved substantial progress in\nrecent years. However, it is observed that the performances of self-labeling\nSSOD methods remain limited. Based on our experimental analysis, we reveal that\nthe reason behind such phenomenon lies in the mutual error amplification\nbetween the pseudo labels and the trained detector. In this study, we propose a\nCross Teaching (CT) method, aiming to mitigate the mutual error amplification\nby introducing a rectification mechanism of pseudo labels. CT simultaneously\ntrains multiple detectors with an identical structure but different parameter\ninitialization. In contrast to existing mutual teaching methods that directly\ntreat predictions from other detectors as pseudo labels, we propose the Label\nRectification Module (LRM), where the bounding boxes predicted by one detector\nare rectified by using the corresponding boxes predicted by all other detectors\nwith higher confidence scores. In this way, CT can enhance the pseudo label\nquality compared with self-labeling and existing mutual teaching methods, and\nreasonably mitigate the mutual error amplification. Over two popular detector\nstructures, i.e., SSD300 and Faster-RCNN-FPN, the proposed CT method obtains\nconsistent improvements and outperforms the state-of-the-art SSOD methods by\n2.2% absolute mAP improvements on the Pascal VOC and MS-COCO benchmarks. The\ncode is available at github.com/machengcheng2016/CrossTeaching-SSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengcheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Convolution Auto-encoder Network for Infrared and Visible Image Fusion. (arXiv:2201.10736v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10736","description":"<p>Background: Leaning redundant and complementary relationships is a critical\nstep in the human visual system. Inspired by the infrared cognition ability of\ncrotalinae animals, we design a joint convolution auto-encoder (JCAE) network\nfor infrared and visible image fusion. Methods: Our key insight is to feed\ninfrared and visible pair images into the network simultaneously and separate\nan encoder stream into two private branches and one common branch, the private\nbranch works for complementary features learning and the common branch does for\nredundant features learning. We also build two fusion rules to integrate\nredundant and complementary features into their fused feature which are then\nfed into the decoder layer to produce the final fused image. We detail the\nstructure, fusion rule and explain its multi-task loss function. Results: Our\nJCAE network achieves good results in terms of both subjective effect and\nobjective evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhancheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuanhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1\">Mengyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10737","description":"<p>Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: 1) existing methods\nfail to capture the important features of the images due to the naive\ntokenization scheme; 2) the models suffer from information loss because they\nonly consider single-scale feature representations; and 3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCA-GANformer, a novel type of generative adversarial transformers, for medical\nimage segmentation. First, we take advantage of the pyramid structure to\nconstruct multi-scale representations and handle multi-scale variations. We\nthen design a novel class-aware transformer module to better learn the\ndiscriminative regions of objects with semantic structures. Lastly, we utilize\nan adversarial training strategy that boosts segmentation accuracy and\ncorrespondingly allows a transformer-based discriminator to capture high-level\nsemantically correlated contents and low-level anatomical features. Our\nexperiments demonstrate that CA-GANformer dramatically outperforms previous\nstate-of-the-art transformer-based approaches on three benchmarks, obtaining\nabsolute 2.54%-5.88% improvements in Dice over previous models. Further\nqualitative experiments provide a more detailed picture of the model's inner\nworkings, shed light on the challenges in improved transparency, and\ndemonstrate that transfer learning can greatly improve performance and reduce\nthe size of medical image datasets in training, making CA-GANformer a strong\nstarting point for downstream medical image analysis tasks. Codes and models\nwill be available to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinchali_S/0/1/0/all/0/1\">Sandeep Chinchali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared and visible image fusion based on Multi-State Contextual Hidden Markov Model. (arXiv:2201.10739v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10739","description":"<p>The traditional two-state hidden Markov model divides the high frequency\ncoefficients only into two states (large and small states). Such scheme is\nprone to produce an inaccurate statistical model for the high frequency subband\nand reduces the quality of fusion result. In this paper, a fine-grained\nmulti-state contextual hidden Markov model (MCHMM) is proposed for infrared and\nvisible image fusion in the non-subsampled Shearlet domain, which takes full\nconsideration of the strong correlations and level of details of NSST\ncoefficients. To this end, an accurate soft context variable is designed\ncorrespondingly from the perspective of context correlation. Then, the\nstatistical features provided by MCHMM are utilized for the fusion of high\nfrequency subbands. To ensure the visual quality, a fusion strategy based on\nthe difference in regional energy is proposed as well for lowfrequency\nsubbands. Experimental results demonstrate that the proposed method can achieve\na superior performance compared with other fusion methods in both subjective\nand objective aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhancheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10747","description":"<p>Unsupervised real world super resolution (USR) aims at restoring\nhigh-resolution (HR) images given low-resolution (LR) inputs when paired data\nis unavailable. One of the most common approaches is synthesizing noisy LR\nimages using GANs and utilizing a synthetic dataset to train the model in a\nsupervised manner. The goal of modeling the degradation generator is to\napproximate the distribution of LR images given a HR image. Previous works\nsimply assumed the conditional distribution as a delta function and learned the\ndeterministic mapping from HR image to a LR image. Instead, we propose the\nprobabilistic degradation generator. Our degradation generator is a deep\nhierarchical latent variable model and more suitable for modeling the complex\ndistribution. Furthermore, we train multiple degradation generators to enhance\nthe mode coverage and apply the novel collaborative learning. We outperform\nseveral baselines on benchmark datasets in terms of PSNR and SSIM and\ndemonstrate the robustness of our method on unseen data distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sangyun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1\">Sewoong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_K/0/1/0/all/0/1\">Kwangjin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Image Inpainting Using Semantic Guidance. (arXiv:2201.10753v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10753","description":"<p>Image inpainting approaches have achieved significant progress with the help\nof deep neural networks. However, existing approaches mainly focus on\nleveraging the priori distribution learned by neural networks to produce a\nsingle inpainting result or further yielding multiple solutions, where the\ncontrollability is not well studied. This paper develops a novel image\ninpainting approach that enables users to customize the inpainting result by\ntheir own preference or memory. Specifically, our approach is composed of two\nstages that utilize the prior of neural network and user's guidance to jointly\ninpaint corrupted images. In the first stage, an autoencoder based on a novel\nexternal spatial attention mechanism is deployed to produce reconstructed\nfeatures of the corrupted image and a coarse inpainting result that provides\nsemantic mask as the medium for user interaction. In the second stage, a\nsemantic decoder that takes the reconstructed features as prior is adopted to\nsynthesize a fine inpainting result guided by user's customized semantic mask,\nso that the final inpainting result will share the same content with user's\nguidance while the textures and colors reconstructed in the first stage are\npreserved. Extensive experiments demonstrate the superiority of our approach in\nterms of inpainting quality and controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wangbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jinhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhu_Y/0/1/0/all/0/1\">Yuesheng zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes. (arXiv:2201.10766v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10766","description":"<p>While datasets with single-label supervision have propelled rapid advances in\nimage classification, additional annotations are necessary in order to\nquantitatively assess how models make predictions. To this end, for a subset of\nImageNet samples, we collect segmentation masks for the entire object and $18$\ninformative attributes. We call this dataset RIVAL10 (RIch Visual Attributes\nwith Localization), consisting of roughly $26k$ instances over $10$ classes.\nUsing RIVAL10, we evaluate the sensitivity of a broad set of models to noise\ncorruptions in foregrounds, backgrounds and attributes. In our analysis, we\nconsider diverse state-of-the-art architectures (ResNets, Transformers) and\ntraining procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that,\nsomewhat surprisingly, in ResNets, adversarial training makes models more\nsensitive to the background compared to foreground than standard training.\nSimilarly, contrastively-trained models also have lower relative foreground\nsensitivity in both transformers and ResNets. Lastly, we observe intriguing\nadaptive abilities of transformers to increase relative foreground sensitivity\nas corruption level increases. Using saliency methods, we automatically\ndiscover spurious features that drive the background sensitivity of models and\nassess alignment of saliency maps with foregrounds. Finally, we quantitatively\nstudy the attribution problem for neural features by comparing feature saliency\nwith ground-truth localization of semantic attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moayeri_M/0/1/0/all/0/1\">Mazda Moayeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_P/0/1/0/all/0/1\">Phillip Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_Y/0/1/0/all/0/1\">Yogesh Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10776","description":"<p>Multi-contrast MRI (MC-MRI) captures multiple complementary imaging\nmodalities to aid in radiological decision-making. Given the need for lowering\nthe time cost of multiple acquisitions, current deep accelerated MRI\nreconstruction networks focus on exploiting the redundancy between multiple\ncontrasts. However, existing works are largely supervised with paired data\nand/or prohibitively expensive fully-sampled MRI sequences. Further,\nreconstruction networks typically rely on convolutional architectures which are\nlimited in their capacity to model long-range interactions and may lead to\nsuboptimal recovery of fine anatomical detail. To these ends, we present a\ndual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI\nreconstruction. DSFormer develops a deep conditional cascade transformer (DCCT)\nconsisting of several cascaded Swin transformer reconstruction networks\n(SwinRN) trained under two deep conditioning strategies to enable MC-MRI\ninformation sharing. We further present a dual-domain (image and k-space)\nself-supervised learning strategy for DCCT to alleviate the costs of acquiring\nfully sampled training data. DSFormer generates high-fidelity reconstructions\nwhich experimentally outperform current fully-supervised baselines. Moreover,\nwe find that DSFormer achieves nearly the same performance when trained either\nwith full supervision or with our proposed dual-domain self-supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlemper_J/0/1/0/all/0/1\">Jo Schlemper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Seyed Sadegh Mohseni Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sofka_M/0/1/0/all/0/1\">Michal Sofka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASFD: Automatic and Scalable Face Detector. (arXiv:2201.10781v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10781","description":"<p>Along with current multi-scale based detectors, Feature Aggregation and\nEnhancement (FAE) modules have shown superior performance gains for\ncutting-edge object detection. However, these hand-crafted FAE modules show\ninconsistent improvements on face detection, which is mainly due to the\nsignificant distribution difference between its training and applying corpus,\nCOCO vs. WIDER Face. To tackle this problem, we essentially analyse the effect\nof data distribution, and consequently propose to search an effective FAE\narchitecture, termed AutoFAE by a differentiable architecture search, which\noutperforms all existing FAE modules in face detection with a considerable\nmargin. Upon the found AutoFAE and existing backbones, a supernet is further\nbuilt and trained, which automatically obtains a family of detectors under the\ndifferent complexity constraints. Extensive experiments conducted on popular\nbenchmarks, WIDER Face and FDDB, demonstrate the state-of-the-art\nperformance-efficiency trade-off for the proposed automatic and scalable face\ndetector (ASFD) family. In particular, our strong ASFD-D6 outperforms the best\ncompetitor with AP 96.7/96.2/92.1 on WIDER Face test, and the lightweight\nASFD-D0 costs about 3.1 ms, more than 320 FPS, on the V100 GPU with\nVGA-resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">ZhenYu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yili Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation. (arXiv:2201.10788v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10788","description":"<p>In the Vision-and-Language Navigation task, the embodied agent follows\nlinguistic instructions and navigates to a specific goal. It is important in\nmany practical scenarios and has attracted extensive attention from both\ncomputer vision and robotics communities. However, most existing works only use\nRGB images but neglect the 3D semantic information of the scene. To this end,\nwe develop a novel self-supervised training framework to encode the voxel-level\n3D semantic reconstruction into a 3D semantic representation. Specifically, a\nregion query task is designed as the pretext task, which predicts the presence\nor absence of objects of a particular class in a specific 3D region. Then, we\nconstruct an LSTM-based navigation model and train it with the proposed 3D\nsemantic representations and BERT language features on vision-language pairs.\nExperiments show that the proposed approach achieves success rates of 68% and\n66% on the validation unseen and test unseen splits of the R2R dataset\nrespectively, which are superior to most of RGB-based methods utilizing\nvision-language transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sinan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Mengmeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Di Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism. (arXiv:2201.10801v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10801","description":"<p>Attention mechanism has been widely believed as the key to success of vision\ntransformers (ViTs), since it provides a flexible and powerful way to model\nspatial relationships. However, is the attention mechanism truly an\nindispensable part of ViT? Can it be replaced by some other alternatives? To\ndemystify the role of attention mechanism, we simplify it into an extremely\nsimple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift\noperation. It does not contain any parameter or arithmetic calculation. The\nonly operation is to exchange a small portion of the channels between\nneighboring features. Based on this simple operation, we construct a new\nbackbone network, namely ShiftViT, where the attention layers in ViT are\nsubstituted by shift operations. Surprisingly, ShiftViT works quite well in\nseveral mainstream tasks, e.g., classification, detection, and segmentation.\nThe performance is on par with or even better than the strong baseline Swin\nTransformer. These results suggest that the attention mechanism might not be\nthe vital factor that makes ViT successful. It can be even replaced by a\nzero-parameter operation. We should pay more attentions to the remaining parts\nof ViT in the future work. Code is available at github.com/microsoft/SPACH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoDistill: Learning Spatial Features for Monocular 3D Object Detection. (arXiv:2201.10830v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10830","description":"<p>3D object detection is a fundamental and challenging task for 3D scene\nunderstanding, and the monocular-based methods can serve as an economical\nalternative to the stereo-based or LiDAR-based methods. However, accurately\ndetecting objects in the 3D space from a single image is extremely difficult\ndue to the lack of spatial cues. To mitigate this issue, we propose a simple\nand effective scheme to introduce the spatial information from LiDAR signals to\nthe monocular 3D detectors, without introducing any extra cost in the inference\nphase. In particular, we first project the LiDAR signals into the image plane\nand align them with the RGB images. After that, we use the resulting data to\ntrain a 3D detector (LiDAR Net) with the same architecture as the baseline\nmodel. Finally, this LiDAR Net can serve as the teacher to transfer the learned\nknowledge to the baseline model. Experimental results show that the proposed\nmethod can significantly boost the performance of the baseline model and ranks\nthe $1^{st}$ place among all monocular-based methods on the KITTI benchmark.\nBesides, extensive ablation studies are conducted, which further prove the\neffectiveness of each part of our designs and illustrate what the baseline\nmodel has learned from the LiDAR Net. Our code will be released at\n\\url{https://github.com/monster-ghost/MonoDistill}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_Z/0/1/0/all/0/1\">Zhiyu Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuxin Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARS: Pseudo-Label Aware Robust Sample Selection for Learning with Noisy Labels. (arXiv:2201.10836v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10836","description":"<p>Acquiring accurate labels on large-scale datasets is both time consuming and\nexpensive. To reduce the dependency of deep learning models on learning from\nclean labeled data, several recent research efforts are focused on learning\nwith noisy labels. These methods typically fall into three design categories to\nlearn a noise robust model: sample selection approaches, noise robust loss\nfunctions, or label correction methods. In this paper, we propose PARS:\nPseudo-Label Aware Robust Sample Selection, a hybrid approach that combines the\nbest from all three worlds in a joint-training framework to achieve robustness\nto noisy labels. Specifically, PARS exploits all training samples using both\nthe raw/noisy labels and estimated/refurbished pseudo-labels via self-training,\ndivides samples into an ambiguous and a noisy subset via loss analysis, and\ndesigns label-dependent noise-aware loss functions for both sets of filtered\nlabels. Results show that PARS significantly outperforms the state of the art\non extensive studies on the noisy CIFAR-10 and CIFAR-100 datasets, particularly\non challenging high-noise and low-resource settings. In particular, PARS\nachieved an absolute 12% improvement in test accuracy on the CIFAR-100 dataset\nwith 90% symmetric label noise, and an absolute 27% improvement in test\naccuracy when only 1/5 of the noisy labels are available during training as an\nadditional restriction. On a real-world noisy dataset, Clothing1M, PARS\nachieves competitive results to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arushi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massiah_J/0/1/0/all/0/1\">Jordan Massiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Depth Estimation Setups from Stereo Endoscopy and Optical Tracking for Point Measurements. (arXiv:2201.10848v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10848","description":"<p>To support minimally-invasive intraoperative mitral valve repair,\nquantitative measurements from the valve can be obtained using an infra-red\ntracked stylus. It is desirable to view such manually measured points together\nwith the endoscopic image for further assistance. Therefore, hand-eye\ncalibration is required that links both coordinate systems and is a\nprerequisite to project the points onto the image plane. A complementary\napproach to this is to use a vision-based endoscopic stereo-setup to detect and\ntriangulate points of interest, to obtain the 3D coordinates. In this paper, we\naim to compare both approaches on a rigid phantom and two patient-individual\nsilicone replica which resemble the intraoperative scenario. The preliminary\nresults indicate that 3D landmark estimation, either labeled manually or\nthrough partly automated detection with a deep learning approach, provides more\naccurate triangulated depth measurements when performed with a tailored\nimage-based method than with stylus measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burger_L/0/1/0/all/0/1\">Lukas Burger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_S/0/1/0/all/0/1\">Samantha Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brand_J/0/1/0/all/0/1\">Julian Brand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hehl_M/0/1/0/all/0/1\">Maximillian Hehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_I/0/1/0/all/0/1\">Ivo Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Knee Osteoarthritis Progression from Structural MRI using Deep Learning. (arXiv:2201.10849v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10849","description":"<p>Accurate prediction of knee osteoarthritis (KOA) progression from structural\nMRI has a potential to enhance disease understanding and support clinical\ntrials. Prior art focused on manually designed imaging biomarkers, which may\nnot fully exploit all disease-related information present in MRI scan. In\ncontrast, our method learns relevant representations from raw data end-to-end\nusing Deep Learning, and uses them for progression prediction. The method\nemploys a 2D CNN to process the data slice-wise and aggregate the extracted\nfeatures using a Transformer. Evaluated on a large cohort (n=4,866), the\nproposed method outperforms conventional 2D and 3D CNN-based models and\nachieves average precision of $0.58\\pm0.03$ and ROC AUC of $0.78\\pm0.01$. This\npaper sets a baseline on end-to-end KOA progression prediction from structural\nMRI. Our code is publicly available at\nhttps://github.com/MIPT-Oulu/OAProgressionMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Panfilov_E/0/1/0/all/0/1\">Egor Panfilov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saarakkala_S/0/1/0/all/0/1\">Simo Saarakkala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nieminen_M/0/1/0/all/0/1\">Miika T. Nieminen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiulpin_A/0/1/0/all/0/1\">Aleksei Tiulpin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing the diversity of representations learned by Bayesian neural networks. (arXiv:2201.10859v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10859","description":"<p>Explainable artificial intelligence (XAI) aims to make learning machines less\nopaque, and offers researchers and practitioners various tools to reveal the\ndecision-making strategies of neural networks. In this work, we investigate how\nXAI methods can be used for exploring and visualizing the diversity of feature\nrepresentations learned by Bayesian neural networks (BNNs). Our goal is to\nprovide a global understanding of BNNs by making their decision-making\nstrategies a) visible and tangible through feature visualizations and b)\nquantitatively measurable with a distance measure learned by contrastive\nlearning. Our work provides new insights into the posterior distribution in\nterms of human-understandable feature information with regard to the underlying\ndecision-making strategies. Our main findings are the following: 1) global XAI\nmethods can be applied to explain the diversity of decision-making strategies\nof BNN instances, 2) Monte Carlo dropout exhibits increased diversity in\nfeature representations compared to the multimodal posterior approximation of\nMultiSWAG, 3) the diversity of learned feature representations highly\ncorrelates with the uncertainty estimates, and 4) the inter-mode diversity of\nthe multimodal posterior decreases as the network width increases, while the\nintra-mode diversity increases. Our findings are consistent with the recent\ndeep neural networks theory, providing additional intuitions about what the\ntheory implies in terms of humanly understandable concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1\">Dennis Grinwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1\">Kirill Bykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across Different iPad Generations. (arXiv:2201.10865v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10865","description":"<p>In 2017 Apple introduced the TrueDepth sensor with the iPhone X release.\nAlthough its primary use case is biometric face recognition, the exploitation\nof accurate depth data for other computer vision tasks like segmentation,\nportrait image generation and metric 3D reconstruction seems natural and lead\nto the development of various applications. In this report, we investigate the\nreliability of TrueDepth data - accessed through two different APIs - on\nvarious devices including different iPhone and iPad generations and reveal two\ndifferent and significant issues on all tested iPads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urban_S/0/1/0/all/0/1\">Steffen Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemeier_T/0/1/0/all/0/1\">Thomas Lindemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbelstein_D/0/1/0/all/0/1\">David Dobbelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haenel_M/0/1/0/all/0/1\">Matthias Haenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransPPG: Two-stream Transformer for Remote Heart Rate Estimate. (arXiv:2201.10873v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10873","description":"<p>Non-contact facial video-based heart rate estimation using remote\nphotoplethysmography (rPPG) has shown great potential in many applications\n(e.g., remote health care) and achieved creditable results in constrained\nscenarios. However, practical applications require results to be accurate even\nunder complex environment with head movement and unstable illumination.\nTherefore, improving the performance of rPPG in complex environment has become\na key challenge. In this paper, we propose a novel video embedding method that\nembeds each facial video sequence into a feature map referred to as Multi-scale\nAdaptive Spatial and Temporal Map with Overlap (MAST_Mop), which contains not\nonly vital information but also surrounding information as reference, which\nacts as the mirror to figure out the homogeneous perturbations imposed on\nforeground and background simultaneously, such as illumination instability.\nCorrespondingly, we propose a two-stream Transformer model to map the MAST_Mop\ninto heart rate (HR), where one stream follows the pulse signal in the facial\narea while the other figures out the perturbation signal from the surrounding\nregion such that the difference of the two channels leads to adaptive noise\ncancellation. Our approach significantly outperforms all current\nstate-of-the-art methods on two public datasets MAHNOB-HCI and VIPL-HR. As far\nas we know, it is the first work with Transformer as backbone to capture the\ntemporal dependencies in rPPGs and apply the two stream scheme to figure out\nthe interference from backgrounds as mirror of the corresponding perturbation\non foreground signals for noise tolerating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jiaqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Su Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weishan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter Optimization for COVID-19 Chest X-Ray Classification. (arXiv:2201.10885v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10885","description":"<p>Despite the introduction of vaccines, Coronavirus disease (COVID-19) remains\na worldwide dilemma, continuously developing new variants such as Delta and the\nrecent Omicron. The current standard for testing is through polymerase chain\nreaction (PCR). However, PCRs can be expensive, slow, and/or inaccessible to\nmany people. X-rays on the other hand have been readily used since the early\n20th century and are relatively cheaper, quicker to obtain, and typically\ncovered by health insurance. With a careful selection of model,\nhyperparameters, and augmentations, we show that it is possible to develop\nmodels with 83% accuracy in binary classification and 64% in multi-class for\ndetecting COVID-19 infections from chest x-rays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hamdi_I/0/1/0/all/0/1\">Ibraheem Hamdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients. (arXiv:2201.10899v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10899","description":"<p>Federated Learning (FL) allows training machine learning models in\nprivacy-constrained scenarios by enabling the cooperation of edge devices\nwithout requiring local data sharing. This approach raises several challenges\ndue to the different statistical distribution of the local datasets and the\nclients' computational heterogeneity. In particular, the presence of highly\nnon-i.i.d. data severely impairs both the performance of the trained neural\nnetwork and its convergence rate, increasing the number of communication rounds\nrequested to reach a performance comparable to that of the centralized\nscenario. As a solution, we propose FedSeq, a novel framework leveraging the\nsequential training of subgroups of heterogeneous clients, i.e. superclients,\nto emulate the centralized paradigm in a privacy-compliant way. Given a fixed\nbudget of communication rounds, we show that FedSeq outperforms or match\nseveral state-of-the-art federated algorithms in terms of final performance and\nspeed of convergence. Finally, our method can be easily integrated with other\napproaches available in the literature. Empirical results show that combining\nexisting algorithms with FedSeq further improves its final performance and\nconvergence speed. We test our method on CIFAR-10 and CIFAR-100 and prove its\neffectiveness in both i.i.d. and non-i.i.d. scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaccone_R/0/1/0/all/0/1\">Riccardo Zaccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzardi_A/0/1/0/all/0/1\">Andrea Rizzardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldarola_D/0/1/0/all/0/1\">Debora Caldarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar Systems. (arXiv:2201.10910v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10910","description":"<p>Deploying 3D single-photon Lidar imaging in real world applications faces\nmultiple challenges including imaging in high noise environments. Several\nalgorithms have been proposed to address these issues based on statistical or\nlearning-based frameworks. Statistical methods provide rich information about\nthe inferred parameters but are limited by the assumed model correlation\nstructures, while deep learning methods show state-of-the-art performance but\nlimited inference guarantees, preventing their extended use in critical\napplications. This paper unrolls a statistical Bayesian algorithm into a new\ndeep learning architecture for robust image reconstruction from single-photon\nLidar data, i.e., the algorithm's iterative steps are converted into neural\nnetwork layers. The resulting algorithm benefits from the advantages of both\nstatistical and learning based frameworks, providing best estimates with\nimproved network interpretability. Compared to existing learning-based\nsolutions, the proposed architecture requires a reduced number of trainable\nparameters, is more robust to noise and mismodelling effects, and provides\nricher information about the estimates including uncertainty measures. Results\non synthetic and real data show competitive results regarding the quality of\nthe inference and computational complexity when compared to state-of-the-art\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koo_J/0/1/0/all/0/1\">Jakeoung Koo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McLaughlin_S/0/1/0/all/0/1\">Stephen McLaughlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting 3D Adversarial Attacks with Attacking On Frequency. (arXiv:2201.10937v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10937","description":"<p>Deep neural networks (DNNs) have been shown to be vulnerable to adversarial\nattacks. Recently, 3D adversarial attacks, especially adversarial attacks on\npoint clouds, have elicited mounting interest. However, adversarial point\nclouds obtained by previous methods show weak transferability and are easy to\ndefend. To address these problems, in this paper we propose a novel point cloud\nattack (dubbed AOF) that pays more attention on the low-frequency component of\npoint clouds. We combine the losses from point cloud and its low-frequency\ncomponent to craft adversarial samples. Extensive experiments validate that AOF\ncan improve the transferability significantly compared to state-of-the-art\n(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,\ncompared to clean point clouds, adversarial point clouds obtained by AOF\ncontain more deformation than outlier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lyujie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projective Urban Texturing. (arXiv:2201.10938v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10938","description":"<p>This paper proposes a method for automatic generation of textures for 3D city\nmeshes in immersive urban environments. Many recent pipelines capture or\nsynthesize large quantities of city geometry using scanners or procedural\nmodeling pipelines. Such geometry is intricate and realistic, however the\ngeneration of photo-realistic textures for such large scenes remains a problem.\nWe propose to generate textures for input target 3D meshes driven by the\ntextural style present in readily available datasets of panoramic photos\ncapturing urban environments. Re-targeting such 2D datasets to 3D geometry is\nchallenging because the underlying shape, size, and layout of the urban\nstructures in the photos do not correspond to the ones in the target meshes.\nPhotos also often have objects (e.g., trees, vehicles) that may not even be\npresent in the target geometry.To address these issues we present a method,\ncalled Projective Urban Texturing (PUT), which re-targets textural style from\nreal-world panoramic images to unseen urban meshes. PUT relies on contrastive\nand adversarial training of a neural architecture designed for unpaired\nimage-to-texture translation. The generated textures are stored in a texture\natlas applied to the target 3D mesh geometry. To promote texture consistency,\nPUT employs an iterative procedure in which texture synthesis is conditioned on\npreviously generated, adjacent textures. We demonstrate both quantitative and\nqualitative evaluation of the generated textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_Y/0/1/0/all/0/1\">Yiangos Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1\">Melinos Averkiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1\">Tom Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10943","description":"<p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that\nreports asynchronous, continuously per-pixel brightness changes called `events'\nwith high temporal resolution and high dynamic range. So far, the event-based\nimage reconstruction methods are based on artificial neural networks (ANN) or\nhand-crafted spatiotemporal smoothing techniques. In this paper, we first\nimplement the image reconstruction work via fully spiking neural network (SNN)\narchitecture. As the bio-inspired neural networks, SNNs operating with\nasynchronous binary spikes distributed over time, can potentially lead to\ngreater computational efficiency on event-driven hardware. We propose a novel\nEvent-based Video reconstruction framework based on a fully Spiking Neural\nNetwork (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and\nMembrane Potential (MP) neuron. We find that the spiking neurons have the\npotential to store useful temporal information (memory) to complete such\ntime-dependent tasks. Furthermore, to better utilize the temporal information,\nwe propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane\npotential of spiking neuron. The proposed neuron is referred as Adaptive\nMembrane Potential (AMP) neuron, which adaptively updates the membrane\npotential according to the input spikes. The experimental results demonstrate\nthat our models achieve comparable performance to ANN-based models on IJRR,\nMVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are\n19.36$\\times$ and 7.75$\\times$ more computationally efficient than their ANN\narchitectures, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10947","description":"<p>Deep learning models have introduced various intelligent applications to edge\ndevices, such as image classification, speech recognition, and augmented\nreality. There is an increasing need of training such models on the devices in\norder to deliver personalized, responsive, and private learning. To address\nthis need, this paper presents a new solution for deploying and training\nstate-of-the-art models on the resource-constrained devices. First, the paper\nproposes a novel filter-pruning-based model compression method to create\nlightweight trainable models from large models trained in the cloud, without\nmuch loss of accuracy. Second, it proposes a novel knowledge transfer method to\nenable the on-device model to update incrementally in real time or near real\ntime using incremental learning on new data and enable the on-device model to\nlearn the unseen categories with the help of the in-cloud model in an\nunsupervised fashion. The results show that 1) our model compression method can\nremove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy\nof over 90% on CIFAR-10; 2) our knowledge transfer method enables the\ncompressed models to achieve more than 90% accuracy on CIFAR-10 and retain good\naccuracy on old categories; 3) it allows the compressed models to converge\nwithin real time (three to six minutes) on the edge for incremental learning\ntasks; 4) it enables the model to classify unseen categories of data (78.92%\nTop-1 accuracy) that it is never trained with.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yitao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10953","description":"<p>Accurate and fine-grained information about the extent of damage to buildings\nis essential for humanitarian relief and disaster response. However, as the\nmost commonly used architecture in remote sensing interpretation tasks,\nConvolutional Neural Networks (CNNs) have limited ability to model the\nnon-local relationship between pixels. Recently, Transformer architecture first\nproposed for modeling long-range dependency in natural language processing has\nshown promising results in computer vision tasks. Considering the frontier\nadvances of Transformer architecture in the computer vision field, in this\npaper, we present the first attempt at designing a Transformer-based damage\nassessment architecture (DamFormer). In DamFormer, a siamese Transformer\nencoder is first constructed to extract non-local and representative deep\nfeatures from input multitemporal image-pairs. Then, a multitemporal fusion\nmodule is designed to fuse information for downstream tasks. Finally, a\nlightweight dual-tasks decoder aggregates multi-level features for final\nprediction. To the best of our knowledge, it is the first time that such a deep\nTransformer-based network is proposed for multitemporal remote sensing\ninterpretation tasks. The experimental results on the large-scale damage\nassessment dataset xBD demonstrate the potential of the Transformer-based\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemni_E/0/1/0/all/0/1\">Edoardo Nemni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallecorsa_S/0/1/0/all/0/1\">Sofia Vallecorsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bromley_L/0/1/0/all/0/1\">Lars Bromley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compose Diversified Prompts for Image Emotion Classification. (arXiv:2201.10963v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10963","description":"<p>Contrastive Language-Image Pre-training (CLIP) represents the latest\nincarnation of pre-trained vision-language models. Although CLIP has recently\nshown its superior power on a wide range of downstream vision-language tasks\nlike Visual Question Answering, it is still underexplored for Image Emotion\nClassification (IEC). Adapting CLIP to the IEC task has three significant\nchallenges, tremendous training objective gap between pretraining and IEC,\nshared suboptimal and invariant prompts for all instances. In this paper, we\npropose a general framework that shows how CLIP can be effectively applied to\nIEC. We first introduce a prompt tuning method that mimics the pretraining\nobjective of CLIP and thus can leverage the rich image and text semantics\nentailed in CLIP. Then we automatically compose instance-specific prompts by\nconditioning them on the categories and image contents of instances,\ndiversifying prompts and avoiding suboptimal problems. Evaluations on six\nwidely-used affective datasets demonstrate that our proposed method outperforms\nthe state-of-the-art methods to a large margin (i.e., up to 9.29% accuracy gain\non EmotionROI dataset) on IEC tasks, with only a few parameters trained. Our\ncodes will be publicly available for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Sinuo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lifang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Ge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lehao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1\">Meng Jian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10972","description":"<p>Data shift robustness is an active research topic, however, it has been\nprimarily investigated from a fully supervised perspective, and robustness of\nzero-shot learning (ZSL) models have been largely neglected. In this paper, we\npresent a novel analysis on the robustness of discriminative ZSL to image\ncorruptions. We leverage the well-known label embedding model and subject it to\na large set of common corruptions and defenses. In order to realize the\ncorruption analysis, we curate and release the first ZSL corruption robustness\ndatasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account\nthe dataset characteristics, class imbalance, class transition trends between\nseen and unseen classes and the discrepancies between ZSL and GZSL\nperformances. Our results show that discriminative ZSL suffer from corruptions\nand this trend is further exacerbated by the severe class imbalance and model\nweakness inherent in ZSL methods. We then combine our findings with those based\non adversarial attacks in ZSL, and highlight the different effects of\ncorruptions and adversarial examples, such as the pseudo-robustness effect\npresent under adversarial attacks. We also obtain new strong baselines for the\nlabel embedding model with certain corruption robustness enhancement methods.\nFinally, our experiments show that although existing methods to improve\nrobustness somewhat work for ZSL models, they do not produce a tangible effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yucel_M/0/1/0/all/0/1\">Mehmet Kerim Yucel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duygulu_P/0/1/0/all/0/1\">Pinar Duygulu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10981","description":"<p>Deep learning-based segmentation of the liver and hepatic lesions therein\nsteadily gains relevance in clinical practice due to the increasing incidence\nof liver cancer each year. Whereas various network variants with overall\npromising results in the field of medical image segmentation have been\ndeveloped over the last years, almost all of them struggle with the challenge\nof accurately segmenting hepatic lesions. This lead to the idea of combining\nelements of convolutional and transformerbased architectures to overcome the\nexisting limitations. This work presents a hybrid network called SWTR-Unet,\nconsisting of a pretrained ResNet, transformer blocks as well as a common\nUnet-style decoder path. This network was applied to clinical liver MRI, as\nwell as to the publicly available CT data of the liver tumor segmentation\n(LiTS) challenge. Additionally, multiple state-of-the-art networks were\nimplemented and applied to both datasets, ensuring a direct comparability.\nFurthermore, correlation analysis and an ablation study were carried out, to\ninvestigate various influencing factors on the segmentation accuracy of our\npresented method. With Dice similarity scores of averaged 98 +- 2 % for liver\nand 81 +- 28 % lesion segmentation on the MRI dataset and 97 +- 2 % and 79 +-\n25 %, respectively on the CT dataset, the proposed SWTR-Unet outperforms each\nof the additionally implemented state-of-the-art networks. The achieved\nsegmentation accuracy was found to be on par with manually performed expert\nsegmentations as indicated by interobserver variabilities for liver lesion\nsegmentation. In conclusion, the presented method could save valuable time and\nresources in clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hille_G/0/1/0/all/0/1\">Georg Hille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agrawal_S/0/1/0/all/0/1\">Shubham Agrawal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wybranski_C/0/1/0/all/0/1\">Christian Wybranski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pech_M/0/1/0/all/0/1\">Maciej Pech</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Surov_A/0/1/0/all/0/1\">Alexey Surov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saalfeld_S/0/1/0/all/0/1\">Sylvia Saalfeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jalisco's multiclass land cover analysis and classification using a novel lightweight convnet with real-world multispectral and relief data. (arXiv:2201.10985v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10985","description":"<p>The understanding of global climate change, agriculture resilience, and\ndeforestation control rely on the timely observations of the Land Use and Land\nCover Change (LULCC). Recently, some deep learning (DL) methods have been\nadapted to make an automatic classification of Land Cover (LC) for global and\nhomogeneous data. However, most of these DL models can not apply effectively to\nreal-world data. i.e. a large number of classes, multi-seasonal data, diverse\nclimate regions, high imbalance label dataset, and low-spatial resolution. In\nthis work, we present our novel lightweight (only 89k parameters) Convolution\nNeural Network (ConvNet) to make LC classification and analysis to handle these\nproblems for the Jalisco region. In contrast to the global approaches, the\nregional data provide the context-specificity that is required for policymakers\nto plan the land use and management, conservation areas, or ecosystem services.\nIn this work, we combine three real-world open data sources to obtain 13\nchannels. Our embedded analysis anticipates the limited performance in some\nclasses and gives us the opportunity to group the most similar, as a result,\nthe test accuracy performance increase from 73 % to 83 %. We hope that this\nresearch helps other regional groups with limited data sources or computational\nresources to attain the United Nations Sustainable Development Goal (SDG)\nconcerning Life on Land.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quevedo_A/0/1/0/all/0/1\">Alexander Quevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Abraham S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanclares_R/0/1/0/all/0/1\">Raul Nancl&#xe1;res</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montoya_D/0/1/0/all/0/1\">Diana P. Montoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacho_J/0/1/0/all/0/1\">Juan Pacho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_J/0/1/0/all/0/1\">Jorge Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_Sanchez_E/0/1/0/all/0/1\">E. Ulises Moya-S&#xe1;nchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10990","description":"<p>In this paper we consider the problem of classifying fine-grained, multi-step\nactivities (e.g., cooking different recipes, making disparate home\nimprovements, creating various forms of arts and crafts) from long videos\nspanning up to several minutes. Accurately categorizing these activities\nrequires not only recognizing the individual steps that compose the task but\nalso capturing their temporal dependencies. This problem is dramatically\ndifferent from traditional action classification, where models are typically\noptimized on videos that span only a few seconds and that are manually trimmed\nto contain simple atomic actions. While step annotations could enable the\ntraining of models to recognize the individual steps of procedural activities,\nexisting large-scale datasets in this area do not include such segment labels\ndue to the prohibitive cost of manually annotating temporal boundaries in long\nvideos. To address this issue, we propose to automatically identify steps in\ninstructional videos by leveraging the distant supervision of a textual\nknowledge base (wikiHow) that includes detailed descriptions of the steps\nneeded for the execution of a wide variety of complex activities. Our method\nuses a language model to match noisy, automatically-transcribed speech from the\nvideo to step descriptions in the knowledge base. We demonstrate that video\nmodels trained to recognize these automatically-labeled steps (without manual\nsupervision) yield a representation that achieves superior generalization\nperformance on four downstream tasks: recognition of procedural activities,\nstep classification, step forecasting and egocentric video classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registration-segmentation of longitudinal thoracic cone beam CTs. (arXiv:2201.11000v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11000","description":"<p>Image-guided adaptive lung radiotherapy requires accurate tumor and organs\nsegmentation from during treatment cone-beam CT (CBCT) images. Thoracic CBCTs\nare hard to segment because of low soft-tissue contrast, imaging artifacts,\nrespiratory motion, and large treatment induced intra-thoracic anatomic\nchanges. Hence, we developed a novel Patient-specific Anatomic Context and\nShape prior or PACS-aware 3D recurrent registration-segmentation network for\nlongitudinal thoracic CBCT segmentation. Segmentation and registration networks\nwere concurrently trained in an end-to-end framework and implemented with\nconvolutional long-short term memory models. The registration network was\ntrained in an unsupervised manner using pairs of planning CT (pCT) and CBCT\nimages and produced a progressively deformed sequence of images. The\nsegmentation network was optimized in a one-shot setting by combining\nprogressively deformed pCT (anatomic context) and pCT delineations (shape\ncontext) with CBCT images. Our method, one-shot PACS was significantly more\naccurate (p$&lt;$0.001) for tumor (DSC of 0.83 $\\pm$ 0.08, surface DSC [sDSC] of\n0.97 $\\pm$ 0.06, and Hausdorff distance at $95^{th}$ percentile [HD95] of\n3.97$\\pm$3.02mm) and the esophagus (DSC of 0.78 $\\pm$ 0.13, sDSC of\n0.90$\\pm$0.14, HD95 of 3.22$\\pm$2.02) segmentation than multiple methods.\nAblation tests and comparative experiments were also done.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures. (arXiv:2201.11002v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11002","description":"<p>Epilepsy is the fourth most common neurological disorder and affects people\nof all ages worldwide. Deep Brain Stimulation (DBS) has emerged as an\nalternative treatment option when anti-epileptic drugs or resective surgery\ncannot lead to satisfactory outcomes. To facilitate the planning of the\nprocedure and for its standardization, it is desirable to develop an algorithm\nto automatically localize the DBS stimulation target, i.e., Anterior Nucleus of\nThalamus (ANT), which is a challenging target to plan. In this work, we perform\nan extensive comparative study by benchmarking various localization methods for\nANT-DBS. Specifically, the methods involved in this study include traditional\nregistration method and deep-learning-based methods including heatmap matching\nand differentiable spatial to numerical transform (DSNT). Our experimental\nresults show that the deep-learning (DL)-based localization methods that are\ntrained with pseudo labels can achieve a performance that is comparable to the\ninter-rater and intra-rater variability and that they are orders of magnitude\nfaster than traditional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holloway_K/0/1/0/all/0/1\">Kathryn L. Holloway</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Englot_D/0/1/0/all/0/1\">Dario J. Englot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M. Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications. (arXiv:2201.11006v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11006","description":"<p>This article presents an overview of image transformation with a secret key\nand its applications. Image transformation with a secret key enables us not\nonly to protect visual information on plain images but also to embed unique\nfeatures controlled with a key into images. In addition, numerous encryption\nmethods can generate encrypted images that are compressible and learnable for\nmachine learning. Various applications of such transformation have been\ndeveloped by using these properties. In this paper, we focus on a class of\nimage transformation referred to as learnable image encryption, which is\napplicable to privacy-preserving machine learning and adversarially robust\ndefense. Detailed descriptions of both transformation algorithms and\nperformances are provided. Moreover, we discuss robustness against various\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MaungMaung_A/0/1/0/all/0/1\">AprilPyone MaungMaung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoko_I/0/1/0/all/0/1\">Imaizumi Shoko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1\">Sayaka Shiota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11014","description":"<p>Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interferences similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1\">Masataka Sawayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphe_M/0/1/0/all/0/1\">Maxime Adolphe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation. (arXiv:2201.11037v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11037","description":"<p>Automatic diabetic retinopathy (DR) lesions segmentation makes great sense of\nassisting ophthalmologists in diagnosis. Although many researches have been\nconducted on this task, most prior works paid too much attention to the designs\nof networks instead of considering the pathological association for lesions.\nThrough investigating the pathogenic causes of DR lesions in advance, we found\nthat certain lesions are closed to specific vessels and present relative\npatterns to each other. Motivated by the observation, we propose a relation\ntransformer block (RTB) to incorporate attention mechanisms at two main levels:\na self-attention transformer exploits global dependencies among lesion\nfeatures, while a cross-attention transformer allows interactions between\nlesion and vessel features by integrating valuable vascular information to\nalleviate ambiguity in lesion detection caused by complex fundus structures. In\naddition, to capture the small lesion patterns first, we propose a global\ntransformer block (GTB) which preserves detailed information in deep network.\nBy integrating the above blocks of dual-branches, our network segments the four\nkinds of lesions simultaneously. Comprehensive experiments on IDRiD and DDR\ndatasets well demonstrate the superiority of our approach, which achieves\ncompetitive performance compared to state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Shiqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuze Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_N/0/1/0/all/0/1\">Ning Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tingfa Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Capsule Networks. (arXiv:2201.11091v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11091","description":"<p>Capsule networks are a class of neural networks that achieved promising\nresults on many computer vision tasks. However, baseline capsule networks have\nfailed to reach state-of-the-art results on more complex datasets due to the\nhigh computation and memory requirements. We tackle this problem by proposing a\nnew network architecture, called Momentum Capsule Network (MoCapsNet).\nMoCapsNets are inspired by Momentum ResNets, a type of network that applies\nreversible residual building blocks. Reversible networks allow for\nrecalculating activations of the forward pass in the backpropagation algorithm,\nso those memory requirements can be drastically reduced. In this paper, we\nprovide a framework on how invertible residual building blocks can be applied\nto capsule networks. We will show that MoCapsNet beats the accuracy of baseline\ncapsule networks on MNIST, SVHN and CIFAR-10 while using considerably less\nmemory. The source code is available on https://github.com/moejoe95/MoCapsNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gugglberger_J/0/1/0/all/0/1\">Josef Gugglberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peer_D/0/1/0/all/0/1\">David Peer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Sanchez_A/0/1/0/all/0/1\">Antonio Rodr&#xed;guez-S&#xe1;nchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attention Neural Bag-of-Features. (arXiv:2201.11092v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11092","description":"<p>In this work, we propose several attention formulations for multivariate\nsequence data. We build on top of the recently introduced 2D-Attention and\nreformulate the attention learning methodology by quantifying the relevance of\nfeature/temporal dimensions through latent spaces based on self-attention\nrather than learning them directly. In addition, we propose a joint\nfeature-temporal attention mechanism that learns a joint 2D attention mask\nhighlighting relevant information without treating feature and temporal\nrepresentations independently. The proposed approaches can be used in various\narchitectures and we specifically evaluate their application together with\nNeural Bag of Features feature extraction module. Experiments on several\nsequence data analysis tasks show the improved performance yielded by our\napproach compared to standard methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chumachenko_K/0/1/0/all/0/1\">Kateryna Chumachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-attention fusion for audiovisual emotion recognition with incomplete data. (arXiv:2201.11095v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11095","description":"<p>In this paper, we consider the problem of multimodal data analysis with a use\ncase of audiovisual emotion recognition. We propose an architecture capable of\nlearning from raw data and describe three variants of it with distinct modality\nfusion mechanisms. While most of the previous works consider the ideal scenario\nof presence of both modalities at all times during inference, we evaluate the\nrobustness of the model in the unconstrained settings where one modality is\nabsent or noisy, and propose a method to mitigate these limitations in a form\nof modality dropout. Most importantly, we find that following this approach not\nonly improves performance drastically under the absence/noisy representations\nof one modality, but also improves the performance in a standard ideal setting,\noutperforming the competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chumachenko_K/0/1/0/all/0/1\">Kateryna Chumachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Instance Distillation for Object Detection in Autonomous Driving. (arXiv:2201.11097v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11097","description":"<p>In recent years, knowledge distillation (KD) has been widely used as an\neffective way to derive efficient models. Through imitating a large teacher\nmodel, a lightweight student model can achieve comparable performance with more\nefficiency. However, most existing knowledge distillation methods are focused\non classification tasks. Only a limited number of studies have applied\nknowledge distillation to object detection, especially in time-sensitive\nautonomous driving scenarios. We propose the Adaptive Instance Distillation\n(AID) method to selectively impart knowledge from the teacher to the student\nfor improving the performance of knowledge distillation. Unlike previous KD\nmethods that treat all instances equally, our AID can attentively adjust the\ndistillation weights of instances based on the teacher model's prediction loss.\nWe verified the effectiveness of our AID method through experiments on the\nKITTI and the COCO traffic datasets. The results show that our method improves\nthe performance of existing state-of-the-art attention-guided and non-local\ndistillation methods and achieves better distillation results on both\nsingle-stage and two-stage detectors. Compared to the baseline, our AID led to\nan average of 2.7% and 2.05% mAP increases for single-stage and two-stage\ndetectors, respectively. Furthermore, our AID is also shown to be useful for\nself-distillation to improve the teacher model's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1\">Qizhen Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qing Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Compressing Subset Pruning for Semantic Image Segmentation. (arXiv:2201.11103v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11103","description":"<p>State-of-the-art semantic segmentation models are characterized by high\nparameter counts and slow inference times, making them unsuitable for\ndeployment in resource-constrained environments. To address this challenge, we\npropose \\textsc{Auto-Compressing Subset Pruning}, \\acosp, as a new online\ncompression method. The core of \\acosp consists of learning a channel selection\nmechanism for individual channels of each convolution in the segmentation model\nbased on an effective temperature annealing schedule. We show a crucial\ninterplay between providing a high-capacity model at the beginning of training\nand the compression pressure forcing the model to compress concepts into\nretained channels. We apply \\acosp to \\segnet and \\pspnet architectures and\nshow its success when trained on the \\camvid, \\city, \\voc, and \\ade datasets.\nThe results are competitive with existing baselines for compression of\nsegmentation models at low compression ratios and outperform them significantly\nat high compression ratios, yielding acceptable results even when removing more\nthan $93\\%$ of the parameters. In addition, \\acosp is conceptually simple, easy\nto implement, and can readily be generalized to other data modalities, tasks,\nand architectures. Our code is available at\n\\url{https://github.com/merantix/acosp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ditschuneit_K/0/1/0/all/0/1\">Konstantin Ditschuneit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes S. Otterbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11114","description":"<p>Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to protected categories like race and gender in models\ntrained on datasets intended to obscure these features. Finally, we use MILAN\nfor editing, improving robustness in an image classifier by deleting neurons\nsensitive to text features spuriously correlated with class labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagashvili_T/0/1/0/all/0/1\">Teona Bagashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating a Fusion Image: One's Identity and Another's Shape. (arXiv:1804.07455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.07455","description":"<p>Generating a novel image by manipulating two input images is an interesting\nresearch problem in the study of generative adversarial networks (GANs). We\npropose a new GAN-based network that generates a fusion image with the identity\nof input image x and the shape of input image y. Our network can simultaneously\ntrain on more than two image datasets in an unsupervised manner. We define an\nidentity loss LI to catch the identity of image x and a shape loss LS to get\nthe shape of y. In addition, we propose a novel training method called\nMin-Patch training to focus the generator on crucial parts of an image, rather\nthan its entirety. We show qualitative results on the VGG Youtube Pose dataset,\nEye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome. (arXiv:1812.10085v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.10085","description":"<p>Adversarial examples pose many security threats to convolutional neural\nnetworks (CNNs). Most defense algorithms prevent these threats by finding\ndifferences between the original images and adversarial examples. However, the\nfound differences do not contain features about the classes, so these defense\nalgorithms can only detect adversarial examples without recovering the correct\nlabels. In this regard, we propose the Adversarial Feature Genome (AFG), a\nnovel type of data that contains both the differences and features about\nclasses. This method is inspired by an observed phenomenon, namely the\nAdversarial Feature Separability (AFS), where the difference between the\nfeature maps of the original images and adversarial examples becomes larger\nwith deeper layers. On top of that, we further develop an adversarial example\nrecognition framework that detects adversarial examples and can recover the\ncorrect labels. In the experiments, the detection and classification of\nadversarial examples by AFGs has an accuracy of more than 90.01\\% in various\nattack scenarios. To the best of our knowledge, our method is the first method\nthat focuses on both attack detecting and recovering. AFG gives a new\ndata-driven perspective to improve the robustness of CNNs. The source code is\navailable at https://github.com/GeoX-Lab/Adv_Fea_Genome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network for Video Relocalization. (arXiv:2007.09877v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.09877","description":"<p>In this paper, we focus on video relocalization task, which uses a query\nvideo clip as input to retrieve a semantic relative video clip in another\nuntrimmed long video. we find that in video relocalization datasets, there\nexists a phenomenon showing that there does not exist consistent relationship\nbetween feature similarity by frame and feature similarity by video, which\naffects the feature fusion among frames. However, existing video relocalization\nmethods do not fully consider it. Taking this phenomenon into account, in this\narticle, we treat video features as a graph by concatenating the query video\nfeature and proposal video feature along time dimension, where each timestep is\ntreated as a node, each row of the feature matrix is treated as feature of each\nnode. Then, with the power of graph neural networks, we propose a Multi-Graph\nFeature Fusion Module to fuse the relation feature of this graph. After\nevaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find\nthat our proposed method outperforms the state of art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_S/0/1/0/all/0/1\">Shuwei Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.12750","description":"<p>Ultrafast ultrasound (US) revolutionized biomedical imaging with its\ncapability of acquiring full-view frames at over 1 kHz, unlocking breakthrough\nmodalities such as shear-wave elastography and functional US neuroimaging. Yet,\nit suffers from strong diffraction artifacts, mainly caused by grating lobes,\nside lobes, or edge waves. Multiple acquisitions are typically required to\nobtain a sufficient image quality, at the cost of a reduced frame rate. To\nanswer the increasing demand for high-quality imaging from single unfocused\nacquisitions, we propose a two-step convolutional neural network (CNN)-based\nimage reconstruction method, compatible with real-time imaging. A low-quality\nestimate is obtained by means of a backprojection-based operation, akin to\nconventional delay-and-sum beamforming, from which a high-quality image is\nrestored using a residual CNN with multi-scale and multi-channel filtering\nproperties, trained specifically to remove the diffraction artifacts inherent\nto ultrafast US imaging. To account for both the high dynamic range and the\noscillating properties of radio frequency US images, we introduce the mean\nsigned logarithmic absolute error (MSLAE) as training loss function.\nExperiments were conducted with a linear transducer array, in single plane wave\n(PW) imaging. Trainings were performed on a simulated dataset, crafted to\ncontain a wide diversity of structures and echogenicities. Extensive numerical\nevaluations demonstrate that the proposed approach can reconstruct images from\nsingle PWs with a quality similar to that of gold-standard synthetic aperture\nimaging, on a dynamic range in excess of 60 dB. In vitro and in vivo\nexperiments show that trainings carried out on simulated data perform well in\nexperimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Perdios_D/0/1/0/all/0/1\">Dimitris Perdios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vonlanthen_M/0/1/0/all/0/1\">Manuel Vonlanthen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_F/0/1/0/all/0/1\">Florian Martinez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arditi_M/0/1/0/all/0/1\">Marcel Arditi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles. (arXiv:2009.09560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09560","description":"<p>Deep neural networks (DNNs) have become the essential components for various\ncommercialized machine learning services, such as Machine Learning as a Service\n(MLaaS). Recent studies show that machine learning services face severe privacy\nthreats - well-trained DNNs owned by MLaaS providers can be stolen through\npublic APIs, namely model stealing attacks. However, most existing works\nundervalued the impact of such attacks, where a successful attack has to\nacquire confidential training data or auxiliary data regarding the victim DNN.\nIn this paper, we propose ES Attack, a novel model stealing attack without any\ndata hurdles. By using heuristically generated synthetic data, ES Attack\niteratively trains a substitute model and eventually achieves a functionally\nequivalent copy of the victim DNN. The experimental results reveal the severity\nof ES Attack: i) ES Attack successfully steals the victim model without data\nhurdles, and ES Attack even outperforms most existing model stealing attacks\nusing auxiliary data in terms of model accuracy; ii) most countermeasures are\nineffective in defending ES Attack; iii) ES Attack facilitates further attacks\nrelying on the stolen model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Leah Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dapeng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-World Semi-Supervised Learning. (arXiv:2102.03526v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.03526","description":"<p>A fundamental limitation of applying semi-supervised learning in real-world\nsettings is the assumption that unlabeled test data contains only classes\npreviously encountered in the labeled training data. However, this assumption\nrarely holds for data in-the-wild, where instances belonging to novel classes\nmay appear at testing time. Here, we introduce a novel open-world\nsemi-supervised learning setting that formalizes the notion that novel classes\nmay appear in the unlabeled test data. In this novel setting, the goal is to\nsolve the class distribution mismatch between labeled and unlabeled data, where\nat the test time every input instance either needs to be classified into one of\nthe existing classes or a new unseen class needs to be initialized. To tackle\nthis challenging problem, we propose ORCA, an end-to-end deep learning approach\nthat introduces uncertainty adaptive margin mechanism to circumvent the bias\ntowards seen classes caused by learning discriminative features for seen\nclasses faster than for the novel classes. In this way, ORCA reduces the gap\nbetween intra-class variance of seen with respect to novel classes. Experiments\non image classification datasets and a single-cell annotation dataset\ndemonstrate that ORCA consistently outperforms alternative baselines, achieving\n25% improvement on seen and 96% improvement on novel classes of the ImageNet\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kaidi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brbic_M/0/1/0/all/0/1\">Maria Brbic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Transformer for Accurate and Reliable Salient Object Detection. (arXiv:2104.10127v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10127","description":"<p>In this paper, we conduct extensive research on exploring the contribution of\ntransformers to salient object detection, achieving both accurate and reliable\nsaliency predictions. We first investigate transformers for accurate salient\nobject detection with deterministic neural networks, and explain that the\neffective structure modeling and global context modeling abilities lead to its\nsuperior performance compared with the CNN based frameworks. Then, we design\nstochastic networks to evaluate the transformers' ability in reliable salient\nobject detection. We observe that both CNN and transformer based frameworks\nsuffer greatly from the over-confidence issue, where the models tend to\ngenerate wrong predictions with high confidence, leading to over-confident\npredictions or a poorly-calibrated model. To estimate the calibration degree of\nboth CNN- and transformer-based frameworks for reliable saliency prediction, we\nintroduce generative adversarial network (GAN) based models to identify the\nover-confident regions by sampling from the latent space. Specifically, we\npresent the inferential generative adversarial network (iGAN). Different from\nthe conventional GAN based framework, which defines the distribution of the\nlatent variable as fixed standard normal distribution N(0,1), the proposed\n\"iGAN\" infers the latent variable by gradient-based Markov Chain Monte Carlo\n(MCMC), namely Langevin dynamics. We apply the proposed inferential generative\nadversarial network (iGAN) to both fully and weakly supervised salient object\ndetection, and explain that iGAN within the transformer framework leads to both\naccurate and reliable salient object detection. The source code and\nexperimental results are publicly available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhexiong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Ternary Quantization. (arXiv:2107.10998v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10998","description":"<p>Inference time, model size, and accuracy are three key factors in deep model\ncompression.\n</p>\n<p>Most of the existing work addresses these three key factors separately as it\nis difficult to optimize them all at the same time.\n</p>\n<p>For example, low-bit quantization aims at obtaining a faster model; weight\nsharing quantization aims at improving compression ratio and accuracy; and\nmixed-precision quantization aims at balancing accuracy and inference time. To\nsimultaneously optimize bit-width, model size, and accuracy, we propose pruning\nternary quantization (PTQ): a simple, effective, symmetric ternary quantization\nmethod. We integrate L2 normalization, pruning, and the weight decay term to\nreduce the weight discrepancy in the gradient estimator during quantization,\nthus producing highly compressed ternary weights. Our method brings the highest\ntest accuracy and the highest compression ratio. For example, it produces a\n939kb (49$\\times$) 2bit ternary ResNet-18 model with only 4\\% accuracy drop on\nthe ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\\times$) with\nonly 2.8\\% average precision drop. Our method is verified on image\nclassification, object detection/segmentation tasks with different network\nstructures such as ResNet-18, ResNet-50, and MobileNetV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01174","description":"<p>Image reconstruction for positron emission tomography (PET) is challenging\nbecause of the ill-conditioned tomographic problem and low counting statistics.\nKernel methods address this challenge by using kernel representation to\nincorporate image prior information in the forward model of iterative PET image\nreconstruction. Existing kernel methods construct the kernels commonly using an\nempirical process, which may lead to suboptimal performance. In this paper, we\ndescribe the equivalence between the kernel representation and a trainable\nneural network model. A deep kernel method is then proposed by exploiting a\ndeep neural network to enable automated learning of an optimized kernel model\nand is directly applicable to single subjects. The training process utilizes\navailable image prior data to seek the best way to form a set of robust kernels\noptimally rather than empirically. The results from computer simulations and a\nreal patient dataset demonstrate that the proposed deep kernel method can\noutperform the existing kernel method and neural network method for dynamic PET\nimage reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guobao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.02636","description":"<p>Diffusion-based inpainting is a powerful tool for the reconstruction of\nimages from sparse data. Its quality strongly depends on the choice of known\ndata. Optimising their spatial location -- the inpainting mask -- is\nchallenging. A commonly used tool for this task are stochastic optimisation\nstrategies. However, they are slow as they compute multiple inpainting results.\nWe provide a remedy in terms of a learned mask generation model. By emulating\nthe complete inpainting pipeline with two networks for mask generation and\nneural surrogate inpainting, we obtain a model for highly efficient adaptive\nmask generation. Experiments indicate that our model can achieve competitive\nquality with an acceleration by as much as four orders of magnitude. Our\nfindings serve as a basis for making diffusion-based inpainting more attractive\nfor applications such as image compression, where fast encoding is highly\ndesirable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06178","description":"<p>Spatial convolutions are widely used in numerous deep video models. It\nfundamentally assumes spatio-temporal invariance, i.e., using shared weights\nfor every location in different frames. This work presents Temporally-Adaptive\nConvolutions (TAdaConv) for video understanding, which shows that adaptive\nweight calibration along the temporal dimension is an efficient way to\nfacilitate modelling complex temporal dynamics in videos. Specifically,\nTAdaConv empowers the spatial convolutions with temporal modelling abilities by\ncalibrating the convolution weights for each frame according to its local and\nglobal temporal context. Compared to previous temporal modelling operations,\nTAdaConv is more efficient as it operates over the convolution kernels instead\nof the features, whose dimension is an order of magnitude smaller than the\nspatial resolutions. Further, the kernel calibration brings an increased model\ncapacity. We construct TAda2D networks by replacing the 2D convolutions in\nResNet with TAdaConv, which leads to at least on par or better performance\ncompared to state-of-the-art approaches on multiple video action recognition\nand localization benchmarks. We also demonstrate that as a readily plug-in\noperation with negligible computation overhead, TAdaConv can effectively\nimprove many existing video models with a convincing margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations. (arXiv:2111.00131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00131","description":"<p>The training data distribution is often biased towards objects in certain\norientations and illumination conditions. While humans have a remarkable\ncapability of recognizing objects in out-of-distribution (OoD) orientations and\nilluminations, Deep Neural Networks (DNNs) severely suffer in this case, even\nwhen large amounts of training examples are available. In this paper, we\ninvestigate three different approaches to improve DNNs in recognizing objects\nin OoD orientations and illuminations. Namely, these are (i) training much\nlonger after convergence of the in-distribution (InD) validation accuracy,\ni.e., late-stopping, (ii) tuning the momentum parameter of the batch\nnormalization layers, and (iii) enforcing invariance of the neural activity in\nan intermediate layer to orientation and illumination conditions. Each of these\napproaches substantially improves the DNN's OoD accuracy (more than 20% in some\ncases). We report results in four datasets: two datasets are modified from the\nMNIST and iLab datasets, and the other two are novel (one of 3D rendered cars\nand another of objects taken from various controlled orientations and\nillumination conditions). These datasets allow to study the effects of\ndifferent amounts of bias and are challenging as DNNs perform poorly in OoD\nconditions. Finally, we demonstrate that even though the three approaches focus\non different aspects of DNNs, they all tend to lead to the same underlying\nneural mechanism to enable OoD accuracy gains --individual neurons in the\nintermediate layers become more selective to a category and also invariant to\nOoD orientations and illuminations. We anticipate this study to be a basis for\nfurther improvement of deep neural networks' OoD generalization performance,\nwhich is highly demanded to achieve safe and fair AI applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakai_A/0/1/0/all/0/1\">Akira Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunagawa_T/0/1/0/all/0/1\">Taro Sunagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kanata Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katoh_T/0/1/0/all/0/1\">Takashi Katoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobashi_H/0/1/0/all/0/1\">Hiromichi Kobashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1\">Pawan Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images. (arXiv:2112.07879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07879","description":"<p>Contactless and efficient systems are implemented rapidly to advocate\npreventive methods in the fight against the COVID-19 pandemic. Despite the\npositive benefits of such systems, there is potential for exploitation by\ninvading user privacy. In this work, we analyse the privacy invasiveness of\nface biometric systems by predicting privacy-sensitive soft-biometrics using\nmasked face images. We train and apply a CNN based on the ResNet-50\narchitecture with 20,003 synthetic masked images and measure the privacy\ninvasiveness. Despite the popular belief of the privacy benefits of wearing a\nmask among people, we show that there is no significant difference to privacy\ninvasiveness when a mask is worn. In our experiments we were able to accurately\npredict sex (94.7%),race (83.1%) and age (MAE 6.21 and RMSE 8.33) from masked\nface images. Our proposed approach can serve as a baseline utility to evaluate\nthe privacy-invasiveness of artificial intelligence systems that make use of\nprivacy-sensitive information. We open-source all contributions for\nre-producibility and broader use by the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sachith Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasthuriarachchi_N/0/1/0/all/0/1\">Nuran Kasthuriarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1\">Sanka Rasnayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hettiachchi_D/0/1/0/all/0/1\">Danula Hettiachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shariffdeen_R/0/1/0/all/0/1\">Ridwan Shariffdeen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a novel transformer-based approach for interactive\nimage segmentation. iSegFormer is built upon existing segmentation transformers\nwith user clicks as an additional input, allowing users to interactively and\niteratively refine the segmentation mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03891","description":"<p>Among the different modalities to assess emotion, electroencephalogram (EEG),\nrepresenting the electrical brain activity, achieved motivating results over\nthe last decade. Emotion estimation from EEG could help in the diagnosis or\nrehabilitation of certain diseases. In this paper, we propose a dual model\nconsidering two different representations of EEG feature maps: 1) a sequential\nbased representation of EEG band power, 2) an image-based representation of the\nfeature vectors. We also propose an innovative method to combine the\ninformation based on a saliency analysis of the image-based model to promote\njoint learning of both model parts. The model has been evaluated on four\npublicly available datasets and achieves similar results to the\nstate-of-the-art approaches. It outperforms results for two of the proposed\ndatasets with a lower standard deviation that reflects higher stability. For\nsake of reproducibility, the codes and models proposed in this paper are\navailable at https://github.com/VDelv/Emotion-EEG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delvigne_V/0/1/0/all/0/1\">Victor Delvigne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facchini_A/0/1/0/all/0/1\">Antoine Facchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wannous_H/0/1/0/all/0/1\">Hazem Wannous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ris_L/0/1/0/all/0/1\">Laurence Ris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandeborre_J/0/1/0/all/0/1\">Jean-Philippe Vandeborre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Proposals Refinement for 3D Object Detection. (arXiv:2201.07070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07070","description":"<p>Recent advances in 3D object detection is made by developing the refinement\nstage for voxel-based Region Proposal Networks (RPN) to better strike the\nbalance between accuracy and efficiency. A popular approach among\nstate-of-the-art frameworks is to divide proposals, or Regions of Interest\n(ROI), into grids and extract feature for each grid location before\nsynthesizing them to form ROI feature. While achieving impressive performances,\nsuch an approach involves a number of hand crafted components (e.g. grid\nsampling, set abstraction) which requires expert knowledge to be tuned\ncorrectly. This paper proposes a data-driven approach to ROI feature computing\nnamed APRO3D-Net which consists of a voxel-based RPN and a refinement stage\nmade of Vector Attention. Unlike the original multi-head attention, Vector\nAttention assigns different weights to different channels within a point\nfeature, thus being able to capture a more sophisticated relation between\npooled points and ROI. Experiments on KITTI \\textit{validation} set show that\nour method achieves competitive performance of 84.84 AP for class Car at\nModerate difficulty while having the least parameters compared to closely\nrelated methods and attaining a quasi-real time inference speed at 15 FPS on\nNVIDIA V100 GPU. The code is released in\nhttps://github.com/quan-dao/APRO3D-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1\">Minh-Quan Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hery_E/0/1/0/all/0/1\">Elwan H&#xe9;ry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fremont_V/0/1/0/all/0/1\">Vincent Fr&#xe9;mont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.08266","description":"<p>A real-time elemental image array (EIA) generation method which does not\nsacrifice accuracy nor rely on high-performance hardware is developed, through\nraytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both\noffline and online working flow, experiments verified the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Quanzhen Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08582","description":"<p>Current research on deep learning for medical image segmentation exposes\ntheir limitations in learning either global semantic information or local\ncontextual information. To tackle these issues, a novel network named\nSegTransVAE is proposed in this paper. SegTransVAE is built upon\nencoder-decoder architecture, exploiting transformer with the variational\nautoencoder (VAE) branch to the network to reconstruct the input images jointly\nwith segmentation. To the best of our knowledge, this is the first method\ncombining the success of CNN, transformer, and VAE. Evaluation on various\nrecently introduced datasets shows that SegTransVAE outperforms previous\nmethods in Dice Score and $95\\%$-Haudorff Distance while having comparable\ninference time to a simple CNN-based architecture network. The source code is\navailable at: https://github.com/itruonghai/SegTransVAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pham_Q/0/1/0/all/0/1\">Quan-Dung Pham</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_Truong_H/0/1/0/all/0/1\">Hai Nguyen-Truong</a> (1, 2 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Phuong_N/0/1/0/all/0/1\">Nam Nguyen Phuong</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoa N. A. Nguyen</a> (1, 2 and 3) ((1) VinBrain JSC., Vietnam, (2) University of Science, Ho Chi Minh City, Vietnam, (3) Vietnam National University, Ho Chi Minh City, Vietnam)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement. (arXiv:2201.10294v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.10294","description":"<p>Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps\nin different energy channels, reflecting energy properties of the scanned\nobject. Due to the limited photon numbers and the non-ideal detector response\nof each energy channel, the reconstructed images usually contain much noise.\nWith the development of Deep Learning (DL) technique, different kinds of\nDL-based models have been proposed for noise reduction. However, most of the\nmodels require clean data set as the training labels, which are not always\navailable in medical imaging field. Inspiring by the similarities of each\nchannel's reconstructed image, we proposed a self-supervised learning based\nPCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS\nframework, both the input and output labels are noisy images. Specifically, one\nsingle channel image was used as output while images of other single channels\nand channel-sum image were used as input to train the network, which can fully\nuse the spectral data information without extra cost. The simulation results\nbased on the AAPM Low-dose CT Challenge database showed that the proposed S2MS\nmodel can suppress the noise and preserve details more effectively in\ncomparison with the traditional DL models, which has potential to improve the\nimage quality of PCCT in clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shaojie Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.10424","description":"<p>The presence of plaques in the coronary arteries are a major risk to the\npatients' life. In particular, non-calcified plaques pose a great challenge, as\nthey are harder to detect and more likely to rupture than calcified plaques.\nWhile current deep learning techniques allow precise segmentation of regular\nimages, the performance in medical images is still low, caused mostly by\nblurriness and ambiguous voxel intensities of unrelated parts that fall on the\nsame range. In this paper, we propose a novel methodology for segmenting\ncalcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The\ninput slices are masked so only the voxels within the wall vessel are\nconsidered for segmentation. We also provide an exhaustive evaluation by\napplying different types of masks, in order to validate the potential of vessel\nmasking for plaque segmentation. Our methodology results in a prominent boost\nin segmentation performance, in both quantitative and qualitative evaluation,\nachieving accurate plaque shapes even for the challenging non-calcified\nplaques. We believe our findings can lead the future research for\nhigh-performance plaque segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1\">Antonio Tejero-de-Pablos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamane_H/0/1/0/all/0/1\">Hiroaki Yamane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurose_Y/0/1/0/all/0/1\">Yusuke Kurose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iho_J/0/1/0/all/0/1\">Junichi Iho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokunaga_Y/0/1/0/all/0/1\">Youji Tokunaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horie_M/0/1/0/all/0/1\">Makoto Horie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishizawa_K/0/1/0/all/0/1\">Keisuke Nishizawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yusaku Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koyama_Y/0/1/0/all/0/1\">Yasushi Koyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}