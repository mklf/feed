{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition. (arXiv:2110.02220v1 [eess.AS])","link":"http://arxiv.org/abs/2110.02220","description":"<p>Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Fan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chua_M/0/1/0/all/0/1\">Mason Chua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR. (arXiv:2110.02267v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02267","description":"<p>We study the inclusion of past conversational context through BERT language\nmodels into a CTC-based Automatic Speech Recognition (ASR) system via N-best\nrescoring. We introduce a data-efficient strategy to fine-tune BERT on\ntranscript disambiguation without external data. Our results show word error\nrate recoveries up to 37.2% with context-augmented BERT rescoring. We do this\nin low-resource data domains, both in language (Norwegian), tone (spontaneous,\nconversational), and topics (parliament proceedings and customer service phone\ncalls). We show how the nature of the data greatly affects the performance of\ncontext-augmented N-best rescoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_P/0/1/0/all/0/1\">Pablo Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burud_S/0/1/0/all/0/1\">Simen Burud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02283","description":"<p>We introduce a method for unsupervised parsing that relies on bootstrapping\nclassifiers to identify if a node dominates a specific span in a sentence.\nThere are two types of classifiers, an inside classifier that acts on a span,\nand an outside classifier that acts on everything outside of a given span.\nThrough self-training and co-training with the two classifiers, we show that\nthe interplay between them helps improve the accuracy of both, and as a result,\neffectively parse. A seed bootstrapping technique prepares the data to train\nthese classifiers. Our analyses further validate that such an approach in\nconjunction with weak supervision using prior branching knowledge of a known\nlanguage (left/right-branching) and minimal heuristics injects strong inductive\nbias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In\naddition, we show the effectiveness of our architecture by evaluating on\ntreebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art\nresults.\\footnote{For code or data, please contact the authors.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maveli_N/0/1/0/all/0/1\">Nickil Maveli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 India Dataset: Parsing Detailed COVID-19 Data in Daily Health Bulletins from States in India. (arXiv:2110.02311v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02311","description":"<p>While India remains one of the hotspots of the COVID-19 pandemic, data about\nthe pandemic from the country has proved to be largely inaccessible for use at\nscale. Much of the data exists in an unstructured form on the web, and limited\naspects of such data are available through public APIs maintained manually\nthrough volunteer efforts. This has proved to be difficult both in terms of\nease of access to detailed data as well as with regards to the maintenance of\nmanual data-keeping over time. This paper reports on a recently launched\nproject aimed at automating the extraction of such data from public health\nbulletins with the help of a combination of classical PDF parsers as well as\nstate-of-the-art ML-based documents extraction APIs. In this paper, we will\ndescribe the automated data-extraction technique, the nature of the generated\ndata, and exciting avenues of ongoing work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mayank Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tathagata Chakraborti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_S/0/1/0/all/0/1\">Sachin Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis. (arXiv:2110.02334v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02334","description":"<p>Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing\nuser-generated reviews to determine (i) the target being evaluated, (ii) the\naspect category to which it belongs, and (iii) the sentiment expressed towards\nthe target and aspect pair. In this article, we propose transforming ABSA into\nan abstract summary-like conditional text generation task that uses targets,\naspects, and polarities to generate auxiliary statements. To demonstrate the\nefficacy of our task formulation and a proposed system, we fine-tune a\npre-trained model for conditional text generation tasks to get new\nstate-of-the-art results on a few restaurant domains and urban neighborhoods\ndomain benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntQA: Entity Linking as Question Answering. (arXiv:2110.02369v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02369","description":"<p>A conventional approach to entity linking is to first find mentions in a\ngiven document and then infer their underlying entities in the knowledge base.\nA well-known limitation of this approach is that it requires finding mentions\nwithout knowing their entities, which is unnatural and difficult. We present a\nnew model that does not suffer from this limitation called EntQA, which stands\nfor Entity linking as Question Answering. EntQA first proposes candidate\nentities with a fast retrieval module, and then scrutinizes the document to\nfind mentions of each candidate with a powerful reader module. Our approach\ncombines progress in entity linking with that in open-domain question answering\nand capitalizes on pretrained models for dense entity retrieval and reading\ncomprehension. Unlike in previous works, we do not rely on a mention-candidates\ndictionary or large-scale weak supervision. EntQA achieves strong results on\nthe GERBIL benchmarking platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning. (arXiv:2110.02370v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02370","description":"<p>Large natural language models (such as GPT-3 or T5) demonstrate impressive\nabilities across a range of general NLP tasks. Here, we show that the knowledge\nembedded in such models provides a useful inductive bias, not just on\ntraditional NLP tasks, but also in the nontraditional task of training a\nsymbolic reasoning engine. We observe that these engines learn quickly and\ngeneralize in a natural way that reflects human intuition. For example,\ntraining such a system to model block-stacking might naturally generalize to\nstacking other types of objects because of structure in the real world that has\nbeen partially captured by the language describing it. We study several\nabstract textual reasoning tasks, such as object manipulation and navigation,\nand demonstrate multiple types of generalization to novel scenarios and the\nsymbols that comprise them. We also demonstrate the surprising utility of\n\\textit{compositional learning}, where a learner dedicated to mastering a\ncomplicated task gains an advantage by training on relevant simpler tasks\ninstead of jumping straight to the complicated task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting intermediate convolutional layers in unsupervised acoustic word classification. (arXiv:2110.02375v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02375","description":"<p>Understanding how deep convolutional neural networks classify data has been\nsubject to extensive research. This paper proposes a technique to visualize and\ninterpret intermediate layers of unsupervised deep convolutional neural\nnetworks by averaging over individual feature maps in each convolutional layer\nand inferring underlying distributions of words with non-linear regression\ntechniques. A GAN-based architecture (ciwGAN <a href=\"/abs/2006.02951\">arXiv:2006.02951</a>) that includes\nthree convolutional networks (a Generator, a Discriminator, and a classifier)\nwas trained on unlabeled sliced lexical items from TIMIT. The training results\nin a deep convolutional network that learns to classify words into discrete\nclasses only from the requirement of the Generator to output informative data.\nThe classifier network has no access to the training data -- only to the\ngenerated data -- which means lexical learning needs to emerge in a fully\nunsupervised manner. We propose a technique to visualize individual\nconvolutional layers in the classifier that yields highly informative\ntime-series data for each convolutional layer and apply it to unobserved test\ndata. Using non-linear regression, we infer underlying distributions for each\nword which allows us to analyze both absolute values and shapes of individual\nwords at different convolutional layers as well as perform hypothesis testing\non their acoustic properties. The technique also allows us to tests individual\nphone contrasts and how they are represented at each layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance. (arXiv:2110.02386v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02386","description":"<p>Multilingual language models achieve impressive zero-shot accuracies in many\nlanguages in complex tasks such as Natural Language Inference (NLI). Examples\nin NLI (and equivalent complex tasks) often pertain to various types of\nsub-tasks, requiring different kinds of reasoning. Certain types of reasoning\nhave proven to be more difficult to learn in a monolingual context, and in the\ncrosslingual context, similar observations may shed light on zero-shot transfer\nefficiency and few-shot sample selection. Hence, to investigate the effects of\ntypes of reasoning on transfer performance, we propose a category-annotated\nmultilingual NLI dataset and discuss the challenges to scale monolingual\nannotations to multiple languages. We statistically observe interesting effects\nthat the confluence of reasoning types and language similarities have on\ntransfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthikeyan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathe_A/0/1/0/all/0/1\">Aalok Sathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modeling using LMUs: 10x Better Data Efficiency or Improved Scaling Compared to Transformers. (arXiv:2110.02402v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02402","description":"<p>Recent studies have demonstrated that the performance of transformers on the\ntask of language modeling obeys a power-law relationship with model size over\nsix orders of magnitude. While transformers exhibit impressive scaling, their\nperformance hinges on processing large amounts of data, and their computational\nand memory requirements grow quadratically with sequence length. Motivated by\nthese considerations, we construct a Legendre Memory Unit based model that\nintroduces a general prior for sequence processing and exhibits an $O(n)$ and\n$O(n \\ln n)$ (or better) dependency for memory and computation respectively.\nOver three orders of magnitude, we show that our new architecture attains the\nsame accuracy as transformers with 10x fewer tokens. We also show that for the\nsame amount of training our model improves the loss over transformers about as\nmuch as transformers improve over LSTMs. Additionally, we demonstrate that\nadding global self-attention complements our architecture and the augmented\nmodel improves performance even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chilkuri_N/0/1/0/all/0/1\">Narsimha Chilkuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunsberger_E/0/1/0/all/0/1\">Eric Hunsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voelker_A/0/1/0/all/0/1\">Aaron Voelker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Gurshaant Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliasmith_C/0/1/0/all/0/1\">Chris Eliasmith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Acquisition in Neural Language Models. (arXiv:2110.02406v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02406","description":"<p>We investigate how neural language models acquire individual words during\ntraining, extracting learning curves and ages of acquisition for over 600 words\non the MacArthur-Bates Communicative Development Inventory (Fenson et al.,\n2007). Drawing on studies of word acquisition in children, we evaluate multiple\npredictors for words' ages of acquisition in LSTMs, BERT, and GPT-2. We find\nthat the effects of concreteness, word length, and lexical class are pointedly\ndifferent in children and language models, reinforcing the importance of\ninteraction and sensorimotor experience in child language acquisition. Language\nmodels rely far more on word frequency than children, but like children, they\nexhibit slower learning of words in longer utterances. Interestingly, models\nfollow consistent patterns during training for both unidirectional and\nbidirectional models, and for both LSTM and Transformer architectures. Models\npredict based on unigram token frequencies early in training, before\ntransitioning loosely to bigram probabilities, eventually converging on more\nnuanced predictions. These results shed light on the role of distributional\nlearning mechanisms in children, while also providing insights for more\nhuman-like language acquisition in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler A. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Aging with Audio-Visual Style Transfer. (arXiv:2110.02411v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02411","description":"<p>Face aging techniques have used generative adversarial networks (GANs) and\nstyle transfer learning to transform one's appearance to look younger/older.\nIdentity is maintained by conditioning these generative networks on a learned\nvector representation of the source content. In this work, we apply a similar\napproach to age a speaker's voice, referred to as voice aging. We first analyze\nthe classification of a speaker's age by training a convolutional neural\nnetwork (CNN) on the speaker's voice and face data from Common Voice and\nVoxCeleb datasets. We generate aged voices from style transfer to transform an\ninput spectrogram to various ages and demonstrate our method on a mobile app.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Justin Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunyeong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Seunghye J. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming C. Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Distillation of Natural Language Understanding with Confident Sinkhorns. (arXiv:2110.02432v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02432","description":"<p>Enhancing the user experience is an essential task for application service\nproviders. For instance, two users living wide apart may have different tastes\nof food. A food recommender mobile application installed on an edge device\nmight want to learn from user feedback (reviews) to satisfy the client's needs\npertaining to distinct domains. Retrieving user data comes at the cost of\nprivacy while asking for model parameters trained on a user device becomes\nspace inefficient at a large scale. In this work, we propose an approach to\nlearn a central (global) model from the federation of (local) models which are\ntrained on user-devices, without disclosing the local data or model parameters\nto the server. We propose a federation mechanism for the problems with natural\nsimilarity metric between the labels which commonly appear in natural language\nunderstanding (NLU) tasks. To learn the global model, the objective is to\nminimize the optimal transport cost of the global model's predictions from the\nconfident sum of soft-targets assigned by local models. The confidence (a model\nweighting scheme) score of a model is defined as the L2 distance of a model's\nprediction from its probability bias. The method improves the global model's\nperformance over the baseline designed on three NLU tasks with intrinsic label\nspace semantics, i.e., fine-grained sentiment analysis, emotion recognition in\nconversation, and natural language inference. We make our codes public at\nhttps://github.com/declare-lab/sinkhorn-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_T/0/1/0/all/0/1\">Tushar Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02442","description":"<p>Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 96.0% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models. (arXiv:2110.02467v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02467","description":"<p>Pre-trained Natural Language Processing (NLP) models can be easily adapted to\na variety of downstream language tasks. This significantly accelerates the\ndevelopment of language models. However, NLP models have been shown to be\nvulnerable to backdoor attacks, where a pre-defined trigger word in the input\ntext causes model misprediction. Previous NLP backdoor attacks mainly focus on\nsome specific tasks. This makes those attacks less general and applicable to\nother kinds of NLP models and tasks. In this work, we propose \\Name, the first\ntask-agnostic backdoor attack against the pre-trained NLP models. The key\nfeature of our attack is that the adversary does not need prior information\nabout the downstream tasks when implanting the backdoor to the pre-trained\nmodel. When this malicious model is released, any downstream models transferred\nfrom it will also inherit the backdoor, even after the extensive transfer\nlearning process. We further design a simple yet effective strategy to bypass a\nstate-of-the-art defense. Experimental results indicate that our approach can\ncompromise a wide range of downstream NLP tasks in an effective and stealthy\nway.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABC: Attention with Bounded-memory Control. (arXiv:2110.02488v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02488","description":"<p>Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier. (arXiv:2110.02523v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02523","description":"<p>Pre-trained models are widely used in fine-tuning downstream tasks with\nlinear classifiers optimized by the cross-entropy loss, which might face\nrobustness and stability problems. These problems can be improved by learning\nrepresentations that focus on similarities in the same class and contradictions\nin different classes when making predictions. In this paper, we utilize the\nK-Nearest Neighbors Classifier in pre-trained model fine-tuning. For this KNN\nclassifier, we introduce a supervised momentum contrastive learning framework\nto learn the clustered representations of the supervised downstream tasks.\nExtensive experiments on text classification tasks and robustness tests show\nthat by incorporating KNNs with the traditional fine-tuning process, we can\nobtain significant improvements on the clean accuracy in both rich-source and\nfew-shot settings and can improve the robustness against adversarial attacks.\n\\footnote{all codes is available at https://github.com/LinyangLee/KNN-BERT}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Demin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Modal Embeddings from Structured Data. (arXiv:2110.02577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02577","description":"<p>Multi-modal word semantics aims to enhance embeddings with perceptual input,\nassuming that human meaning representation is grounded in sensory experience.\nMost research focuses on evaluation involving direct visual input, however,\nvisual grounding can contribute to linguistic applications as well. Another\nmotivation for this paper is the growing need for more interpretable models and\nfor evaluating model efficiency regarding size and performance. This work\nexplores the impact of visual information for semantics when the evaluation\ninvolves no direct visual input, specifically semantic similarity and\nrelatedness. We investigate a new embedding type in-between linguistic and\nvisual modalities, based on the structured annotations of Visual Genome. We\ncompare uni- and multi-modal models including structured, linguistic and image\nbased representations. We measure the efficiency of each model with regard to\ndata and model size, modality / data distribution and information gain. The\nanalysis includes an interpretation of embedding structures. We found that this\nnew embedding conveys complementary information for text based embeddings. It\nachieves comparable performance in an economic way, using orders of magnitude\nless resources than visual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vero_A/0/1/0/all/0/1\">Anita L. Ver&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Text Classification Based on Keyword Graph. (arXiv:2110.02591v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02591","description":"<p>Weakly-supervised text classification has received much attention in recent\nyears for it can alleviate the heavy burden of annotating massive data. Among\nthem, keyword-driven methods are the mainstream where user-provided keywords\nare exploited to generate pseudo-labels for unlabeled texts. However, existing\nmethods treat keywords independently, thus ignore the correlation among them,\nwhich should be useful if properly exploited. In this paper, we propose a novel\nframework called ClassKG to explore keyword-keyword correlation on keyword\ngraph by GNN. Our framework is an iterative process. In each iteration, we\nfirst construct a keyword graph, so the task of assigning pseudo labels is\ntransformed to annotating keyword subgraphs. To improve the annotation quality,\nwe introduce a self-supervised task to pretrain a subgraph annotator, and then\nfinetune it. With the pseudo labels generated by the subgraph annotator, we\nthen train a text classifier to classify the unlabeled texts. Finally, we\nre-extract keywords from the classified texts. Extensive experiments on both\nlong-text and short-text datasets show that our method substantially\noutperforms the existing ones\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiandong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02600","description":"<p>Multilingual models jointly pretrained on multiple languages have achieved\nremarkable performance on various multilingual downstream tasks. Moreover,\nmodels finetuned on a single monolingual downstream task have shown to\ngeneralize to unseen languages. In this paper, we first show that it is crucial\nfor those tasks to align gradients between them in order to maximize knowledge\ntransfer while minimizing negative transfer. Despite its importance, the\nexisting methods for gradient alignment either have a completely different\npurpose, ignore inter-task alignment, or aim to solve continual learning\nproblems in rather inefficient ways. As a result of the misaligned gradients\nbetween tasks, the model suffers from severe negative transfer in the form of\ncatastrophic forgetting of the knowledge acquired from the pretraining. To\novercome the limitations, we propose a simple yet effective method that can\nefficiently align gradients between tasks. Specifically, we perform each\ninner-optimization by sequentially sampling batches from all the tasks,\nfollowed by a Reptile outer update. Thanks to the gradients aligned between\ntasks by our method, the model becomes less vulnerable to negative transfer and\ncatastrophic forgetting. We extensively validate our method on various\nmulti-task learning and zero-shot cross-lingual transfer tasks, where our\nmethod largely outperforms all the relevant baselines we consider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hae Beom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of the interactive Leipzig Corpus Miner as a generic research platform for the use in the social sciences. (arXiv:2110.02708v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02708","description":"<p>This article introduces to the interactive Leipzig Corpus Miner (iLCM) - a\nnewly released, open-source software to perform automatic content analysis.\nSince the iLCM is based on the R-programming language, its generic text mining\nprocedures provided via a user-friendly graphical user interface (GUI) can\neasily be extended using the integrated IDE RStudio-Server or numerous other\ninterfaces in the tool. Furthermore, the iLCM offers various possibilities to\nuse quantitative and qualitative research approaches in combination. Some of\nthese possibilities will be presented in more detail in the following.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahmann_C/0/1/0/all/0/1\">Christian Kahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiedemann_G/0/1/0/all/0/1\">Gregor Wiedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How BPE Affects Memorization in Transformers. (arXiv:2110.02782v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02782","description":"<p>Training data memorization in NLP can both be beneficial (e.g., closed-book\nQA) and undesirable (personal data extraction). In any case, successful model\ntraining requires a non-trivial amount of memorization to store word spellings,\nvarious linguistic idiosyncrasies and common knowledge. However, little is\nknown about what affects the memorization behavior of NLP models, as the field\ntends to focus on the equally important question of generalization. In this\nwork, we demonstrate that the size of the subword vocabulary learned by\nByte-Pair Encoding (BPE) greatly affects both ability and tendency of standard\nTransformer models to memorize training data, even when we control for the\nnumber of learned parameters. We find that with a large subword vocabulary\nsize, Transformer models fit random mappings more easily and are more\nvulnerable to membership inference attacks. Similarly, given a prompt,\nTransformer-based language models with large subword vocabularies reproduce the\ntraining data more often. We conjecture this effect is caused by reduction in\nthe sequences' length that happens as the BPE vocabulary grows. Our findings\ncan allow a more informed choice of hyper-parameters, that is better tailored\nfor a particular use-case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spell my name: keyword boosted speech recognition. (arXiv:2110.02791v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02791","description":"<p>Recognition of uncommon words such as names and technical terminology is\nimportant to understanding conversations in context. However, the ability to\nrecognise such words remains a challenge in modern automatic speech recognition\n(ASR) systems.\n</p>\n<p>In this paper, we propose a simple but powerful ASR decoding method that can\nbetter recognise these uncommon keywords, which in turn enables better\nreadability of the results. The method boosts the probabilities of given\nkeywords in a beam search based on acoustic model predictions. The method does\nnot require any training in advance.\n</p>\n<p>We demonstrate the effectiveness of our method on the LibriSpeeech test sets\nand also internal data of real-world conversations. Our method significantly\nboosts keyword accuracy on the test sets, while maintaining the accuracy of the\nother words, and as well as providing significant qualitative improvements.\nThis method is applicable to other tasks such as machine translation, or\nwherever unseen and difficult keywords need to be recognised in beam search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_N/0/1/0/all/0/1\">Namkyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioning pre-trained language models. (arXiv:2110.02802v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02802","description":"<p>We study the presence of expert units in pre-trained Transformer-based\nLanguage Models (TLMs), and how they can be used to condition text generation\nto contain specific concepts. We define expert units to be neurons that are\nable to detect a concept in the input with a given average precision. A concept\nis represented with a set of sentences that either do or do not contain the\nconcept. Leveraging the OneSec dataset, we compile a dataset of 1344 concepts\nthat allows diverse expert units in TLMs to be discovered. Our experiments\ndemonstrate that off-the-shelf pre-trained TLMs can be conditioned on their own\nknowledge (self-conditioning) to generate text that contains a given concept.\nTo this end, we intervene on the top expert units by fixing their output during\ninference, and we show experimentally that this is an effective method to\ncondition TLMs. Our method does not require fine-tuning the model or using\nadditional parameters, which allows conditioning large TLM with minimal compute\nresources. Furthermore, by intervening on a small number of experts in GPT2, we\ncan achieve parity with respect to two concepts at generation time. The\nspecific case of gender bias is explored, and we show that, for given contexts,\ngender parity is achieved while maintaining the model's perplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations. (arXiv:2110.02834v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02834","description":"<p>Learning good representations on multi-relational graphs is essential to\nknowledge base completion (KBC). In this paper, we propose a new\nself-supervised training objective for multi-relational graph representation\nlearning, via simply incorporating relation prediction into the commonly used\n1vsAll objective. The new training objective contains not only terms for\npredicting the subject and object of a given triple, but also a term for\npredicting the relation type. We analyse how this new objective impacts\nmulti-relational learning in KBC: experiments on a variety of datasets and\nmodels show that relation prediction can significantly improve entity ranking,\nthe most widely used evaluation task for KBC, yielding a 6.1% increase in MRR\nand 9.9% increase in Hits@1 on FB15k-237 as well as a 3.1% increase in MRR and\n3.4% in Hits@1 on Aristo-v4. Moreover, we observe that the proposed objective\nis especially effective on highly multi-relational datasets, i.e. datasets with\na large number of predicates, and generates better representations when larger\nembedding sizes are used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Composition of Weighted Finite-State Transducers. (arXiv:2110.02848v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02848","description":"<p>Finite-state transducers (FSTs) are frequently used in speech recognition.\nTransducer composition is an essential operation for combining different\nsources of information at different granularities. However, composition is also\none of the more computationally expensive operations. Due to the heterogeneous\nstructure of FSTs, parallel algorithms for composition are suboptimal in\nefficiency, generality, or both. We propose an algorithm for parallel\ncomposition and implement it on graphics processing units. We benchmark our\nparallel algorithm on the composition of random graphs and the composition of\ngraphs commonly used in speech recognition. The parallel composition scales\nbetter with the size of the input graphs and for large graphs can be as much as\n10 to 30 times faster than a sequential CPU algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubho Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSG HASOC-Dravidian CodeMixFIRE2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02852","description":"<p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in GitHub 1\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Structural Locality in Non-parametric Language Models. (arXiv:2110.02870v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02870","description":"<p>Structural locality is a ubiquitous feature of real-world datasets, wherein\ndata points are organized into local hierarchies. Some examples include topical\nclusters in text or project hierarchies in source code repositories. In this\npaper, we explore utilizing this structural locality within non-parametric\nlanguage models, which generate sequences that reference retrieved examples\nfrom an external source. We propose a simple yet effective approach for adding\nlocality information into such models by adding learned parameters that improve\nthe likelihood of retrieving examples from local neighborhoods. Experiments on\ntwo different domains, Java source code and Wikipedia text, demonstrate that\nlocality features improve model efficacy over models without access to these\nfeatures, with interesting differences. We also perform an analysis of how and\nwhere locality features contribute to improved performance and why the\ntraditionally used contextual similarity metrics alone are not enough to grasp\nthe locality structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-Loop Refinement of Word Embeddings. (arXiv:2110.02884v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02884","description":"<p>Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. Despite their proven\nutility in machine learning tasks, word embedding models may capture uneven\nsemantic and syntactic representations, and can inadvertently reflect various\nkinds of bias present within corpora upon which they were trained. It has been\ndemonstrated that post-processing of word embeddings to apply information found\nin lexical dictionaries can improve the semantic associations, thus improving\ntheir quality. Building on this idea, we propose a system that incorporates an\nadaptation of word embedding post-processing, which we call \"interactive\nrefitting\", to address some of the most daunting qualitative problems found in\nword embeddings. Our approach allows a human to identify and address potential\nquality issues with word embeddings interactively. This has the advantage of\nnegating the question of who decides what constitutes bias or what other\nquality issues may affect downstream tasks. It allows each organization or\nentity to address concerns they may have at a fine grained level and to do so\nin an iterative and interactive fashion. It also allows for better insight into\nwhat effect word embeddings, and refinements to word embeddings, have on\nmachine learning pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Powell_J/0/1/0/all/0/1\">James Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sentz_K/0/1/0/all/0/1\">Kari Sentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_M/0/1/0/all/0/1\">Martin Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings. (arXiv:2110.02887v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02887","description":"<p>Recent studies have proposed different methods to improve multilingual word\nrepresentations in contextualized settings including techniques that align\nbetween source and target embedding spaces. For contextualized embeddings,\nalignment becomes more complex as we additionally take context into\nconsideration. In this work, we propose using Optimal Transport (OT) as an\nalignment objective during fine-tuning to further improve multilingual\ncontextualized representations for downstream cross-lingual transfer. This\napproach does not require word-alignment pairs prior to fine-tuning that may\nlead to sub-optimal matching and instead learns the word alignments within\ncontext in an unsupervised manner. It also allows different types of mappings\ndue to soft matching between source and target sentences. We benchmark our\nproposed method on two tasks (XNLI and XQuAD) and achieve improvements over\nbaselines as well as competitive results compared to similar recent works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alqahtani_S/0/1/0/all/0/1\">Sawsan Alqahtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalwani_G/0/1/0/all/0/1\">Garima Lalwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer. (arXiv:2110.02950v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02950","description":"<p>Expert-layman text style transfer technologies have the potential to improve\ncommunication between members of scientific communities and the general public.\nHigh-quality information produced by experts is often filled with difficult\njargon laypeople struggle to understand. This is a particularly notable issue\nin the medical domain, where layman are often confused by medical text online.\nAt present, two bottlenecks interfere with the goal of building high-quality\nmedical expert-layman style transfer systems: a dearth of pretrained\nmedical-domain language models spanning both expert and layman terminologies\nand a lack of parallel corpora for training the transfer task itself. To\nmitigate the first issue, we propose a novel language model (LM) pretraining\ntask, Knowledge Base Assimilation, to synthesize pretraining data from the\nedges of a graph of expert- and layman-style medical terminology terms into an\nLM during self-supervised learning. To mitigate the second issue, we build a\nlarge-scale parallel corpus in the medical expert-layman domain using a\nmargin-based criterion. Our experiments show that transformer-based models\npretrained on knowledge base assimilation and other well-established\npretraining tasks fine-tuning on our new parallel corpus leads to considerable\nimprovement against expert-layman transfer benchmarks, gaining an average\nrelative improvement of our human evaluation, the Overall Success Rate (OSR),\nby 106%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS. (arXiv:2110.02952v1 [eess.AS])","link":"http://arxiv.org/abs/2110.02952","description":"<p>Neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the synthetic speech often\nrepresents the average prosodic style of the database instead of having more\nversatile prosodic variation. Moreover, many models lack the ability to control\nthe output prosody, which does not allow for different styles for the same text\ninput. In this work, we train a non-autoregressive parallel neural TTS model\nhierarchically conditioned on both coarse and fine-grained acoustic speech\nfeatures to learn a latent prosody space with intuitive and meaningful\ndimensions. Experiments show that a non-autoregressive TTS model hierarchically\nconditioned on utterance-wise pitch, pitch range, duration, energy, and\nspectral tilt can effectively control each prosodic dimension, generate a wide\nvariety of speaking styles, and provide word-wise emphasis control, while\nmaintaining equal or better quality to the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On learning an interpreted language with recurrent models. (arXiv:1809.04128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1809.04128","description":"<p>Can recurrent neural nets, inspired by human sequential data processing,\nlearn to understand language? We construct simplified datasets reflecting core\nproperties of natural language as modeled in formal syntax and semantics:\nrecursive syntactic structure and compositionality. We find LSTM and GRU\nnetworks to generalise to compositional interpretation well, but only in the\nmost favorable learning settings, with a well-paced curriculum, extensive\ntraining data, and left-to-right (but not right-to-left) composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.06658","description":"<p>Humans can systematically generalize to novel compositions of existing\nconcepts. There have been extensive conjectures into the extent to which neural\nnetworks can do the same. Recent arguments supported by evidence on the SCAN\ndataset claim that neural networks are inherently ineffective in such cognitive\ncapacity. In this paper, we revisit systematic generalization from the\nperspective of meaningful learning, an exceptional capability of humans to\nlearn new concepts by connecting them with other previously known knowledge. We\npropose to augment a training dataset in either an inductive or deductive\nmanner to build semantic links between new and old concepts. Our observations\non SCAN suggest that, following the meaningful learning principle, modern\nsequence-to-sequence models, including RNNs, CNNs, and Transformers, can\nsuccessfully generalize to compositions of new concepts. We further validate\nour findings on two real-world datasets on semantic parsing and consistent\ncompositional generalization is also observed. Moreover, our experiments\ndemonstrate that both prior knowledge and semantic linking play a key role to\nachieve systematic generalization. Meanwhile, inductive learning generally\nworks better than deductive learning in our experiments. Finally, we provide an\nexplanation for data augmentation techniques by concluding them into either\ninductive-based or deductive-based meaningful learning. We hope our findings\nwill encourage excavating existing neural networks' potential in systematic\ngeneralization through more advanced learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HittER: Hierarchical Transformers for Knowledge Graph Embeddings. (arXiv:2008.12813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.12813","description":"<p>This paper examines the challenging problem of learning representations of\nentities and relations in a complex multi-relational knowledge graph. We\npropose HittER, a Hierarchical Transformer model to jointly learn\nEntity-relation composition and Relational contextualization based on a source\nentity's neighborhood. Our proposed model consists of two different Transformer\nblocks: the bottom block extracts features of each entity-relation pair in the\nlocal neighborhood of the source entity and the top block aggregates the\nrelational information from outputs of the bottom block. We further design a\nmasked entity prediction task to balance information from the relational\ncontext and the source entity itself. Experimental results show that HittER\nachieves new state-of-the-art results on multiple link prediction datasets. We\nadditionally propose a simple approach to integrate HittER into BERT and\ndemonstrate its effectiveness on two Freebase factoid question answering\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterDrop: On the Efficiency of Adapters in Transformers. (arXiv:2010.11918v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.11918","description":"<p>Massively pre-trained transformer models are computationally expensive to\nfine-tune, slow for inference, and have large storage requirements. Recent\napproaches tackle these shortcomings by training smaller models, dynamically\nreducing the model size, and by training light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters from lower transformer layers during\ntraining and inference, which incorporates concepts from all three directions.\nWe show that AdapterDrop can dynamically reduce the computational overhead when\nperforming inference over multiple tasks simultaneously, with minimal decrease\nin task performances. We further prune adapters from AdapterFusion, which\nimproves the inference efficiency while maintaining the task performances\nentirely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glockner_M/0/1/0/all/0/1\">Max Glockner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs. (arXiv:2012.02821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02821","description":"<p>Multilabel conditional image generation is a challenging problem in computer\nvision. In this work we propose Multi-ingredient Pizza Generator (MPG), a\nconditional Generative Neural Network (GAN) framework for synthesizing\nmultilabel images. We design MPG based on a state-of-the-art GAN structure\ncalled StyleGAN2, in which we develop a new conditioning technique by enforcing\nintermediate feature maps to learn scalewise label information. Because of the\ncomplex nature of the multilabel image generation problem, we also regularize\nsynthetic image by predicting the corresponding ingredients as well as\nencourage the discriminator to distinguish between matched image and mismatched\nimage. To verify the efficacy of MPG, we test it on Pizza10, which is a\ncarefully annotated multi-ingredient pizza image dataset. MPG can successfully\ngenerate photo-realist pizza images with desired ingredients. The framework can\nbe easily extend to other multilabel image generation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guoyao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast WordPiece Tokenization. (arXiv:2012.15524v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15524","description":"<p>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In\nthis paper, we propose efficient algorithms for the WordPiece tokenization used\nin BERT, from single-word tokenization to general text (e.g., sentence)\ntokenization. When tokenizing a single word, WordPiece uses a\nlongest-match-first strategy, known as maximum matching. The best known\nalgorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is\nthe maximum vocabulary token length). We propose a novel algorithm whose\ntokenization complexity is strictly O(n). Our method is inspired by the\nAho-Corasick algorithm. We introduce additional linkages on top of the trie\nbuilt from the vocabulary, allowing smart transitions when the trie matching\ncannot continue. For general text, we further propose an algorithm that\ncombines pre-tokenization (splitting the text into words) and our linear-time\nWordPiece method into a single pass. Experimental results show that our method\nis 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text\non average for general text tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1\">Alex Salcianu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopson_D/0/1/0/all/0/1\">Dave Dopson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records. (arXiv:2102.02340v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.02340","description":"<p>One important challenge of applying deep learning to electronic health\nrecords (EHR) is the complexity of their multimodal structure. EHR usually\ncontains a mixture of structured (codes) and unstructured (free-text) data with\nsparse and irregular longitudinal features -- all of which doctors utilize when\nmaking decisions. In the deep learning regime, determining how different\nmodality representations should be fused together is a difficult problem, which\nis often addressed by handcrafted modeling and intuition. In this work, we\nextend state-of-the-art neural architecture search (NAS) methods and propose\nMUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across\nmultimodal fusion strategies and modality-specific architectures for the first\ntime. We demonstrate empirically that our MUFASA method outperforms established\nunimodal NAS on public EHR data with comparable computation costs. In addition,\nMUFASA produces architectures that outperform Transformer and Evolved\nTransformer. Compared with these baselines on CCS diagnosis code prediction,\nour discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate\nthe ability to generalize to other EHR tasks. Studying our top architecture in\ndepth, we provide empirical evidence that MUFASA's improvements are derived\nfrom its ability to both customize modeling for each data modality and find\neffective fusion strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Attention with Linear Units. (arXiv:2104.07012v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07012","description":"<p>Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques. (arXiv:2104.13225v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.13225","description":"<p>This survey provides an overview of the evolution of visually grounded models\nof spoken language over the last 20 years. Such models are inspired by the\nobservation that when children pick up a language, they rely on a wide range of\nindirect and noisy clues, crucially including signals from the visual modality\nco-occurring with spoken utterances. Several fields have made important\ncontributions to this approach to modeling or mimicking the process of learning\nlanguage: Machine Learning, Natural Language and Speech Processing, Computer\nVision and Cognitive Science. The current paper brings together these\ncontributions in order to provide a useful introduction and overview for\npractitioners in all these areas. We discuss the central research questions\naddressed, the timeline of developments, and the datasets which enabled much of\nthis work. We then summarize the main modeling architectures and offer an\nexhaustive overview of the evaluation metrics and analysis techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Ackermannian lower bound for the Petri nets reachability problem. (arXiv:2105.08551v3 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2105.08551","description":"<p>Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n</p>\n<p>Our contribution is an improvement of the former construction, making it\nconceptually simpler and more direct. On the way we improve the lower bound for\nvector addition systems with states in fixed dimension (or, equivalently, Petri\nnets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski prove\n$F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in dimension\n$6k$, and Leroux in dimension $4k+5$, our simplified construction yields\n$F_k$-hardness already in dimension $3k+2$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Itihasa: A large-scale corpus for Sanskrit to English translation. (arXiv:2106.03269v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03269","description":"<p>This work introduces Itihasa, a large-scale translation dataset containing\n93,000 pairs of Sanskrit shlokas and their English translations. The shlokas\nare extracted from two Indian epics viz., The Ramayana and The Mahabharata. We\nfirst describe the motivation behind the curation of such a dataset and follow\nup with empirical analysis to bring out its nuances. We then benchmark the\nperformance of standard translation models on this corpus and show that even\nstate-of-the-art transformer architectures perform poorly, emphasizing the\ncomplexity of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07704","description":"<p>Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many emerging applications, such as generating\nadversarial attacks or generating prompts to control language models.\nReinforcement learning (RL) on the other hand offers a more flexible solution\nby allowing users to plug in arbitrary task metrics as reward. Yet previous RL\nalgorithms for text generation, such as policy gradient (on-policy RL) and\nQ-learning (off-policy RL), are often notoriously inefficient or unstable to\ntrain due to the large sequence space and the sparse reward received only at\nthe end of sequences. In this paper, we introduce a new RL formulation for text\ngeneration from the soft Q-learning (SQL) perspective. It enables us to draw\nfrom the latest RL advances, such as path consistency learning, to combine the\nbest of on-/off-policy updates, and learn effectively from sparse reward. We\napply the approach to a wide range of text generation tasks, including learning\nfrom noisy/negative examples, adversarial attacks, and prompt generation.\nExperiments show our approach consistently outperforms both task-specialized\nalgorithms and the previous RL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence is not perfect,\ne.g., when the target sequence is corrupted with noises, or when only weak\nsequence supervision is available. To address this challenge, we propose a\nnovel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of\na target n-gram with all n-grams in the generated sequence. Drawing\ninspirations from the classical convolutional networks (ConvNets) which capture\nshift-invariance in image modeling, EISL is designed to be robust to the shift\nof n-grams to tolerate various noises and edits in the target sequences.\nMoreover, the EISL computation is essentially a convolution operation with\ntarget n-grams as kernels, which is easy to implement and efficient to compute\nwith existing libraries. To demonstrate the effectiveness of EISL, we conduct\nexperiments on a wide range of tasks, including machine translation with noisy\ntarget sequences, unsupervised text style transfer with only weak training\nsignals, and non-autoregressive generation with non-predefined generation\norder. Experimental results show our method significantly outperforms the\ncommon cross-entropy loss and other strong baselines on all the tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14638","description":"<p>It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution. (arXiv:2108.12777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12777","description":"<p>Recent studies have shown that deep neural networks are vulnerable to\nintentionally crafted adversarial examples, and various methods have been\nproposed to defend against adversarial word-substitution attacks for neural NLP\nmodels. However, there is a lack of systematic study on comparing different\ndefense approaches under the same attacking setting. In this paper, we seek to\nfill the gap of systematic studies through comprehensive researches on\nunderstanding the behavior of neural text classifiers trained by various\ndefense methods under representative adversarial attacks. In addition, we\npropose an effective method to further improve the robustness of neural text\nclassifiers against such attacks and achieved the highest accuracy on both\nclean and adversarial examples on AGNEWS and IMDB datasets by a significant\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14788","description":"<p>Modern medical diagnosis relies on precise pain assessment tools in\ntranslating clinical information from patient to physician. The McGill Pain\nQuestionnaire (MPQ) is a clinical pain assessment technique that utilizes 78\nadjectives of different intensities in 20 different categories to quantity a\npatient's pain. The questionnaire's efficacy depends on a predictable pattern\nof adjective use by patients experiencing pain. In this study, I recreate the\nMPQ's adjective intensity orderings using data gathered from patient forums and\nmodern NLP techniques. I extract adjective intensity relationships by searching\nfor key linguistic contexts, and then combine the relationship information to\nform robust adjective scales. Of 17 adjective relationships predicted by this\nresearch, only 4 diverge from the MPQ's orderings, which is statistically\nsignificant at the 0.1 alpha level. The results suggest predictable patterns of\nadjective use by people experiencing pain, but call into question the MPQ's\ncategories for grouping adjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1\">Miriam Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer Language Models. (arXiv:2110.02058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy. (arXiv:2110.02204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02204","description":"<p>Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hybrid Classical-Quantum method for Diabetic Foot Ulcer Classification. (arXiv:2110.02222v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02222","description":"<p>Diabetes is a raising problem that affects many people globally. Diabetic\npatients are at risk of developing foot ulcer that usually leads to limb\namputation, causing significant morbidity, and psychological distress. In order\nto develop a self monitoring mobile application, it is necessary to be able to\nclassify such ulcers into either of the following classes: Infection,\nIschaemia, None, or Both. In this work, we compare the performance of a\nclassical transfer-learning-based method, with the performance of a hybrid\nclassical-quantum Classifier on diabetic foot ulcer classification task. As\nsuch, we merge the pre-trained Xception network with a multi-class variational\nclassifier. Thus, after modifying and re-training the Xception network, we\nextract the output of a mid-layer and employ it as deep-features presenters of\nthe given images. Finally, we use those deep-features to train multi-class\nvariational classifier, where each classifier is implemented on an individual\nvariational circuit. The method is then evaluated on the blind test set\nDFUC2021. The results proves that our proposed hybrid classical-quantum\nClassifier leads to considerable improvement compared to solely relying on\ntransfer learning concept through training the modified version of Xception\nnetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alavi_A/0/1/0/all/0/1\">Azadeh Alavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akhoundi_H/0/1/0/all/0/1\">Hossein Akhoundi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Assisted Convolutional Network for Cell Instance Segmentation. (arXiv:2110.02270v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02270","description":"<p>Region proposal based methods like R-CNN and Faster R-CNN models have proven\nto be extremely successful in object detection and segmentation tasks.\nRecently, Transformers have also gained popularity in the domain of Computer\nVision, and are being utilised to improve the performance of conventional\nmodels. In this paper, we present a relatively new transformer based approach\nto enhance the performance of the conventional convolutional feature extractor\nin the existing region proposal based methods. Our approach merges the\nconvolutional feature maps with transformer-based token embeddings by applying\na projection operation similar to self-attention in transformers. The results\nof our experiments show that transformer assisted feature extractor achieves a\nsignificant improvement in mIoU (mean Intersection over Union) scores compared\nto vanilla convolutional backbone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_D/0/1/0/all/0/1\">Deepanshu Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pradyumna Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sumit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Aman Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rohit Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints. (arXiv:2110.02273v1 [math.OC])","link":"http://arxiv.org/abs/2110.02273","description":"<p>We investigate a family of bilevel imaging learning problems where the\nlower-level instance corresponds to a convex variational model involving first-\nand second-order nonsmooth regularizers. By using geometric properties of the\nprimal-dual reformulation of the lower-level problem and introducing suitable\nchanges of variables, we are able to reformulate the original bilevel problems\nas Mathematical Programs with Complementarity Constraints (MPCC). For the\nlatter, we prove tight constraint qualification conditions (MPCC-MFCQ and\npartial MPCC-LICQ) and derive Mordukovich (M-) and Strong (S-) stationarity\nconditions. The S-stationarity system for the MPCC turns also into\nS-stationarity conditions for the original formulation. Second-order sufficient\noptimality conditions are derived as well. The proposed reformulation may be\nextended to problems in function spaces, leading to MPCC's with additional\nconstraints on the gradient of the state. Finally, we report on some numerical\nresults obtained by using the proposed MPCC reformulations together with\navailable large-scale nonlinear programming solvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Reyes_J/0/1/0/all/0/1\">Juan Carlos De los Reyes</a>, <a href=\"http://arxiv.org/find/math/1/au:+Villacis_D/0/1/0/all/0/1\">David Villac&#xed;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling up instance annotation via label propagation. (arXiv:2110.02277v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02277","description":"<p>Manually annotating object segmentation masks is very time-consuming. While\ninteractive segmentation methods offer a more efficient alternative, they\nbecome unaffordable at a large scale because the cost grows linearly with the\nnumber of annotated masks. In this paper, we propose a highly efficient\nannotation scheme for building large datasets with object segmentation masks.\nAt a large scale, images contain many object instances with similar appearance.\nWe exploit these similarities by using hierarchical clustering on mask\npredictions made by a segmentation model. We propose a scheme that efficiently\nsearches through the hierarchy of clusters and selects which clusters to\nannotate. Humans manually verify only a few masks per cluster, and the labels\nare propagated to the whole cluster. Through a large-scale experiment to\npopulate 1M unlabeled images with object segmentation masks for 80 object\nclasses, we show that (1) we obtain 1M object segmentation masks with an total\nannotation time of only 290 hours; (2) we reduce annotation time by 76x\ncompared to manual annotation; (3) the segmentation quality of our masks is on\npar with those from manually annotated datasets. Code, data, and models are\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1\">Dim P. Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_E/0/1/0/all/0/1\">Ethan Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turing approximations, toric isometric embeddings & manifold convolutions. (arXiv:2110.02279v1 [math.DG])","link":"http://arxiv.org/abs/2110.02279","description":"<p>Convolutions are fundamental elements in deep learning architectures. Here,\nwe present a theoretical framework for combining extrinsic and intrinsic\napproaches to manifold convolution through isometric embeddings into tori. In\nthis way, we define a convolution operator for a manifold of arbitrary topology\nand dimension. We also explain geometric and topological conditions that make\nsome local definitions of convolutions which rely on translating filters along\ngeodesic paths on a manifold, computationally intractable. A result of Alan\nTuring from 1938 underscores the need for such a toric isometric embedding\napproach to achieve a global definition of convolution on computable, finite\nmetric space approximations to a smooth manifold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Suarez_Serrato_P/0/1/0/all/0/1\">P. Su&#xe1;rez-Serrato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the Facial Growth Direction is Challenging. (arXiv:2110.02316v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02316","description":"<p>Facial dysmorphology or malocclusion is frequently associated with abnormal\ngrowth of the face. The ability to predict facial growth (FG) direction would\nallow clinicians to prepare individualized therapy to increase the chance for\nsuccessful treatment. Prediction of FG direction is a novel problem in the\nmachine learning (ML) domain. In this paper, we perform feature selection and\npoint the attribute that plays a central role in the abovementioned problem.\nThen we successfully apply data augmentation (DA) methods and improve the\npreviously reported classification accuracy by 2.81%. Finally, we present the\nresults of two experienced clinicians that were asked to solve a similar task\nto ours and show how tough is solving this problem for human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazmierczak_S/0/1/0/all/0/1\">Stanis&#x142;aw Ka&#x17a;mierczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juszka_Z/0/1/0/all/0/1\">Zofia Juszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandevska_Radunovic_V/0/1/0/all/0/1\">Vaska Vandevska-Radunovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maal_T/0/1/0/all/0/1\">Thomas JJ Maal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudalej_P/0/1/0/all/0/1\">Piotr Fudalej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancement of Anime Imaging Enlargement using Modified Super-Resolution CNN. (arXiv:2110.02321v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02321","description":"<p>Anime is a storytelling medium similar to movies and books. Anime images are\na kind of artworks, which are almost entirely drawn by hand. Hence, reproducing\nexisting Anime with larger sizes and higher quality images is expensive.\nTherefore, we proposed a model based on convolutional neural networks to\nextract outstanding features of images, enlarge those images, and enhance the\nquality of Anime images. We trained the model with a training set of 160 images\nand a validation set of 20 images. We tested the trained model with a testing\nset of 20 images. The experimental results indicated that our model\nsuccessfully enhanced the image quality with a larger image-size when compared\nwith the common existing image enlargement and the original SRCNN method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Intaniyom_T/0/1/0/all/0/1\">Tanakit Intaniyom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thananporn_W/0/1/0/all/0/1\">Warinthorn Thananporn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woraratpanya_K/0/1/0/all/0/1\">Kuntpong Woraratpanya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-aware Multi-Person Pose Estimation from Multi-View Images. (arXiv:2110.02330v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02330","description":"<p>In this paper we contribute a simple yet effective approach for estimating 3D\nposes of multiple people from multi-view images. Our proposed coarse-to-fine\npipeline first aggregates noisy 2D observations from multiple camera views into\n3D space and then associates them into individual instances based on a\nconfidence-aware majority voting technique. The final pose estimates are\nattained from a novel optimization scheme which links high-confidence\nmulti-view 2D observations and 3D joint candidates. Moreover, a statistical\nparametric body model such as SMPL is leveraged as a regularizing prior for\nthese 3D joint candidates. Specifically, both 3D poses and SMPL parameters are\noptimized jointly in an alternating fashion. Here the parametric models help in\ncorrecting implausible 3D pose estimates and filling in missing joint\ndetections while updated 3D poses in turn guide obtaining better SMPL\nestimations. By linking 2D and 3D observations, our method is both accurate and\ngeneralizes to different data sources because it better decouples the final 3D\npose from the inter-person constellation and is more robust to noisy 2D\ndetections. We systematically evaluate our method on public datasets and\nachieve state-of-the-art performance. The code and video will be available on\nthe project page: https://ait.ethz.ch/projects/2021/multi-human-pose/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zijian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Algebra Attention Networks for Small Point Clouds. (arXiv:2110.02393v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02393","description":"<p>Much of the success of deep learning is drawn from building architectures\nthat properly respect underlying symmetry and structure in the data on which\nthey operate - a set of considerations that have been united under the banner\nof geometric deep learning. Often problems in the physical sciences deal with\nrelatively small sets of points in two- or three-dimensional space wherein\ntranslation, rotation, and permutation equivariance are important or even vital\nfor models to be useful in practice. In this work, we present rotation- and\npermutation-equivariant architectures for deep learning on these small point\nclouds, composed of a set of products of terms from the geometric algebra and\nreductions over those products using an attention mechanism. The geometric\nalgebra provides valuable mathematical structure by which to combine vector,\nscalar, and other types of geometric inputs in a systematic way to account for\nrotation invariance or covariance, while attention yields a powerful way to\nimpose permutation equivariance. We demonstrate the usefulness of these\narchitectures by training models to solve sample problems relevant to physics,\nchemistry, and biology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spellings_M/0/1/0/all/0/1\">Matthew Spellings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Affinity with Maximum Bipartite Matching in Few-Shot Learning. (arXiv:2110.02399v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02399","description":"<p>We propose an asymmetric affinity score for representing the complexity of\nutilizing the knowledge of one task for learning another one. Our method is\nbased on the maximum bipartite matching algorithm and utilizes the Fisher\nInformation matrix. We provide theoretical analyses demonstrating that the\nproposed score is mathematically well-defined, and subsequently use the\naffinity score to propose a novel algorithm for the few-shot learning problem.\nIn particular, using this score, we find relevant training data labels to the\ntest data and leverage the discovered relevant data for episodically\nfine-tuning a few-shot model. Results on various few-shot benchmark datasets\ndemonstrate the efficacy of the proposed approach by improving the\nclassification accuracy over the state-of-the-art methods even when using\nsmaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Juncheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-MOV: Audio-Visual LSTM Autoencoder for 3D Reconstruction of Multiple Objects from Video. (arXiv:2110.02404v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02404","description":"<p>3D object reconstructions of transparent and concave structured objects, with\ninferred material properties, remains an open research problem for robot\nnavigation in unstructured environments. In this paper, we propose a multimodal\nsingle- and multi-frame neural network for 3D reconstructions using\naudio-visual inputs. Our trained reconstruction LSTM autoencoder 3D-MOV accepts\nmultiple inputs to account for a variety of surface types and views. Our neural\nnetwork produces high-quality 3D reconstructions using voxel representation.\nBased on Intersection-over-Union (IoU), we evaluate against other baseline\nmethods using synthetic audio-visual datasets ShapeNet and Sound20K with impact\nsounds and bounding box annotations. To the best of our knowledge, our single-\nand multi-frame model is the first audio-visual reconstruction neural network\nfor 3D geometry and material representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Justin Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming C. Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction. (arXiv:2110.02405v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02405","description":"<p>Reflective and textureless surfaces such as windows, mirrors, and walls can\nbe a challenge for object and scene reconstruction. These surfaces are often\npoorly reconstructed and filled with depth discontinuities and holes, making it\ndifficult to cohesively reconstruct scenes that contain these planar\ndiscontinuities. We propose Echoreconstruction, an audio-visual method that\nuses the reflections of sound to aid in geometry and audio reconstruction for\nvirtual conferencing, teleimmersion, and other AR/VR experience. The mobile\nphone prototype emits pulsed audio, while recording video for RGB-based 3D\nreconstruction and audio-visual classification. Reflected sound and images from\nthe video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV)\nconvolutional neural networks for surface and sound source detection, depth\nestimation, and material classification. The inferences from these\nclassifications enhance scene 3D reconstructions containing open spaces and\nreflective surfaces by depth filtering, inpainting, and placement of unmixed\nsound sources in the scene. Our prototype, VR demo, and experimental results\nfrom real-world and virtual scenes with challenging surfaces and sound indicate\nhigh success rates on classification of material, depth estimation, and\nclosed/open surfaces, leading to considerable visual and audio improvement in\n3D scenes (see Figure 1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Justin Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1\">Nicholas Rewkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming C. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Scale A Contrario method for Unsupervised Image Anomaly Detection. (arXiv:2110.02407v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02407","description":"<p>Anomalies can be defined as any non-random structure which deviates from\nnormality. Anomaly detection methods reported in the literature are numerous\nand diverse, as what is considered anomalous usually varies depending on\nparticular scenarios and applications. In this work we propose an a contrario\nframework to detect anomalies in images applying statistical analysis to\nfeature maps obtained via convolutions. We evaluate filters learned from the\nimage under analysis via patch PCA, Gabor filters and the feature maps obtained\nfrom a pre-trained deep neural network (Resnet). The proposed method is\nmulti-scale and fully unsupervised and is able to detect anomalies in a wide\nvariety of scenarios. While the end goal of this work is the detection of\nsubtle defects in leather samples for the automotive industry, we show that the\nsame algorithm achieves state of the art results in public anomalies datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tailanian_M/0/1/0/all/0/1\">Matias Tailanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muse_P/0/1/0/all/0/1\">Pablo Mus&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">&#xc1;lvaro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADA: Multi-scale Collaborative Adversarial Domain Adaptation for Unsupervised Optic Disc and Cup Segmentation. (arXiv:2110.02417v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02417","description":"<p>The diversity of retinal imaging devices poses a significant challenge:\ndomain shift, which leads to performance degradation when applying the deep\nlearning models trained on one domain to new testing domains. In this paper, we\npropose a multi-scale input along with multiple domain adaptors applied\nhierarchically in both feature and output spaces. The proposed training\nstrategy and novel unsupervised domain adaptation framework, called\nCollaborative Adversarial Domain Adaptation (CADA), can effectively overcome\nthe challenge. Multi-scale inputs can reduce the information loss due to the\npooling layers used in the network for feature extraction, while our proposed\nCADA is an interactive paradigm that presents an exquisite collaborative\nadaptation through both adversarial learning and ensembling weights at\ndifferent network layers. In particular, to produce a better prediction for the\nunlabeled target domain data, we simultaneously achieve domain invariance and\nmodel generalizability via adversarial learning at multi-scale outputs from\ndifferent levels of network layers and maintaining an exponential moving\naverage (EMA) of the historical weights during training. Without annotating any\nsample from the target domain, multiple adversarial losses in encoder and\ndecoder layers guide the extraction of domain-invariant features to confuse the\ndomain classifier. Meanwhile, the ensembling of weights via EMA reduces the\nuncertainty of adapting multiple discriminator learning. Comprehensive\nexperimental results demonstrate that our CADA model incorporating multi-scale\ninput training can overcome performance degradation and outperform\nstate-of-the-art domain adaptation methods in segmenting retinal optic disc and\ncup from fundus images stemming from the REFUGE, Drishti-GS, and Rim-One-r3\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_C/0/1/0/all/0/1\">Charlie T. Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_B/0/1/0/all/0/1\">Bin Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_R/0/1/0/all/0/1\">Ruogu Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence-Balanced Loss for Imbalanced Visual Classification. (arXiv:2110.02444v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02444","description":"<p>In this paper, we propose a balancing training method to address problems in\nimbalanced data learning. To this end, we derive a new loss used in the\nbalancing training phase that alleviates the influence of samples that cause an\noverfitted decision boundary. The proposed loss efficiently improves the\nperformance of any type of imbalance learning methods. In experiments on\nmultiple benchmark data sets, we demonstrate the validity of our method and\nreveal that the proposed loss outperforms the state-of-the-art cost-sensitive\nloss methods. Furthermore, since our loss is not restricted to a specific task,\nmodel, or training method, it can be easily used in combination with other\nrecent re-sampling, meta-learning, and cost-sensitive learning methods for\nclass-imbalance problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seulki Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jongin Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Younghan Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jin Young Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ripple Attention for Visual Perception with Sub-quadratic Complexity. (arXiv:2110.02453v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02453","description":"<p>Transformer architectures are now central to modeling in natural language\nprocessing tasks. At its heart is the attention mechanism, which enables\neffective modeling of long-term dependencies in a sequence. Recently,\ntransformers have been successfully applied in the computer vision domain,\nwhere 2D images are first segmented into patches and then treated as 1D\nsequences. Such linearization, however, impairs the notion of spatial locality\nin images, which bears important visual clues. To bridge the gap, we propose\nripple attention, a sub-quadratic attention mechanism for visual perception. In\nripple attention, contributions of different tokens to a query are weighted\nwith respect to their relative spatial distances in the 2D space. To favor\ncorrelations with vicinal tokens yet permit long-term dependencies, we derive\nthe spatial weights through a stick-breaking transformation. We further design\na dynamic programming algorithm that computes weighted contributions for all\nqueries in linear observed time, taking advantage of the summed-area table and\nrecent advances in linearized attention. Extensive experiments and analyses\ndemonstrate the effectiveness of ripple attention on various visual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huijie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc Models for Performance Estimation of Machine Learning Inference. (arXiv:2110.02459v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02459","description":"<p>Estimating how well a machine learning model performs during inference is\ncritical in a variety of scenarios (for example, to quantify uncertainty, or to\nchoose from a library of available models). However, the standard accuracy\nestimate of softmax confidence is not versatile and cannot reliably predict\ndifferent performance metrics (e.g., F1-score, recall) or the performance in\ndifferent application scenarios or input domains. In this work, we\nsystematically generalize performance estimation to a diverse set of metrics\nand scenarios and discuss generalized notions of uncertainty calibration. We\npropose the use of post-hoc models to accomplish this goal and investigate\ndesign parameters, including the model type, feature engineering, and\nperformance metric, to achieve the best estimation quality. Emphasis is given\nto object detection problems and, unlike prior work, our approach enables the\nestimation of per-image metrics such as recall and F1-score. Through extensive\nexperiments with computer vision models and datasets in three use cases --\nmobile edge offloading, model selection, and dataset shift -- we find that\nproposed post-hoc models consistently outperform the standard calibrated\nconfidence baselines. To the best of our knowledge, this is the first work to\ndevelop a unified framework to address different performance estimation\nproblems for machine learning inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuechen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiasi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image Enhancement. (arXiv:2110.02477v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02477","description":"<p>Low-light image enhancement is a challenging low-level computer vision task\nbecause after we enhance the brightness of the image, we have to deal with\namplified noise, color distortion, detail loss, blurred edges, shadow blocks\nand halo artifacts. In this paper, we propose a Two-Stage Network with Channel\nAttention (denoted as TSN-CA) to enhance the brightness of the low-light image\nand restore the enhanced images from various kinds of degradation. In the first\nstage, we enhance the brightness of the low-light image in HSV space and use\nthe information of H and S channels to help the recovery of details in V\nchannel. In the second stage, we integrate Channel Attention (CA) mechanism\ninto the skip connection of U-Net in order to restore the brightness-enhanced\nimage from severe kinds of degradation in RGB space. We train and evaluate the\nperformance of our proposed model on the LOL real-world and synthetic datasets.\nIn addition, we test our model on several other commonly used datasets without\nGround-Truth. We conduct extensive experiments to demonstrate that our method\nachieves excellent effect on brightness enhancement as well as denoising,\ndetails preservation and halo artifacts elimination. Our method outperforms\nmany other state-of-the-art methods qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attack as the Best Defense: Nullifying Image-to-image Translation GANs via Limit-aware Adversarial Attack. (arXiv:2110.02516v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02516","description":"<p>With the successful creation of high-quality image-to-image (Img2Img)\ntranslation GANs comes the non-ethical applications of DeepFake and DeepNude.\nSuch misuses of img2img techniques present a challenging problem for society.\nIn this work, we tackle the problem by introducing the Limit-Aware Self-Guiding\nGradient Sliding Attack (LaS-GSA). LaS-GSA follows the Nullifying Attack to\ncancel the img2img translation process under a black-box setting. In other\nwords, by processing input images with the proposed LaS-GSA before publishing,\nany targeted img2img GANs can be nullified, preventing the model from\nmaliciously manipulating the images. To improve efficiency, we introduce the\nlimit-aware random gradient-free estimation and the gradient sliding mechanism\nto estimate the gradient that adheres to the adversarial limit, i.e., the pixel\nvalue limitations of the adversarial example. Theoretical justifications\nvalidate how the above techniques prevent inefficiency caused by the\nadversarial limit in both the direction and the step length. Furthermore, an\neffective self-guiding prior is extracted solely from the threat model and the\ntarget image to efficiently leverage the prior information and guide the\ngradient estimation process. Extensive experiments demonstrate that LaS-GSA\nrequires fewer queries to nullify the image translation process with higher\nsuccess rates than 4 state-of-the-art black-box methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chin-Yuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsi-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">De-Nian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming-Syan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActiveMatch: End-to-end Semi-supervised Active Representation Learning. (arXiv:2110.02521v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02521","description":"<p>Semi-supervised learning (SSL) is an efficient framework that can train\nmodels with both labeled and unlabeled data. However, constrained by the\nlimited number of labels, the learned representations of SSL are ambiguous and\nnot distinguishable for inter-class samples. Moreover, the performance of SSL\nis also largely dependent on the model initialization. To deal with the\ndrawbacks of SSL, in this paper, we propose a novel end-to-end representation\nlearning method, namely ActiveMatch, which combines SSL with contrastive\nlearning and active learning to fully leverage the limited labels. Starting\nfrom a small amount of labeled data with unsupervised contrastive learning as a\nwarm-up, ActiveMatch then combines SSL and supervised contrastive learning, and\nactively selects the most representative samples for labeling during the\ntraining, resulting in better representations towards the classification.\nCompared with MixMatch and FixMatch, we show that ActiveMatch achieves the\nstate-of-the-art performance, with 89.24 accuracy on CIFAR-10 with 100\ncollected labels, and 92.20 accuracy with 200 collected labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xinkai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zilinghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a> (Zhejiang University-University of Illinois at Urbana-Champaign Institute, Zhejiang University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Reasoning for Visual Question Answering. (arXiv:2110.02526v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02526","description":"<p>Bridging the semantic gap between image and question is an important step to\nimprove the accuracy of the Visual Question Answering (VQA) task. However, most\nof the existing VQA methods focus on attention mechanisms or visual relations\nfor reasoning the answer, while the features at different semantic levels are\nnot fully utilized. In this paper, we present a new reasoning framework to fill\nthe gap between visual features and semantic clues in the VQA task. Our method\nfirst extracts the features and predicates from the image and question. We then\npropose a new reasoning framework to effectively jointly learn these features\nand predicates in a coarse-to-fine manner. The intensively experimental results\non three large-scale VQA datasets show that our proposed approach achieves\nsuperior accuracy comparing with other state-of-the-art methods. Furthermore,\nour reasoning framework also provides an explainable way to understand the\ndecision of the deep neural network when predicting the answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Firth Bias Reduction in Few-Shot Classification. (arXiv:2110.02529v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02529","description":"<p>Learning accurate classifiers for novel categories from very few examples,\nknown as few-shot image classification, is a challenging task in statistical\nmachine learning and computer vision. The performance in few-shot\nclassification suffers from the bias in the estimation of classifier\nparameters; however, an effective underlying bias reduction technique that\ncould alleviate this issue in training few-shot classifiers has been\noverlooked. In this work, we demonstrate the effectiveness of Firth bias\nreduction in few-shot classification. Theoretically, Firth bias reduction\nremoves the first order term $O(N^{-1})$ from the small-sample bias of the\nMaximum Likelihood Estimator. Here we show that the general Firth bias\nreduction technique simplifies to encouraging uniform class assignment\nprobabilities for multinomial logistic classification, and almost has the same\neffect in cosine classifiers. We derive the optimization objective for Firth\npenalized multinomial logistic and cosine classifiers, and empirically evaluate\nthat it is consistently effective across the board for few-shot image\nclassification, regardless of (1) the feature representations from different\nbackbones, (2) the number of samples per class, and (3) the number of classes.\nFinally, we show the robustness of Firth bias reduction, in the case of\nimbalanced data distribution. Our implementation is available at\nhttps://github.com/ehsansaleh/firth_bias_reduction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_S/0/1/0/all/0/1\">Saba Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_E/0/1/0/all/0/1\">Ehsan Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-xiong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-FCT: Simultaneous 3D Object Detection and Tracking Using Feature Correlation. (arXiv:2110.02531v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02531","description":"<p>3D object detection using LiDAR data remains a key task for applications like\nautonomous driving and robotics. Unlike in the case of 2D images, LiDAR data is\nalmost always collected over a period of time. However, most work in this area\nhas focused on performing detection independent of the temporal domain. In this\npaper we present 3D-FCT, a Siamese network architecture that utilizes temporal\ninformation to simultaneously perform the related tasks of 3D object detection\nand tracking. The network is trained to predict the movement of an object based\non the correlation features of extracted keypoints across time. Calculating\ncorrelation across keypoints only allows for real-time object detection. We\nfurther extend the multi-task objective to include a tracking regression loss.\nFinally, we produce high accuracy detections by linking short-term object\ntracklets into long term tracks based on the predicted tracks. Our proposed\nmethod is evaluated on the KITTI tracking dataset where it is shown to provide\nan improvement of 5.57% mAP over a state-of-the-art approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1\">Naman Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hocksoon Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See Yourself in Others: Attending Multiple Tasks for Own Failure Detection. (arXiv:2110.02549v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02549","description":"<p>Autonomous robots deal with unexpected scenarios in real environments. Given\ninput images, various visual perception tasks can be performed, e.g., semantic\nsegmentation, depth estimation and normal estimation. These different tasks\nprovide rich information for the whole robotic perception system. All tasks\nhave their own characteristics while sharing some latent correlations. However,\nsome of the task predictions may suffer from the unreliability dealing with\ncomplex scenes and anomalies. We propose an attention-based failure detection\napproach by exploiting the correlations among multiple tasks. The proposed\nframework infers task failures by evaluating the individual prediction, across\nmultiple visual perception tasks for different regions in an image. The\nformulation of the evaluations is based on an attention network supervised by\nmulti-task uncertainty estimation and their corresponding prediction errors.\nOur proposed framework generates more accurate estimations of the prediction\nerror for the different task's predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Boyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jiaxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1\">Hermann Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Computer Vision Technologies for Fish Tracking. (arXiv:2110.02551v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02551","description":"<p>Fish tracking based on computer vision is a complex and challenging task in\nfishery production and ecological studies. Most of the applications of fish\ntracking use classic filtering algorithms, which lack in accuracy and\nefficiency. To solve this issue, deep learning methods utilized deep neural\nnetworks to extract the features, which achieve a good performance in the fish\ntracking. Some one-stage detection algorithms have gradually been adopted in\nthis area for the real-time applications. The transfer learning to fish target\nis the current development direction. At present, fish tracking technology is\nnot enough to cover actual application requirements. According to the\nliterature data collected by us, there has not been any extensive review about\nvision-based fish tracking in the community. In this paper, we introduced the\ndevelopment and application prospects of fish tracking technology in last ten\nyears. Firstly, we introduced the open source datasets of fish, and summarized\nthe preprocessing technologies of underwater images. Secondly, we analyzed the\ndetection and tracking algorithms for fish, and sorted out some transferable\nfrontier tracking model. Thirdly, we listed the actual applications, metrics\nand bottlenecks of the fish tracking such as occlusion and multi-scale.\nFinally, we give the discussion for fish tracking datasets, solutions of the\nbottlenecks, and improvements. We expect that our work can help the fish\ntracking models to achieve higher accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTCD: Cataract Detection via Near Infrared Eye Images. (arXiv:2110.02564v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02564","description":"<p>Globally, cataract is a common eye disease and one of the leading causes of\nblindness and vision impairment. The traditional process of detecting cataracts\ninvolves eye examination using a slit-lamp microscope or ophthalmoscope by an\nophthalmologist, who checks for clouding of the normally clear lens of the eye.\nThe lack of resources and unavailability of a sufficient number of experts pose\na burden to the healthcare system throughout the world, and researchers are\nexploring the use of AI solutions for assisting the experts. Inspired by the\nprogress in iris recognition, in this research, we present a novel algorithm\nfor cataract detection using near-infrared eye images. The NIR cameras, which\nare popularly used in iris recognition, are of relatively low cost and easy to\noperate compared to ophthalmoscope setup for data capture. However, such NIR\nimages have not been explored for cataract detection. We present deep\nlearning-based eye segmentation and multitask network classification networks\nfor cataract detection using NIR images as input. The proposed segmentation\nalgorithm efficiently and effectively detects non-ideal eye boundaries and is\ncost-effective, and the classification network yields very high classification\nperformance on the cataract dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_P/0/1/0/all/0/1\">Pavani Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhter_Y/0/1/0/all/0/1\">Yasmeena Akhter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurshid_M/0/1/0/all/0/1\">Mahapara Khurshid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakra_A/0/1/0/all/0/1\">Aditya Lakra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshari_R/0/1/0/all/0/1\">Rohit Keshari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1\">Mayank Vatsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Modal Embeddings from Structured Data. (arXiv:2110.02577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02577","description":"<p>Multi-modal word semantics aims to enhance embeddings with perceptual input,\nassuming that human meaning representation is grounded in sensory experience.\nMost research focuses on evaluation involving direct visual input, however,\nvisual grounding can contribute to linguistic applications as well. Another\nmotivation for this paper is the growing need for more interpretable models and\nfor evaluating model efficiency regarding size and performance. This work\nexplores the impact of visual information for semantics when the evaluation\ninvolves no direct visual input, specifically semantic similarity and\nrelatedness. We investigate a new embedding type in-between linguistic and\nvisual modalities, based on the structured annotations of Visual Genome. We\ncompare uni- and multi-modal models including structured, linguistic and image\nbased representations. We measure the efficiency of each model with regard to\ndata and model size, modality / data distribution and information gain. The\nanalysis includes an interpretation of embedding structures. We found that this\nnew embedding conveys complementary information for text based embeddings. It\nachieves comparable performance in an economic way, using orders of magnitude\nless resources than visual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vero_A/0/1/0/all/0/1\">Anita L. Ver&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Adaptation for Cross-Domain Object Detection. (arXiv:2110.02578v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02578","description":"<p>Cross-domain object detection is more challenging than object classification\nsince multiple objects exist in an image and the location of each object is\nunknown in the unlabeled target domain. As a result, when we adapt features of\ndifferent objects to enhance the transferability of the detector, the features\nof the foreground and the background are easy to be confused, which may hurt\nthe discriminability of the detector. Besides, previous methods focused on\ncategory adaptation but ignored another important part for object detection,\ni.e., the adaptation on bounding box regression. To this end, we propose\nD-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation\nand the training of the detector. Besides, we fill the blank of regression\ndomain adaptation in object detection by introducing a bounding box adaptor.\nExperiments show that D-adapt achieves state-of-the-art results on four\ncross-domain object detection tasks and yields 17% and 21% relative improvement\non benchmark datasets Clipart1k and Comic2k in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning for Land Use Land Cover Classification: A Comparative Study. (arXiv:2110.02580v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02580","description":"<p>Efficiently implementing remote sensing image classification with high\nspatial resolution imagery can provide great significant value in land-use\nland-cover classification (LULC). The developments in remote sensing and deep\nlearning technologies have facilitated the extraction of spatiotemporal\ninformation for LULC classification. Moreover, the diverse disciplines of\nscience, including remote sensing, have utilised tremendous improvements in\nimage classification by CNNs with Transfer Learning. In this study, instead of\ntraining CNNs from scratch, we make use of transfer learning to fine-tune\npre-trained networks a) VGG16 and b) Wide Residual Networks (WRNs), by\nreplacing the final layer with additional layers, for LULC classification with\nEuroSAT dataset. Further, the performance and computational time were compared\nand optimized with techniques like early stopping, gradient clipping, adaptive\nlearning rates and data augmentation. With the proposed approaches we were able\nto address the limited-data problem and achieved very good accuracy.\nComprehensive comparisons over the EuroSAT RGB version benchmark have\nsuccessfully established that our method outperforms the previous best-stated\nresults, with a significant improvement over the accuracy from 98.57% to\n99.17%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naushad_R/0/1/0/all/0/1\">Raoof Naushad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_T/0/1/0/all/0/1\">Tarunpreet Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FADNet++: Real-Time and Accurate Disparity Estimation with Configurable Networks. (arXiv:2110.02582v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02582","description":"<p>Deep neural networks (DNNs) have achieved great success in the area of\ncomputer vision. The disparity estimation problem tends to be addressed by DNNs\nwhich achieve much better prediction accuracy than traditional hand-crafted\nfeature-based methods. However, the existing DNNs hardly serve both efficient\ncomputation and rich expression capability, which makes them difficult for\ndeployment in real-time and high-quality applications, especially on mobile\ndevices. To this end, we propose an efficient, accurate, and configurable deep\nnetwork for disparity estimation named FADNet++. Leveraging several liberal\nnetwork design and training techniques, FADNet++ can boost its accuracy with a\nfast model inference speed for real-time applications. Besides, it enables\nusers to easily configure different sizes of models for balancing accuracy and\ninference efficiency. We conduct extensive experiments to demonstrate the\neffectiveness of FADNet++ on both synthetic and realistic datasets among six\nGPU devices varying from server to mobile platforms. Experimental results show\nthat FADNet++ and its variants achieve state-of-the-art prediction accuracy,\nand run at a significant order of magnitude faster speed than existing 3D\nmodels. With the constraint of running at above 15 frames per second (FPS) on a\nmobile GPU, FADNet++ achieves a new state-of-the-art result for the SceneFlow\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaohuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shizhen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaiyong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on the Common Good: Group Distributional Robustness Follows. (arXiv:2110.02619v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02619","description":"<p>We consider the problem of training a classification model with group\nannotated training data. Recent work has established that, if there is\ndistribution shift across different groups, models trained using the standard\nempirical risk minimization (ERM) objective suffer from poor performance on\nminority groups and that group distributionally robust optimization (Group-DRO)\nobjective is a better alternative. The starting point of this paper is the\nobservation that though Group-DRO performs better than ERM on minority groups\nfor some benchmark datasets, there are several other datasets where it performs\nmuch worse than ERM. Inspired by ideas from the closely related problem of\ndomain generalization, this paper proposes a new and simple algorithm that\nexplicitly encourages learning of features that are shared across various\ngroups. The key insight behind our proposed algorithm is that while Group-DRO\nfocuses on groups with worst regularized loss, focusing instead, on groups that\nenable better performance even on other groups, could lead to learning of\nshared/common features, thereby enhancing minority performance beyond what is\nachieved by Group-DRO. Empirically, we show that our proposed algorithm matches\nor achieves better performance compared to strong contemporary baselines\nincluding ERM and Group-DRO on standard benchmarks on both minority groups and\nacross all groups. Theoretically, we show that the proposed algorithm is a\ndescent method and finds first order stationary points of smooth nonconvex\nfunctions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching. (arXiv:2110.02623v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02623","description":"<p>The task of image-text matching aims to map representations from different\nmodalities into a common joint visual-textual embedding. However, the most\nwidely used datasets for this task, MSCOCO and Flickr30K, are actually image\ncaptioning datasets that offer a very limited set of relationships between\nimages and sentences in their ground-truth annotations. This limited ground\ntruth information forces us to use evaluation metrics based on binary\nrelevance: given a sentence query we consider only one image as relevant.\nHowever, many other relevant images or captions may be present in the dataset.\nIn this work, we propose two metrics that evaluate the degree of semantic\nrelevance of retrieved items, independently of their annotated binary\nrelevance. Additionally, we incorporate a novel strategy that uses an image\ncaptioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be\noptimized in a standard triplet loss. By incorporating our formulation to\nexisting models, a \\emph{large} improvement is obtained in scenarios where\navailable training data is limited. We also demonstrate that the performance on\nthe annotated image-caption pairs is maintained while improving on other\nnon-annotated relevant items when employing the full training set. Code with\nour metrics and adaptive margin formulation will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mafla_A/0/1/0/all/0/1\">Andres Mafla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation. (arXiv:2110.02624v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02624","description":"<p>While recent progress has been made in text-to-image generation,\ntext-to-shape generation remains a challenging problem due to the\nunavailability of paired text and shape data at a large scale. We present a\nsimple yet effective method for zero-shot text-to-shape generation based on a\ntwo-stage training process, which only depends on an unlabelled shape dataset\nand a pre-trained image-text network such as CLIP. Our method not only\ndemonstrates promising zero-shot generalization, but also avoids expensive\ninference time optimization and can generate multiple shapes for a given text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph G. Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1\">Marco Fumero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02627","description":"<p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)\nis the latest frontier of e-fashion, referred to as \"video-to-shop\" in the\ncomputer vision literature. In this paper we present MovingFashion, the first\npublicly available dataset to cope with this challenge. MovingFashion is\ncomposed of 14855 social videos, each one of them associated to e-commerce\n\"shop\" images where the corresponding clothing items are clearly portrayed. In\naddition, we present a network for retrieving the shop images in this scenario,\ndubbed SEAM Match-RCNN. The model is trained by image-to-video domain\nadaptation, allowing to use video sequences where only their association with a\nshop image is given, eliminating the need of millions of annotated bounding\nboxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted\nsum of few frames (10) of a social video is enough to individuate the correct\nproduct within the first 5 retrieved items in a 14K+ shop element gallery with\nan accuracy of 80%. This provides the best performance on MovingFashion,\ncomparing exhaustively against the related state-of-the-art approaches and\nalternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godi_M/0/1/0/all/0/1\">Marco Godi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02636","description":"<p>Diffusion-based inpainting is a powerful tool for the reconstruction of\nimages from sparse data. Its quality strongly depends on the choice of known\ndata. Optimising their spatial location -- the inpainting mask -- is\nchallenging. A commonly used tool for this task are stochastic optimisation\nstrategies. However, they are slow as they compute multiple inpainting results.\nWe provide a remedy in terms of a learned mask generation model. By emulating\nthe complete inpainting pipeline with two networks for mask generation and\nneural surrogate inpainting, we obtain a model for highly efficient adaptive\nmask generation. Experiments indicate that our model can achieve competitive\nquality with an acceleration by as much as four orders of magnitude. Our\nfindings serve as a basis for making diffusion-based inpainting more attractive\nfor various applications such as image compression, where fast encoding is\nhighly desirable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2nd Place Solution to Google Landmark Recognition Competition 2021. (arXiv:2110.02638v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02638","description":"<p>As Transformer-based architectures have recently shown encouraging progresses\nin computer vision. In this work, we present the solution to the Google\nLandmark Recognition 2021 Challenge held on Kaggle, which is an improvement on\nour last year's solution by changing three designs, including (1) Using Swin\nand CSWin as backbone for feature extraction, (2) Train on full GLDv2, and (3)\nUsing full GLDv2 images as index image set for kNN search.\n</p>\n<p>With these modifications, our solution significantly improves last year\nsolution on this year competition. Our full pipeline, after ensembling Swin,\nCSWin, EfficientNet B7 models, scores 0.4907 on the private leaderboard which\nhelp us to get the 2nd place in the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shubin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weighted Generalized Coherence Approach for Sensing Matrix Design. (arXiv:2110.02645v1 [cs.IT])","link":"http://arxiv.org/abs/2110.02645","description":"<p>As compared to using randomly generated sensing matrices, optimizing the\nsensing matrix w.r.t. a carefully designed criterion is known to lead to better\nquality signal recovery given a set of compressive measurements. In this paper,\nwe propose generalizations of the well-known mutual coherence criterion for\noptimizing sensing matrices starting from random initial conditions. We term\nthese generalizations as bi-coherence or tri-coherence and they are based on a\ncriterion that discourages any one column of the sensing matrix from being\nclose to a sparse linear combination of other columns. We also incorporate\ntraining data to further improve the sensing matrices through weighted\ncoherence, weighted bi-coherence, or weighted tri-coherence criteria, which\nassign weights to sensing matrix columns as per their importance. An algorithm\nis also presented to solve the optimization problems. Finally, the\neffectiveness of the proposed algorithm is demonstrated through empirical\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anjarlekar_A/0/1/0/all/0/1\">Ameya Anjarlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajwade_A/0/1/0/all/0/1\">Ajit Rajwade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weak Novel Categories without Tears: A Survey on Weak-Shot Learning. (arXiv:2110.02651v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02651","description":"<p>Deep learning is a data-hungry approach, which requires massive training\ndata. However, it is time-consuming and labor-intensive to collect abundant\nfully-annotated training data for all categories. Assuming the existence of\nbase categories with adequate fully-annotated training samples, different\nparadigms requiring fewer training samples or weaker annotations for novel\ncategories have attracted growing research interest. Among them, zero-shot\n(resp., few-shot) learning explores using zero (resp., a few) training samples\nfor novel categories, which lowers the quantity requirement for novel\ncategories. Instead, weak-shot learning lowers the quality requirement for\nnovel categories. Specifically, sufficient training samples are collected for\nnovel categories but they only have weak annotations. In different tasks, weak\nannotations are presented in different forms (e.g., noisy labels for image\nclassification, image labels for object detection, bounding boxes for\nsegmentation), similar to the definitions in weakly supervised learning.\nTherefore, weak-shot learning can also be treated as weakly supervised learning\nwith auxiliary fully supervised categories. In this paper, we discuss the\nexisting weak-shot learning methodologies in different tasks and summarize the\ncodes at https://github.com/bcmi/Awesome-Weak-Shot-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robotic Knee Arthroscopy: Multi-Scale Network for Tissue-Tool Segmentation. (arXiv:2110.02657v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02657","description":"<p>Tissue awareness has a great demand to improve surgical accuracy in minimally\ninvasive procedures. In arthroscopy, it is one of the challenging tasks due to\nsurgical sites exhibit limited features and textures. Moreover, arthroscopic\nsurgical video shows high intra-class variations. Arthroscopic videos are\nrecorded with endoscope known as arthroscope which records tissue structures at\nproximity, therefore, frames contain minimal joint structure. As consequences,\nfully conventional network-based segmentation model suffers from long- and\nshort- term dependency problems. In this study, we present a densely connected\nshape aware multi-scale segmentation model which captures multi-scale features\nand integrates shape features to achieve tissue-tool segmentations. The model\nhas been evaluated with three distinct datasets. Moreover, with the publicly\navailable polyp dataset our proposed model achieved 5.09 % accuracy\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shahnewaz Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crawford_P/0/1/0/all/0/1\">Prof. Ross Crawford</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maire_D/0/1/0/all/0/1\">Dr. Frederic Maire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandey_A/0/1/0/all/0/1\">Assoc. Prof. Ajay K. Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S-Extension Patch: A simple and efficient way to extend an object detection model. (arXiv:2110.02670v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02670","description":"<p>While building convolutional network-based systems, the toll it takes to\ntrain the network is something that cannot be ignored. In cases where we need\nto append additional capabilities to the existing model, the attention\nimmediately goes towards retraining techniques. In this paper, I show how to\nleverage knowledge about the dataset to append the class faster while\nmaintaining the speed of inference as well as the accuracies; while reducing\nthe amount of time and data required. The method can extend a class in the\nexisting object detection model in 1/10th of the time compared to the other\nexisting methods. S-Extension patch not only offers faster training but also\nspeed and ease of adaptation, as it can be appended to any existing system,\ngiven it fulfills the similarity threshold condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tailed Distribution Adaptation. (arXiv:2110.02686v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02686","description":"<p>Recognizing images with long-tailed distributions remains a challenging\nproblem while there lacks an interpretable mechanism to solve this problem. In\nthis study, we formulate Long-tailed recognition as Domain Adaption (LDA), by\nmodeling the long-tailed distribution as an unbalanced domain and the general\ndistribution as a balanced domain. Within the balanced domain, we propose to\nslack the generalization error bound, which is defined upon the empirical risks\nof unbalanced and balanced domains and the divergence between them. We propose\nto jointly optimize empirical risks of the unbalanced and balanced domains and\napproximate their domain divergence by intra-class and inter-class distances,\nwith the aim to adapt models trained on the long-tailed distribution to general\ndistributions in an interpretable way. Experiments on benchmark datasets for\nimage recognition, object detection, and instance segmentation validate that\nour LDA approach, beyond its interpretability, achieves state-of-the-art\nperformance. Code is available at https://github.com/pengzhiliang/LDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zonghao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaosong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Objects in Semantic Topology. (arXiv:2110.02687v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02687","description":"<p>A more realistic object detection paradigm, Open-World Object Detection, has\narisen increasing research interests in the community recently. A qualified\nopen-world object detector can not only identify objects of known categories,\nbut also discover unknown objects, and incrementally learn to categorize them\nwhen their annotations progressively arrive. Previous works rely on independent\nmodules to recognize unknown categories and perform incremental learning,\nrespectively. In this paper, we provide a unified perspective: Semantic\nTopology. During the life-long learning of an open-world object detector, all\nobject instances from the same category are assigned to their corresponding\npre-defined node in the semantic topology, including the `unknown' category.\nThis constraint builds up discriminative feature representations and consistent\nrelationships among objects, thus enabling the detector to distinguish unknown\nobjects out of the known categories, as well as making learned features of\nknown objects undistorted when learning new categories incrementally. Extensive\nexperiments demonstrate that semantic topology, either randomly-generated or\nderived from a well-trained language model, could outperform the current\nstate-of-the-art open-world object detectors by a large margin, e.g., the\nabsolute open-set error is reduced from 7832 to 2546, exhibiting the inherent\nsuperiority of semantic topology on open-world object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reversible adversarial examples against local visual perturbation. (arXiv:2110.02700v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02700","description":"<p>Recently, studies have indicated that adversarial attacks pose a threat to\ndeep learning systems. However, when there are only adversarial examples,\npeople cannot get the original images, so there is research on reversible\nadversarial attacks. However, the existing strategies are aimed at invisible\nadversarial perturbation, and do not consider the case of locally visible\nadversarial perturbation. In this article, we generate reversible adversarial\nexamples for local visual adversarial perturbation, and use reversible data\nembedding technology to embed the information needed to restore the original\nimage into the adversarial examples to generate examples that are both\nadversarial and reversible. Experiments on ImageNet dataset show that our\nmethod can restore the original image losslessly while ensuring the attack\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shaowei Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models. (arXiv:2110.02711v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02711","description":"<p>Diffusion models are recent generative models that have shown great success\nin image generation with the state-of-the-art performance. However, only a few\nresearches have been conducted for image manipulation with diffusion models.\nHere, we present a novel DiffusionCLIP which performs text-driven image\nmanipulation with diffusion models using Contrastive Language-Image\nPre-training (CLIP) loss. Our method has a performance comparable to that of\nthe modern GAN-based image processing methods for in and out-of-domain image\nprocessing tasks, with the advantage of almost perfect inversion even without\nadditional encoders or optimization. Furthermore, our method can be easily used\nfor various novel applications, enabling image translation from an unseen\ndomain to another unseen domain or stroke-conditioned image generation in an\nunseen domain, etc. Finally, we present a novel multiple attribute control with\nDiffusionCLIPby combining multiple fine-tuned diffusion models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaDiS: Parallelly Distributable Slimmable Neural Networks. (arXiv:2110.02724v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02724","description":"<p>When several limited power devices are available, one of the most efficient\nways to make profit of these resources, while reducing the processing latency\nand communication load, is to run in parallel several neural sub-networks and\nto fuse the result at the end of processing. However, such a combination of\nsub-networks must be trained specifically for each particular configuration of\ndevices (characterized by number of devices and their capacities) which may\nvary over different model deployments and even within the same deployment. In\nthis work we introduce parallelly distributable slimmable (ParaDiS) neural\nnetworks that are splittable in parallel among various device configurations\nwithout retraining. While inspired by slimmable networks allowing instant\nadaptation to resources on just one device, ParaDiS networks consist of several\nmulti-device distributable configurations or switches that strongly share the\nparameters between them. We evaluate ParaDiS framework on MobileNet v1 and\nResNet-50 architectures on ImageNet classification task. We show that ParaDiS\nswitches achieve similar or better accuracy than the individual models, i.e.,\ndistributed models of the same structure trained individually. Moreover, we\nshow that, as compared to universally slimmable networks that are not\ndistributable, the accuracy of distributable ParaDiS switches either does not\ndrop at all or drops by a maximum of 1 % only in the worst cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozerov_A/0/1/0/all/0/1\">Alexey Ozerov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_A/0/1/0/all/0/1\">Anne Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraswamy_S/0/1/0/all/0/1\">Suresh Kirthi Kumaraswamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Step Towards Efficient Evaluation of Complex Perception Tasks in Simulation. (arXiv:2110.02739v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02739","description":"<p>There has been increasing interest in characterising the error behaviour of\nsystems which contain deep learning models before deploying them into any\nsafety-critical scenario. However, characterising such behaviour usually\nrequires large-scale testing of the model that can be extremely computationally\nexpensive for complex real-world tasks. For example, tasks involving compute\nintensive object detectors as one of their components. In this work, we propose\nan approach that enables efficient large-scale testing using simplified\nlow-fidelity simulators and without the computational cost of executing\nexpensive deep learning models. Our approach relies on designing an efficient\nsurrogate model corresponding to the compute intensive components of the task\nunder test. We demonstrate the efficacy of our methodology by evaluating the\nperformance of an autonomous driving task in the Carla simulator with reduced\ncomputational expense by training efficient surrogate models for PIXOR and\nCenterPoint LiDAR detectors, whilst demonstrating that the accuracy of the\nsimulation is maintained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_J/0/1/0/all/0/1\">Jonathan Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_B/0/1/0/all/0/1\">Blaine Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunn_J/0/1/0/all/0/1\">James Gunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_T/0/1/0/all/0/1\">Thomas Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samangooei_S/0/1/0/all/0/1\">Sina Samangooei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet Kumar Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1\">John Redford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Unsupervised Radar Place Recognition. (arXiv:2110.02744v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02744","description":"<p>We learn, in an unsupervised way, an embedding from sequences of radar images\nthat is suitable for solving the place recognition problem with complex radar\ndata. Our method is based on invariant instance feature learning but is\ntailored for the task of re-localisation by exploiting for data augmentation\nthe temporal successivity of data as collected by a mobile platform moving\nthrough the scene smoothly. We experiment across two prominent urban radar\ndatasets totalling over 400 km of driving and show that we achieve a new radar\nplace recognition state-of-the-art. Specifically, the proposed system proves\ncorrect for 98.38% of the queries that it is presented with over a challenging\nre-localisation sequence, using only the single nearest neighbour in the\nlearned metric space. We also find that our learned model shows better\nunderstanding of out-of-lane loop closures at arbitrary orientation than\nnon-learned radar scan descriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1\">Matthew Gadd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1\">Daniele De Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1\">Paul Newman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice. (arXiv:2110.02750v1 [cs.DS])","link":"http://arxiv.org/abs/2110.02750","description":"<p>The minimum graph cut and minimum $s$-$t$-cut problems are important\nprimitives in the modeling of combinatorial problems in computer science,\nincluding in computer vision and machine learning. Some of the most efficient\nalgorithms for finding global minimum cuts are randomized algorithms based on\nKarger's groundbreaking contraction algorithm. Here, we study whether Karger's\nalgorithm can be successfully generalized to other cut problems. We first prove\nthat a wide class of natural generalizations of Karger's algorithm cannot\nefficiently solve the $s$-$t$-mincut or the normalized cut problem to\noptimality. However, we then present a simple new algorithm for seeded\nsegmentation / graph-based semi-supervised learning that is closely based on\nKarger's original algorithm, showing that for these problems, extensions of\nKarger's algorithm can be useful. The new algorithm has linear asymptotic\nruntime and yields a potential that can be interpreted as the posterior\nprobability of a sample belonging to a given seed / class. We clarify its\nrelation to the random walker algorithm / harmonic energy minimization in terms\nof distributions over spanning forests. On classical problems from seeded image\nsegmentation and graph-based semi-supervised learning on image data, the method\nperforms at least as well as the random walker / harmonic energy minimization /\nGaussian processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanmartin_E/0/1/0/all/0/1\">Enrique Fita Sanmart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred A. Hamprecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Challenge of Appearance-Free Object Tracking with Feedforward Neural Networks. (arXiv:2110.02772v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02772","description":"<p>Nearly all models for object tracking with artificial neural networks depend\non appearance features extracted from a \"backbone\" architecture, designed for\nobject recognition. Indeed, significant progress on object tracking has been\nspurred by introducing backbones that are better able to discriminate objects\nby their appearance. However, extensive neurophysiology and psychophysics\nevidence suggests that biological visual systems track objects using both\nappearance and motion features. Here, we introduce $\\textit{PathTracker}$, a\nvisual challenge inspired by cognitive psychology, which tests the ability of\nobservers to learn to track objects solely by their motion. We find that\nstandard 3D-convolutional deep network models struggle to solve this task when\nclutter is introduced into the generated scenes, or when objects travel long\ndistances. This challenge reveals that tracing the path of object motion is a\nblind spot of feedforward neural networks. We expect that strategies for\nappearance-free object tracking from biological vision can inspire solutions\nthese failures of deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Girik Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1\">Drew Linsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingolla_E/0/1/0/all/0/1\">Ennio Mingolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIRe-Networks: Skip Connections over Interlaced Multi-Task Learning and Residual Connections for Structure Preserving Object Classification. (arXiv:2110.02776v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02776","description":"<p>Improving existing neural network architectures can involve several design\nchoices such as manipulating the loss functions, employing a diverse learning\nstrategy, exploiting gradient evolution at training time, optimizing the\nnetwork hyper-parameters, or increasing the architecture depth. The latter\napproach is a straightforward solution, since it directly enhances the\nrepresentation capabilities of a network; however, the increased depth\ngenerally incurs in the well-known vanishing gradient problem. In this paper,\nborrowing from different methods addressing this issue, we introduce an\ninterlaced multi-task learning strategy, defined SIRe, to reduce the vanishing\ngradient in relation to the object classification task. The presented\nmethodology directly improves a convolutional neural network (CNN) by enforcing\nthe input image structure preservation through interlaced auto-encoders, and\nfurther refines the base network architecture by means of skip and residual\nconnections. To validate the presented methodology, a simple CNN and various\nimplementations of famous networks are extended via the SIRe strategy and\nextensively tested on the CIFAR100 dataset; where the SIRe-extended\narchitectures achieve significantly increased performances across all models,\nthus confirming the presented approach effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study on Transfer Learning Capabilities for Pneumonia Classification in Chest-X-Rays Image. (arXiv:2110.02780v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02780","description":"<p>Over the last year, the severe acute respiratory syndrome coronavirus-2\n(SARS-CoV-2) and its variants have highlighted the importance of screening\ntools with high diagnostic accuracy for new illnesses such as COVID-19. To that\nregard, deep learning approaches have proven as effective solutions for\npneumonia classification, especially when considering chest-x-rays images.\nHowever, this lung infection can also be caused by other viral, bacterial or\nfungi pathogens. Consequently, efforts are being poured toward distinguishing\nthe infection source to help clinicians to diagnose the correct disease origin.\nFollowing this tendency, this study further explores the effectiveness of\nestablished neural network architectures on the pneumonia classification task\nthrough the transfer learning paradigm. To present a comprehensive comparison,\n12 well-known ImageNet pre-trained models were fine-tuned and used to\ndiscriminate among chest-x-rays of healthy people, and those showing pneumonia\nsymptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial\nsource. Furthermore, since a common public collection distinguishing between\nsuch categories is currently not available, two distinct datasets of\nchest-x-rays images, describing the aforementioned sources, were combined and\nemployed to evaluate the various architectures. The experiments were performed\nusing a total of 6330 images split between train, validation and test sets. For\nall models, common classification metrics were computed (e.g., precision,\nf1-score) and most architectures obtained significant performances, reaching,\namong the others, up to 84.46% average f1-score when discriminating the 4\nidentified classes. Moreover, confusion matrices and activation maps computed\nvia the Grad-CAM algorithm were also reported to present an informed discussion\non the networks classifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacciu_A/0/1/0/all/0/1\">Andrea Bacciu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marini_M/0/1/0/all/0/1\">Marco Raoul Marini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3rd Place Solution to Google Landmark Recognition Competition 2021. (arXiv:2110.02794v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02794","description":"<p>In this paper, we show our solution to the Google Landmark Recognition 2021\nCompetition. Firstly, embeddings of images are extracted via various\narchitectures (i.e. CNN-, Transformer- and hybrid-based), which are optimized\nby ArcFace loss. Then we apply an efficient pipeline to re-rank predictions by\nadjusting the retrieval score with classification logits and non-landmark\ndistractors. Finally, the ensembled model scores 0.489 on the private\nleaderboard, achieving the 3rd place in the 2021 edition of the Google Landmark\nRecognition Competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_T/0/1/0/all/0/1\">Tianling Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yanyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs. (arXiv:2110.02797v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02797","description":"<p>Convolutional Neural Networks (CNNs) have become the de facto gold standard\nin computer vision applications in the past years. Recently, however, new model\narchitectures have been proposed challenging the status quo. The Vision\nTransformer (ViT) relies solely on attention modules, while the MLP-Mixer\narchitecture substitutes the self-attention modules with Multi-Layer\nPerceptrons (MLPs). Despite their great success, CNNs have been widely known to\nbe vulnerable to adversarial attacks, causing serious concerns for\nsecurity-sensitive applications. Thus, it is critical for the community to know\nwhether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial\nattacks. To this end, we empirically evaluate their adversarial robustness\nunder several adversarial attack setups and benchmark them against the widely\nused CNNs. Overall, we find that the two architectures, especially ViT, are\nmore robust than their CNN models. Using a toy example, we also provide\nempirical evidence that the lower adversarial robustness of CNNs can be\npartially attributed to their shift-invariant property. Our frequency analysis\nsuggests that the most robust ViT architectures tend to rely more on\nlow-frequency features compared with CNNs. Additionally, we have an intriguing\nfinding that MLP-Mixer is extremely vulnerable to universal adversarial\nperturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benz_P/0/1/0/all/0/1\">Philipp Benz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_S/0/1/0/all/0/1\">Soomin Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karjauv_A/0/1/0/all/0/1\">Adil Karjauv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated First Order Methods for Variational Imaging. (arXiv:2110.02813v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02813","description":"<p>In this thesis, we offer a thorough investigation of different regularisation\nterms used in variational imaging problems, together with detailed optimisation\nprocesses of these problems. We begin by studying smooth problems and partially\nnon-smooth problems in the form of Tikhonov denoising and Total Variation (TV)\ndenoising, respectively.\n</p>\n<p>For Tikhonov denoising, we study an accelerated gradient method with adaptive\nrestart, which shows a very rapid convergence rate. However, it is not\nstraightforward to apply this fast algorithm to TV denoising, due to the\nnon-smoothness of its built-in regularisation. To tackle this issue, we propose\nto utilise duality to convert such a non-smooth problem into a smooth one so\nthat the accelerated gradient method with restart applies naturally.\n</p>\n<p>However, we notice that both Tikhonov and TV regularisations have drawbacks,\nin the form of blurred image edges and staircase artefacts, respectively. To\novercome these drawbacks, we propose a novel adaption to Total Generalised\nVariation (TGV) regularisation called Total Smooth Variation (TSV), which\nretains edges and meanwhile does not produce results which contain staircase\nartefacts. To optimise TSV effectively, we then propose the Accelerated\nProximal Gradient Algorithm (APGA) which also utilises adaptive restart\ntechniques. Compared to existing state-of-the-art regularisations (e.g. TV),\nTSV is shown to obtain more effective results on denoising problems as well as\nadvanced imaging applications such as magnetic resonance imaging (MRI)\nreconstruction and optical flow. TSV removes the staircase artefacts observed\nwhen using TV regularisation, but has the added advantage over TGV that it can\nbe efficiently optimised using gradient based methods with Nesterov\nacceleration and adaptive restart. Code is available at\nhttps://github.com/Jbartlett6/Accelerated-First-Order-Method-for-Variational-Imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartlett_J/0/1/0/all/0/1\">Joseph Bartlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinming Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Prediction: Which One Should Come First, Recognition or Prediction?. (arXiv:2110.02829v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02829","description":"<p>The ultimate goal of video prediction is not forecasting future pixel-values\ngiven some previous frames. Rather, the end goal of video prediction is to\ndiscover valuable internal representations from the vast amount of available\nunlabeled video data in a self-supervised fashion for downstream tasks. One of\nthe primary downstream tasks is interpreting the scene's semantic composition\nand using it for decision-making. For example, by predicting human movements,\nan observer can anticipate human activities and collaborate in a shared\nworkspace. There are two main ways to achieve the same outcome, given a\npre-trained video prediction and pre-trained semantic extraction model; one can\nfirst apply predictions and then extract semantics or first extract semantics\nand then predict. We investigate these configurations using the Local Frequency\nDomain Transformer Network (LFDTN) as the video prediction model and U-Net as\nthe semantic extraction model on synthetic and real datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farazi_H/0/1/0/all/0/1\">Hafez Farazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogga_J/0/1/0/all/0/1\">Jan Nogga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_a/0/1/0/all/0/1\">and Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shallow Features Guide Unsupervised Domain Adaptation for Semantic Segmentation at Class Boundaries. (arXiv:2110.02833v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02833","description":"<p>Although deep neural networks have achieved remarkable results for the task\nof semantic segmentation, they usually fail to generalize towards new domains,\nespecially when performing synthetic-to-real adaptation. Such domain shift is\nparticularly noticeable along class boundaries, invalidating one of the main\ngoals of semantic segmentation that consists in obtaining sharp segmentation\nmasks. In this work, we specifically address this core problem in the context\nof Unsupervised Domain Adaptation and present a novel low-level adaptation\nstrategy that allows us to obtain sharp predictions. Moreover, inspired by\nrecent self-training techniques, we introduce an effective data augmentation\nthat alleviates the noise typically present at semantic boundaries when\nemploying pseudo-labels for self-training. Our contributions can be easily\nintegrated into other popular adaptation frameworks, and extensive experiments\nshow that they effectively improve performance along class boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardace_A/0/1/0/all/0/1\">Adriano Cardace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1\">Pierluigi Zama Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHO-Hand Hygiene Gesture Classification System. (arXiv:2110.02842v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02842","description":"<p>The recent ongoing coronavirus pandemic highlights the importance of hand\nhygiene practices in our daily lives, with governments and worldwide health\nauthorities promoting good hand hygiene practices. More than one million cases\nof hospital-acquired infections occur in Europe annually. Hand hygiene\ncompliance may reduce the risk of transmission by reducing the number of\ninfections as well as healthcare expenditures. In this paper, the World Health\nOrganization, hand hygiene gestures are recorded and analyzed with the\nconstruction of an aluminum frame, placed at the laboratory sink. The hand\nhygiene gestures are recorded for thirty participants after conducting a\ntraining session about hand hygiene gestures demonstration. The video\nrecordings are converted into image files and are organized into six different\nhand hygiene classes. The Resnet50 framework selection for the classification\nof multiclass hand hygiene stages. The model is trained with the first set of\nclasses; Fingers Interlaced, P2PFingers Interlaced, and Rotational Rub for 25\nepochs. An accuracy of 44 percent for the first set of experiments with a loss\nscore greater than 1.5 in the validation set is achieved. The training steps\nfor the second set of classes; Rub hands palm to palm, Fingers Interlocked,\nThumb Rub are 50 epochs. An accuracy of 72 percent is achieved for the second\nset with a loss score of less than 0.8 for the validation set. In this work, a\npreliminary analysis of a robust hand hygiene dataset with transfer learning\ntakes place. The future aim for deploying a hand hygiene prediction system for\nhealthcare workers in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Identification of the End-Diastolic and End-Systolic Cardiac Frames from Invasive Coronary Angiography Videos. (arXiv:2110.02844v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02844","description":"<p>Automatic identification of proper image frames at the end-diastolic (ED) and\nend-systolic (ES) frames during the review of invasive coronary angiograms\n(ICA) is important to assess blood flow during a cardiac cycle, reconstruct the\n3D arterial anatomy from bi-planar views, and generate the complementary fusion\nmap with myocardial images. The current identification method primarily relies\non visual interpretation, making it not only time-consuming but also less\nreproducible. In this paper, we propose a new method to automatically identify\nangiographic image frames associated with the ED and ES cardiac phases by using\nthe trajectories of key vessel points (i.e. landmarks). More specifically, a\ndetection algorithm is first used to detect the key points of coronary\narteries, and then an optical flow method is employed to track the trajectories\nof the selected key points. The ED and ES frames are identified based on all\nthese trajectories. Our method was tested with 62 ICA videos from two separate\nmedical centers (22 and 9 patients in sites 1 and 2, respectively). Comparing\nconsensus interpretations by two human expert readers, excellent agreement was\nachieved by the proposed algorithm: the agreement rates within a one-frame\nrange were 92.99% and 92.73% for the automatic identification of the ED and ES\nimage frames, respectively. In conclusion, the proposed automated method showed\ngreat potential for being an integral part of automated ICA image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yinghui Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1\">Minghao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_X/0/1/0/all/0/1\">Xumin Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1\">Haipeng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jingfeng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu1_F/0/1/0/all/0/1\">Fubao Zhu1</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhihui Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weihua Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seed Classification using Synthetic Image Datasets Generated from Low-Altitude UAV Imagery. (arXiv:2110.02846v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02846","description":"<p>Plant breeding programs extensively monitor the evolution of seed kernels for\nseed certification, wherein lies the need to appropriately label the seed\nkernels by type and quality. However, the breeding environments are large where\nthe monitoring of seed kernels can be challenging due to the minuscule size of\nseed kernels. The use of unmanned aerial vehicles aids in seed monitoring and\nlabeling since they can capture images at low altitudes whilst being able to\naccess even the remotest areas in the environment. A key bottleneck in the\nlabeling of seeds using UAV imagery is drone altitude i.e. the classification\naccuracy decreases as the altitude increases due to lower image detail.\nConvolutional neural networks are a great tool for multi-class image\nclassification when there is a training dataset that closely represents the\ndifferent scenarios that the network might encounter during evaluation. The\narticle addresses the challenge of training data creation using Domain\nRandomization wherein synthetic image datasets are generated from a meager\nsample of seeds captured by the bottom camera of an autonomously driven Parrot\nAR Drone 2.0. Besides, the article proposes a seed classification framework as\na proof-of-concept using the convolutional neural networks of Microsoft's\nResNet-100, Oxford's VGG-16, and VGG-19. To enhance the classification accuracy\nof the framework, an ensemble model is developed resulting in an overall\naccuracy of 94.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margapuri_V/0/1/0/all/0/1\">Venkat Margapuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penumajji_N/0/1/0/all/0/1\">Niketa Penumajji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neilsen_M/0/1/0/all/0/1\">Mitchell Neilsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Convolutional Cross-Scale-Flows for Image-based Defect Detection. (arXiv:2110.02855v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02855","description":"<p>In industrial manufacturing processes, errors frequently occur at\nunpredictable times and in unknown manifestations. We tackle the problem of\nautomatic defect detection without requiring any image samples of defective\nparts. Recent works model the distribution of defect-free image data, using\neither strong statistical priors or overly simplified data representations. In\ncontrast, our approach handles fine-grained representations incorporating the\nglobal and local image context while flexibly estimating the density. To this\nend, we propose a novel fully convolutional cross-scale normalizing flow\n(CS-Flow) that jointly processes multiple feature maps of different scales.\nUsing normalizing flows to assign meaningful likelihoods to input samples\nallows for efficient defect detection on image-level. Moreover, due to the\npreserved spatial arrangement the latent space of the normalizing flow is\ninterpretable which enables to localize defective regions in the image. Our\nwork sets a new state-of-the-art in image-level defect detection on the\nbenchmark datasets Magnetic Tile Defects and MVTec AD showing a 100% AUROC on 4\nout of 15 classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Marco Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehrbein_T/0/1/0/all/0/1\">Tom Wehrbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Common Principal Subspace of Deep Features in Neural Networks. (arXiv:2110.02863v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02863","description":"<p>We find that different Deep Neural Networks (DNNs) trained with the same\ndataset share a common principal subspace in latent spaces, no matter in which\narchitectures (e.g., Convolutional Neural Networks (CNNs), Multi-Layer\nPreceptors (MLPs) and Autoencoders (AEs)) the DNNs were built or even whether\nlabels have been used in training (e.g., supervised, unsupervised, and\nself-supervised learning). Specifically, we design a new metric\n$\\mathcal{P}$-vector to represent the principal subspace of deep features\nlearned in a DNN, and propose to measure angles between the principal subspaces\nusing $\\mathcal{P}$-vectors. Small angles (with cosine close to $1.0$) have\nbeen found in the comparisons between any two DNNs trained with different\nalgorithms/architectures. Furthermore, during the training procedure from\nrandom scratch, the angle decrease from a larger one ($70^\\circ-80^\\circ$\nusually) to the small one, which coincides the progress of feature space\nlearning from scratch to convergence. Then, we carry out case studies to\nmeasure the angle between the $\\mathcal{P}$-vector and the principal subspace\nof training dataset, and connect such angle with generalization performance.\nExtensive experiments with practically-used Multi-Layer Perceptron (MLPs), AEs\nand CNNs for classification, image reconstruction, and self-supervised learning\ntasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our\nclaims with solid evidences.\n</p>\n<p>Interpretability of Deep Learning, Feature Learning, and Subspaces of Deep\nFeatures\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haozhe An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks. (arXiv:2110.02865v1 [cs.NE])","link":"http://arxiv.org/abs/2110.02865","description":"<p>Biological spiking neural networks (SNNs) can temporally encode information\nin their outputs, e.g. in the rank order in which neurons fire, whereas\nartificial neural networks (ANNs) conventionally do not. As a result, models of\nSNNs for neuromorphic computing are regarded as potentially more rapid and\nefficient than ANNs when dealing with temporal input. On the other hand, ANNs\nare simpler to train, and usually achieve superior performance. Here we show\nthat temporal coding such as rank coding (RC) inspired by SNNs can also be\napplied to conventional ANNs such as LSTMs, and leads to computational savings\nand speedups. In our RC for ANNs, we apply backpropagation through time using\nthe standard real-valued activations, but only from a strategically early time\nstep of each sequential input example, decided by a threshold-crossing event.\nLearning then incorporates naturally also _when_ to produce an output, without\nother changes to the model or the algorithm. Both the forward and the backward\ntraining pass can be significantly shortened by skipping the remaining input\nsequence after that first event. RC-training also significantly reduces\ntime-to-insight during inference, with a minimal decrease in accuracy. The\ndesired speed-accuracy trade-off is tunable by varying the threshold or a\nregularization parameter that rewards output entropy. We demonstrate these in\ntwo toy problems of sequence classification, and in a temporally-encoded MNIST\ndataset where our RC model achieves 99.19% accuracy after the first input\ntime-step, outperforming the state of the art in temporal coding with SNNs, as\nwell as in spoken-word classification of Google Speech Commands, outperforming\nnon-RC-trained early inference with LSTMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeffares_A/0/1/0/all/0/1\">Alan Jeffares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods. (arXiv:2110.02871v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02871","description":"<p>Climate change is a major threat to humanity, and the actions required to\nprevent its catastrophic consequences include changes in both policy-making and\nindividual behaviour. However, taking action requires understanding the effects\nof climate change, even though they may seem abstract and distant. Projecting\nthe potential consequences of extreme climate events such as flooding in\nfamiliar places can help make the abstract impacts of climate change more\nconcrete and encourage action. As part of a larger initiative to build a\nwebsite that projects extreme climate events onto user-chosen photos, we\npresent our solution to simulate photo-realistic floods on authentic images. To\naddress this complex task in the absence of suitable training data, we propose\nClimateGAN, a model that leverages both simulated and real data for\nunsupervised domain adaptation and conditional image generation. In this paper,\nwe describe the details of our framework, thoroughly evaluate components of our\narchitecture and demonstrate that our model is capable of robustly generating\nphoto-realistic flooding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_V/0/1/0/all/0/1\">Victor Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1\">M&#xe9;lisande Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_A/0/1/0/all/0/1\">Alexia Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghupathi_S/0/1/0/all/0/1\">Sunand Raghupathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosne_G/0/1/0/all/0/1\">Gautier Cosne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraver_A/0/1/0/all/0/1\">Adrien Juraver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardanyan_V/0/1/0/all/0/1\">Vahe Vardanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1\">Alex Hernandez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDA-GAN: Unsupervised Image Translation Using Spectral Domain Attention-Guided Generative Adversarial Network. (arXiv:2110.02873v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02873","description":"<p>This work introduced a novel GAN architecture for unsupervised image\ntranslation on the task of face style transform. A spectral attention-based\nmechanism is embedded into the design along with spatial attention on the image\ncontents. We proved that neural network has the potential of learning complex\ntransformations such as Fourier transform, within considerable computational\ncost. The model is trained and tested in comparison to the baseline model,\nwhich only uses spatial attention. The performance improvement of our approach\nis significant especially when the source and target domain include different\ncomplexity (reduced FID to 49.18 from 142.84). In the translation process, a\nspectra filling effect was introduced due to the implementation of FFT and\nspectral attention. Another style transfer task and real-world object\ntranslation are also studied in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarenko_M/0/1/0/all/0/1\">Maksim Makarenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Internal Learning. (arXiv:2110.02900v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02900","description":"<p>Internal learning for single-image generation is a framework, where a\ngenerator is trained to produce novel images based on a single image. Since\nthese models are trained on a single image, they are limited in their scale and\napplication. To overcome these issues, we propose a meta-learning approach that\nenables training over a collection of images, in order to model the internal\nstatistics of the sample image more effectively. In the presented meta-learning\napproach, a single-image GAN model is generated given an input image, via a\nconvolutional feedforward hypernetwork $f$. This network is trained over a\ndataset of images, allowing for feature sharing among different models, and for\ninterpolation in the space of generative models. The generated single-image\nmodel contains a hierarchy of multiple generators and discriminators. It is\ntherefore required to train the meta-learner in an adversarial manner, which\nrequires careful design choices that we justify by a theoretical analysis. Our\nresults show that the models obtained are as suitable as single-image GANs for\nmany common image applications, significantly reduce the training time per\nimage without loss in performance, and introduce novel capabilities, such as\ninterpolation and feedforward modeling of novel images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bensadoun_R/0/1/0/all/0/1\">Raphael Bensadoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_S/0/1/0/all/0/1\">Shir Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1\">Tomer Galanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIC_Cambridge-HuPBA-FBK Submission to the EPIC-Kitchens-100 Action Recognition Challenge 2021. (arXiv:2110.02902v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02902","description":"<p>This report presents the technical details of our submission to the\nEPIC-Kitchens-100 Action Recognition Challenge 2021. To participate in the\nchallenge we deployed spatio-temporal feature extraction and aggregation models\nwe have developed recently: GSF and XViT. GSF is an efficient spatio-temporal\nfeature extracting module that can be plugged into 2D CNNs for video action\nrecognition. XViT is a convolution free video feature extractor based on\ntransformer architecture. We design an ensemble of GSF and XViT model families\nwith different backbones and pretraining to generate the prediction scores. Our\nsubmission, visible on the public leaderboard, achieved a top-1 action\nrecognition accuracy of 44.82%, using only RGB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Swathikiran Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1\">Juan-Manuel Perez-Rua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1\">Alex Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grasp-Oriented Fine-grained Cloth Segmentation without Real Supervision. (arXiv:2110.02903v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02903","description":"<p>Automatically detecting graspable regions from a single depth image is a key\ningredient in cloth manipulation. The large variability of cloth deformations\nhas motivated most of the current approaches to focus on identifying specific\ngrasping points rather than semantic parts, as the appearance and depth\nvariations of local regions are smaller and easier to model than the larger\nones. However, tasks like cloth folding or assisted dressing require\nrecognising larger segments, such as semantic edges that carry more information\nthan points. The first goal of this paper is therefore to tackle the problem of\nfine-grained region detection in deformed clothes using only a depth image. As\na proof of concept, we implement an approach for T-shirts, and define up to 6\nsemantic regions of varying extent, including edges on the neckline, sleeve\ncuffs, and hem, plus top and bottom grasping points. We introduce a U-net based\nnetwork to segment and label these parts. The second contribution of our work\nis concerned with the level of supervision that we require to train the\nproposed network. While most approaches learn to detect grasping points by\ncombining real and synthetic annotations, in this work we defy the limitations\nof the synthetic data, and propose a multilayered domain adaptation (DA)\nstrategy that does not use real annotations at all. We thoroughly evaluate our\napproach on real depth images of a T-shirt annotated with fine-grained labels.\nWe show that training our network solely with synthetic data and the proposed\nDA yields results competitive with models trained on real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruijie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_M/0/1/0/all/0/1\">Mohit Gurnani Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Riera_J/0/1/0/all/0/1\">Jordi Sanchez-Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yurun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiris_Y/0/1/0/all/0/1\">Yiannis Demiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Capsule Networks from the Cloud to the Deep Edge. (arXiv:2110.02911v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02911","description":"<p>Capsule networks (CapsNets) are an emerging trend in image processing. In\ncontrast to a convolutional neural network, CapsNets are not vulnerable to\nobject deformation, as the relative spatial information of the objects is\npreserved across the network. However, their complexity is mainly related with\nthe capsule structure and the dynamic routing mechanism, which makes it almost\nunreasonable to deploy a CapsNet, in its original form, in a\nresource-constrained device powered by a small microcontroller (MCU). In an era\nwhere intelligence is rapidly shifting from the cloud to the edge, this high\ncomplexity imposes serious challenges to the adoption of CapsNets at the very\nedge. To tackle this issue, we present an API for the execution of quantized\nCapsNets in Cortex-M and RISC-V MCUs. Our software kernels extend the Arm\nCMSIS-NN and RISC-V PULP-NN, to support capsule operations with 8-bit integers\nas operands. Along with it, we propose a framework to perform post training\nquantization of a CapsNet. Results show a reduction in memory footprint of\nalmost 75%, with a maximum accuracy loss of 1%. In terms of throughput, our\nsoftware kernels for the Arm Cortex-M are, at least, 5.70x faster than a\npre-quantized CapsNet running on an NVIDIA GTX 980 Ti graphics card. For\nRISC-V, the throughout gain increases to 26.28x and 56.91x for a single- and\nocta-core configuration, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1\">Miguel Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diogo Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_T/0/1/0/all/0/1\">Tiago Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_S/0/1/0/all/0/1\">Sandro Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting RANSAC via Dual Principal Component Pursuit. (arXiv:2110.02918v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02918","description":"<p>In this paper, we revisit the problem of local optimization in RANSAC. Once a\nso-far-the-best model has been found, we refine it via Dual Principal Component\nPursuit (DPCP), a robust subspace learning method with strong theoretical\nsupport and efficient algorithms. The proposed DPCP-RANSAC has far fewer\nparameters than existing methods and is scalable. Experiments on estimating\ntwo-view homographies, fundamental and essential matrices, and three-view\nhomographic tensors using large-scale datasets show that our approach\nconsistently has higher accuracy than state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunchen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianjiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_D/0/1/0/all/0/1\">Daniel P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Rene Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1\">Manolis C. Tsakiris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision. (arXiv:2110.02929v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02929","description":"<p>Event-based sensing using dynamic vision sensors is gaining traction in\nlow-power vision applications. Spiking neural networks work well with the\nsparse nature of event-based data and suit deployment on low-power neuromorphic\nhardware. Being a nascent field, the sensitivity of spiking neural networks to\npotentially malicious adversarial attacks has received very little attention so\nfar. In this work, we show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and to\nthe continuous-time setting of spiking neural networks. We test our methods on\nthe N-MNIST and IBM Gestures neuromorphic vision datasets and show adversarial\nperturbations achieve a high success rate, by injecting a relatively small\nnumber of appropriately placed events. We also verify, for the first time, the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations and possible\nfuture directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1\">Julian B&#xfc;chel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yalun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheik_S/0/1/0/all/0/1\">Sadique Sheik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorbaro_M/0/1/0/all/0/1\">Martino Sorbaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Cropped versus Uncropped Training Sets in Tabular Structure Detection. (arXiv:2110.02933v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02933","description":"<p>Automated document processing for tabular information extraction is highly\ndesired in many organizations, from industry to government. Prior works have\naddressed this problem under table detection and table structure detection\ntasks. Proposed solutions leveraging deep learning approaches have been giving\npromising results in these tasks. However, the impact of dataset structures on\ntable structure detection has not been investigated. In this study, we provide\na comparison of table structure detection performance with cropped and\nuncropped datasets. The cropped set consists of only table images that are\ncropped from documents assuming tables are detected perfectly. The uncropped\nset consists of regular document images. Experiments show that deep learning\nmodels can improve the detection performance by up to 9% in average precision\nand average recall on the cropped versions. Furthermore, the impact of cropped\nimages is negligible under the Intersection over Union (IoU) values of 50%-70%\nwhen compared to the uncropped versions. However, beyond 70% IoU thresholds,\ncropped datasets provide significantly higher detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akkaya_Y/0/1/0/all/0/1\">Yakup Akkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1\">Murat Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1\">Burak Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shahzad Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topologically Consistent Multi-View Face Inference Using Volumetric Sampling. (arXiv:2110.02948v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02948","description":"<p>High-fidelity face digitization solutions often combine multi-view stereo\n(MVS) techniques for 3D reconstruction and a non-rigid registration step to\nestablish dense correspondence across identities and expressions. A common\nproblem is the need for manual clean-up after the MVS step, as 3D scans are\ntypically affected by noise and outliers and contain hairy surface regions that\nneed to be cleaned up by artists. Furthermore, mesh registration tends to fail\nfor extreme facial expressions. Most learning-based methods use an underlying\n3D morphable model (3DMM) to ensure robustness, but this limits the output\naccuracy for extreme facial expressions. In addition, the global bottleneck of\nregression architectures cannot produce meshes that tightly fit the ground\ntruth surfaces. We propose ToFu, Topologically consistent Face from multi-view,\na geometry inference framework that can produce topologically consistent meshes\nacross facial identities and expressions using a volumetric representation\ninstead of an explicit underlying 3DMM. Our novel progressive mesh generation\nnetwork embeds the topological structure of the face in a feature volume,\nsampled from geometry-aware local features. A coarse-to-fine architecture\nfacilitates dense and accurate facial mesh predictions in a consistent mesh\ntopology. ToFu further captures displacement maps for pore-level geometric\ndetails and facilitates high-quality rendering in the form of albedo and\nspecular reflectance maps. These high-quality assets are readily usable by\nproduction studios for avatar creation, animation and physically-based skin\nrendering. We demonstrate state-of-the-art geometric and correspondence\naccuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices,\nwhich is three orders of magnitude faster than traditional techniques. The code\nand the model are available for research purposes at\nhttps://tianyeli.github.io/tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yajie Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Autoencoder: self-supervised disentanglement of static 3D structure and motion. (arXiv:2110.02951v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02951","description":"<p>A video autoencoder is proposed for learning disentan- gled representations\nof 3D structure and camera pose from videos in a self-supervised manner.\nRelying on temporal continuity in videos, our work assumes that the 3D scene\nstructure in nearby video frames remains static. Given a sequence of video\nframes as input, the video autoencoder extracts a disentangled representation\nof the scene includ- ing: (i) a temporally-consistent deep voxel feature to\nrepresent the 3D structure and (ii) a 3D trajectory of camera pose for each\nframe. These two representations will then be re-entangled for rendering the\ninput video frames. This video autoencoder can be trained directly using a\npixel reconstruction loss, without any ground truth 3D or camera pose\nannotations. The disentangled representation can be applied to a range of\ntasks, including novel view synthesis, camera pose estimation, and video\ngeneration by motion following. We evaluate our method on several large- scale\nnatural video datasets, and show generalization results on out-of-domain\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zihang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning. (arXiv:1912.07773v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.07773","description":"<p>Inspired by human visual attention, we propose a novel inverse reinforcement\nlearning formulation using Maximum Entropy Deep Inverse Reinforcement Learning\n(MEDIRL) for predicting the visual attention of drivers in accident-prone\nsituations. MEDIRL predicts fixation locations that lead to maximal rewards by\nlearning a task-sensitive reward function from eye fixation patterns recorded\nfrom attentive drivers. Additionally, we introduce EyeCar, a new driver\nattention dataset in accident-prone situations. We conduct comprehensive\nexperiments to evaluate our proposed model on three common benchmarks:\n(DR(eye)VE, BDD-A, DADA-2000), and our EyeCar dataset. Results indicate that\nMEDIRL outperforms existing models for predicting attention and achieves\nstate-of-the-art performance. We present extensive ablation studies to provide\nmore insights into different features of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baee_S/0/1/0/all/0/1\">Sonia Baee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakdamanian_E/0/1/0/all/0/1\">Erfan Pakdamanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Inki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1\">Laura Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORPH-DSLAM: Model Order Reduction for PHysics-based Deformable SLAM. (arXiv:2009.00576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.00576","description":"<p>We propose a new methodology to estimate the 3D displacement field of\ndeformable objects from video sequences using standard monocular cameras. We\nsolve in real time the complete (possibly visco-)hyperelasticity problem to\nproperly describe the strain and stress fields that are consistent with the\ndisplacements captured by the images, constrained by real physics. We do not\nimpose any ad-hoc prior or energy minimization in the external surface, since\nthe real and complete mechanics problem is solved. This means that we can also\nestimate the internal state of the objects, even in occluded areas, just by\nobserving the external surface and the knowledge of material properties and\ngeometry. Solving this problem in real time using a realistic constitutive law,\nusually non-linear, is out of reach for current systems. To overcome this\ndifficulty, we solve off-line a parametrized problem that considers each source\nof variability in the problem as a new parameter and, consequently, as a new\ndimension in the formulation. Model Order Reduction methods allow us to reduce\nthe dimensionality of the problem, and therefore, its computational cost, while\npreserving the visualization of the solution in the high-dimensionality space.\nThis allows an accurate estimation of the object deformations, improving also\nthe robustness in the 3D points estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_I/0/1/0/all/0/1\">Iciar Alfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-Based Clustering for Enhancing Image Classification Architectures. (arXiv:2011.04728v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04728","description":"<p>Convolutional networks are at the center of best-in-class computer vision\napplications for a wide assortment of undertakings. Since 2014, a profound\namount of work began to make better convolutional architectures, yielding\ngenerous additions in different benchmarks. Albeit expanded model size and\ncomputational cost will, in general, mean prompt quality increases for most\nundertakings but, the architectures now need to have some additional\ninformation to increase the performance. I show evidence that with the\namalgamation of content-based image similarity and deep learning models, we can\nprovide the flow of information which can be used in making clustered learning\npossible. The paper shows how training of sub-dataset clusters not only reduces\nthe cost of computation but also increases the speed of evaluating and tuning a\nmodel on the given dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Gabor modulated complex-valued networks for orientation robustness. (arXiv:2011.11734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11734","description":"<p>Robustness to transformation is desirable in many computer vision tasks,\ngiven that input data often exhibits pose variance. While translation\ninvariance and equivariance is a documented phenomenon of CNNs, sensitivity to\nother transformations is typically encouraged through data augmentation. We\ninvestigate the modulation of complex valued convolutional weights with learned\nGabor filters to enable orientation robustness. The resulting network can\ngenerate orientation dependent features free of interpolation with a single set\nof learnable rotation-governing parameters. By choosing to either retain or\npool orientation channels, the choice of equivariance versus invariance can be\ndirectly controlled. Moreover, we introduce rotational weight-tying through a\nproposed cyclic Gabor convolution, further enabling generalisation over\nrotations. We combine these innovations into Learnable Gabor Convolutional\nNetworks (LGCNs), that are parameter-efficient and offer increased model\ncomplexity. We demonstrate their rotation invariance and equivariance on MNIST,\nBSD and a dataset of simulated and real astronomical images of Galactic cirri.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richards_F/0/1/0/all/0/1\">Felix Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiement_A/0/1/0/all/0/1\">Adeline Paiement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sola_E/0/1/0/all/0/1\">Elisabeth Sola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duc_P/0/1/0/all/0/1\">Pierre-Alain Duc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$DA^3$:Dynamic Additive Attention Adaption for Memory-EfficientOn-Device Multi-Domain Learning. (arXiv:2012.01362v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01362","description":"<p>Nowadays, one practical limitation of deep neural network (DNN) is its high\ndegree of specialization to a single task or domain (e.g., one visual domain).\nIt motivates researchers to develop algorithms that can adapt DNN model to\nmultiple domains sequentially, while still performing well on the past domains,\nwhich is known as multi-domain learning. Almost all conventional methods only\nfocus on improving accuracy with minimal parameter update, while ignoring high\ncomputing and memory cost during training, which makes it difficult to deploy\nmulti-domain learning into more and more widely used resource-limited edge\ndevices, like mobile phones, IoT, embedded systems, etc. We observe that large\nmemory used for activation storage is the bottleneck that largely limits the\ntraining time and cost on edge devices. To reduce training memory usage, while\nkeeping the domain adaption accuracy performance, we propose Dynamic Additive\nAttention Adaption ($DA^3$), a novel memory-efficient on-device multi-domain\nlearning method. $DA^3$ learns a novel additive attention adaptor module, while\nfreezing the weights of the pre-trained backbone model for each domain.\nDifferentiating from prior works, such module not only mitigates activation\nmemory buffering for reducing memory usage during training but also serves as a\ndynamic gating mechanism to reduce the computation cost for fast inference. We\nvalidate $DA^3$ on multiple datasets against state-of-the-art methods, which\nshows great improvement in both accuracy and training time. Moreover, we\ndeployed $DA^3$ into the popular NIVDIA Jetson Nano edge GPU, where the\nmeasured experimental results show our proposed $DA^3$ reduces the on-device\ntraining memory consumption by 19-37x, and training time by 2x, in comparison\nto the baseline methods (e.g., standard fine-tuning, Parallel and Series Res.\nadaptor, and Piggyback).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakin_A/0/1/0/all/0/1\">Adnan Siraj Rakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deliang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs. (arXiv:2012.02821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02821","description":"<p>Multilabel conditional image generation is a challenging problem in computer\nvision. In this work we propose Multi-ingredient Pizza Generator (MPG), a\nconditional Generative Neural Network (GAN) framework for synthesizing\nmultilabel images. We design MPG based on a state-of-the-art GAN structure\ncalled StyleGAN2, in which we develop a new conditioning technique by enforcing\nintermediate feature maps to learn scalewise label information. Because of the\ncomplex nature of the multilabel image generation problem, we also regularize\nsynthetic image by predicting the corresponding ingredients as well as\nencourage the discriminator to distinguish between matched image and mismatched\nimage. To verify the efficacy of MPG, we test it on Pizza10, which is a\ncarefully annotated multi-ingredient pizza image dataset. MPG can successfully\ngenerate photo-realist pizza images with desired ingredients. The framework can\nbe easily extend to other multilabel image generation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guoyao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Robust Unpaired Image Translation for Data with Unmatched Semantics Statistics. (arXiv:2012.04932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04932","description":"<p>Many applications of unpaired image-to-image translation require the input\ncontents to be preserved semantically during translations. Unaware of the\ninherently unmatched semantics distributions between source and target domains,\nexisting distribution matching methods (i.e., GAN-based) can give undesired\nsolutions. In particular, although producing visually reasonable outputs, the\nlearned models usually flip the semantics of the inputs. To tackle this without\nusing extra supervision, we propose to enforce the translated outputs to be\nsemantically invariant w.r.t. small perceptual variations of the inputs, a\nproperty we call \"semantic robustness\". By optimizing a robustness loss w.r.t.\nmulti-scale feature space perturbations of the inputs, our method effectively\nreduces semantics flipping and produces translations that outperform existing\nmethods both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bodi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_D/0/1/0/all/0/1\">David Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhiqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Disentanglement of Structured Latent Representations. (arXiv:2101.04041v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.04041","description":"<p>We introduce the first metric for evaluating disentanglement at individual\nhierarchy levels of a structured latent representation. Applied to\nobject-centric generative models, this offers a systematic, unified approach to\nevaluating (i) object separation between latent slots (ii) disentanglement of\nobject properties inside individual slots (iii) disentanglement of intrinsic\nand extrinsic object properties. We theoretically show that our framework gives\nstronger guarantees of selecting a good model than previous disentanglement\nmetrics. Experimentally, we demonstrate that viewing object compositionality as\na disentanglement problem addresses several issues with prior visual metrics of\nobject separation. As a core technical component, we present the first\nrepresentation probing algorithm handling slot permutation invariance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Nhu_R/0/1/0/all/0/1\">Rapha&#xeb;l Dang-Nhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05260","description":"<p>In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1\">Ritesh Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Text-to-Image Synthesis: A Review. (arXiv:2101.09983v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09983","description":"<p>With the advent of generative adversarial networks, synthesizing images from\ntextual descriptions has recently become an active research area. It is a\nflexible and intuitive way for conditional image generation with significant\nprogress in the last years regarding visual realism, diversity, and semantic\nalignment. However, the field still faces several challenges that require\nfurther research efforts such as enabling the generation of high-resolution\nimages with multiple objects, and developing suitable and reliable evaluation\nmetrics that correlate with human judgement. In this review, we contextualize\nthe state of the art of adversarial text-to-image synthesis models, their\ndevelopment since their inception five years ago, and propose a taxonomy based\non the level of supervision. We critically examine current strategies to\nevaluate text-to-image synthesis models, highlight shortcomings, and identify\nnew areas of research, ranging from the development of better datasets and\nevaluation metrics to possible improvements in architectural design and model\ntraining. This review complements previous surveys on generative adversarial\nnetworks with a focus on text-to-image synthesis which we believe will help\nresearchers to further advance the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_T/0/1/0/all/0/1\">Tobias Hinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs. (arXiv:2102.02828v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02828","description":"<p>Convolutional neural networks (CNNs) constructed natively on the sphere have\nbeen developed recently and shown to be highly effective for the analysis of\nspherical data. While an efficient framework has been formulated, spherical\nCNNs are nevertheless highly computationally demanding; typically they cannot\nscale beyond spherical signals of thousands of pixels. We develop scattering\nnetworks constructed natively on the sphere that provide a powerful\nrepresentational space for spherical data. Spherical scattering networks are\ncomputationally scalable and exhibit rotational equivariance, while their\nrepresentational space is invariant to isometries and provides efficient and\nstable signal representations. By integrating scattering networks as an\nadditional type of layer in the generalized spherical CNN framework, we show\nhow they can be leveraged to scale spherical CNNs to the high-resolution data\ntypical of many practical applications, with spherical signals of many tens of\nmegapixels and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McEwen_J/0/1/0/all/0/1\">Jason D. McEwen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_C/0/1/0/all/0/1\">Christopher G. R. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavor_Parker_A/0/1/0/all/0/1\">Augustine N. Mavor-Parker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Models Are More Interpretable Because Attributions Look Normal. (arXiv:2103.11257v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.11257","description":"<p>Recent work has found that adversarially-robust deep networks used for image\nclassification are more interpretable: their feature attributions tend to be\nsharper, and are more concentrated on the objects associated with the image's\nground-truth class. We show that smooth decision boundaries play an important\nrole in this enhanced interpretability, as the model's input gradients around\ndata points will more closely align with boundaries' normal vectors when they\nare smooth. Thus, because robust models have smoother boundaries, the results\nof gradient-based attribution methods, like Integrated Gradients and DeepLift,\nwill capture more accurate information about nearby decision boundaries. This\nunderstanding of robust interpretability leads to our second contribution:\n\\emph{boundary attributions}, which aggregate information about the normal\nvectors of local decision boundaries to explain a classification outcome. We\nshow that by leveraging the key factors underpinning robust interpretability,\nboundary attributions produce sharper, more concentrated visual explanations --\neven on non-robust models. Any example implementation can be found at\n\\url{https://github.com/zifanw/boundary}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization for Arbitrary-Oriented Object Detection via Representation Invariance Loss. (arXiv:2103.11636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11636","description":"<p>Arbitrary-oriented objects exist widely in natural scenes, and thus the\noriented object detection has received extensive attention in recent years. The\nmainstream rotation detectors use oriented bounding boxes (OBB) or\nquadrilateral bounding boxes (QBB) to represent the rotating objects. However,\nthese methods suffer from the representation ambiguity for oriented object\ndefinition, which leads to suboptimal regression optimization and the\ninconsistency between the loss metric and the localization accuracy of the\npredictions. In this paper, we propose a Representation Invariance Loss (RIL)\nto optimize the bounding box regression for the rotating objects. Specifically,\nRIL treats multiple representations of an oriented object as multiple\nequivalent local minima, and hence transforms bounding box regression into an\nadaptive matching process with these local minima. Then, the Hungarian matching\nalgorithm is adopted to obtain the optimal regression strategy. We also propose\na normalized rotation loss to alleviate the weak correlation between different\nvariables and their unbalanced loss contribution in OBB representation.\nExtensive experiments on remote sensing datasets and scene text datasets show\nthat our method achieves consistent and substantial improvement. The source\ncode and trained models are available at https://github.com/ming71/RIDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1\">Lingjuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yunpeng Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Sketch Tensor Space for Image Inpainting of Man-made Scenes. (arXiv:2103.15087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15087","description":"<p>This paper studies the task of inpainting man-made scenes. It is very\nchallenging due to the difficulty in preserving the visual patterns of images,\nsuch as edges, lines, and junctions. Especially, most previous works are failed\nto restore the object/building structures for images of man-made scenes. To\nthis end, this paper proposes learning a Sketch Tensor (ST) space for\ninpainting man-made scenes. Such a space is learned to restore the edges,\nlines, and junctions in images, and thus makes reliable predictions of the\nholistic image structures. To facilitate the structure refinement, we propose a\nMulti-scale Sketch Tensor inpainting (MST) network, with a novel\nencoder-decoder structure. The encoder extracts lines and edges from the input\nimages to project them into an ST space. From this space, the decoder is\nlearned to restore the input images. Extensive experiments validate the\nefficacy of our model. Furthermore, our model can also achieve competitive\nperformance in inpainting general nature images over the competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions. (arXiv:2104.00820v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.00820","description":"<p>Recent research has shown that it is possible to find interpretable\ndirections in the latent spaces of pre-trained Generative Adversarial Networks\n(GANs). These directions enable controllable image generation and support a\nwide range of semantic editing operations, such as zoom or rotation. The\ndiscovery of such directions is often done in a supervised or semi-supervised\nmanner and requires manual annotations which limits their use in practice. In\ncomparison, unsupervised discovery allows finding subtle directions that are\ndifficult to detect a priori. In this work, we propose a contrastive\nlearning-based approach to discover semantic directions in the latent space of\npre-trained GANs in a self-supervised manner. Our approach finds semantically\nmeaningful dimensions comparable with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuksel_O/0/1/0/all/0/1\">O&#x11f;uz Kaan Y&#xfc;ksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsar_E/0/1/0/all/0/1\">Enis Simsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_E/0/1/0/all/0/1\">Ezgi G&#xfc;lperi Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Stealthy Adversarial Attacks against Segmentation Models. (arXiv:2104.01732v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01732","description":"<p>Segmentation models have been found to be vulnerable to targeted/non-targeted\nadversarial attacks. However, damaged predictions make it easy to unearth an\nattack. In this paper, we propose semantically stealthy adversarial attacks\nwhich can manipulate targeted labels as designed and preserve non-targeted\nlabels at the same time. In this way, we may hide the corresponding attack\nbehaviors. One challenge is making semantically meaningful manipulations across\ndatasets/models. Another challenge is avoiding damaging non-targeted labels. To\nsolve the above challenges, we consider each input image as prior knowledge to\ngenerate perturbations. We also design a special regularizer to help extract\nfeatures. To evaluate our model's performance, we design three basic attack\ntypes, namely `vanishing into the context', `embedding fake labels', and\n`displacing target objects'. The experiments show that our stealthy adversarial\nmodel can attack segmentation models with a relatively high success rate on\nCityscapes, Mapillary, and BDD100K. Finally, our framework also shows good\ngeneralizations across datasets/models empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David J. Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relating Adversarially Robust Generalization to Flat Minima. (arXiv:2104.04448v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.04448","description":"<p>Adversarial training (AT) has become the de-facto standard to obtain models\nrobust against adversarial examples. However, AT exhibits severe robust\noverfitting: cross-entropy loss on adversarial examples, so-called robust loss,\ndecreases continuously on training examples, while eventually increasing on\ntest examples. In practice, this leads to poor robust generalization, i.e.,\nadversarial robustness does not generalize well to new examples. In this paper,\nwe study the relationship between robust generalization and flatness of the\nrobust loss landscape in weight space, i.e., whether robust loss changes\nsignificantly when perturbing weights. To this end, we propose average- and\nworst-case metrics to measure flatness in the robust loss landscape and show a\ncorrelation between good robust generalization and flatness. For example,\nthroughout training, flatness reduces significantly during overfitting such\nthat early stopping effectively finds flatter minima in the robust loss\nlandscape. Similarly, AT variants achieving higher adversarial robustness also\ncorrespond to flatter minima. This holds for many popular choices, e.g.,\nAT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled\nexamples, as well as simple regularization techniques, e.g., AutoAugment,\nweight decay or label noise. For fair comparison across these approaches, our\nflatness measures are specifically designed to be scale-invariant and we\nconduct extensive experiments to validate our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-rays with Radiomics using a Feedback Loop. (arXiv:2104.04968v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04968","description":"<p>Building a highly accurate predictive model for these tasks usually requires\na large number of manually annotated labels and pixel regions (bounding boxes)\nof abnormalities. However, it is expensive to acquire such annotations,\nespecially the bounding boxes. Recently, contrastive learning has shown strong\npromise in leveraging unlabeled natural images to produce highly generalizable\nand discriminative features. However, extending its power to the medical image\ndomain is under-explored and highly non-trivial, since medical images are much\nless amendable to data augmentations. In contrast, their prior knowledge, as\nwell as radiomic features, is often crucial. To bridge this gap, we propose an\nend-to-end semi-supervised knowledge-augmented contrastive learning framework,\nthat simultaneously performs disease classification and localization tasks. The\nkey knob of our framework is a unique positive sampling approach tailored for\nthe medical images, by seamlessly integrating radiomic features as a knowledge\naugmentation. Specifically, we first apply an image encoder to classify the\nchest X-rays and to generate the image features. We next leverage Grad-CAM to\nhighlight the crucial (abnormal) regions for chest X-rays (even when\nunannotated), from which we extract radiomic features. The radiomic features\nare then passed through another dedicated encoder to act as the positive sample\nfor the image features generated from the same chest X-ray. In this way, our\nframework constitutes a feedback loop for image and radiomic modality features\nto mutually reinforce each other. Their contrasting yields knowledge-augmented\nrepresentations that are both robust and interpretable. Extensive experiments\non the NIH Chest X-ray dataset demonstrate that our approach outperforms\nexisting baselines in both classification and localization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glicksberg_B/0/1/0/all/0/1\">Benjamin Glicksberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The hidden label-marginal biases of segmentation losses. (arXiv:2104.08717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08717","description":"<p>Most segmentation losses are arguably variants of the Cross-Entropy (CE) or\nDice losses. In the abundant segmentation literature, there is no clear\nconsensus as to which of these losses is a better choice, with varying\nperformances for each across different benchmarks and applications. In this\nwork, we develop a theoretical analysis that links these two types of losses,\nexposing their advantages and weaknesses. First, we provide a\nconstrained-optimization perspective showing that CE and Dice share a much\ndeeper connection than previously thought: They both decompose into\nlabel-marginal penalties and closely related ground-truth matching penalties.\nThen, we provide bound relationships and an information-theoretic analysis,\nwhich uncover hidden label-marginal biases: Dice has an intrinsic bias towards\nspecific extremely imbalanced solutions, whereas CE implicitly encourages the\nground-truth region proportions. Our theoretical results explain the wide\nexperimental evidence in the medical-imaging literature, whereby Dice losses\nbring improvements for imbalanced segmentation. It also explains why CE\ndominates natural-image problems with diverse class proportions, in which case\nDice might have difficulty adapting to different label-marginal distributions.\nBased on our theoretical analysis, we propose a principled and simple solution,\nwhich enables to control explicitly the label-marginal bias. Our loss\nintegrates CE with explicit ${\\cal L}_1$ regularization, which encourages label\nmarginals to match target class proportions, thereby mitigating class imbalance\nbut without losing generality. Comprehensive experiments and ablation studies\nover different losses and applications validate our theoretical analysis, as\nwell as the effectiveness of our explicit label-marginal regularizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbi_R/0/1/0/all/0/1\">Riadh Kobbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust 3D Cell Segmentation: Extending the View of Cellpose. (arXiv:2105.00794v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.00794","description":"<p>Increasing data set sizes of 3D microscopy imaging experiments demand for an\nautomation of segmentation processes to be able to extract meaningful\nbiomedical information. Due to the shortage of annotated 3D image data that can\nbe used for machine learning-based approaches, 3D segmentation approaches are\nrequired to be robust and to generalize well to unseen data. The Cellpose\napproach proposed by Stringer \\textit{et al.} \\cite{stringer2020} proved to be\nsuch a generalist approach for cell instance segmentation tasks. In this paper,\nwe extend the Cellpose approach to improve segmentation accuracy on 3D image\ndata and we further show how the formulation of the gradient maps can be\nsimplified while still being robust and reaching similar segmentation accuracy.\nThe code is publicly available and was integrated into two established\nopen-source applications that allow using the 3D extension of Cellpose without\nany programming knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_R/0/1/0/all/0/1\">Richard S. Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fast Partial Video Copy Detection Using KNN and Global Feature Database. (arXiv:2105.01713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01713","description":"<p>We propose a fast partial video copy detection framework in this paper. In\nthis framework all frame features of the reference videos are organized in a\nKNN searchable database. Instead of scanning all reference videos, the query\nvideo segment does a fast KNN search in the global feature database. The\nreturned results are used to generate a short list of candidate videos. A\nmodified temporal network is then used to localize the copy segment in the\ncandidate videos. We evaluate different choice of CNN features on the VCDB\ndataset. Our benchmark F1 score exceeds the state of the art by a big margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rushuai Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training with Rectified Rejection. (arXiv:2105.14785v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14785","description":"<p>Adversarial training (AT) is one of the most effective strategies for\npromoting model robustness, whereas even the state-of-the-art adversarially\ntrained models struggle to exceed 65% robust test accuracy on CIFAR-10 without\nadditional data, which is far from practical. A natural way to improve beyond\nthis accuracy bottleneck is to introduce a rejection option, where confidence\nis a commonly used certainty proxy. However, the vanilla confidence can\noverestimate the model certainty if the input is wrongly classified. To this\nend, we propose to use true confidence (T-Con) (i.e., predicted probability of\nthe true class) as a certainty oracle, and learn to predict T-Con by rectifying\nconfidence. Intriguingly, we prove that under mild conditions, a rectified\nconfidence (R-Con) rejector and a confidence rejector can be coupled to\ndistinguish any wrongly classified input from correctly classified ones. We\nalso quantify that training R-Con to be aligned with T-Con could be an easier\ntask than learning robust classifiers. In our experiments, we evaluate our\nrectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 under\nseveral attacks, and demonstrate that the RR module is well compatible with\ndifferent AT frameworks on improving robustness, with little extra computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.02229","description":"<p>Recently, Differentiable Architecture Search (DARTS) has become one of the\nmost popular Neural Architecture Search (NAS) methods successfully applied in\nsupervised learning (SL). However, its applications in other domains, in\nparticular for reinforcement learning (RL), has seldom been studied. This is\ndue in part to RL possessing a significantly different optimization paradigm\nthan SL, especially with regards to the notion of replay data, which is\ncontinually generated via inference in RL. In this paper, we introduce\nRL-DARTS, one of the first applications of end-to-end DARTS in RL to search for\nconvolutional cells, applied to the challenging, infinitely procedurally\ngenerated Procgen benchmark. We demonstrate that the benefits of DARTS become\namplified when applied to RL, namely search efficiency in terms of time and\ncompute, as well as simplicity in integration with complex preexisting RL code\nvia simply replacing the image encoder with a DARTS supernet, compatible with\nboth off-policy and on-policy RL algorithms. At the same time however, we\nprovide one of the first extensive studies of DARTS outside of the standard\nfixed dataset setting in SL via RL-DARTS. We show that throughout training, the\nsupernet gradually learns better cells, leading to alternative architectures\nwhich can be highly competitive against manually designed policies, but also\nverify previous design choices for RL policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yingjie Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daiyi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Summer Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1\">John D. Co-Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1\">Eugene Brevdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03719","description":"<p>Self-supervised learning has recently shown great potential in vision tasks\nthrough contrastive learning which aims to discriminate each image, or\ninstance, in the dataset. However, such instance-level learning ignores the\nsemantic relationship among instances and sometimes undesirably repels the\nanchor from the semantically similar samples, termed as \"false negatives\". In\nthis work, we show that the unfavorable effect from false negatives is more\nsignificant for the large-scale datasets with more semantic concepts. To\naddress the issue, we propose a novel self-supervised contrastive learning\nframework that incrementally detects and explicitly removes the false negative\nsamples. Specifically, following the training process, our method dynamically\ndetects increasing high-quality false negatives considering that the encoder\ngradually improves and the embedding space becomes more semantically\nstructural. Next, we discuss two strategies to explicitly remove the detected\nfalse negatives during contrastive learning. Extensive experiments show that\nour framework outperforms other self-supervised contrastive learning methods on\nmultiple benchmarks in a limited resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsai-Shien Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04144","description":"<p>Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet-Packet Powered Deepfake Image Detection. (arXiv:2106.09369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09369","description":"<p>As neural networks become able to generate realistic artificial images, they\nhave the potential to improve movies, music, video games and make the internet\nan even more creative and inspiring place. Yet, at the same time, the latest\ntechnology potentially enables new digital ways to lie. In response, the need\nfor a diverse and reliable method toolbox arises to identify artificial images\nand other content. Previous work primarily relies on pixel-space CNN or the\nFourier transform. To the best of our knowledge, synthesized fake image\nanalysis and detection methods based on a multi-scale wavelet representation,\nwhich is localized in both space and frequency, have been absent thus far. This\npaper proposes to learn a model for the detection of synthetic images based on\nthe wavelet-packet representation of natural and GAN-generated images. We\nevaluate our method on FFHQ, CelebA, and LSUN source identification problems\nand find improved or competitive performance. Our forensic classifier has a\nsmall network size and can be learned efficiently. Furthermore, a comparison of\nthe wavelet coefficients from these two sources of images allows an\ninterpretation and identifies significant differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolter_M/0/1/0/all/0/1\">Moritz Wolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanke_F/0/1/0/all/0/1\">Felix Blanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Charles Tapley Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1\">Jochen Garcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Visual Robustness by Causal Intervention. (arXiv:2106.09534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09534","description":"<p>Adversarial training is the de facto most promising defense against\nadversarial examples. Yet, its passive nature inevitably prevents it from being\nimmune to unknown attackers. To achieve a proactive defense, we need a more\nfundamental understanding of adversarial examples, beyond the popular bounded\nthreat model. In this paper, we provide a causal viewpoint of adversarial\nvulnerability: the cause is the spurious correlation ubiquitously existing in\nlearning, i.e., the confounding effect, where attackers are precisely\nexploiting these effects. Therefore, a fundamental solution for adversarial\nrobustness is by causal intervention. As these visual confounders are\nimperceptible in general, we propose to use the instrumental variable that\nachieves causal intervention without the need for confounder observation. We\nterm our robust training method as Causal intervention by instrumental Variable\n(CiiV). It's a causal regularization that 1) augments the image with multiple\nretinotopic centers and 2) encourages the model to learn causal features,\nrather than local confounding patterns, by favoring features linearly\nresponding to spatial interpolations. Extensive experiments on a wide spectrum\nof attackers and settings applied in CIFAR-10, CIFAR-100, and mini-ImageNet\ndemonstrate that CiiV is robust to adaptive attacks, including the recent\nAutoAttack. Besides, as a general causal regularization, it can be easily\nplugged into other methods to further boost the robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaihua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Correspondence Hallucination. (arXiv:2106.09711v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09711","description":"<p>Given a pair of partially overlapping source and target images and a keypoint\nin the source image, the keypoint's correspondent in the target image can be\neither visible, occluded or outside the field of view. Local feature matching\nmethods are only able to identify the correspondent's location when it is\nvisible, while humans can also hallucinate its location when it is occluded or\noutside the field of view through geometric reasoning. In this paper, we bridge\nthis gap by training a network to output a peaked probability distribution over\nthe correspondent's location, regardless of this correspondent being visible,\noccluded, or outside the field of view. We experimentally demonstrate that this\nnetwork is indeed able to hallucinate correspondences on pairs of images\ncaptured in scenes that were not seen at training-time. We also apply this\nnetwork to an absolute camera pose estimation problem and find it is\nsignificantly more robust than state-of-the-art local feature matching-based\ncompetitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Germain_H/0/1/0/all/0/1\">Hugo Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourmaud_G/0/1/0/all/0/1\">Guillaume Bourmaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of spherical decision\nsurfaces and operates on point clouds. Focusing on 3D geometry, we derive a 3D\nsteerability constraint for hypersphere neurons, which are obtained by\nconformal embedding of Euclidean space and have recently been revisited in the\ncontext of learning representations of point sets. Exploiting the rotational\nequivariance, we show how our model parameters are fully steerable at inference\ntime. We use a synthetic point set and real-world 3D skeleton data to show how\nthe proposed spherical filter banks enable making equivariant and, after online\noptimization, invariant class predictions for known point sets in unknown\norientations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Technical Document Classification. (arXiv:2106.14269v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14269","description":"<p>In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have only focused on processing text for\nclassification, whereas technical documents often contain multimodal\ninformation. This paper presents a novel multimodal deep learning architecture,\nTechDoc, for technical document classification, which utilizes three types of\ninformation, including natural language texts and descriptive images within\ndocuments and the associations among the documents. The architecture\nsynthesizes the convolutional neural network, recurrent neural network, and\ngraph neural network through an integrated multimodal training process. We\napplied the architecture to a large multimodal technical document database and\ntrained the model for classifying documents based on the hierarchical\nInternational Patent Classification system. Our results show that TechDoc\npresents a greater classification accuracy than the unimodal methods and other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Clothing as a Separate Layer for an Animatable Human Avatar. (arXiv:2106.14879v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14879","description":"<p>We have recently seen great progress in building photorealistic animatable\nfull-body codec avatars, but generating high-fidelity animation of clothing is\nstill difficult. To address these difficulties, we propose a method to build an\nanimatable clothed body avatar with an explicit representation of the clothing\non the upper body from multi-view captured videos. We use a two-layer mesh\nrepresentation to register each 3D scan separately with the body and clothing\ntemplates. In order to improve the photometric correspondence across different\nframes, texture alignment is then performed through inverse rendering of the\nclothing geometry and texture predicted by a variational autoencoder. We then\ntrain a new two-layer codec avatar with separate modeling of the upper clothing\nand the inner body layer. To learn the interaction between the body dynamics\nand clothing states, we use a temporal convolution network to predict the\nclothing latent code based on a sequence of input skeletal poses. We show\nphotorealistic animation output for three different actors, and demonstrate the\nadvantage of our clothed-body avatars over the single-layer avatars used in\nprevious work. We also show the benefit of an explicit clothing model that\nallows the clothing texture to be edited in the animation output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">He Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1\">Jessica Hodgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00166","description":"<p>There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Large Circular Kernels into CNNs through Neural Architecture Search. (arXiv:2107.02451v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02451","description":"<p>The square kernel is a standard unit for contemporary Convolutional Neural\nNetworks (CNNs), as it fits well on the tensor computation for the convolution\noperation. However, the retinal ganglion cells in the biological visual system\nhave approximately concentric receptive fields. Motivated by this observation,\nwe propose using the circular kernel with a concentric and isotropic receptive\nfield as an option for convolution operation. We first substitute the $3 \\times\n3$ square kernels with the corresponding circular kernels or our proposed\nintegrated kernels in the typical ResNet architecture, and the modified models\nafter training yield similar or even competitive performance. We then show the\nadvantages of large circular kernels over the corresponding square kernels in\nthat the difference and the improvement are more distinct. Hence, we speculate\nthat large circular kernels would help find advanced neural network models by\nthe Neural Architecture Search (NAS). To validate our hypothesis, we expand the\noperation space in several typical NAS methods with convolutions of large\ncircular kernels. Experimental results show that the searched new neural\nnetwork models contain large circular kernels and significantly outperform the\noriginal searched models. The additional empirical analysis also reveals that\nthe large circular kernel help the model to be more robust to rotated or\nsheared images due to its rotation invariance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopcroft_J/0/1/0/all/0/1\">John E. Hopcroft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis. (arXiv:2107.02790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02790","description":"<p>How would a static scene react to a local poke? What are the effects on other\nparts of an object if you could locally push it? There will be distinctive\nmovement, despite evident variations caused by the stochastic nature of our\nworld. These outcomes are governed by the characteristic kinematics of objects\nthat dictate their overall motion caused by a local interaction. Conversely,\nthe movement of an object provides crucial information about its underlying\ndistinctive kinematics and the interdependencies between its parts. This\ntwo-way relation motivates learning a bijective mapping between object\nkinematics and plausible future image sequences. Therefore, we propose iPOKE --\ninvertible Prediction of Object Kinematics -- that, conditioned on an initial\nframe and a local poke, allows to sample object kinematics and establishes a\none-to-one correspondence to the corresponding plausible videos, thereby\nproviding a controlled stochastic video synthesis. In contrast to previous\nworks, we do not generate arbitrary realistic videos, but provide efficient\ncontrol of movements, while still capturing the stochastic nature of our\nenvironment and the diversity of plausible outcomes it entails. Moreover, our\napproach can transfer kinematics onto novel object instances and is not\nconfined to particular object classes. Our project page is available at\nhttps://bit.ly/3dJN4Lf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorkenwald_M/0/1/0/all/0/1\">Michael Dorkenwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05747","description":"<p>State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, a modern objective\noptimization theory for WTA networks has been missing, except under very\nlimiting assumptions. Here we derive formally such a theory, based on\nbiologically plausible but generic ANN elements. Through Hebbian learning,\nnetwork parameters maintain a Bayesian generative model of the data. There is\nno supervisory loss function, but the network does minimize cross-entropy\nbetween its activations and the input distribution. The key is a \"soft\" WTA\nwhere there is no absolute \"hard\" winner neuron, and a specific type of\nHebbian-like plasticity of weights and biases. We confirm our theory in\npractice, where, in handwritten digit (MNIST) recognition, our Hebbian\nalgorithm, SoftHebb, minimizes cross-entropy without having access to it, and\noutperforms the more frequently used, hard-WTA-based method. Strikingly, it\neven outperforms supervised end-to-end backpropagation, under certain\nconditions. Specifically, in a two-layered network, SoftHebb outperforms\nbackpropagation when the training dataset is only presented once, when the\ntesting data is noisy, and under gradient-based adversarial attacks. Notably,\nadversarial attacks that confuse SoftHebb are also confusing to the human eye.\nFinally, the model can generate interpolations of objects from its input\ndistribution. All in all, SoftHebb extends Hebbian WTA theory with modern\nmachine learning tools, thus making these networks relevant to pertinent issues\nin deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1\">Dmitry Toichkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1\">Yansong Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters. (arXiv:2107.09783v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09783","description":"<p>In this paper, we focus on a less explored, but more realistic and complex\nproblem of domain adaptation in LiDAR semantic segmentation. There is a\nsignificant drop in performance of an existing segmentation model when training\n(source domain) and testing (target domain) data originate from different LiDAR\nsensors. To overcome this shortcoming, we propose an unsupervised domain\nadaptation framework that leverages unlabeled target domain data for\nself-supervision, coupled with an unpaired mask transfer strategy to mitigate\nthe impact of domain shifts. Furthermore, we introduce gated adapter modules\nwith a small number of parameters into the network to account for target\ndomain-specific information. Experiments adapting from both real-to-real and\nsynthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the\nsignificant improvement over prior arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1\">Mrigank Rochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1\">Shubhra Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1\">Eduardo R. Corral-Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabatchian_A/0/1/0/all/0/1\">Amir Nabatchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Self-Supervised Learning Can be Used for Fine-Grained Head Pose Estimation?. (arXiv:2108.04893v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04893","description":"<p>The cost of Head View point labels is the main hurdle in the improving of\nfine-grained Head Pose estimation algorithm. One solution to the lack of huge\nnumber of labels is using Self-Supervised Learning (SSL). SSL can extract good\nfeatures from unlabeled data for a downstream task. Accordingly, this article\nhas tried to answer a question: How Self-Supervised Learning (SSL) can be used\nfor Head Pose estimation? In general, there are two main approaches to use SSL:\n(1) Using it to pre-train the weights, (2) Leveraging SSL as an auxiliary task\nbesides of Supervised Learning (SL) in one training session. In this study, we\ncompared two approaches by designing a Hybrid Multi-Task Learning (HMTL)\narchitecture and assessing it with two SSL pre-text tasks, the rotation and\npuzzling. Results showed that the combination of both methods in which using\nrotation for pre-training and using puzzling for auxiliary head were the best.\nTogether, the error rate was reduced up to 13% compared to the baseline which\nis comparable with current SOTA methods. Finally, we compared the impact of\ninitial weights on the HMTL and SL. Subsequently, by HMTL, the error was\nreduced with all kinds of initial weights: random, ImageNet and SSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourmirzaei_M/0/1/0/all/0/1\">Mahdi Pourmirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montazer_G/0/1/0/all/0/1\">Gholam Ali Montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaili_F/0/1/0/all/0/1\">Farzaneh Esmaili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Self-supervised Learning with Hardness-aware Dynamic Curriculum Learning: An Application to Digital Pathology. (arXiv:2108.07183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07183","description":"<p>Self-supervised learning (SSL) has recently shown tremendous potential to\nlearn generic visual representations useful for many image analysis tasks.\nDespite their notable success, the existing SSL methods fail to generalize to\ndownstream tasks when the number of labeled training instances is small or if\nthe domain shift between the transfer domains is significant. In this paper, we\nattempt to improve self-supervised pretrained representations through the lens\nof curriculum learning by proposing a hardness-aware dynamic curriculum\nlearning (HaDCL) approach. To improve the robustness and generalizability of\nSSL, we dynamically leverage progressive harder examples via easy-to-hard and\nhard-to-very-hard samples during mini-batch downstream fine-tuning. We discover\nthat by progressive stage-wise curriculum learning, the pretrained\nrepresentations are significantly enhanced and adaptable to both in-domain and\nout-of-domain distribution data.\n</p>\n<p>We performed extensive validation on three histology benchmark datasets on\nboth patch-wise and slide-level classification problems. Our curriculum based\nfine-tuning yields a significant improvement over standard fine-tuning, with a\nminimum improvement in area-under-the-curve (AUC) score of 1.7% and 2.2% on\nin-domain and out-of-domain distribution data, respectively. Further, we\nempirically show that our approach is more generic and adaptable to any SSL\nmethods and does not impose any additional overhead complexity. Besides, we\nalso outline the role of patch-based versus slide-based curriculum learning in\nhistopathology to provide practical insights into the success of curriculum\nbased fine-tuning of SSL methods. Code is released at\nhttps://github.com/srinidhiPY/ICCV-CDPATH2021-ID-8\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinidhi_C/0/1/0/all/0/1\">Chetan L Srinidhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12659","description":"<p>Deep neural network (DNN) model compression for efficient on-device inference\nis becoming increasingly important to reduce memory requirements and keep user\ndata on-device. To this end, we propose a novel differentiable k-means\nclustering layer (DKM) and its application to train-time weight\nclustering-based DNN model compression. DKM casts k-means clustering as an\nattention problem and enables joint optimization of the DNN parameters and\nclustering centroids. Unlike prior works that rely on additional regularizers\nand parameters, DKM-based compression keeps the original loss function and\nmodel architecture fixed. We evaluated DKM-based compression on various DNN\nmodels for computer vision and natural language processing (NLP) tasks. Our\nresults demonstrate that DKM delivers superior compression and accuracy\ntrade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression\ncan offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB\nmodel size (29.4x model compression factor). For MobileNet-v1, which is a\nchallenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with\n0.72 MB model size (22.4x model compression factor). This result is 6.8% higher\ntop-1accuracy and 33% relatively smaller model size than the current\nstate-of-the-art DNN compression algorithms. Additionally, DKM enables\ncompression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on\nGLUE NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1\">Keivan A. Vahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1\">Saurabh Adya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does deep learning model calibration improve performance in class-imbalanced medical image classification?. (arXiv:2110.00918v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.00918","description":"<p>In medical image classification tasks, it is common to find that the number\nof normal samples far exceeds the number of abnormal samples. In such\nclass-imbalanced situations, reliable training of deep neural networks\ncontinues to be a major challenge. Under these circumstances, the predicted\nclass probabilities may be biased toward the majority class. Calibration has\nbeen suggested to alleviate some of these effects. However, there is\ninsufficient analysis explaining when and whether calibrating a model would be\nbeneficial in improving performance. In this study, we perform a systematic\nanalysis of the effect of model calibration on its performance on two medical\nimage modalities, namely, chest X-rays and fundus images, using various deep\nlearning classifier backbones. For this, we study the following variations: (i)\nthe degree of imbalances in the dataset used for training; (ii) calibration\nmethods; and (iii) two classification thresholds, namely, default decision\nthreshold of 0.5, and optimal threshold from precision-recall curves. Our\nresults indicate that at the default operating threshold of 0.5, the\nperformance achieved through calibration is significantly superior (p &lt; 0.05)\nto using uncalibrated probabilities. However, at the PR-guided threshold, these\ngains are not significantly different (p &gt; 0.05). This finding holds for both\nimage modalities and at varying degrees of imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1\">Sivaramakrishnan Rajaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_P/0/1/0/all/0/1\">Prasanth Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement. (arXiv:2110.00984v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.00984","description":"<p>Real-world low-light images suffer from two main degradations, namely,\ninevitable noise and poor visibility. Since the noise exhibits different\nlevels, its estimation has been implemented in recent works when enhancing\nlow-light images from raw Bayer space. When it comes to sRGB color space, the\nnoise estimation becomes more complicated due to the effect of the image\nprocessing pipeline. Nevertheless, most existing enhancing algorithms in sRGB\nspace only focus on the low visibility problem or suppress the noise under a\nhypothetical noise level, leading them impractical due to the lack of\nrobustness. To address this issue,we propose an adaptive unfolding total\nvariation network (UTVNet), which approximates the noise level from the real\nsRGB low-light image by learning the balancing parameter in the model-based\ndenoising method with total variation regularization. Meanwhile, we learn the\nnoise level map by unrolling the corresponding minimization process for\nproviding the inferences of smoothness and fidelity constraints. Guided by the\nnoise level map, our UTVNet can recover finer details and is more capable to\nsuppress noise in real captured low-light scenes. Extensive experiments on\nreal-world low-light images clearly demonstrate the superior performance of\nUTVNet over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanjun Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_D/0/1/0/all/0/1\">Daming Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wentian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new weakly supervised approach for ALS point cloud semantic segmentation. (arXiv:2110.01462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01462","description":"<p>While there are novel point cloud semantic segmentation schemes that\ncontinuously surpass state-of-the-art results, the success of learning an\neffective model usually rely on the availability of abundant labeled data.\nHowever, data annotation is a time-consuming and labor-intensive task,\nparticularly for large-scale airborne laser scanning (ALS) point clouds\ninvolving multiple classes in urban areas. Thus, how to attain promising\nresults while largely reducing labeling works become an essential issue. In\nthis study, we propose a deep-learning based weakly supervised framework for\nsemantic segmentation of ALS point clouds, exploiting potential information\nfrom unlabeled data subject to incomplete and sparse labels. Entropy\nregularization is introduced to penalize the class overlap in predictive\nprobability. Additionally, a consistency constraint by minimizing the\ndiscrepancy distance between instant and ensemble predictions is designed to\nimprove the robustness of predictions. Finally, we propose an online soft\npseudo-labeling strategy to create extra supervisory sources in an efficient\nand nonpaprametric way. Extensive experimental analysis using three benchmark\ndatasets demonstrates that in case of sparse point annotations, our proposed\nmethod significantly boosts the classification performance without compromising\nthe computational efficiency. It outperforms current weakly supervised methods\nand achieves a comparable result against full supervision competitors. For the\nISPRS 3D Labeling Vaihingen data, by using only 0.1% of labels, our method\nachieves an overall accuracy of 83.0% and an average F1 score of 70.0%, which\nhave increased by 6.9% and 12.8% respectively, compared to model trained by\nsparse label information only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Puzuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wei Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Diagnosis in Two-View Mammography Using End-to-End Trained EfficientNet-Based Convolutional Network. (arXiv:2110.01606v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01606","description":"<p>Some recent studies have described deep convolutional neural networks to\ndiagnose breast cancer in mammograms with similar or even superior performance\nto that of human experts. Shen et al. (2019) present one of the best techniques\nthat consists of two transfer learnings. The first uses a model trained on\nnatural images to create a \"patch classifier\" that categorizes small subimages.\nThe second uses the patch classifier to scan the whole mammogram and create the\n\"single-view whole-image classifier\". We propose to make a third transfer\nlearning to obtain a \"two-view classifier\" to use the two mammographic views:\nbilateral craniocaudal and mediolateral oblique. We use modern EfficientNet as\nthe basis of our model. We \"end-to-end\" train the entire system using CBIS-DDSM\ndataset. To ensure statistical robustness, we test our system twice using: (a)\n5-fold cross validation; and (b) the original training/test division of the\ndataset. Our technique reached an AUC of 0.934 using 5-fold cross validation\n(sensitivity and specificity are 85.13% at the equal error rate of ROC). Using\nthe original dataset division, our technique achieved an AUC of 0.8483, the\nlargest AUC reported for this problem, as far as we know.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Petrini_D/0/1/0/all/0/1\">Daniel G.P. Petrini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shimizu_C/0/1/0/all/0/1\">Carlos Shimizu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roela_R/0/1/0/all/0/1\">Rosimeire A. Roela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valente_G/0/1/0/all/0/1\">Gabriel V. Valente</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Folgueira_M/0/1/0/all/0/1\">Maria A.A.K. Folgueira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hae Yong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression. (arXiv:2110.01805v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01805","description":"<p>Block based motion estimation is integral to inter prediction processes\nperformed in hybrid video codecs. Prevalent block matching based methods that\nare used to compute block motion vectors (MVs) rely on computationally\nintensive search procedures. They also suffer from the aperture problem, which\ncan worsen as the block size is reduced. Moreover, the block matching criteria\nused in typical codecs do not account for the resulting levels of perceptual\nquality of the motion compensated pictures that are created upon decoding.\nTowards achieving the elusive goal of perceptually optimized motion estimation,\nwe propose a search-free block motion estimation framework using a multi-stage\nconvolutional neural network, which is able to conduct motion estimation on\nmultiple block sizes simultaneously, using a triplet of frames as input. This\ncomposite block translation network (CBT-Net) is trained in a self-supervised\nmanner on a large database that we created from publicly available uncompressed\nvideo content. We deploy the multi-scale structural similarity (MS-SSIM) loss\nfunction to optimize the perceptual quality of the motion compensated predicted\nframes. Our experimental results highlight the computational efficiency of our\nproposed model relative to conventional block matching based motion estimation\nalgorithms, for comparable prediction errors. Further, when used to perform\ninter prediction in AV1, the MV predictions of the perceptually optimized model\nresult in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and\n-1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion\n(VMAF) quality metrics, respectively as compared to the block matching based\nmotion estimation system employed in the SVT-AV1 encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1\">Somdyuti Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norkin_A/0/1/0/all/0/1\">Andrey Norkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning attack on copy detection patterns: are 1x1 patterns cloneable?. (arXiv:2110.02176v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.02176","description":"<p>Nowadays, the modern economy critically requires reliable yet cheap\nprotection solutions against product counterfeiting for the mass market. Copy\ndetection patterns (CDP) are considered as such solution in several\napplications. It is assumed that being printed at the maximum achievable limit\nof a printing resolution of an industrial printer with the smallest symbol size\n1x1 elements, the CDP cannot be copied with sufficient accuracy and thus are\nunclonable. In this paper, we challenge this hypothesis and consider a copy\nattack against the CDP based on machine learning. The experimental based on\nsamples produced on two industrial printers demonstrate that simple detection\nmetrics used in the CDP authentication cannot reliably distinguish the original\nCDP from their fakes. Thus, the paper calls for a need of careful\nreconsideration of CDP cloneability and search for new authentication\ntechniques and CDP optimization because of the current attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaban_R/0/1/0/all/0/1\">Roman Chaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taran_O/0/1/0/all/0/1\">Olga Taran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutt_J/0/1/0/all/0/1\">Joakim Tutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holotyak_T/0/1/0/all/0/1\">Taras Holotyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_S/0/1/0/all/0/1\">Slavi Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Slava Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}