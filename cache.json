{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Privacy Policy Question Answering Assistant: A Query-Guided Extractive Summarization Approach. (arXiv:2109.14638v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14638","description":"<p>Existing work on making privacy policies accessible has explored new\npresentation forms such as color-coding based on the risk factors or\nsummarization to assist users with conscious agreement. To facilitate a more\npersonalized interaction with the policies, in this work, we propose an\nautomated privacy policy question answering assistant that extracts a summary\nin response to the input user query. This is a challenging task because users\narticulate their privacy-related questions in a very different language than\nthe legal language of the policy, making it difficult for the system to\nunderstand their inquiry. Moreover, existing annotated data in this domain are\nlimited. We address these problems by paraphrasing to bring the style and\nlanguage of the user's question closer to the language of privacy policies. Our\ncontent scoring module uses the existing in-domain data to find relevant\ninformation in the policy and incorporates it in a summary. Our pipeline is\nable to find an answer for 89% of the user queries in the privacyQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1\">Moniba Keymanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1\">Micha Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Tweet Sentiment Using the Hidden State and Attention Matrix of a Fine-tuned BERTweet Model. (arXiv:2109.14692v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14692","description":"<p>This paper introduces a study on tweet sentiment classification. Our task is\nto classify a tweet as either positive or negative. We approach the problem in\ntwo steps, namely embedding and classifying. Our baseline methods include\nseveral combinations of traditional embedding methods and classification\nalgorithms. Furthermore, we explore the current state-of-the-art tweet analysis\nmodel, BERTweet, and propose a novel approach in which features are engineered\nfrom the hidden states and attention matrices of the model, inspired by\nempirical study of the tweets. Using a multi-layer perceptron trained with a\nhigh dropout rate for classification, our proposed approach achieves a\nvalidation accuracy of 0.9111.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macri_T/0/1/0/all/0/1\">Tommaso Macr&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_F/0/1/0/all/0/1\">Freya Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yunfan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zumbach_Y/0/1/0/all/0/1\">Yves Zumbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Approach For Sparse Representations Using The Locally Competitive Algorithm For Audio. (arXiv:2109.14705v1 [cs.SD])","link":"http://arxiv.org/abs/2109.14705","description":"<p>Gammachirp filterbank has been used to approximate the cochlea in sparse\ncoding algorithms. An oriented grid search optimization was applied to adapt\nthe gammachirp's parameters and improve the Matching Pursuit (MP) algorithm's\nsparsity along with the reconstruction quality. However, this combination of a\ngreedy algorithm with a grid search at each iteration is computationally\ndemanding and not suitable for real-time applications. This paper presents an\nadaptive approach to optimize the gammachirp's parameters but in the context of\nthe Locally Competitive Algorithm (LCA) that requires much fewer computations\nthan MP. The proposed method consists of taking advantage of the LCA's neural\narchitecture to automatically adapt the gammachirp's filterbank using the\nbackpropagation algorithm. Results demonstrate an improvement in the LCA's\nperformance with our approach in terms of sparsity, reconstruction quality, and\nconvergence time. This approach can yield a significant advantage over existing\napproaches for real-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahadi_S/0/1/0/all/0/1\">Soufiyan Bahadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1\">Jean Rouat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plourde_E/0/1/0/all/0/1\">&#xc9;ric Plourde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. (arXiv:2109.14723v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14723","description":"<p>Although pretrained language models (PTLMs) contain significant amounts of\nworld knowledge, they can still produce inconsistent answers to questions when\nprobed, even after specialized training. As a result, it can be hard to\nidentify what the model actually \"believes\" about the world, making it\nsusceptible to inconsistent behavior and simple errors. Our goal is to reduce\nthese problems. Our approach is to embed a PTLM in a broader system that also\nincludes an evolving, symbolic memory of beliefs -- a BeliefBank -- that\nrecords but then may modify the raw PTLM answers. We describe two mechanisms to\nimprove belief consistency in the overall system. First, a reasoning component\n-- a weighted MaxSAT solver -- revises beliefs that significantly clash with\nothers. Second, a feedback component issues future queries to the PTLM using\nknown beliefs as context. We show that, in a controlled experimental setting,\nthese two mechanisms result in more consistent beliefs in the overall system,\nimproving both the accuracy and consistency of its answers over time. This is\nsignificant as it is a first step towards PTLM-based architectures with a\nsystematic notion of belief, enabling them to construct a more coherent picture\nof the world, and improve over time without model retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14739","description":"<p>Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14776","description":"<p>Certainty and uncertainty are fundamental to science communication. Hedges\nhave widely been used as proxies for uncertainty. However, certainty is a\ncomplex construct, with authors expressing not only the degree but the type and\naspects of uncertainty in order to give the reader a certain impression of what\nis known. Here, we introduce a new study of certainty that models both the\nlevel and the aspects of certainty in scientific findings. Using a new dataset\nof 2167 annotated scientific findings, we demonstrate that hedges alone account\nfor only a partial explanation of certainty. We show that both the overall\ncertainty and individual aspects can be predicted with pre-trained language\nmodels, providing a more complete picture of the author's intended\ncommunication. Downstream analyses on 431K scientific findings from news and\nscientific abstracts demonstrate that modeling sentence-level and aspect-level\ncertainty is meaningful for areas like science communication. Both the model\nand datasets used in this paper are released at\nhttps://blablablab.si.umich.edu/projects/certainty/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14788","description":"<p>Modern medical diagnosis relies on precise pain assessment tools in\ntranslating clinical information from patient to physician. The McGill Pain\nQuestionnaire (MPQ) is a clinical pain assessment technique that utilizes 78\nadjectives of different intensities in 20 different categories to quantity a\npatient's pain. The questionnaire's efficacy depends on a predictable pattern\nof adjective use by patients experiencing pain. In this study, I recreate the\nMPQ's adjective intensity orderings using data gathered from patient forums and\nmodern NLP techniques. I extract adjective intensity relationships by searching\nfor key linguistic contexts, and then combine the relationship information to\nform robust adjective scales. Of 17 adjective relationships predicted by this\nresearch, 10 show agreement with the MPQ, which is statistically significant at\nthe .1 alpha level. The results suggest predictable patterns of adjective use\nby people experiencing pain, but call into question the MPQ's categories for\ngrouping adjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1\">Miriam Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonetic Word Embeddings. (arXiv:2109.14796v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14796","description":"<p>This work presents a novel methodology for calculating the phonetic\nsimilarity between words taking motivation from the human perception of sounds.\nThis metric is employed to learn a continuous vector embedding space that\ngroups similar sounding words together and can be used for various downstream\ncomputational phonology tasks. The efficacy of the method is presented for two\ndifferent languages (English, Hindi) and performance gains over previous\nreported works are discussed on established tests for predicting phonetic\nsimilarity. To address limited benchmarking mechanisms in this field, we also\nintroduce a heterographic pun dataset based evaluation methodology to compare\nthe effectiveness of acoustic similarity algorithms. Further, a visualization\nof the embedding space is presented with a discussion on the various possible\nuse-cases of this novel algorithm. An open-source implementation is also shared\nto aid reproducibility and enable adoption in related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhawan_K/0/1/0/all/0/1\">Kunal Dhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_B/0/1/0/all/0/1\">Balakrishna Pailla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Fake News Detection Using Bidirectiona lEncoder Representations from Transformers Based Models. (arXiv:2109.14816v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14816","description":"<p>Nowadays, the development of social media allows people to access the latest\nnews easily. During the COVID-19 pandemic, it is important for people to access\nthe news so that they can take corresponding protective measures. However, the\nfake news is flooding and is a serious issue especially under the global\npandemic. The misleading fake news can cause significant loss in terms of the\nindividuals and the society. COVID-19 fake news detection has become a novel\nand important task in the NLP field. However, fake news always contain the\ncorrect portion and the incorrect portion. This fact increases the difficulty\nof the classification task. In this paper, we fine tune the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model as our\nbase model. We add BiLSTM layers and CNN layers on the top of the finetuned\nBERT model with frozen parameters or not frozen parameters methods\nrespectively. The model performance evaluation results showcase that our best\nmodel (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves\nstate-of-the-art results towards COVID-19 fake news detection task. We also\nexplore keywords evaluation methods using our best model and evaluate the model\nperformance after removing keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuebo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Process discovery on deviant traces and other stranger things. (arXiv:2109.14883v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14883","description":"<p>As the need to understand and formalise business processes into a model has\ngrown over the last years, the process discovery research field has gained more\nand more importance, developing two different classes of approaches to model\nrepresentation: procedural and declarative. Orthogonally to this\nclassification, the vast majority of works envisage the discovery task as a\none-class supervised learning process guided by the traces that are recorded\ninto an input log. In this work instead, we focus on declarative processes and\nembrace the less-popular view of process discovery as a binary supervised\nlearning task, where the input log reports both examples of the normal system\nexecution, and traces representing \"stranger\" behaviours according to the\ndomain semantics. We therefore deepen how the valuable information brought by\nboth these two sets can be extracted and formalised into a model that is\n\"optimal\" according to user-defined goals. Our approach, namely NegDis, is\nevaluated w.r.t. other relevant works in this field, and shows promising\nresults as regards both the performance and the quality of the obtained\nsolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chesani_F/0/1/0/all/0/1\">Federico Chesani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francescomarino_C/0/1/0/all/0/1\">Chiara Di Francescomarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreti_D/0/1/0/all/0/1\">Daniela Loreti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1\">Fabrizio Maria Maggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1\">Paola Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1\">Marco Montali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessaris_S/0/1/0/all/0/1\">Sergio Tessaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14895","description":"<p>In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantavy_A/0/1/0/all/0/1\">Ashraf Tantavy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DICoE@FinSim-3: Financial Hypernym Detection using Augmented Terms and Distance-based Features. (arXiv:2109.14906v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14906","description":"<p>We present the submission of team DICoE for FinSim-3, the 3rd Shared Task on\nLearning Semantic Similarities for the Financial Domain. The task provides a\nset of terms in the financial domain and requires to classify them into the\nmost relevant hypernym from a financial ontology. After augmenting the terms\nwith their Investopedia definitions, our system employs a Logistic Regression\nclassifier over financial word embeddings and a mix of hand-crafted and\ndistance-based features. Also, for the first time in this task, we employ\ndifferent replacement methods for out-of-vocabulary terms, leading to improved\nperformance. Finally, we have also experimented with word representations\ngenerated from various financial corpora. Our best-performing submission ranked\n4th on the task's leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougiatiotis_K/0/1/0/all/0/1\">Konstantinos Bougiatiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroeidis_D/0/1/0/all/0/1\">Dimitris Mavroeidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavitsanos_E/0/1/0/all/0/1\">Elias Zavitsanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14927","description":"<p>Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. After\nstudying variants of token classification and encoder-decoder architectures, we\nultimately present a transformer encoder-decoder model using RoBERTa language\nmodel as our best performing system. By supplementing training resources with\nweakly labeled data from rule-based systems, our model surpasses previous works\nin temporal tagging and type classification, especially on rare classes.\nAdditionally, we make the code and pre-trained experiment publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almasian_S/0/1/0/all/0/1\">Satya Almasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prose2Poem: The blessing of Transformer-based Language Models in translating Prose to Persian Poetry. (arXiv:2109.14934v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14934","description":"<p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouryabi_Y/0/1/0/all/0/1\">Yazdan Rezaee Jouryabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14989","description":"<p>We investigate the extent to which modern, neural language models are\nsusceptible to syntactic priming, the phenomenon where the syntactic structure\nof a sentence makes the same structure more probable in a follow-up sentence.\nWe explore how priming can be used to study the nature of the syntactic\nknowledge acquired by these models. We introduce a novel metric and release\nPrime-LM, a large corpus where we control for various linguistic factors which\ninteract with priming strength. We find that recent large Transformer models\nindeed show evidence of syntactic priming, but also that the syntactic\ngeneralisations learned by these models are to some extent modulated by\nsemantic information. We report surprisingly strong priming effects when\npriming with multiple sentences, each with different words and meaning but with\nidentical syntactic structure. We conclude that the syntactic priming paradigm\nis a highly useful, additional tool for gaining insights into the capacities of\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1\">Arabella Sinclair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumelet_J/0/1/0/all/0/1\">Jaap Jumelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A surprisal--duration trade-off across and within the world's languages. (arXiv:2109.15000v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15000","description":"<p>While there exist scores of natural languages, each with its unique features\nand idiosyncrasies, they all share a unifying theme: enabling human\ncommunication. We may thus reasonably predict that human cognition shapes how\nthese languages evolve and are used. Assuming that the capacity to process\ninformation is roughly constant across human populations, we expect a\nsurprisal--duration trade-off to arise both across and within languages. We\nanalyse this trade-off using a corpus of 600 languages and, after controlling\nfor several potential confounds, we find strong supporting evidence in both\nsettings. Specifically, we find that, on average, phones are produced faster in\nlanguages where they are less surprising, and vice versa. Further, we confirm\nthat more surprising phones are longer, on average, in 319 languages out of the\n600. We thus conclude that there is strong evidence of a surprisal--duration\ntrade-off in operation, both across and within the world's languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_S/0/1/0/all/0/1\">Simone Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1\">Dami&#xe1;n Blasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Compression Via Concurrent Pruning and Self-Distillation. (arXiv:2109.15014v1 [cs.LG])","link":"http://arxiv.org/abs/2109.15014","description":"<p>Pruning aims to reduce the number of parameters while maintaining performance\nclose to the original network. This work proposes a novel\n\\emph{self-distillation} based pruning strategy, whereby the representational\nsimilarity between the pruned and unpruned versions of the same network is\nmaximized. Unlike previous approaches that treat distillation and pruning\nseparately, we use distillation to inform the pruning criteria, without\nrequiring a separate student network as in knowledge distillation. We show that\nthe proposed {\\em cross-correlation objective for self-distilled pruning}\nimplicitly encourages sparse solutions, naturally complementing magnitude-based\npruning criteria. Experiments on the GLUE and XGLUE benchmarks show that\nself-distilled pruning increases mono- and cross-lingual language model\nperformance. Self-distilled pruned models also outperform smaller Transformers\nwith an equal number of parameters and are competitive against (6 times) larger\ndistilled networks. We also observe that self-distillation (1) maximizes class\nseparability, (2) increases the signal-to-noise ratio, and (3) converges faster\nafter pruning steps, providing further insights into why self-distilled pruning\nimproves generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1\">James O&#x27; Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assem_H/0/1/0/all/0/1\">Haytham Assem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient Post-training Quantization of Pre-trained Language Models. (arXiv:2109.15082v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15082","description":"<p>Network quantization has gained increasing attention with the rapid growth of\nlarge pre-trained language models~(PLMs). However, most existing quantization\nmethods for PLMs follow quantization-aware training~(QAT) that requires\nend-to-end training with full access to the entire dataset. Therefore, they\nsuffer from slow training, large memory overhead, and data security issues. In\nthis paper, we study post-training quantization~(PTQ) of PLMs, and propose\nmodule-wise quantization error minimization~(MREM), an efficient solution to\nmitigate these issues. By partitioning the PLM into multiple modules, we\nminimize the reconstruction error incurred by quantization for each module. In\naddition, we design a new model parallel training strategy such that each\nmodule can be trained locally on separate computing devices without waiting for\npreceding modules, which brings nearly the theoretical training speed-up (e.g.,\n$4\\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our\nproposed PTQ solution not only performs close to QAT, but also enjoys\nsignificant reductions in training time, memory overhead, and data consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15086","description":"<p>Key point analysis is the task of extracting a set of concise and high-level\nstatements from a given collection of arguments, representing the gist of these\narguments. This paper presents our proposed approach to the Key Point Analysis\nshared task, collocated with the 8th Workshop on Argument Mining. The approach\nintegrates two complementary components. One component employs contrastive\nlearning via a siamese neural network for matching arguments to key points; the\nother is a graph-based extractive summarization model for generating key\npoints. In both automatic and manual evaluation, our approach was ranked best\namong all submissions to the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurke_T/0/1/0/all/0/1\">Timon Gurke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_P/0/1/0/all/0/1\">Philipp Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spliethover_M/0/1/0/all/0/1\">Maximilian Splieth&#xf6;ver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimiano_P/0/1/0/all/0/1\">Philipp Cimiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15101","description":"<p>Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models. (arXiv:2109.15107v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15107","description":"<p>Fact verification datasets are typically constructed using crowdsourcing\ntechniques due to the lack of text sources with veracity labels. However, the\ncrowdsourcing process often produces undesired biases in data that cause models\nto learn spurious patterns. In this paper, we propose CrossAug, a contrastive\ndata augmentation method for debiasing fact verification models. Specifically,\nwe employ a two-stage augmentation pipeline to generate new claims and\nevidences from existing samples. The generated samples are then paired\ncross-wise with the original pair, forming contrastive samples that facilitate\nthe model to rely less on spurious patterns and learn more robust\nrepresentations. Experimental results show that our method outperforms the\nprevious state-of-the-art debiasing technique by 3.6% on the debiased extension\nof the FEVER dataset, with a total performance boost of 10.13% from the\nbaseline. Furthermore, we evaluate our approach in data-scarce settings, where\nmodels can be more susceptible to biases due to the lack of training data.\nExperimental results demonstrate that our approach is also effective at\ndebiasing in these low-resource conditions, exceeding the baseline performance\non the Symmetric dataset with just 1% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_S/0/1/0/all/0/1\">Seungpil Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheoneum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims. (arXiv:2109.15118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15118","description":"<p>We present an overview of the second edition of the CheckThat! Lab at CLEF\n2019. The lab featured two tasks in two different languages: English and\nArabic. Task 1 (English) challenged the participating systems to predict which\nclaims in a political debate or speech should be prioritized for fact-checking.\nTask 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a\ncheck-worthy claim based on their usefulness for fact-checking that claim, (B)\nclassify these same Web pages according to their degree of usefulness for\nfact-checking the target claim, (C) identify useful passages from these pages,\nand (D) use the useful pages to predict the claim's factuality. CheckThat!\nprovided a full evaluation framework, consisting of data in English (derived\nfrom fact-checking sources) and Arabic (gathered and annotated from scratch)\nand evaluation based on mean average precision (MAP) and normalized discounted\ncumulative gain (nDCG) for ranking, and F1 for classification. A total of 47\nteams registered to participate in this lab, and fourteen of them actually\nsubmitted runs (compared to nine last year). The evaluation results show that\nthe most successful approaches to Task 1 used various neural networks and\nlogistic regression. As for Task 2, learning-to-rank was used by the highest\nscoring runs for subtask A, while different classifiers were used in the other\nsubtasks. We release to the research community all datasets from the lab as\nwell as the evaluation scripts, which should enable further research in the\nimportant tasks of check-worthiness estimation and automatic claim\nverification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1\">Alberto Barr&#xf3;n-Cede&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwaileh_R/0/1/0/all/0/1\">Reem Suwaileh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved statistical machine translation using monolingual paraphrases. (arXiv:2109.15119v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15119","description":"<p>We propose a novel monolingual sentence paraphrasing method for augmenting\nthe training data for statistical machine translation systems \"for free\" -- by\ncreating it from data that is already available rather than having to create\nmore aligned data. Starting with a syntactic tree, we recursively generate new\nsentence variants where noun compounds are paraphrased using suitable\nprepositions, and vice-versa -- preposition-containing noun phrases are turned\ninto noun compounds. The evaluation shows an improvement equivalent to 33%-50%\nof that of doubling the amount of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering. (arXiv:2109.15120v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15120","description":"<p>We present the system we built for participating in SemEval-2016 Task 3 on\nCommunity Question Answering. We achieved the best results on subtask C, and\nstrong results on subtasks A and B, by combining a rich set of various types of\nfeatures: semantic, lexical, metadata, and user-related. The most important\ngroup turned out to be the metadata for the question and for the comment,\nsemantic vectors trained on QatarLiving data and similarities between the\nquestion and the comment for subtasks A and C, and between the original and the\nrelated question for Subtask B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylova_T/0/1/0/all/0/1\">Tsvetomila Mihaylova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencheva_P/0/1/0/all/0/1\">Pepa Gencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyanov_M/0/1/0/all/0/1\">Martin Boyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yovcheva_I/0/1/0/all/0/1\">Ivana Yovcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiprov_Y/0/1/0/all/0/1\">Yasen Kiprov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balchev_D/0/1/0/all/0/1\">Daniel Balchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolova_I/0/1/0/all/0/1\">Ivelina Nikolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_G/0/1/0/all/0/1\">Galia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields. (arXiv:2109.15121v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15121","description":"<p>The paper presents a feature-rich approach to the automatic recognition and\ncategorization of named entities (persons, organizations, locations, and\nmiscellaneous) in news text for Bulgarian. We combine well-established features\nused for other languages with language-specific lexical, syntactic and\nmorphological information. In particular, we make use of the rich tagset\nannotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive\nsuitable task-specific tagsets (local and nonlocal). We further add\ndomain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,\nwhich is comparable to the state-of-the-art results for English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osenova_P/0/1/0/all/0/1\">Petya Osenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simov_K/0/1/0/all/0/1\">Kiril Ivanov Simov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Text Style Transfer using Deep Learning. (arXiv:2109.15144v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15144","description":"<p>Style is an integral component of a sentence indicated by the choice of words\na person makes. Different people have different ways of expressing themselves,\nhowever, they adjust their speaking and writing style to a social context, an\naudience, an interlocutor or the formality of an occasion. Text style transfer\nis defined as a task of adapting and/or changing the stylistic manner in which\na sentence is written, while preserving the meaning of the original sentence.\n</p>\n<p>A systematic review of text style transfer methodologies using deep learning\nis presented in this paper. We point out the technological advances in deep\nneural networks that have been the driving force behind current successes in\nthe fields of natural language understanding and generation. The review is\nstructured around two key stages in the text style transfer process, namely,\nrepresentation learning and sentence generation in a new style. The discussion\nhighlights the commonalities and differences between proposed solutions as well\nas challenges and opportunities that are expected to direct and foster further\nresearch in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toshevska_M/0/1/0/all/0/1\">Martina Toshevska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gievska_S/0/1/0/all/0/1\">Sonja Gievska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Sarcasm Detection Based on Contrastive Attention Mechanism. (arXiv:2109.15153v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15153","description":"<p>In the past decade, sarcasm detection has been intensively conducted in a\ntextual scenario. With the popularization of video communication, the analysis\nin multi-modal scenarios has received much attention in recent years.\nTherefore, multi-modal sarcasm detection, which aims at detecting sarcasm in\nvideo conversations, becomes increasingly hot in both the natural language\nprocessing community and the multi-modal analysis community. In this paper,\nconsidering that sarcasm is often conveyed through incongruity between\nmodalities (e.g., text expressing a compliment while acoustic tone indicating a\ngrumble), we construct a Contras-tive-Attention-based Sarcasm Detection\n(ConAttSD) model, which uses an inter-modality contrastive attention mechanism\nto extract several contrastive features for an utterance. A contrastive feature\nrepresents the incongruity of information between two modalities. Our\nexperiments on MUStARD, a benchmark multi-modal sarcasm dataset, demonstrate\nthe effectiveness of the proposed ConAttSD model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Contrastive Training for Test-based Constituency Analysis. (arXiv:2109.15159v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15159","description":"<p>We propose a scheme for self-training of grammaticality models for\nconstituency analysis based on linguistic tests. A pre-trained language model\nis fine-tuned by contrastive estimation of grammatical sentences from a corpus,\nand ungrammatical sentences that were perturbed by a syntactic test, a\ntransformation that is motivated by constituency theory. We show that\nconsistent gains can be achieved if only certain positive instances are chosen\nfor training, depending on whether they could be the result of a test\ntransformation. This way, the positives, and negatives exhibit similar\ncharacteristics, which makes the objective more challenging for the language\nmodel, and also allows for additional markup that indicates the position of the\ntest application within the sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15196","description":"<p>We study multilingual AMR parsing from the perspective of knowledge\ndistillation, where the aim is to learn and improve a multilingual AMR parser\nby using an existing English parser as its teacher. We constrain our\nexploration in a strict multilingual setting: there is but one model to parse\nall different languages including English. We identify that noisy input and\nprecise output are the key to successful distillation. Together with extensive\npre-training, we obtain an AMR parser whose performances surpass all previously\npublished results on four different foreign languages, including German,\nSpanish, Italian, and Chinese, by large margins (up to 18.8 \\textsc{Smatch}\npoints on Chinese and on average 11.3 \\textsc{Smatch} points). Our parser also\nachieves comparable performance on English to the latest state-of-the-art\nEnglish-only parser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jackie Chun-Sing Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15207","description":"<p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates\na 3D environment, following natural language instructions. A challenge in this\ntask is how to handle 'off the path' scenarios where an agent veers from a\nreference path. Prior work supervises the agent with actions based on the\nshortest path from the agent's location to the goal, but such goal-oriented\nsupervision is often not in alignment with the instruction. Furthermore, the\nevaluation metrics employed by prior work do not measure how much of a language\ninstruction the agent is able to follow. In this work, we propose a simple and\neffective language-aligned supervision scheme, and a new metric that measures\nthe number of sub-instructions the agent has completed during navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_S/0/1/0/all/0/1\">Sonia Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wani_S/0/1/0/all/0/1\">Saim Wani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivansh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1\">Unnat Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A formal model for ledger management systems based on contracts and temporal logic. (arXiv:2109.15212v1 [cs.CR])","link":"http://arxiv.org/abs/2109.15212","description":"<p>A key component of blockchain technology is the ledger, viz., a database\nthat, unlike standard databases, keeps in memory the complete history of past\ntransactions as in a notarial archive for the benefit of any future test. In\nsecond-generation blockchains such as Ethereum the ledger is coupled with smart\ncontracts, which enable the automation of transactions associated with\nagreements between the parties of a financial or commercial nature. The\ncoupling of smart contracts and ledgers provides the technological background\nfor very innovative application areas, such as Decentralized Autonomous\nOrganizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance\n(DeFi), which propelled blockchains beyond cryptocurrencies that were the only\nfocus of first generation blockchains such as the Bitcoin. However, the\ncurrently used implementation of smart contracts as arbitrary programming\nconstructs has made them susceptible to dangerous bugs that can be exploited\nmaliciously and has moved their semantics away from that of legal contracts. We\npropose here to recompose the split and recover the reliability of databases by\nformalizing a notion of contract modelled as a finite-state automaton with\nwell-defined computational characteristics derived from an encoding in terms of\nallocations of resources to actors, as an alternative to the approach based on\nprogramming. To complete the work, we use temporal logic as the basis for an\nabstract query language that is effectively suited to the historical nature of\nthe information kept in the ledger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bottoni_P/0/1/0/all/0/1\">Paolo Bottoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labella_A/0/1/0/all/0/1\">Anna Labella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_R/0/1/0/all/0/1\">Remo Pareschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15254","description":"<p>We introduce a new Slovak masked language model called SlovakBERT in this\npaper. It is the first Slovak-only transformers-based model trained on a\nsizeable corpus. We evaluate the model on several NLP tasks and achieve\nstate-of-the-art results. We publish the masked language model, as well as the\nsubsequently fine-tuned models for part-of-speech tagging, sentiment analysis\nand semantic textual similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grivalsky_S/0/1/0/all/0/1\">&#x160;tefan Grivalsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopka_M/0/1/0/all/0/1\">Martin Kon&#xf4;pka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blstak_M/0/1/0/all/0/1\">Miroslav Bl&#x161;t&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamajka_M/0/1/0/all/0/1\">Martin Tamajka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachraty_V/0/1/0/all/0/1\">Viktor Bachrat&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_M/0/1/0/all/0/1\">Mari&#xe1;n &#x160;imko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazik_P/0/1/0/all/0/1\">Pavol Bal&#xe1;&#x17e;ik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trnka_M/0/1/0/all/0/1\">Michal Trnka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlarik_F/0/1/0/all/0/1\">Filip Uhl&#xe1;rik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks. (arXiv:2109.15256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15256","description":"<p>Systematic compositionality is an essential mechanism in human language,\nallowing the recombination of known parts to create novel expressions. However,\nexisting neural models have been shown to lack this basic ability in learning\nsymbolic structures. Motivated by the failure of a Transformer model on the\nSCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing\na command into actions, we propose two auxiliary sequence prediction tasks that\ntrack the progress of function and argument semantics, as additional training\nsupervision. These automatically-generated sequences are more representative of\nthe underlying compositional symbolic structures of the input data. During\ninference, the model jointly predicts the next action and the next tokens in\nthe auxiliary sequences at each step. Experiments on the SCAN dataset show that\nour method encourages the Transformer to understand compositional structures of\nthe command, improving its accuracy on multiple challenging splits from &lt;= 10%\nto 100%. With only 418 (5%) training instances, our approach still achieves\n97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can\nbe induced in Transformers given minimal but proper guidance. We also show that\na better result is achieved using less contextualized vectors as the\nattention's query, providing insights into architecture choices in achieving\nsystematic compositionality. Finally, we show positive generalization results\non the groundedSCAN task (Ruis et al., 2020). Our code is publicly available\nat: https://github.com/jiangycTarheel/compositional-auxseq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yichen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction. (arXiv:2109.15290v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15290","description":"<p>An overwhelmingly large amount of knowledge in the materials domain is\ngenerated and stored as text published in peer-reviewed scientific literature.\nRecent developments in natural language processing, such as bidirectional\nencoder representations from transformers (BERT) models, provide promising\ntools to extract information from these texts. However, direct application of\nthese models in the materials domain may yield suboptimal results as the models\nthemselves may not be trained on notations and jargon that are specific to the\ndomain. Here, we present a materials-aware language model, namely, MatSciBERT,\nwhich is trained on a large corpus of scientific literature published in the\nmaterials domain. We further evaluate the performance of MatSciBERT on three\ndownstream tasks, namely, abstract classification, named entity recognition,\nand relation extraction, on different materials datasets. We show that\nMatSciBERT outperforms SciBERT, a language model trained on science corpus, on\nall the tasks. Further, we discuss some of the applications of MatSciBERT in\nthe materials domain for extracting information, which can, in turn, contribute\nto materials discovery or optimization. Finally, to make the work accessible to\nthe larger materials community, we make the pretrained and finetuned weights\nand the models of MatSciBERT freely accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granular Legal Topic Classification on Greek Legislation. (arXiv:2109.15298v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15298","description":"<p>In this work, we study the task of classifying legal texts written in the\nGreek language. We introduce and make publicly available a novel dataset based\non Greek legislation, consisting of more than 47 thousand official, categorized\nGreek legislation resources. We experiment with this dataset and evaluate a\nbattery of advanced methods and classifiers, ranging from traditional machine\nlearning and RNN-based methods to state-of-the-art Transformer-based methods.\nWe show that recurrent architectures with domain-specific word embeddings offer\nimproved overall performance while being competitive even to transformer-based\nmodels. Finally, we show that cutting-edge multilingual and monolingual\ntransformer-based models brawl on the top of the classifiers' ranking, making\nus question the necessity of training monolingual transfer learning models as a\nrule of thumb. To the best of our knowledge, this is the first time the task of\nGreek legal text classification is considered in an open research project,\nwhile also Greek is a language with very limited NLP resources in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papaloukas_C/0/1/0/all/0/1\">Christos Papaloukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athinaios_K/0/1/0/all/0/1\">Konstantinos Athinaios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantazi_D/0/1/0/all/0/1\">Despina-Athanasia Pantazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubarakis_M/0/1/0/all/0/1\">Manolis Koubarakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Text Classification via Self-Pretraining. (arXiv:2109.15300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15300","description":"<p>We present a neural semi-supervised learning model termed Self-Pretraining.\nOur model is inspired by the classic self-training algorithm. However, as\nopposed to self-training, Self-Pretraining is threshold-free, it can\npotentially update its belief about previously labeled documents, and can cope\nwith the semantic drift problem. Self-Pretraining is iterative and consists of\ntwo classifiers. In each iteration, one classifier draws a random set of\nunlabeled documents and labels them. This set is used to initialize the second\nclassifier, to be further trained by the set of labeled documents. The\nalgorithm proceeds to the next iteration and the classifiers' roles are\nreversed. To improve the flow of information across the iterations and also to\ncope with the semantic drift problem, Self-Pretraining employs an iterative\ndistillation process, transfers hypotheses across the iterations, utilizes a\ntwo-stage training model, uses an efficient learning rate schedule, and employs\na pseudo-label transformation heuristic. We have evaluated our model in three\npublicly available social media datasets. Our experiments show that\nSelf-Pretraining outperforms the existing state-of-the-art semi-supervised\nclassifiers across multiple settings. Our code is available at\nhttps://github.com/p-karisani/self_pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karisani_N/0/1/0/all/0/1\">Negin Karisani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Graph Contextualized Knowledge into Pre-trained Language Models. (arXiv:1912.00147v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.00147","description":"<p>Complex node interactions are common in knowledge graphs, and these\ninteractions also contain rich knowledge information. However, traditional\nmethods usually treat a triple as a training unit during the knowledge\nrepresentation learning (KRL) procedure, neglecting contextualized information\nof the nodes in knowledge graphs (KGs). We generalize the modeling object to a\nvery general form, which theoretically supports any subgraph extracted from the\nknowledge graph, and these subgraphs are fed into a novel transformer-based\nmodel to learn the knowledge embeddings. To broaden usage scenarios of\nknowledge, pre-trained language models are utilized to build a model that\nincorporates the learned knowledge representations. Experimental results\ndemonstrate that our model achieves the state-of-the-art performance on several\nmedical NLP tasks, and improvement above TransE indicates that our KRL method\ncaptures the graph contextualized information effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Di Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jiang_X/0/1/0/all/0/1\">Xin jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Sequence Learning for Generating Adequate Question-Answer Pairs. (arXiv:2010.01620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01620","description":"<p>Creating multiple-choice questions to assess reading comprehension of a given\narticle involves generating question-answer pairs (QAPs) on the main points of\nthe document. We present a learning scheme to generate adequate QAPs via\nmeta-sequence representations of sentences. A meta sequence is a sequence of\nvectors comprising semantic and syntactic tags. In particular, we devise a\nscheme called MetaQA to learn meta sequences from training data to form pairs\nof a meta sequence for a declarative sentence (MD) and a corresponding\ninterrogative sentence (MIs). On a given declarative sentence, a trained MetaQA\nmodel converts it to a meta sequence, finds a matched MD, and uses the\ncorresponding MIs and the input sentence to generate QAPs. We implement MetaQA\nfor the English language using semantic-role labeling, part-of-speech tagging,\nand named-entity recognition, and show that trained on a small dataset, MetaQA\ngenerates efficiently over the official SAT practice reading tests a large\nnumber of syntactically and semantically correct QAPs with over 97\\% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.09697","description":"<p>The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does My Representation Capture X? Probe-Ably. (arXiv:2104.05807v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.05807","description":"<p>Probing (or diagnostic classification) has become a popular strategy for\ninvestigating whether a given set of intermediate features is present in the\nrepresentations of neural models. Probing studies may have misleading results,\nbut various recent works have suggested more reliable methodologies that\ncompensate for the possible pitfalls of probing. However, these best practices\nare numerous and fast-evolving. To simplify the process of running a set of\nprobing experiments in line with suggested methodologies, we introduce\nProbe-Ably: an extendable probing framework which supports and automates the\napplication of probing methods to the user's inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media. (arXiv:2104.06999v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06999","description":"<p>Online social media platforms increasingly rely on Natural Language\nProcessing (NLP) techniques to detect abusive content at scale in order to\nmitigate the harms it causes to their users. However, these techniques suffer\nfrom various sampling and association biases present in training data, often\nresulting in sub-par performance on content relevant to marginalized groups,\npotentially furthering disproportionate harms towards them. Studies on such\nbiases so far have focused on only a handful of axes of disparities and\nsubgroups that have annotations/lexicons available. Consequently, biases\nconcerning non-Western contexts are largely ignored in the literature. In this\npaper, we introduce a weakly supervised method to robustly detect lexical\nbiases in broader geocultural contexts. Through a case study on a publicly\navailable toxicity detection model, we demonstrate that our method identifies\nsalient groups of cross-geographic errors, and, in a follow up, demonstrate\nthat these groupings reflect human judgments of offensive and inoffensive\nlanguage in those geographic contexts. We also conduct analysis of a model\ntrained on a dataset with ground truth labels to better understand these\nbiases, and present preliminary mitigation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_D/0/1/0/all/0/1\">Dylan Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. (arXiv:2104.08758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08758","description":"<p>Large language models have led to remarkable progress on many NLP tasks, and\nresearchers are turning to ever-larger text corpora to train them. Some of the\nlargest corpora available are made by scraping significant portions of the\ninternet, and are frequently introduced with only minimal documentation. In\nthis work we provide some of the first documentation for the Colossal Clean\nCrawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set\nof filters to a single snapshot of Common Crawl. We begin by investigating\nwhere the data came from, and find a significant amount of text from unexpected\nsources like patents and US military websites. Then we explore the content of\nthe text itself, and find machine-generated text (e.g., from machine\ntranslation systems) and evaluation examples from other benchmark NLP datasets.\nTo understand the impact of the filters applied to create this dataset, we\nevaluate the text that was removed, and show that blocklist filtering\ndisproportionately removes text from and about minority individuals. Finally,\nwe conclude with some recommendations for how to created and document web-scale\ndatasets from a scrape of the internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1\">William Agnew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groeneveld_D/0/1/0/all/0/1\">Dirk Groeneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01542","description":"<p>Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: <a href=\"http://nlp.uit.edu.vn/datasets/\">this http URL</a> for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02605","description":"<p>The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, making each node's\nsemantic accurately comprehended from the global perspective. In addition, a\nprogressive learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation. (arXiv:2106.11096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11096","description":"<p>In this work, we propose a novel and easy-to-apply data augmentation\nstrategy, namely Bilateral Generation (BiG), with a contrastive training\nobjective for improving the performance of ranking question answer pairs with\nexisting labeled data. In specific, we synthesize pseudo-positive QA pairs in\ncontrast to the original negative QA pairs with two pre-trained generation\nmodels, one for question generation, the other for answer generation, which are\nfine-tuned on the limited positive QA pairs from the original dataset. With the\naugmented dataset, we design a contrastive training objective for learning to\nrank question answer pairs. Experimental results on three benchmark datasets\nshow that our method significantly improves the performance of ranking models\nby making full use of existing labeled data and can be easily applied to\ndifferent ranking models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13658","description":"<p>The understanding of time expressions includes two sub-tasks: recognition and\nnormalization. In recent years, significant progress has been made in the\nrecognition of time expressions while research on normalization has lagged\nbehind. Existing SOTA normalization methods highly rely on rules or grammars\ndesigned by experts, which limits their performance on emerging corpora, such\nas social media texts. In this paper, we model time expression normalization as\na sequence of operations to construct the normalized temporal value, and we\npresent a novel method called ARTime, which can automatically generate\nnormalization rules from training data without expert interventions.\nSpecifically, ARTime automatically captures possible operation sequences from\nannotated data and generates normalization rules on time expressions with\ncommon surface forms. The experimental results show that ARTime can\nsignificantly surpass SOTA methods on the Tweets benchmark, and achieves\ncompetitive results with existing expert-engineered rule methods on the\nTempEval-3 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>In this paper, a BERT based neural network model is applied to the JIGSAW\ndata set in order to create a model identifying hateful and toxic comments\n(strictly seperated from offensive language) in online social platforms\n(English language), in this case Twitter. Three other neural network\narchitectures and a GPT-2 model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set and the data set HASOC 2019 which includes Twitter and\nalso Facebook comments; we focus on the English HASOC 2019 data. In addition,\nit can be shown that by fine-tuning the trained BERT model on these two data\nsets by applying different transfer learning scenarios via retraining partial\nor all layers the predictive scores improve compared to simply applying the\nmodel pre-trained on the JIGSAW data set. With our results, we get precisions\nfrom 64% to around 90% while still achieving acceptable recall values of at\nleast lower 60s%, proving that BERT is suitable for real use cases in social\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11321","description":"<p>Large language models are known to suffer from the hallucination problem in\nthat they are prone to output statements that are false or inconsistent,\nindicating a lack of knowledge. A proposed solution to this is to provide the\nmodel with additional data modalities that complements the knowledge obtained\nthrough text. We investigate the use of visual data to complement the knowledge\nof large language models by proposing a method for evaluating visual knowledge\ntransfer to text for uni- or multimodal language models. The method is based on\ntwo steps, 1) a novel task querying for knowledge of memory colors, i.e.\ntypical colors of well-known objects, and 2) filtering of model training data\nto clearly separate knowledge contributions. Additionally, we introduce a model\narchitecture that involves a visual imagination step and evaluate it with our\nproposed method. We find that our method can successfully be used to measure\nvisual knowledge transfer capabilities in models and that our novel model\narchitecture shows promising results for leveraging multimodal knowledge in a\nunimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13066","description":"<p>Existing text-to-SQL research only considers complete questions as the input,\nbut lay-users might strive to formulate a complete question. To build a smarter\nnatural language interface to database systems (NLIDB) that also processes\nincomplete questions, we propose a new task, prefix-to-SQL which takes question\nprefix from users as the input and predicts the intended SQL. We construct a\nnew benchmark called PAGSAS that contains 124K user question prefixes and the\nintended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.\nAdditionally, we propose a new metric SAVE to measure how much effort can be\nsaved by users. Experimental results show that PAGSAS is challenging even for\nstrong baseline models such as T5. As we observe the difficulty of\nprefix-to-SQL is related to the number of omitted tokens, we incorporate\ncurriculum learning of feeding examples with an increasing number of omitted\ntokens. This improves scores on various sub-tasks by as much as 9% recall\nscores on sub-task GeoQuery in PAGSAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13123","description":"<p>Digital learning platforms enable students to learn on a flexible and\nindividual schedule as well as providing instant feedback mechanisms. The field\nof STEM education requires students to solve numerous training exercises to\ngrasp underlying concepts. It is apparent that there are restrictions in\ncurrent online education in terms of exercise diversity and individuality. Many\nexercises show little variance in structure and content, hindering the adoption\nof abstraction capabilities by students. This thesis proposes an approach to\ngenerate diverse, context rich word problems. In addition to requiring the\ngenerated language to be grammatically correct, the nature of word problems\nimplies additional constraints on the validity of contents. The proposed\napproach is proven to be effective in generating valid word problems for\nmathematical statistics. The experimental results present a tradeoff between\ngeneration time and exercise validity. The system can easily be parametrized to\nhandle this tradeoff according to the requirements of specific use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1\">Stanley Uros Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Federated Self-Supervised Contrastive Learning via Ensemble Similarity Distillation. (arXiv:2109.14611v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14611","description":"<p>This paper investigates the feasibility of learning good representation space\nwith unlabeled client data in the federated scenario. Existing works trivially\ninherit the supervised federated learning methods, which does not apply to the\nmodel heterogeneity and has the potential risk of privacy exposure. To tackle\nthe problems above, we first identify that self-supervised contrastive local\ntraining is more robust against the non-i.i.d.-ness than the traditional\nsupervised learning paradigm. Then we propose a novel federated self-supervised\ncontrastive learning framework FLESD that supports architecture-agnostic local\ntraining and communication-efficient global aggregation. At each round of\ncommunication, the server first gathers a fraction of the clients' inferred\nsimilarity matrices on a public dataset. Then FLESD ensembles the similarity\nmatrices and trains the global model via similarity distillation. We verify the\neffectiveness of our proposed framework by a series of empirical experiments\nand show that FLESD has three main advantages over the existing methods: it\nhandles the model heterogeneity, is less prone to privacy leak, and is more\ncommunication-efficient. We will release the code of this paper in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zijin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean. (arXiv:2109.14646v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14646","description":"<p>Ocean-going platforms are integrating high-resolution camera feeds for\nobservation and navigation, producing a deluge of visual data. The volume and\nrate of this data collection can rapidly outpace researchers' abilities to\nprocess and analyze them. Recent advances in machine learning enable fast,\nsophisticated analysis of visual data, but have had limited success in the\noceanographic world due to lack of dataset standardization, sparse annotation\ntools, and insufficient formatting and aggregation of existing, expertly\ncurated imagery for use by data scientists. To address this need, we have built\nFathomNet, a public platform that makes use of existing (and future), expertly\ncurated data. Initial efforts have leveraged MBARI's Video Annotation and\nReference System and annotated deep sea video database, which has more than 7M\nannotations, 1M framegrabs, and 5k terms in the knowledgebase, with additional\ncontributions by National Geographic Society (NGS) and NOAA's Office of Ocean\nExploration and Research. FathomNet has over 100k localizations of 1k midwater\nand benthic classes, and contains iconic and non-iconic views of marine\nanimals, underwater equipment, debris, etc. We will demonstrate how machine\nlearning models trained on FathomNet data can be applied across different\ninstitutional video data, (e.g., NGS' Deep Sea Camera System and NOAA's ROV\nDeep Discoverer), and enable automated acquisition and tracking of midwater\nanimals using MBARI's ROV MiniROV. As FathomNet continues to develop and\nincorporate more image data from other oceanographic community members, this\neffort will enable scientists, explorers, policymakers, storytellers, and the\npublic to understand and care for our ocean.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katija_K/0/1/0/all/0/1\">Kakani Katija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_E/0/1/0/all/0/1\">Eric Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlining_B/0/1/0/all/0/1\">Brian Schlining</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundsten_L/0/1/0/all/0/1\">Lonny Lundsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kevin Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_G/0/1/0/all/0/1\">Giovanna Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oceane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_B/0/1/0/all/0/1\">Benjamin Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_K/0/1/0/all/0/1\">Katy Croff Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Mean Teacher for Source-free Unsupervised Domain Adaptive 3D Object Detection. (arXiv:2109.14651v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14651","description":"<p>Pseudo-label based self training approaches are a popular method for\nsource-free unsupervised domain adaptation. However, their efficacy depends on\nthe quality of the labels generated by the source trained model. These labels\nmay be incorrect with high confidence, rendering thresholding methods\nineffective. In order to avoid reinforcing errors caused by label noise, we\npropose an uncertainty-aware mean teacher framework which implicitly filters\nincorrect pseudo-labels during training. Leveraging model uncertainty allows\nthe mean teacher network to perform implicit filtering by down-weighing losses\ncorresponding uncertain pseudo-labels. Effectively, we perform automatic\nsoft-sampling of pseudo-labeled data while aligning predictions from the\nstudent and teacher networks. We demonstrate our method on several domain\nadaptation scenarios, from cross-dataset to cross-weather conditions, and\nachieve state-of-the-art performance in these cases, on the KITTI lidar target\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1\">Deepti Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1\">Velat Kilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Brinton Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1\">Mark Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation. (arXiv:2109.14657v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14657","description":"<p>In this paper, we address the problem of estimating the hand pose from the\negocentric view when the hand is interacting with objects. Specifically, we\npropose a method to label a dataset Ego-Siam which contains the egocentric\nimages pair-wisely. We also use the collected pairwise data to train our\nencoder-decoder style network which has been proven efficient in. This could\nbring extra training efficiency and testing accuracy. Our network is\nlightweight and can be performed with over 30 FPS with an outdated GPU. We\ndemonstrate that our method outperforms Mueller et al. which is the state of\nthe art work dealing with egocentric hand-object interaction problems on the\nGANerated dataset. To show the ability to preserve the semantic information of\nour method, we also report the performance of grasp type classification on\nGUN-71 dataset and outperforms the benchmark by only using the predicted 3-d\nhand pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayol_Cuevas_W/0/1/0/all/0/1\">Walterio W. Mayol-Cuevas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Roads in Satellite Images using specially modified U-Net CNNs. (arXiv:2109.14671v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14671","description":"<p>The image classification problem has been deeply investigated by the research\ncommunity, with computer vision algorithms and with the help of Neural\nNetworks. The aim of this paper is to build an image classifier for satellite\nimages of urban scenes that identifies the portions of the images in which a\nroad is located, separating these portions from the rest. Unlike conventional\ncomputer vision algorithms, convolutional neural networks (CNNs) provide\naccurate and reliable results on this task. Our novel approach uses a sliding\nwindow to extract patches out of the whole image, data augmentation for\ngenerating more training/testing data and lastly a series of specially modified\nU-Net CNNs. This proposed technique outperforms all other baselines tested in\nterms of mean F-score metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yihang She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhehan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macri_T/0/1/0/all/0/1\">Tommaso Macr&#xec;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning. (arXiv:2109.14685v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14685","description":"<p>Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized\nby relapsing inflammation of the large intestine. The severity of UC is often\nrepresented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal\ndisease activity from endoscopy videos. In clinical trials, an endoscopy video\nis assigned an MES based upon the most severe disease activity observed in the\nvideo. For this reason, severe inflammation spread throughout the colon will\nreceive the same MES as an otherwise healthy colon with severe inflammation\nrestricted to a small, localized segment. Therefore, the extent of disease\nactivity throughout the large intestine, and overall response to treatment, may\nnot be completely captured by the MES. In this work, we aim to automatically\nestimate UC severity for each frame in an endoscopy video to provide a higher\nresolution assessment of disease activity throughout the colon. Because\nannotating severity at the frame-level is expensive, labor-intensive, and\nhighly subjective, we propose a novel weakly supervised, ordinal classification\nmethod to estimate frame severity from video MES labels alone. Using clinical\ntrial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing\nand remission of UC, respectively. Then, for severity estimation, we\ndemonstrate that our models achieve substantial Cohen's Kappa agreement with\nground truth MES labels, comparable to the inter-rater agreement of expert\nclinicians. These findings indicate that our framework could serve as a\nfoundation for novel clinical endpoints, based on a more localized scoring\nsystem, to better evaluate UC drug efficacy in clinical trials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schwab_E/0/1/0/all/0/1\">Evan Schwab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cula_G/0/1/0/all/0/1\">Gabriela Oana Cula</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Standish_K/0/1/0/all/0/1\">Kristopher Standish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yip_S/0/1/0/all/0/1\">Stephen S. F. Yip</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stojmirovic_A/0/1/0/all/0/1\">Aleksandar Stojmirovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghanem_L/0/1/0/all/0/1\">Louis Ghanem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chehoud_C/0/1/0/all/0/1\">Christel Chehoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Aided Beam Tracking: Explore the Proper Use of Camera Images with Deep Learning. (arXiv:2109.14686v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14686","description":"<p>We investigate the problem of wireless beam tracking on mmWave bands with the\nassistance of camera images. In particular, based on the user's beam indices\nused and camera images taken in the trajectory, we predict the optimal beam\nindices in the next few time spots. To resolve this problem, we first\nreformulate the \"ViWi\" dataset in [1] to get rid of the image repetition\nproblem. Then we develop a deep learning approach and investigate various model\ncomponents to achieve the best performance. Finally, we explore whether, when,\nand how to use the image for better beam prediction. To answer this question,\nwe split the dataset into three clusters -- (LOS, light NLOS, serious\nNLOS)-like -- based on the standard deviation of the beam sequence. With\nexperiments we demonstrate that using the image indeed helps beam tracking\nespecially when the user is in serious NLOS, and the solution relies on\ncarefully-designed dataset for training a model. Generally speaking, including\nNLOS-like data for training a model does not benefit beam tracking of the user\nin LOS, but including light NLOS-like data for training a model benefits beam\ntracking of the user in serious NLOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LR-to-HR Face Hallucination with an Adversarial Progressive Attribute-Induced Network. (arXiv:2109.14690v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14690","description":"<p>Face super-resolution is a challenging and highly ill-posed problem since a\nlow-resolution (LR) face image may correspond to multiple high-resolution (HR)\nones during the hallucination process and cause a dramatic identity change for\nthe final super-resolved results. Thus, to address this problem, we propose an\nend-to-end progressive learning framework incorporating facial attributes and\nenforcing additional supervision from multi-scale discriminators. By\nincorporating facial attributes into the learning process and progressively\nresolving the facial image, the mapping between LR and HR images is constrained\nmore, and this significantly helps to reduce the ambiguity and uncertainty in\none-to-many mapping. In addition, we conduct thorough evaluations on the CelebA\ndataset following the settings of previous works (i.e. super-resolving by a\nfactor of 8x from tiny 16x16 face images.), and the results demonstrate that\nthe proposed approach can yield satisfactory face hallucination images\noutperforming other state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_N/0/1/0/all/0/1\">Nitin Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition. (arXiv:2109.14710v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14710","description":"<p>Modern Convolutional Neural Network (CNN) architectures, despite their\nsuperiority in solving various problems, are generally too large to be deployed\non resource constrained edge devices. In this paper, we reduce memory usage and\nfloating-point operations required by convolutional layers in CNNs. We compress\nthese layers by generalizing the Kronecker Product Decomposition to apply to\nmultidimensional tensors, leading to the Generalized Kronecker Product\nDecomposition(GKPD). Our approach yields a plug-and-play module that can be\nused as a drop-in replacement for any convolutional layer. Experimental results\nfor image classification on CIFAR-10 and ImageNet datasets using ResNet,\nMobileNetv2 and SeNet architectures substantiate the effectiveness of our\nproposed approach. We find that GKPD outperforms state-of-the-art decomposition\nmethods including Tensor-Train and Tensor-Ring as well as other relevant\ncompression methods such as pruning and knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1\">Marawan Gamal Abdel Hameed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh S. Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosleh_A/0/1/0/all/0/1\">Ali Mosleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USIS: Unsupervised Semantic Image Synthesis. (arXiv:2109.14715v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14715","description":"<p>Semantic Image Synthesis (SIS) is a subclass of image-to-image translation\nwhere a photorealistic image is synthesized from a segmentation mask. SIS has\nmostly been addressed as a supervised problem. However, state-of-the-art\nmethods depend on a huge amount of labeled data and cannot be applied in an\nunpaired setting. On the other hand, generic unpaired image-to-image\ntranslation frameworks underperform in comparison, because they color-code\nsemantic layouts and feed them to traditional convolutional networks, which\nthen learn correspondences in appearance instead of semantic content. In this\ninitial work, we propose a new Unsupervised paradigm for Semantic Image\nSynthesis (USIS) as a first step towards closing the performance gap between\npaired and unpaired settings. Notably, the framework deploys a SPADE generator\nthat learns to output images with visually separable semantic classes using a\nself-supervised segmentation loss. Furthermore, in order to match the color and\ntexture distribution of real images without losing high-frequency information,\nwe propose to use whole image wavelet-based discrimination. We test our\nmethodology on 3 challenging datasets and demonstrate its ability to generate\nmultimodal photorealistic images with an improved quality in the unpaired\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eskandar_G/0/1/0/all/0/1\">George Eskandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelsamad_M/0/1/0/all/0/1\">Mohamed Abdelsamad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armanious_K/0/1/0/all/0/1\">Karim Armanious</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning. (arXiv:2109.14729v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14729","description":"<p>A convolutional neural network (ConvNet) is usually trained and then tested\nusing images drawn from the same distribution. To generalize a ConvNet to\nvarious tasks often requires a complete training dataset that consists of\nimages drawn from different tasks. In most scenarios, it is nearly impossible\nto collect every possible representative dataset as a priori. The new data may\nonly become available after the ConvNet is deployed in clinical practice.\nConvNet, however, may generate artifacts on out-of-distribution testing\nsamples. In this study, we present Targeted Gradient Descent (TGD), a novel\nfine-tuning method that can extend a pre-trained network to a new task without\nrevisiting data from the previous task while preserving the knowledge acquired\nfrom previous training. To a further extent, the proposed method also enables\nonline learning of patient-specific data. The method is built on the idea of\nreusing a pre-trained ConvNet's redundant kernels to learn new knowledge. We\ncompare the performance of TGD to several commonly used training approaches on\nthe task of Positron emission tomography (PET) image denoising. Results from\nclinical images show that TGD generated results on par with\ntraining-from-scratch while significantly reducing data preparation and network\ntraining time. More importantly, it enables online learning on the testing\nstudy to enhance the network's generalization capability in real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asma_E/0/1/0/all/0/1\">Evren Asma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chung Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Hand-object Interaction Detection and Application. (arXiv:2109.14734v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14734","description":"<p>In this paper, we present a method to detect the hand-object interaction from\nan egocentric perspective. In contrast to massive data-driven discriminator\nbased method like \\cite{Shan20}, we propose a novel workflow that utilises the\ncues of hand and object. Specifically, we train networks predicting hand pose,\nhand mask and in-hand object mask to jointly predict the hand-object\ninteraction status. We compare our method with the most recent work from Shan\net al. \\cite{Shan20} on selected images from EPIC-KITCHENS\n\\cite{damen2018scaling} dataset and achieve $89\\%$ accuracy on HOI (hand-object\ninteraction) detection which is comparative to Shan's ($92\\%$). However, for\nreal-time performance, with the same machine, our method can run over\n$\\textbf{30}$ FPS which is much efficient than Shan's\n($\\textbf{1}\\sim\\textbf{2}$ FPS). Furthermore, with our approach, we are able\nto segment script-less activities from where we extract the frames with the HOI\nstatus detection. We achieve $\\textbf{68.2\\%}$ and $\\textbf{82.8\\%}$ F1 score\non GTEA \\cite{fathi2011learning} and the UTGrasp \\cite{cai2015scalable} dataset\nrespectively which are all comparative to the SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayol_Cuevas_W/0/1/0/all/0/1\">Walterio W. Mayol-Cuevas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the potential of deep learning for marine ecology: overview, applications, and outlook. (arXiv:2109.14737v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14737","description":"<p>The deep learning revolution is touching all scientific disciplines and\ncorners of our lives as a means of harnessing the power of big data. Marine\necology is no exception. These new methods provide analysis of data from\nsensors, cameras, and acoustic recorders, even in real time, in ways that are\nreproducible and rapid. Off-the-shelf algorithms can find, count, and classify\nspecies from digital images or video and detect cryptic patterns in noisy data.\nUsing these opportunities requires collaboration across ecological and data\nscience disciplines, which can be challenging to initiate. To facilitate these\ncollaborations and promote the use of deep learning towards ecosystem-based\nmanagement of the sea, this paper aims to bridge the gap between marine\necologists and computer scientists. We provide insight into popular deep\nlearning approaches for ecological data analysis in plain language, focusing on\nthe techniques of supervised learning with deep neural networks, and illustrate\nchallenges and opportunities through established and emerging applications of\ndeep learning to marine ecology. We use established and future-looking case\nstudies on plankton, fishes, marine mammals, pollution, and nutrient cycling\nthat involve object detection, classification, tracking, and segmentation of\nvisualized data. We conclude with a broad outlook of the field's opportunities\nand challenges, including potential technological advances and issues with\nmanaging complex data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_K/0/1/0/all/0/1\">Kim Tallaksen Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knausgaard_K/0/1/0/all/0/1\">Kristian Muri Knausg&#xe5;rd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Angela Helen Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyano_M/0/1/0/all/0/1\">Marta Moyano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oomen_R/0/1/0/all/0/1\">Rebekah A. Oomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasmussen_J/0/1/0/all/0/1\">Jeppe Have Rasmussen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordalen_T/0/1/0/all/0/1\">Tonje Knutsen S&#xf8;rdalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorbjornsen_S/0/1/0/all/0/1\">Susanna Huneide Thorbj&#xf8;rnsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions. (arXiv:2109.14744v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14744","description":"<p>In this paper, we concern with the problem of how to automatically extract\nthe steps that compose real-life hand activities. This is a key competence\ntowards processing, monitoring and providing video guidance in Mixed Reality\nsystems. We use egocentric vision to observe hand-object interactions in\nreal-world tasks and automatically decompose a video into its constituent\nsteps. Our approach combines hand-object interaction (HOI) detection, object\nsimilarity measurement and a finite state machine (FSM) representation to\nautomatically edit videos into steps. We use a combination of Convolutional\nNeural Networks (CNNs) and the FSM to discover, edit cuts and merge segments\nwhile observing real hand activities. We evaluate quantitatively and\nqualitatively our algorithm on two datasets: the GTEA\\cite{li2015delving}, and\na new dataset we introduce for Chinese Tea making. Results show our method is\nable to segment hand-object interaction videos into key step segments with high\nlevels of precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayol_Cuevas_W/0/1/0/all/0/1\">Walterio W. Mayol-Cuevas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improvising the Learning of Neural Networks on Hyperspherical Manifold. (arXiv:2109.14746v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14746","description":"<p>The impact of convolution neural networks (CNNs) in the supervised settings\nprovided tremendous increment in performance. The representations learned from\nCNN's operated on hyperspherical manifold led to insightful outcomes in face\nrecognition, face identification and other supervised tasks. A broad range of\nactivation functions is developed with hypersphere intuition which performs\nsuperior to softmax in euclidean space. The main motive of this research is to\nprovide insights. First, the stereographic projection is implied to transform\ndata from Euclidean space ($\\mathbb{R}^{n}$) to hyperspherical manifold\n($\\mathbb{S}^{n}$) to analyze the performance of angular margin losses.\nSecondly, proving both theoretically and practically that decision boundaries\nconstructed on hypersphere using stereographic projection obliges the learning\nof neural networks. Experiments have proved that applying stereographic\nprojection on existing state-of-the-art angular margin objective functions led\nto improve performance for standard image classification data sets\n(CIFAR-10,100). The code is publicly available at:\nhttps://github.com/barulalithb/stereo-angular-margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baru_L/0/1/0/all/0/1\">Lalith Bharadwaj Baru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanumolu_S/0/1/0/all/0/1\">Sai Vardhan Kanumolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shilhora_A/0/1/0/all/0/1\">Akshay Patel Shilhora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaHistoSeg: A Python Framework for Meta Learning in Histopathology Image Segmentation. (arXiv:2109.14754v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14754","description":"<p>Few-shot learning is a standard practice in most deep learning based\nhistopathology image segmentation, given the relatively low number of digitized\nslides that are generally available. While many models have been developed for\ndomain specific histopathology image segmentation, cross-domain generalization\nremains a key challenge for properly validating models. Here, tooling and\ndatasets to benchmark model performance across histopathological domains are\nlacking. To address this limitation, we introduce MetaHistoSeg - a Python\nframework that implements unique scenarios in both meta learning and instance\nbased transfer learning. Designed for easy extension to customized datasets and\ntask sampling schemes, the framework empowers researchers with the ability of\nrapid model design and experimentation. We also curate a histopathology meta\ndataset - a benchmark dataset for training and validating models on\nout-of-distribution performance across a range of cancer types. In experiments\nwe showcase the usage of MetaHistoSeg with the meta dataset and find that both\nmeta-learning and instance based transfer learning deliver comparable results\non average, but in some cases tasks can greatly benefit from one over the\nother.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteva_A/0/1/0/all/0/1\">Andre Esteva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chest X-Rays Image Classification from beta-Variational Autoencoders Latent Features. (arXiv:2109.14760v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14760","description":"<p>Chest X-Ray (CXR) is one of the most common diagnostic techniques used in\neveryday clinical practice all around the world. We hereby present a work which\nintends to investigate and analyse the use of Deep Learning (DL) techniques to\nextract information from such images and allow to classify them, trying to keep\nour methodology as general as possible and possibly also usable in a real world\nscenario without much effort, in the future. To move in this direction, we\ntrained several beta-Variational Autoencoder (beta-VAE) models on the CheXpert\ndataset, one of the largest publicly available collection of labeled CXR\nimages; from these models, latent features have been extracted and used to\ntrain other Machine Learning models, able to classify the original images from\nthe features extracted by the beta-VAE. Lastly, tree-based models have been\ncombined together in ensemblings to improve the results without the necessity\nof further training or models engineering. Expecting some drop in pure\nperformance with the respect to state of the art classification specific\nmodels, we obtained encouraging results, which show the viability of our\napproach and the usability of the high level features extracted by the\nautoencoders for classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Crespi_L/0/1/0/all/0/1\">Leonardo Crespi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loiacono_D/0/1/0/all/0/1\">Daniele Loiacono</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiti_A/0/1/0/all/0/1\">Arturo Chiti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prior Knowledge Based Tumor and Tumoral Subregion Segmentation Tool for Pediatric Brain Tumors. (arXiv:2109.14775v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14775","description":"<p>In the past few years, deep learning (DL) models have drawn great attention\nand shown superior performance on brain tumor and subregion segmentation tasks.\nHowever, the success is limited to segmentation of adult gliomas, where\nsufficient data have been collected, manually labeled, and published for\ntraining DL models. It is still challenging to segment pediatric tumors,\nbecause the appearances are different from adult gliomas. Hence, directly\napplying a pretained DL model on pediatric data usually generates unacceptable\nresults. Because pediatric data is very limited, both labeled and unlabeled, we\npresent a brain tumor segmentation model that is based on knowledge rather than\nlearning from data. We also provide segmentation of more subregions for super\nheterogeneous tumor like atypical teratoid rhabdoid tumor (ATRT). Our proposed\napproach showed superior performance on both whole tumor and subregion\nsegmentation tasks to DL based models on our pediatric data when training data\nis not available for transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Silu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_A/0/1/0/all/0/1\">Angela Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shubo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patay_Z/0/1/0/all/0/1\">Zoltan Patay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bag_A/0/1/0/all/0/1\">Asim Bag</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scoggins_M/0/1/0/all/0/1\">Matthew A. Scoggins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated airway segmentation by learning graphical structure. (arXiv:2109.14792v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14792","description":"<p>In this research project, we put forward an advanced method for airway\nsegmentation based on the existent convolutional neural network (CNN) and graph\nneural network (GNN). The method is originated from the vessel segmentation,\nbut we ameliorate it and enable the novel model to perform better for datasets\nfrom computed tomography (CT) scans. Current methods for airway segmentation\nare considering the regular grid only. No matter what the detailed model is,\nincluding the 3-dimensional CNN or 2-dimensional CNN in three directions, the\noverall graph structures are not taken into consideration. In our model, with\nthe neighbourhoods of airway taken into account, the graph structure is\nincorporated and the segmentation of airways are improved compared with the\ntraditional CNN methods. We perform experiments on the chest CT scans, where\nthe ground truth segmentation labels are produced manually. The proposed model\nshows that compared with the CNN-only method, the combination of CNN and GNN\nhas a better performance in that the bronchi in the chest CT scans can be\ndetected in most cases. In addition, the model we propose has a wide extension\nsince the architecture is also utilitarian in fulfilling similar aims in other\ndatasets. Hence, the state-of-the-art model is of great significance and highly\napplicable in our daily lives.\n</p>\n<p>Keywords: Airway segmentation, Convolutional neural network, Graph neural\nnetwork\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yihua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images. (arXiv:2109.14805v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14805","description":"<p>Motion estimation is a fundamental step in dynamic medical image processing\nfor the assessment of target organ anatomy and function. However, existing\nimage-based motion estimation methods, which optimize the motion field by\nevaluating the local image similarity, are prone to produce implausible\nestimation, especially in the presence of large motion. In this study, we\nprovide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which\ncomprises two stages. In the first stage, we process the raw dense image to\nextract sparse landmarks to represent the target organ anatomical topology and\ndiscard the redundant information that is unnecessary for motion estimation.\nFor this purpose, we introduce an unsupervised 3D landmark detection network to\nextract spatially sparse but representative landmarks for the target organ\nmotion estimation. In the second stage, we derive the sparse motion\ndisplacement from the extracted sparse landmarks of two images of different\ntime points. Then, we present a motion reconstruction network to construct the\nmotion field by projecting the sparse landmarks displacement back into the\ndense image domain. Furthermore, we employ the estimated motion field from our\ntwo-stage DSD framework as initialization and boost the motion estimation\nquality in light-weight yet effective iterative optimization. We evaluate our\nmethod on two dynamic medical imaging tasks to model cardiac motion and lung\nrespiratory motion, respectively. Our method has produced superior motion\nestimation accuracy compared to existing comparative methods. Besides, the\nextensive experimental results demonstrate that our solution can extract well\nrepresentative anatomical landmarks without any requirement of manual\nannotation. Our code is publicly available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dongming Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liyun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhengbin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation. (arXiv:2109.14813v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14813","description":"<p>To achieve an accurate assessment of root canal therapy, a fundamental step\nis to perform tooth root segmentation on oral X-ray images, in that the\nposition of tooth root boundary is significant anatomy information in root\ncanal therapy evaluation. However, the fuzzy boundary makes the tooth root\nsegmentation very challenging. In this paper, we propose a novel end-to-end\nU-Net like Group Transformer Network (GT U-Net) for the tooth root\nsegmentation. The proposed network retains the essential structure of U-Net but\neach of the encoders and decoders is replaced by a group Transformer, which\nsignificantly reduces the computational cost of traditional Transformer\narchitectures by using the grouping structure and the bottleneck structure. In\naddition, the proposed GT U-Net is composed of a hybrid structure of\nconvolution and Transformer, which makes it independent of pre-training\nweights. For optimization, we also propose a shape-sensitive Fourier Descriptor\n(FD) loss function to make use of shape prior knowledge. Experimental results\nshow that our proposed network achieves the state-of-the-art performance on our\ncollected tooth root segmentation dataset and the public retina dataset DRIVE.\nCode has been released at https://github.com/Kent0n-Li/GT-U-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guodong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qun Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spark in the Dark: Evaluating Encoder-Decoder Pairs for COVID-19 CT's Semantic Segmentation. (arXiv:2109.14818v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14818","description":"<p>With the COVID-19 global pandemic, computerassisted diagnoses of medical\nimages have gained a lot of attention, and robust methods of Semantic\nSegmentation of Computed Tomography (CT) turned highly desirable. Semantic\nSegmentation of CT is one of many research fields of automatic detection of\nCovid-19 and was widely explored since the Covid19 outbreak. In the robotic\nfield, Semantic Segmentation of organs and CTs are widely used in robots\ndeveloped for surgery tasks. As new methods and new datasets are proposed\nquickly, it becomes apparent the necessity of providing an extensive evaluation\nof those methods. To provide a standardized comparison of different\narchitectures across multiple recently proposed datasets, we propose in this\npaper an extensive benchmark of multiple encoders and decoders with a total of\n120 architectures evaluated in five datasets, with each dataset being validated\nthrough a five-fold cross-validation strategy, totaling 3.000 experiments. To\nthe best of our knowledge, this is the largest evaluation in number of\nencoders, decoders, and datasets proposed in the field of Covid-19 CT\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Krinski_B/0/1/0/all/0/1\">Bruno A. Krinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruiz_D/0/1/0/all/0/1\">Daniel V. Ruiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Todt_E/0/1/0/all/0/1\">Eduardo Todt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Dense Reconstruction with Consistent Scene Segments. (arXiv:2109.14821v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14821","description":"<p>In this paper, a method for dense semantic 3D scene reconstruction from an\nRGB-D sequence is proposed to solve high-level scene understanding tasks.\nFirst, each RGB-D pair is consistently segmented into 2D semantic maps based on\na camera tracking backbone that propagates objects' labels with high\nprobabilities from full scans to corresponding ones of partial views. Then a\ndense 3D mesh model of an unknown environment is incrementally generated from\nthe input RGB-D sequence. Benefiting from 2D consistent semantic segments and\nthe 3D model, a novel semantic projection block (SP-Block) is proposed to\nextract deep feature volumes from 2D segments of different views. Moreover, the\nsemantic volumes are fused into deep volumes from a point cloud encoder to make\nthe final semantic segmentation. Extensive experimental evaluations on public\ndatasets show that our system achieves accurate 3D dense reconstruction and\nstate-of-the-art semantic prediction performances simultaneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yingcai Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yingxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Cheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Lijin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntentVizor: Towards Generic Query Guided Interactive Video Summarization Using Slow-Fast Graph Convolutional Networks. (arXiv:2109.14834v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14834","description":"<p>The target of automatic Video summarization is to create a short skim of the\noriginal long video while preserving the major content/events. There is a\ngrowing interest in the integration of user's queries into video summarization,\nor query-driven video summarization. This video summarization method predicts a\nconcise synopsis of the original video based on the user query, which is\ncommonly represented by the input text. However, two inherent problems exist in\nthis query-driven way. First, the query text might not be enough to describe\nthe exact and diverse needs of the user. Second, the user cannot edit once the\nsummaries are produced, limiting this summarization technique's practical\nvalue. We assume the needs of the user should be subtle and need to be adjusted\ninteractively. To solve these two problems, we propose a novel IntentVizor\nframework, which is an interactive video summarization framework guided by\ngenric multi-modality queries. The input query that describes the user's needs\nis not limited to text but also the video snippets. We further conclude these\nmulti-modality finer-grained queries as user `intent', which is a newly\nproposed concept in this paper. This intent is interpretable, interactable, and\nbetter quantifies/describes the user's needs. To be more specific, We use a set\nof intents to represent the inputs of users to design our new interactive\nvisual analytic interface. Users can interactively control and adjust these\nmixed-initiative intents to obtain a more satisfying summary of this newly\nproposed interface. Also, as algorithms help users achieve their summarization\ngoal via video understanding, we propose two novel intent/scoring networks\nbased on the slow-fast feature for our algorithm part. We conduct our\nexperiments on two benchmark datasets. The comparison with the state-of-the-art\nmethods verifies the effectiveness of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guande Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio T. Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Image Compression with Probabilistic Decoding. (arXiv:2109.14837v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14837","description":"<p>Lossy image compression is a many-to-one process, thus one bitstream\ncorresponds to multiple possible original images, especially at low bit rates.\nHowever, this nature was seldom considered in previous studies on image\ncompression, which usually chose one possible image as reconstruction, e.g. the\none with the maximal a posteriori probability. We propose a learned image\ncompression framework to natively support probabilistic decoding. The\ncompressed bitstream is decoded into a series of parameters that instantiate a\npre-chosen distribution; then the distribution is used by the decoder to sample\nand reconstruct images. The decoder may adopt different sampling strategies and\nproduce diverse reconstructions, among which some have higher signal fidelity\nand some others have better visual quality. The proposed framework is dependent\non a revertible neural network-based transform to convert pixels into\ncoefficients that obey the pre-chosen distribution as much as possible. Our\ncode and models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Haichuan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Cunhui Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AffectGAN: Affect-Based Generative Art Driven by Semantics. (arXiv:2109.14845v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14845","description":"<p>This paper introduces a novel method for generating artistic images that\nexpress particular affective states. Leveraging state-of-the-art deep learning\nmethods for visual generation (through generative adversarial networks),\nsemantic models from OpenAI, and the annotated dataset of the visual art\nencyclopedia WikiArt, our AffectGAN model is able to generate images based on\nspecific or broad semantic prompts and intended affective outcomes. A small\ndataset of 32 images generated by AffectGAN is annotated by 50 participants in\nterms of the particular emotion they elicit, as well as their quality and\nnovelty. Results show that for most instances the intended emotion used as a\nprompt for image generation matches the participants' responses. This\nsmall-scale study brings forth a new vision towards blending affective\ncomputing with computational creativity, enabling generative systems with\nintentionality in terms of the emotions they wish their output to elicit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galanos_T/0/1/0/all/0/1\">Theodoros Galanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HLIC: Harmonizing Optimization Metrics in Learned Image Compression by Reinforcement Learning. (arXiv:2109.14863v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14863","description":"<p>Learned image compression is making good progress in recent years. Peak\nsignal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM)\nare the two most popular evaluation metrics. As different metrics only reflect\ncertain aspects of human perception, works in this field normally optimize two\nmodels using PSNR and MS-SSIM as loss function separately, which is suboptimal\nand makes it difficult to select the model with best visual quality or overall\nperformance. Towards solving this problem, we propose to Harmonize optimization\nmetrics in Learned Image Compression (HLIC) using online loss function\nadaptation by reinforcement learning. By doing so, we are able to leverage the\nadvantages of both PSNR and MS-SSIM, achieving better visual quality and higher\nVMAF score. To our knowledge, our work is the first to explore automatic loss\nfunction adaptation for harmonizing optimization metrics in low level vision\ntasks like learned image compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baocheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Meng Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dailan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tongda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Segmentation Models using an Uncertainty Slice Sampling Based Annotation Workflow. (arXiv:2109.14879v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14879","description":"<p>Semantic segmentation neural networks require pixel-level annotations in\nlarge quantities to achieve a good performance. In the medical domain, such\nannotations are expensive, because they are time-consuming and require expert\nknowledge. Active learning optimizes the annotation effort by devising\nstrategies to select cases for labeling that are most informative to the model.\nIn this work, we propose an uncertainty slice sampling (USS) strategy for\nsemantic segmentation of 3D medical volumes that selects 2D image slices for\nannotation and compare it with various other strategies. We demonstrate the\nefficiency of USS on a CT liver segmentation task using multi-site data. After\nfive iterations, the training data resulting from USS consisted of 2410 slices\n(4% of all slices in the data pool) compared to 8121 (13%), 8641 (14%), and\n3730 (6%) for uncertainty volume (UVS), random volume (RVS), and random slice\n(RSS) sampling, respectively. Despite being trained on the smallest amount of\ndata, the model based on the USS strategy evaluated on 234 test volumes\nsignificantly outperformed models trained according to other strategies and\nachieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean\nsurface distance of 1.35 mm, and a Hausdorff distance of 23.4 mm. This was only\nslightly inferior to 0.967, 3.8%, 1.18 mm, and 22.9 mm achieved by a model\ntrained on all available data, but the robustness analysis using the 5th\npercentile of Dice and the 95th percentile of the remaining metrics\ndemonstrated that USS resulted not only in the most robust model compared to\nother sampling schemes, but also outperformed the model trained on all data\naccording to Dice (0.946 vs. 0.945) and mean surface distance (1.92 mm vs. 2.03\nmm).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chlebus_G/0/1/0/all/0/1\">Grzegorz Chlebus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schenk_A/0/1/0/all/0/1\">Andrea Schenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_H/0/1/0/all/0/1\">Horst K. Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meine_H/0/1/0/all/0/1\">Hans Meine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations. (arXiv:2109.14910v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14910","description":"<p>Contrastive learning allows us to flexibly define powerful losses by\ncontrasting positive pairs from sets of negative samples. Recently, the\nprinciple has also been used to learn cross-modal embeddings for video and\ntext, yet without exploiting its full potential. In particular, previous losses\ndo not take the intra-modality similarities into account, which leads to\ninefficient embeddings, as the same content is mapped to multiple points in the\nembedding space. With CrossCLR, we present a contrastive loss that fixes this\nissue. Moreover, we define sets of highly related samples in terms of their\ninput embeddings and exclude them from the negative samples to avoid issues\nwith false negatives. We show that these principles consistently improve the\nquality of the learned embeddings. The joint embeddings learned with CrossCLR\nextend the state of the art in video-text retrieval on Youcook2 and LSMDC\ndatasets and in video captioning on Youcook2 dataset by a large margin. We also\ndemonstrate the generality of the concept by learning improved joint embeddings\nfor other pairs of modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zolfaghari_M/0/1/0/all/0/1\">Mohammadreza Zolfaghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forming a sparse representation for visual place recognition using a neurorobotic approach. (arXiv:2109.14916v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14916","description":"<p>This paper introduces a novel unsupervised neural network model for visual\ninformation encoding which aims to address the problem of large-scale visual\nlocalization. Inspired by the structure of the visual cortex, the model (namely\nHSD) alternates layers of topologic sparse coding and pooling to build a more\ncompact code of visual information. Intended for visual place recognition (VPR)\nsystems that use local descriptors, the impact of its integration in a\nbio-inpired model for self-localization (LPMP) is evaluated. Our experimental\nresults on the KITTI dataset show that HSD improves the runtime speed of LPMP\nby a factor of at least 2 and its localization accuracy by 10%. A comparison\nwith CoHog, a state-of-the-art VPR approach, showed that our method achieves\nslightly better results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colomer_S/0/1/0/all/0/1\">Sylvain Colomer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuperlier_N/0/1/0/all/0/1\">Nicolas Cuperlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresson_G/0/1/0/all/0/1\">Guillaume Bresson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romain_O/0/1/0/all/0/1\">Olivier Romain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Localization Method for Measuring Abdominal Muscle Dimensions in Ultrasound Images. (arXiv:2109.14919v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14919","description":"<p>Health professionals extensively use Two- Dimensional (2D) Ultrasound (US)\nvideos and images to visualize and measure internal organs for various purposes\nincluding evaluation of muscle architectural changes. US images can be used to\nmeasure abdominal muscles dimensions for the diagnosis and creation of\ncustomized treatment plans for patients with Low Back Pain (LBP), however, they\nare difficult to interpret. Due to high variability, skilled professionals with\nspecialized training are required to take measurements to avoid low\nintra-observer reliability. This variability stems from the challenging nature\nof accurately finding the correct spatial location of measurement endpoints in\nabdominal US images. In this paper, we use a Deep Learning (DL) approach to\nautomate the measurement of the abdominal muscle thickness in 2D US images. By\ntreating the problem as a localization task, we develop a modified Fully\nConvolutional Network (FCN) architecture to generate blobs of coordinate\nlocations of measurement endpoints, similar to what a human operator does. We\ndemonstrate that using the TrA400 US image dataset, our network achieves a Mean\nAbsolute Error (MAE) of 0.3125 on the test set, which almost matches the\nperformance of skilled ultrasound technicians. Our approach can facilitate next\nsteps for automating the process of measurements in 2D US images, while\nreducing inter-observer as well as intra-observer variability for more\neffective clinical outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saleh_A/0/1/0/all/0/1\">Alzayat Saleh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laradji_I/0/1/0/all/0/1\">Issam H. Laradji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lammie_C/0/1/0/all/0/1\">Corey Lammie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Flavell_C/0/1/0/all/0/1\">Carol A Flavell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azghadi_M/0/1/0/all/0/1\">Mostafa Rahimi Azghadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark. (arXiv:2109.14956v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14956","description":"<p>PURPOSE: Surgical workflow and skill analysis are key technologies for the\nnext generation of cognitive surgical assistance systems. These systems could\nincrease the safety of the operation through context-sensitive warnings and\nsemi-autonomous robotic assistance or improve training of surgeons via\ndata-driven feedback. In surgical workflow analysis up to 91% average precision\nhas been reported for phase recognition on an open data single-center dataset.\nIn this work we investigated the generalizability of phase recognition\nalgorithms in a multi-center setting including more difficult recognition tasks\nsuch as surgical action and surgical skill. METHODS: To achieve this goal, a\ndataset with 33 laparoscopic cholecystectomy videos from three surgical centers\nwith a total operation time of 22 hours was created. Labels included annotation\nof seven surgical phases with 250 phase transitions, 5514 occurences of four\nsurgical actions, 6980 occurences of 21 surgical instruments from seven\ninstrument categories and 495 skill classifications in five skill dimensions.\nThe dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for\nsurgical workflow and skill analysis. Here, 12 teams submitted their machine\nlearning algorithms for recognition of phase, action, instrument and/or skill\nassessment. RESULTS: F1-scores were achieved for phase recognition between\n23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5%\nand 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3%\n(n=5 teams). The average absolute error for skill assessment was 0.78 (n=1\nteam). CONCLUSION: Surgical workflow and skill analysis are promising\ntechnologies to support the surgical team, but are not solved yet, as shown by\nour comparison of algorithms. This novel benchmark can be used for comparable\nevaluation and validation of future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_M/0/1/0/all/0/1\">Martin Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat-Peter M&#xfc;ller-Stich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kisilenko_A/0/1/0/all/0/1\">Anna Kisilenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_D/0/1/0/all/0/1\">Duc Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heger_P/0/1/0/all/0/1\">Patrick Heger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mundermann_L/0/1/0/all/0/1\">Lars M&#xfc;ndermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lubotsky_D/0/1/0/all/0/1\">David M Lubotsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Davitashvili_T/0/1/0/all/0/1\">Tornike Davitashvili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capek_M/0/1/0/all/0/1\">Manuela Capek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardazaryan_A/0/1/0/all/0/1\">Armine Vardazaryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eung-Joo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Disch_C/0/1/0/all/0/1\">Constantin Disch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meine_H/0/1/0/all/0/1\">Hans Meine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fucang Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reiter_W/0/1/0/all/0/1\">Wolfgang Reiter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Twick_I/0/1/0/all/0/1\">Isabell Twick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirtac_K/0/1/0/all/0/1\">Kadir Kirtac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosgor_E/0/1/0/all/0/1\">Enes Hosgor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bolmgren_J/0/1/0/all/0/1\">Jon Lindstr&#xf6;m Bolmgren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stenzel_M/0/1/0/all/0/1\">Michael Stenzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siemens_B/0/1/0/all/0/1\">Bj&#xf6;rn von Siemens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes G. Kenngott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_F/0/1/0/all/0/1\">Felix Nickel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frankenberg_M/0/1/0/all/0/1\">Moritz von Frankenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathis_Ullrich_F/0/1/0/all/0/1\">Franziska Mathis-Ullrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodenstedt_S/0/1/0/all/0/1\">Sebastian Bodenstedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented reality navigation system for visual prosthesis. (arXiv:2109.14957v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14957","description":"<p>The visual functions of visual prostheses such as field of view, resolution\nand dynamic range, seriously restrict the person's ability to navigate in\nunknown environments. Implanted patients still require constant assistance for\nnavigating from one location to another. Hence, there is a need for a system\nthat is able to assist them safely during their journey. In this work, we\npropose an augmented reality navigation system for visual prosthesis that\nincorporates a software of reactive navigation and path planning which guides\nthe subject through convenient, obstacle-free route. It consists on four steps:\nlocating the subject on a map, planning the subject trajectory, showing it to\nthe subject and re-planning without obstacles. We have also designed a\nsimulated prosthetic vision environment which allows us to systematically study\nnavigation performance. Twelve subjects participated in the experiment.\nSubjects were guided by the augmented reality navigation system and their\ninstruction was to navigate through different environments until they reached\ntwo goals, cross the door and find an object (bin), as fast and accurately as\npossible. Results show how our augmented navigation system help navigation\nperformance by reducing the time and distance to reach the goals, even\nsignificantly reducing the number of obstacles collisions, compared to other\nbaseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_M/0/1/0/all/0/1\">Melani Sanchez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Yus_A/0/1/0/all/0/1\">Alejandro Perez-Yus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cantin_R/0/1/0/all/0/1\">Ruben Martinez-Cantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_J/0/1/0/all/0/1\">Jose J. Guerrero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Object Detection for Event-based vision using Graph Spectral Clustering. (arXiv:2109.14979v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14979","description":"<p>Moving object detection has been a central topic of discussion in computer\nvision for its wide range of applications like in self-driving cars, video\nsurveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are\nbio-inspired sensors that mimic the working of the human eye. Unlike\nconventional frame-based cameras, these sensors capture a stream of\nasynchronous 'events' that pose multiple advantages over the former, like high\ndynamic range, low latency, low power consumption, and reduced motion blur.\nHowever, these advantages come at a high cost, as the event camera data\ntypically contains more noise and has low resolution. Moreover, as event-based\ncameras can only capture the relative changes in brightness of a scene, event\ndata do not contain usual visual information (like texture and color) as\navailable in video data from normal cameras. So, moving object detection in\nevent-based cameras becomes an extremely challenging task. In this paper, we\npresent an unsupervised Graph Spectral Clustering technique for Moving Object\nDetection in Event-based data (GSCEventMOD). We additionally show how the\noptimum number of moving objects can be automatically determined. Experimental\ncomparisons on publicly available datasets show that the proposed GSCEventMOD\nalgorithm outperforms a number of state-of-the-art techniques by a maximum\nmargin of 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Anindya Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_S/0/1/0/all/0/1\">Shashant R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraldo_J/0/1/0/all/0/1\">Jhony H. Giraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouwmans_T/0/1/0/all/0/1\">Thierry Bouwmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Ananda S. Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Point Cloud Simplification: A Learnable Feature Preserving Approach. (arXiv:2109.14982v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14982","description":"<p>The recent advances in 3D sensing technology have made possible the capture\nof point clouds in significantly high resolution. However, increased detail\nusually comes at the expense of high storage, as well as computational costs in\nterms of processing and visualization operations. Mesh and Point Cloud\nsimplification methods aim to reduce the complexity of 3D models while\nretaining visual quality and relevant salient features. Traditional\nsimplification techniques usually rely on solving a time-consuming optimization\nproblem, hence they are impractical for large-scale datasets. In an attempt to\nalleviate this computational burden, we propose a fast point cloud\nsimplification method by learning to sample salient points. The proposed method\nrelies on a graph neural network architecture trained to select an arbitrary,\nuser-defined, number of points from the input space and to re-arrange their\npositions so as to minimize the visual perception error. The approach is\nextensively evaluated on various datasets using several perceptual metrics.\nImportantly, our method is able to generalize to out-of-distribution shapes,\nhence demonstrating zero-shot capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1\">Rolandos Alexandros Potamias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouritsas_G/0/1/0/all/0/1\">Giorgos Bouritsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Semantic Contour for Object Detection. (arXiv:2109.15009v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15009","description":"<p>Modern object detectors are vulnerable to adversarial examples, which brings\npotential risks to numerous applications, e.g., self-driving car. Among attacks\nregularized by $\\ell_p$ norm, $\\ell_0$-attack aims to modify as few pixels as\npossible. Nevertheless, the problem is nontrivial since it generally requires\nto optimize the shape along with the texture simultaneously, which is an\nNP-hard problem. To address this issue, we propose a novel method of\nAdversarial Semantic Contour (ASC) guided by object contour as prior. With this\nprior, we reduce the searching space to accelerate the $\\ell_0$ optimization,\nand also introduce more semantic information which should affect the detectors\nmore. Based on the contour, we optimize the selection of modified pixels via\nsampling and their colors with gradient descent alternately. Extensive\nexperiments demonstrate that our proposed ASC outperforms the most commonly\nmanually designed patterns (e.g., square patches and grids) on task of\ndisappearing. By modifying no more than 5\\% and 3.5\\% of the object area\nrespectively, our proposed ASC can successfully mislead the mainstream object\ndetectors including the SSD512, Yolov4, Mask RCNN, Faster RCNN, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zijian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15025","description":"<p>3D pose transfer is one of the most challenging 3D generation tasks. It aims\nto transfer the pose of a source mesh to a target mesh and keep the identity\n(e.g., body shape) of the target mesh. Some previous works require key point\nannotations to build reliable correspondence between the source and target\nmeshes, while other methods do not consider any shape correspondence between\nsources and targets, which leads to limited generation quality. In this work,\nwe propose a correspondence-refinement network to help the 3D pose transfer for\nboth human and animal meshes. The correspondence between source and target\nmeshes is first established by solving an optimal transport problem. Then, we\nwarp the source mesh according to the dense correspondence and obtain a coarse\nwarped mesh. The warped mesh will be better refined with our proposed\n\\textit{Elastic Instance Normalization}, which is a conditional normalization\nlayer and can help to generate high-quality meshes. Extensive experimental\nresults show that the proposed architecture can effectively transfer the poses\nfrom source to target meshes and produce better results with satisfied visual\nperformance than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chaoyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Riedones3D: a celtic coin dataset for registration and fine-grained clustering. (arXiv:2109.15033v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15033","description":"<p>Clustering coins with respect to their die is an important component of\nnumismatic research and crucial for understanding the economic history of\ntribes (especially when literary production does not exist, in celtic culture).\nIt is a very hard task that requires a lot of times and expertise. To cluster\nthousands of coins, automatic methods are becoming necessary. Nevertheless,\npublic datasets for coin die clustering evaluation are too rare, though they\nare very important for the development of new methods. Therefore, we propose a\nnew 3D dataset of 2 070 scans of coins. With this dataset, we propose two\nbenchmarks, one for point cloud registration, essential for coin die\nrecognition, and a benchmark of coin die clustering. We show how we\nautomatically cluster coins to help experts, and perform a preliminary\nevaluation for these two tasks. The code of the baseline and the dataset will\nbe publicly available at https://www.npm3d.fr/coins-riedones3d and\nhttps://www.chronocarto.eu/spip.php?article84&amp;lang=fr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horache_S/0/1/0/all/0/1\">Sofiane Horache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruel_K/0/1/0/all/0/1\">Katherine Gruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lejars_T/0/1/0/all/0/1\">Thierry Lejars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masson_O/0/1/0/all/0/1\">Olivier Masson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss. (arXiv:2109.15044v1 [cs.LG])","link":"http://arxiv.org/abs/2109.15044","description":"<p>From ecology to atmospheric sciences, many academic disciplines deal with\ndata characterized by intricate spatio-temporal complexities, the modeling of\nwhich often requires specialized approaches. Generative models of these data\nare of particular interest, as they enable a range of impactful downstream\napplications like simulation or creating synthetic training data. Recent work\nhas highlighted the potential of generative adversarial nets (GANs) for\ngenerating spatio-temporal data. A new GAN algorithm COT-GAN, inspired by the\ntheory of causal optimal transport (COT), was proposed in an attempt to better\ntackle this challenge. However, the task of learning more complex\nspatio-temporal patterns requires additional knowledge of their specific data\nstructures. In this study, we propose a novel loss objective combined with\nCOT-GAN based on an autoregressive embedding to reinforce the learning of\nspatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new\nmetric measuring spatio-temporal autocorrelation by using the deviance of\nobservations from their expected values. We compute SPATE for real and\nsynthetic data samples and use it to compute an embedding loss that considers\nspace-time interactions, nudging the GAN to learn outputs that are faithful to\nthe observed dynamics. We test this new objective on a diverse set of complex\nspatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and\nglobal weather data. We show that our novel embedding loss improves performance\nwithout any changes to the architecture of the COT-GAN backbone, highlighting\nour model's increased capacity for capturing autoregressive structures. We also\ncontextualize our work with respect to recent advances in physics-informed deep\nlearning and interdisciplinary work connecting neural networks with geographic\nand geophysical sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acciaio_B/0/1/0/all/0/1\">Beatrice Acciaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B. Neill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Contextual Video Compression. (arXiv:2109.15047v1 [eess.IV])","link":"http://arxiv.org/abs/2109.15047","description":"<p>Most of the existing neural video compression methods adopt the predictive\ncoding framework, which first generates the predicted frame and then encodes\nits residue with the current frame. However, as for compression ratio,\npredictive coding is only a sub-optimal solution as it uses simple subtraction\noperation to remove the redundancy across frames. In this paper, we propose a\ndeep contextual video compression framework to enable a paradigm shift from\npredictive coding to conditional coding. In particular, we try to answer the\nfollowing questions: how to define, use, and learn condition under a deep video\ncompression framework. To tap the potential of conditional coding, we propose\nusing feature domain context as condition. This enables us to leverage the high\ndimension context to carry rich information to both the encoder and the\ndecoder, which helps reconstruct the high-frequency contents for higher video\nquality. Our framework is also extensible, in which the condition can be\nflexibly designed. Experiments show that our method can significantly\noutperform the previous state-of-the-art (SOTA) deep video compression methods.\nWhen compared with x265 using veryslow preset, we can achieve 26.0% bitrate\nsaving for 1080P standard test videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Workflow Augmentation of Video Data for Event Recognition with Time-Sensitive Neural Networks. (arXiv:2109.15063v1 [eess.IV])","link":"http://arxiv.org/abs/2109.15063","description":"<p>Supervised training of neural networks requires large, diverse and well\nannotated data sets. In the medical field, this is often difficult to achieve\ndue to constraints in time, expert knowledge and prevalence of an event.\nArtificial data augmentation can help to prevent overfitting and improve the\ndetection of rare events as well as overall performance. However, most\naugmentation techniques use purely spatial transformations, which are not\nsufficient for video data with temporal correlations. In this paper, we present\na novel methodology for workflow augmentation and demonstrate its benefit for\nevent recognition in cataract surgery. The proposed approach increases the\nfrequency of event alternation by creating artificial videos. The original\nvideo is split into event segments and a workflow graph is extracted from the\noriginal annotations. Finally, the segments are assembled into new videos based\non the workflow graph. Compared to the original videos, the frequency of event\nalternation in the augmented cataract surgery videos increased by 26%. Further,\na 3% higher classification accuracy and a 7.8% higher precision was achieved\ncompared to a state-of-the-art approach. Our approach is particularly helpful\nto increase the occurrence of rare but important events and can be applied to a\nlarge variety of use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wachter_A/0/1/0/all/0/1\">Andreas Wachter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahm_W/0/1/0/all/0/1\">Werner Nahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iShape: A First Step Towards Irregular Shape Instance Segmentation. (arXiv:2109.15068v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15068","description":"<p>In this paper, we introduce a brand new dataset to promote the study of\ninstance segmentation for objects with irregular shapes. Our key observation is\nthat though irregularly shaped objects widely exist in daily life and\nindustrial scenarios, they received little attention in the instance\nsegmentation field due to the lack of corresponding datasets. To fill this gap,\nwe propose iShape, an irregular shape dataset for instance segmentation. iShape\ncontains six sub-datasets with one real and five synthetics, each represents a\nscene of a typical irregular shape. Unlike most existing instance segmentation\ndatasets of regular objects, iShape has many characteristics that challenge\nexisting instance segmentation algorithms, such as large overlaps between\nbounding boxes of instances, extreme aspect ratios, and large numbers of\nconnected components per instance. We benchmark popular instance segmentation\nmethods on iShape and find their performance drop dramatically. Hence, we\npropose an affinity-based instance segmentation algorithm, called ASIS, as a\nstronger baseline. ASIS explicitly combines perception and reasoning to solve\nArbitrary Shape Instance Segmentation including irregular objects. Experimental\nresults show that ASIS outperforms the state-of-the-art on iShape. Dataset and\ncode are available at https://ishape.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yan Zi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HE_Y/0/1/0/all/0/1\">Yisheng HE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenhang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-Domain Mitosis Detection. (arXiv:2109.15092v1 [eess.IV])","link":"http://arxiv.org/abs/2109.15092","description":"<p>Domain variability is a common bottle neck in developing generalisable\nalgorithms for various medical applications. Motivated by the observation that\nthe domain variability of the medical images is to some extent compact, we\npropose to learn a target representative feature space through unpaired image\nto image translation (CycleGAN). We comprehensively evaluate the performanceand\nusefulness by utilising the transformation to mitosis detection with candidate\nproposal and classification. This work presents a simple yet effective\nmulti-step mitotic figure detection algorithm developed as a baseline for the\nMIDOG challenge. On the preliminary test set, the algorithm scoresan F1 score\nof 0.52.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Mustaffa Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangnani_R/0/1/0/all/0/1\">Ritesh Gangnani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kadiyala_S/0/1/0/all/0/1\">Sasidhar Kadiyala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction. (arXiv:2109.15098v1 [eess.IV])","link":"http://arxiv.org/abs/2109.15098","description":"<p>Current laparoscopic camera motion automation relies on rule-based approaches\nor only focuses on surgical tools. Imitation Learning (IL) methods could\nalleviate these shortcomings, but have so far been applied to oversimplified\nsetups. Instead of extracting actions from oversimplified setups, in this work\nwe introduce a method that allows to extract a laparoscope holder's actions\nfrom videos of laparoscopic interventions. We synthetically add camera motion\nto a newly acquired dataset of camera motion free da Vinci surgery image\nsequences through the introduction of a novel homography generation algorithm.\nThe synthetic camera motion serves as a supervisory signal for camera motion\nestimation that is invariant to object and tool motion. We perform an extensive\nevaluation of state-of-the-art (SOTA) Deep Neural Networks (DNNs) across\nmultiple compute regimes, finding our method transfers from our camera motion\nfree da Vinci surgery dataset to videos of laparoscopic interventions,\noutperforming classical homography estimation approaches in both, precision by\n41%, and runtime on a CPU by 43%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Martin Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-LCNet: A Lightweight CPU Convolutional Neural Network. (arXiv:2109.15099v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15099","description":"<p>We propose a lightweight CPU network based on the MKLDNN acceleration\nstrategy, named PP-LCNet, which improves the performance of lightweight models\non multiple tasks. This paper lists technologies which can improve network\naccuracy while the latency is almost constant. With these improvements, the\naccuracy of PP-LCNet can greatly surpass the previous network structure with\nthe same inference time for classification. As shown in Figure 1, it\noutperforms the most state-of-the-art models. And for downstream tasks of\ncomputer vision, it also performs very well, such as object detection, semantic\nsegmentation, etc. All our experiments are implemented based on PaddlePaddle.\nCode and pretrained models are available at PaddleClas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tingquan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shengyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xueying Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake It Till You Make It: Face analysis in the wild using synthetic data alone. (arXiv:2109.15102v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15102","description":"<p>We demonstrate that it is possible to perform face-related computer vision in\nthe wild using synthetic data alone. The community has long enjoyed the\nbenefits of synthesizing training data with graphics, but the domain gap\nbetween real and synthetic data has remained a problem, especially for human\nfaces. Researchers have tried to bridge this gap with data mixing, domain\nadaptation, and domain-adversarial training, but we show that it is possible to\nsynthesize data with minimal domain gap, so that models trained on synthetic\ndata generalize to real in-the-wild datasets. We describe how to combine a\nprocedurally-generated parametric 3D face model with a comprehensive library of\nhand-crafted assets to render training images with unprecedented realism and\ndiversity. We train machine learning systems for face-related tasks such as\nlandmark localization and face parsing, showing that synthetic data can both\nmatch real data in accuracy as well as open up new approaches where manual\nlabelling would be impossible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wood_E/0/1/0/all/0/1\">Erroll Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1\">Tadas Baltru&#x161;aitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_C/0/1/0/all/0/1\">Charlie Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziadzio_S/0/1/0/all/0/1\">Sebastian Dziadzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estellers_V/0/1/0/all/0/1\">Virginia Estellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_T/0/1/0/all/0/1\">Thomas J. Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1\">Jamie Shotton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-aware Self-supervised Video Representation Learning via Foreground-background Merging. (arXiv:2109.15130v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15130","description":"<p>In light of the success of contrastive learning in the image domain, current\nself-supervised video representation learning methods usually employ\ncontrastive loss to facilitate video representation learning. When naively\npulling two augmented views of a video closer, the model however tends to learn\nthe common static background as a shortcut but fails to capture the motion\ninformation, a phenomenon dubbed as background bias. This bias makes the model\nsuffer from weak generalization ability, leading to worse performance on\ndownstream tasks such as action recognition. To alleviate such bias, we propose\nForeground-background Merging (FAME) to deliberately compose the foreground\nregion of the selected video onto the background of others. Specifically,\nwithout any off-the-shelf detector, we extract the foreground and background\nregions via the frame difference and color statistics, and shuffle the\nbackground regions among the videos. By leveraging the semantic consistency\nbetween the original clips and the fused ones, the model focuses more on the\nforeground motion pattern and is thus more robust to the background context.\nExtensive experiments demonstrate that FAME can significantly boost the\nperformance in different downstream tasks with various backbones. When\nintegrated with MoCo, FAME reaches 84.8% and 53.5% accuracy on UCF101 and\nHMDB51, respectively, achieving the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maomao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning. (arXiv:2109.15163v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15163","description":"<p>Zero-shot learning (ZSL) tackles the unseen class recognition problem,\ntransferring semantic knowledge from seen classes to unseen ones. Typically, to\nguarantee desirable knowledge transfer, a common (latent) space is adopted for\nassociating the visual and semantic domains in ZSL. However, existing common\nspace learning methods align the semantic and visual domains by merely\nmitigating distribution disagreement through one-step adaptation. This strategy\nis usually ineffective due to the heterogeneous nature of the feature\nrepresentations in the two domains, which intrinsically contain both\ndistribution and structure variations. To address this and advance ZSL, we\npropose a novel hierarchical semantic-visual adaptation (HSVA) framework.\nSpecifically, HSVA aligns the semantic and visual domains by adopting a\nhierarchical two-step adaptation, i.e., structure adaptation and distribution\nadaptation. In the structure adaptation step, we take two task-specific\nencoders to encode the source data (visual domain) and the target data\n(semantic domain) into a structure-aligned common space. To this end, a\nsupervised adversarial discrepancy (SAD) module is proposed to adversarially\nminimize the discrepancy between the predictions of two task-specific\nclassifiers, thus making the visual and semantic feature manifolds more closely\naligned. In the distribution adaptation step, we directly minimize the\nWasserstein distance between the latent multivariate Gaussian distributions to\nalign the visual and semantic distributions using a common encoder. Finally,\nthe structure and distribution adaptation are derived in a unified framework\nunder two partially-aligned variational autoencoders. Extensive experiments on\nfour benchmark datasets demonstrate that HSVA achieves superior performance on\nboth conventional and generalized ZSL. The code is available at\n\\url{https://github.com/shiming-chen/HSVA} .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Technical Report for ICCV 2021 VIPriors Re-identification Challenge. (arXiv:2109.15164v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15164","description":"<p>Person re-identification has always been a hot and challenging task. This\npaper introduces our solution for the re-identification track in VIPriors\nChallenge 2021. In this challenge, the difficulty is how to train the model\nfrom scratch without any pretrained weight. In our method, we show use\nstate-of-the-art data processing strategies, model designs, and post-processing\nensemble methods, it is possible to overcome the difficulty of data shortage\nand obtain competitive results. (1) Both image augmentation strategy and novel\npre-processing method for occluded images can help the model learn more\ndiscriminative features. (2) Several strong backbones and multiple loss\nfunctions are used to learn more representative features. (3) Post-processing\ntechniques including re-ranking, automatic query expansion, ensemble learning,\netc., significantly improve the final performance. The final score of our team\n(ALONG) is 96.5154% mAP, ranking first in the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yunbo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSeg: Cognitively Inspired Unsupervised Generic Event Segmentation. (arXiv:2109.15170v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15170","description":"<p>Some cognitive research has discovered that humans accomplish event\nsegmentation as a side effect of event anticipation. Inspired by this\ndiscovery, we propose a simple yet effective end-to-end self-supervised\nlearning framework for event segmentation/boundary detection. Unlike the\nmainstream clustering-based methods, our framework exploits a transformer-based\nfeature reconstruction scheme to detect event boundary by reconstruction\nerrors. This is consistent with the fact that humans spot new events by\nleveraging the deviation between their prediction and what is actually\nperceived. Thanks to their heterogeneity in semantics, the frames at boundaries\nare difficult to be reconstructed (generally with large reconstruction errors),\nwhich is favorable for event boundary detection. Additionally, since the\nreconstruction occurs on the semantic feature level instead of pixel level, we\ndevelop a temporal contrastive feature embedding module to learn the semantic\nvisual representation for frame feature reconstruction. This procedure is like\nhumans building up experiences with \"long-term memory\". The goal of our work is\nto segment generic events rather than localize some specific ones. We focus on\nachieving accurate event boundaries. As a result, we adopt F1 score\n(Precision/Recall) as our primary evaluation metric for a fair comparison with\nprevious approaches. Meanwhile, we also calculate the conventional frame-based\nMoF and IoU metric. We thoroughly benchmark our work on four publicly available\ndatasets and demonstrate much better results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors. (arXiv:2109.15177v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15177","description":"<p>Blind spots or outright deceit can bedevil and deceive machine learning\nmodels. Unidentified objects such as digital \"stickers,\" also known as\nadversarial patches, can fool facial recognition systems, surveillance systems\nand self-driving cars. Fortunately, most existing adversarial patches can be\noutwitted, disabled and rejected by a simple classification network called an\nadversarial patch detector, which distinguishes adversarial patches from\noriginal images. An object detector classifies and predicts the types of\nobjects within an image, such as by distinguishing a motorcyclist from the\nmotorcycle, while also localizing each object's placement within the image by\n\"drawing\" so-called bounding boxes around each object, once again separating\nthe motorcyclist from the motorcycle. To train detectors even better, however,\nwe need to keep subjecting them to confusing or deceitful adversarial patches\nas we probe for the models' blind spots. For such probes, we came up with a\nnovel approach, a Low-Detectable Adversarial Patch, which attacks an object\ndetector with small and texture-consistent adversarial patches, making these\nadversaries less likely to be recognized. Concretely, we use several geometric\nprimitives to model the shapes and positions of the patches. To enhance our\nattack performance, we also assign different weights to the bounding boxes in\nterms of loss function. Our experiments on the common detection dataset COCO as\nwell as the driving-video dataset D2-City show that LDAP is an effective attack\nmethod, and can resist the adversarial patch detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zijian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shibao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15207","description":"<p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates\na 3D environment, following natural language instructions. A challenge in this\ntask is how to handle 'off the path' scenarios where an agent veers from a\nreference path. Prior work supervises the agent with actions based on the\nshortest path from the agent's location to the goal, but such goal-oriented\nsupervision is often not in alignment with the instruction. Furthermore, the\nevaluation metrics employed by prior work do not measure how much of a language\ninstruction the agent is able to follow. In this work, we propose a simple and\neffective language-aligned supervision scheme, and a new metric that measures\nthe number of sub-instructions the agent has completed during navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_S/0/1/0/all/0/1\">Sonia Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wani_S/0/1/0/all/0/1\">Saim Wani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivansh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1\">Unnat Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Out-of-Distribution Detection and Localization with Natural Synthetic Anomalies (NSA). (arXiv:2109.15222v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15222","description":"<p>We introduce a new self-supervised task, NSA, for training an end-to-end\nmodel for anomaly detection and localization using only normal data. NSA uses\nPoisson image editing to seamlessly blend scaled patches of various sizes from\nseparate images. This creates a wide range of synthetic anomalies which are\nmore similar to natural sub-image irregularities than previous\ndata-augmentation strategies for self-supervised anomaly detection. We evaluate\nthe proposed method using natural and medical images. Our experiments with the\nMVTec AD dataset show that a model trained to localize NSA anomalies\ngeneralizes well to detecting real-world a priori unknown types of\nmanufacturing defects. Our method achieves an overall detection AUROC of 97.2\noutperforming all previous methods that learn from scratch without pre-training\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schluter_H/0/1/0/all/0/1\">Hannah M. Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Tactile Grasp Force Sensing Using Fingernail Imaging via Deep Neural Networks. (arXiv:2109.15231v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15231","description":"<p>This paper has introduced a novel approach for the real-time estimation of 3D\ntactile forces exerted by human fingertips via vision only. The introduced\napproach is entirely monocular vision-based and does not require any physical\nforce sensor. Therefore, it is scalable, non-intrusive, and easily fused with\nother perception systems such as body pose estimation, making it ideal for HRI\napplications where force sensing is necessary. The introduced approach consists\nof three main modules: finger tracking for detection and tracking of each\nindividual finger, image alignment for preserving the spatial information in\nthe images, and the force model for estimating the 3D forces from coloration\npatterns in the images. The model has been implemented experimentally, and the\nresults have shown a maximum RMS error of 8.4% (for the entire range of force\nlevels) along all three directions. The estimation accuracy is comparable to\nthe offline models in the literature, such as EigneNail, while, this model is\ncapable of performing force estimation at 30 frames per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fallahinia_N/0/1/0/all/0/1\">Navid Fallahinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascaro_S/0/1/0/all/0/1\">Stephen Mascaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability Estimation for Semantic Segmentation Task. (arXiv:2109.15242v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15242","description":"<p>Transferability estimation is a fundamental problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. With the guidance of transferability score, we can\nefficiently select the highly transferable source models without performing the\nreal transfer in practice. Recent analytical transferability metrics are mainly\ndesigned for image classification, and currently there is no specific\ninvestigation for the transferability estimation of semantic segmentation task,\nwhich is an essential problem in autonomous driving, medical image analysis,\netc. Consequently, we further extend the recent analytical transferability\nmetric OTCE (Optimal Transport based Conditional Entropy) score to the semantic\nsegmentation task. The challenge in applying the OTCE score is the high\ndimensional segmentation output, which is difficult to find the optimal\ncoupling between so many pixels under an acceptable computation cost. Thus we\npropose to randomly sample N pixels for computing OTCE score and take the\nexpectation over K repetitions as the final transferability score. Experimental\nevaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE\nscore highly correlates with the transfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T\\\"oRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis. (arXiv:2109.15271v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15271","description":"<p>Neural networks can represent and accurately reconstruct radiance fields for\nstatic 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes\ncaptured with monocular video, with promising performance. However, the\nmonocular setting is known to be an under-constrained problem, and so methods\nrely on data-driven priors for reconstructing dynamic content. We replace these\npriors with measurements from a time-of-flight (ToF) camera, and introduce a\nneural representation based on an image formation model for continuous-wave ToF\ncameras. Instead of working with processed depth maps, we model the raw ToF\nsensor measurements to improve reconstruction quality and avoid issues with low\nreflectance regions, multi-path interference, and a sensor's limited\nunambiguous depth range. We show that this approach improves robustness of\ndynamic scene reconstruction to erroneous calibration and large motions, and\ndiscuss the benefits and limitations of integrating RGB+ToF sensors that are\nnow available on modern smartphones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attal_B/0/1/0/all/0/1\">Benjamin Attal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlaw_E/0/1/0/all/0/1\">Eliot Laidlaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1\">Christian Richardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_M/0/1/0/all/0/1\">Matthew O&#x27;Toole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAAS: Differentiable Architecture and Augmentation Policy Search. (arXiv:2109.15273v1 [cs.LG])","link":"http://arxiv.org/abs/2109.15273","description":"<p>Neural architecture search (NAS) has been an active direction of automatic\nmachine learning (Auto-ML), aiming to explore efficient network structures. The\nsearched architecture is evaluated by training on datasets with fixed data\naugmentation policies. However, recent works on auto-augmentation show that the\nsuited augmentation policies can vary over different structures. Therefore,\nthis work considers the possible coupling between neural architectures and data\naugmentation and proposes an effective algorithm jointly searching for them.\nSpecifically, 1) for the NAS task, we adopt a single-path based differentiable\nmethod with Gumbel-softmax reparameterization strategy due to its memory\nefficiency; 2) for the auto-augmentation task, we introduce a novel search\nmethod based on policy gradient algorithm, which can significantly reduce the\ncomputation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and\n76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance\nof our search algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bend-Net: Bending Loss Regularized Multitask Learning Network for Nuclei Segmentation in Histopathology Images. (arXiv:2109.15283v1 [eess.IV])","link":"http://arxiv.org/abs/2109.15283","description":"<p>Separating overlapped nuclei is a major challenge in histopathology image\nanalysis. Recently published approaches have achieved promising overall\nperformance on nuclei segmentation; however, their performance on separating\noverlapped nuclei is quite limited. To address the issue, we propose a novel\nmultitask learning network with a bending loss regularizer to separate\noverlapped nuclei accurately. The newly proposed multitask learning\narchitecture enhances the generalization by learning shared representation from\nthree tasks: instance segmentation, nuclei distance map prediction, and\noverlapped nuclei distance map prediction. The proposed bending loss defines\nhigh penalties to concave contour points with large curvatures, and applies\nsmall penalties to convex contour points with small curvatures. Minimizing the\nbending loss avoids generating contours that encompass multiple nuclei. In\naddition, two new quantitative metrics, Aggregated Jaccard Index of overlapped\nnuclei (AJIO) and Accuracy of overlapped nuclei (ACCO), are designed for the\nevaluation of overlapped nuclei segmentation. We validate the proposed approach\non the CoNSeP and MoNuSegv1 datasets using seven quantitative metrics:\nAggregate Jaccard Index, Dice, Segmentation Quality, Recognition Quality,\nPanoptic Quality, AJIO, and ACCO. Extensive experiments demonstrate that the\nproposed Bend-Net outperforms eight state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation. (arXiv:2109.15286v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15286","description":"<p>Scene understanding is a pivotal task for autonomous vehicles to safely\nnavigate in the environment. Recent advances in deep learning enable accurate\nsemantic reconstruction of the surroundings from LiDAR data. However, these\nmodels encounter a large domain gap while deploying them on vehicles equipped\nwith different LiDAR setups which drastically decreases their performance.\nFine-tuning the model for every new setup is infeasible due to the expensive\nand cumbersome process of recording and manually labeling new data.\nUnsupervised Domain Adaptation (UDA) techniques are thus essential to fill this\ndomain gap and retain the performance of models on new sensor setups without\nthe need for additional data labeling. In this paper, we propose AdaptLPS, a\nnovel UDA approach for LiDAR panoptic segmentation that leverages task-specific\nknowledge and accounts for variation in the number of scan lines, mounting\nposition, intensity distribution, and environmental conditions. We tackle the\nUDA task by employing two complementary domain adaptation strategies,\ndata-based and model-based. While data-based adaptations reduce the domain gap\nby processing the raw LiDAR scans to resemble the scans in the target domain,\nmodel-based techniques guide the network in extracting features that are\nrepresentative for both domains. Extensive evaluations on three pairs of\nreal-world autonomous driving datasets demonstrate that AdaptLPS outperforms\nexisting UDA approaches by up to 6.41 pp in terms of the PQ score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besic_B/0/1/0/all/0/1\">Borna Be&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity-Disentangled Neural Deformation Model for Dynamic Meshes. (arXiv:2109.15299v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15299","description":"<p>Neural shape models can represent complex 3D shapes with a compact latent\nspace. When applied to dynamically deforming shapes such as the human hands,\nhowever, they would need to preserve temporal coherence of the deformation as\nwell as the intrinsic identity of the subject. These properties are difficult\nto regularize with manually designed loss functions. In this paper, we learn a\nneural deformation model that disentangles the identity-induced shape\nvariations from pose-dependent deformations using implicit neural functions. We\nperform template-free unsupervised learning on 3D scans without explicit mesh\ncorrespondence or semantic correspondences of shapes across subjects. We can\nthen apply the learned model to reconstruct partial dynamic 4D scans of novel\nsubjects performing unseen actions. We propose two methods to integrate global\npose alignment with our neural deformation model. Experiments demonstrate the\nefficacy of our method in the disentanglement of identities and pose. Our\nmethod also outperforms traditional skeleton-driven models in reconstructing\nsurface details such as palm prints or tendons without limitations from a fixed\ntemplate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_T/0/1/0/all/0/1\">Tanner Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1\">Christopher D. Twigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovegrove_S/0/1/0/all/0/1\">Steven Lovegrove</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation. (arXiv:2109.15317v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15317","description":"<p>We present MetaUVFS as the first Unsupervised Meta-learning algorithm for\nVideo Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled\nvideos to train a two-stream 2D and 3D CNN architecture via contrastive\nlearning to capture the appearance-specific spatial and action-specific\nspatio-temporal video features respectively. MetaUVFS comprises a novel\nAction-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on\nthe action-oriented video features in relation to the appearance features via\nexplicit few-shot episodic meta-learning over unsupervised hard-mined episodes.\nOur action-appearance alignment and explicit few-shot learner conditions the\nunsupervised training to mimic the downstream few-shot task, enabling MetaUVFS\nto significantly outperform all unsupervised methods on few-shot benchmarks.\nMoreover, unlike previous few-shot action recognition methods that are\nsupervised, MetaUVFS needs neither base-class labels nor a supervised\npretrained backbone. Thus, we need to train MetaUVFS just once to perform\ncompetitively or sometimes even outperform state-of-the-art supervised methods\non popular HMDB51, UCF101, and Kinetics100 few-shot datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patravali_J/0/1/0/all/0/1\">Jay Patravali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor-Guided Optical Flow. (arXiv:2109.15321v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15321","description":"<p>This paper proposes a framework to guide an optical flow network with\nexternal cues to achieve superior accuracy either on known or unseen domains.\nGiven the availability of sparse yet accurate optical flow hints from an\nexternal source, these are injected to modulate the correlation scores computed\nby a state-of-the-art optical flow network and guide it towards more accurate\npredictions. Although no real sensor can provide sparse flow hints, we show how\nthese can be obtained by combining depth measurements from active sensors with\ngeometry and hand-crafted optical flow algorithms, leading to accurate enough\nhints for our purpose. Experimental results with a state-of-the-art flow\nnetwork on standard benchmarks support the effectiveness of our framework, both\nin simulated and real conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aleotti_F/0/1/0/all/0/1\">Filippo Aleotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1\">Stefano Mattoccia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.02620","description":"<p>How will my face look when I get older? Or, for a more challenging question:\nHow will my brain look when I get older? To answer this question one must\ndevise (and learn from data) a multivariate auto-regressive function which\ngiven an image and a desired target age generates an output image. While\ncollecting data for faces may be easier, collecting longitudinal brain data is\nnot trivial. We propose a deep learning-based method that learns to simulate\nsubject-specific brain ageing trajectories without relying on longitudinal\ndata. Our method synthesises images conditioned on two factors: age (a\ncontinuous variable), and status of Alzheimer's Disease (AD, an ordinal\nvariable). With an adversarial formulation we learn the joint distribution of\nbrain appearance, age and AD status, and define reconstruction losses to\naddress the challenging problem of preserving subject identity. We compare with\nseveral benchmarks using two widely used datasets. We evaluate the quality and\nrealism of synthesised images using ground-truth longitudinal data and a\npre-trained age predictor. We show that, despite the use of cross-sectional\ndata, our model learns patterns of gray matter atrophy in the middle temporal\ngyrus in patients with AD. To demonstrate generalisation ability, we train on\none dataset and evaluate predictions on the other. In conclusion, our model\nshows an ability to separate age, disease influence and anatomy using only 2D\ncross-sectional data that should be useful in large studies into\nneurodegenerative disease, that aim to combine several data sources. To\nfacilitate such future studies by the community at large our code is made\navailable at https://github.com/xiat0616/BrainAgeing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengjia Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond. (arXiv:2004.14557v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.14557","description":"<p>Conventional deformable registration methods aim at solving an optimization\nmodel carefully designed on image pairs and their computational costs are\nexceptionally high. In contrast, recent deep learning based approaches can\nprovide fast deformation estimation. These heuristic network architectures are\nfully data-driven and thus lack explicit geometric constraints, e.g.,\ntopology-preserving, which are indispensable to generate plausible\ndeformations. We design a new deep learning based framework to optimize a\ndiffeomorphic model via multi-scale propagation in order to integrate\nadvantages and avoid limitations of these two categories of approaches.\nSpecifically, we introduce a generic optimization model to formulate\ndiffeomorphic registration and develop a series of learnable architectures to\nobtain propagative updating in the coarse-to-fine feature space. Moreover, we\npropose a novel bilevel self-tuned training strategy, allowing efficient search\nof task-specific hyper-parameters. This training strategy increases the\nflexibility to various types of data while reduces computational and human\nburdens. We conduct two groups of image registration experiments on 3D volume\ndatasets including image-to-atlas registration on brain MRI data and\nimage-to-image registration on liver CT data. Extensive results demonstrate the\nstate-of-the-art performance of the proposed method with diffeomorphic\nguarantee and extreme efficiency. We also apply our framework to challenging\nmulti-modal image registration, and investigate how our registration to support\nthe down-streaming tasks for medical image analysis including multi-modal\nfusion and image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongxuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutTransformer: Layout Generation and Completion with Self-attention. (arXiv:2006.14615v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.14615","description":"<p>We address the problem of scene layout generation for diverse domains such as\nimages, mobile applications, documents, and 3D objects. Most complex scenes,\nnatural or human-designed, can be expressed as a meaningful arrangement of\nsimpler compositional graphical primitives. Generating a new layout or\nextending an existing layout requires understanding the relationships between\nthese primitives. To do this, we propose LayoutTransformer, a novel framework\nthat leverages self-attention to learn contextual relationships between layout\nelements and generate novel layouts in a given domain. Our framework allows us\nto generate a new layout either from an empty set or from an initial seed set\nof primitives, and can easily scale to support an arbitrary of primitives per\nlayout. Furthermore, our analyses show that the model is able to automatically\ncapture the semantic properties of the primitives. We propose simple\nimprovements in both representation of layout primitives, as well as training\nmethods to demonstrate competitive performance in very diverse data domains\nsuch as object bounding boxes in natural images(COCO bounding box), documents\n(PubLayNet), mobile applications (RICO dataset) as well as 3D shapes\n(Part-Net). Code and other materials will be made available at\nhttps://kampta.github.io/layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kamal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazarow_J/0/1/0/all/0/1\">Justin Lazarow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-based Hyperparameter Optimization Over Long Horizons. (arXiv:2007.07869v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.07869","description":"<p>Gradient-based hyperparameter optimization has earned a widespread popularity\nin the context of few-shot meta-learning, but remains broadly impractical for\ntasks with long horizons (many gradient steps), due to memory scaling and\ngradient degradation issues. A common workaround is to learn hyperparameters\nonline, but this introduces greediness which comes with a significant\nperformance drop. We propose forward-mode differentiation with sharing (FDS), a\nsimple and efficient algorithm which tackles memory scaling issues with\nforward-mode differentiation, and gradient degradation issues by sharing\nhyperparameters that are contiguous in time. We provide theoretical guarantees\nabout the noise reduction properties of our algorithm, and demonstrate its\nefficiency empirically by differentiating through $\\sim 10^4$ gradient steps of\nunrolled optimization. We consider large hyperparameter search ranges on\nCIFAR-10 where we significantly outperform greedy gradient-based alternatives,\nwhile achieving $\\times 20$ speedups compared to the state-of-the-art black-box\nmethods. Code is available at: \\url{https://github.com/polo5/FDS}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micaelli_P/0/1/0/all/0/1\">Paul Micaelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suppression of Independent and Correlated Noise with Similarity-based Unsupervised Deep Learning. (arXiv:2011.03384v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.03384","description":"<p>Denoising is one of the most important data processing tasks and is generally\na prerequisite for downstream image analysis in many fields. Despite their\nsuperior denoising performance, supervised deep denoising methods require\npaired noise-clean or noise-noise samples often unavailable in practice. On the\nother hand, unsupervised deep denoising methods such as Noise2Void and its\nvariants predict masked pixels from their neighboring pixels in single noisy\nimages. However, these unsupervised algorithms only work under the independent\nnoise assumption while real noise distributions are usually correlated with\ncomplex structural patterns. Here we propose the first-of-its-kind feature\nsimilarity-based unsupervised denoising approach that works in a nonlocal and\nnonlinear fashion to suppress not only independent but also correlated noise.\nOur approach is referred to as Noise2Sim since different noisy sub-images with\nsimilar signals are extracted to form as many as possible training pairs so\nthat the parameters of a deep denoising network can be optimized in a\nself-learning fashion. Theoretically, the theorem is established that Noise2Sim\nis equivalent to the supervised learning methods under mild conditions.\nExperimentally, Noise2Sim achieves excellent results on natural, microscopic,\nlow-dose CT and photon-counting micro-CT images, removing image noise\nindependent or not and being superior to the competitive denoising methods.\nPotentially, Noise2Sim would open a new direction of research and lead to the\ndevelopment of adaptive denoising tools in diverse applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning. (arXiv:2012.01345v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01345","description":"<p>Computational food analysis (CFA) naturally requires multi-modal evidence of\na particular food, e.g., images, recipe text, etc. A key to making CFA possible\nis multi-modal shared representation learning, which aims to create a joint\nrepresentation of the multiple views (text and image) of the data. In this work\nwe propose a method for food domain cross-modal shared representation learning\nthat preserves the vast semantic richness present in the food data. Our\nproposed method employs an effective transformer-based multilingual recipe\nencoder coupled with a traditional image embedding architecture. Here, we\npropose the use of imperfect multilingual translations to effectively\nregularize the model while at the same time adding functionality across\nmultiple languages and alphabets. Experimental analysis on the public Recipe1M\ndataset shows that the representation learned via the proposed method\nsignificantly outperforms the current state-of-the-arts (SOTA) on retrieval\ntasks. Furthermore, the representational power of the learned representation is\ndemonstrated through a generative food image synthesis model conditioned on\nrecipe embeddings. Synthesized images can effectively reproduce the visual\nappearance of paired samples, indicating that the learned representation\ncaptures the joint semantics of both the textual recipe and its visual content,\nthus narrowing the modality gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hai Xuan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-to-real for high-resolution optical tactile sensing: From images to 3D contact force distributions. (arXiv:2012.11295v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2012.11295","description":"<p>The images captured by vision-based tactile sensors carry information about\nhigh-resolution tactile fields, such as the distribution of the contact forces\napplied to their soft sensing surface. However, extracting the information\nencoded in the images is challenging and often addressed with learning-based\napproaches, which generally require a large amount of training data. This\narticle proposes a strategy to generate tactile images in simulation for a\nvision-based tactile sensor based on an internal camera that tracks the motion\nof spherical particles within a soft material. The deformation of the material\nis simulated in a finite element environment under a diverse set of contact\nconditions, and spherical particles are projected to a simulated image.\nFeatures extracted from the images are mapped to the 3D contact force\ndistribution, with the ground truth also obtained via finite-element\nsimulations, with an artificial neural network that is therefore entirely\ntrained on synthetic data avoiding the need for real-world data collection. The\nresulting model exhibits high accuracy when evaluated on real-world tactile\nimages, is transferable across multiple tactile sensors without further\ntraining, and is suitable for efficient real-time inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAndrea_R/0/1/0/all/0/1\">Raffaello D&#x27;Andrea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.04353","description":"<p>There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and the potential for observational overfitting\nthrough spurious correlations. A promising approach to solve both of these\nproblems is an attention bottleneck, which provides a simple and effective\nframework for learning high performing policies, even in the presence of\ndistractions. However, due to poor scalability of attention architectures,\nthese methods cannot be applied beyond low resolution visual inputs, using\nlarge patches (thus small attention matrices). In this paper we make use of new\nefficient attention algorithms, recently shown to be highly effective for\nTransformers, and demonstrate that these techniques can be successfully adopted\nfor the RL setting. This allows our attention-based controllers to scale to\nlarger visual inputs, and facilitate the use of smaller patches, even\nindividual pixels, improving generalization. We show this on a range of tasks\nfrom the Distracting Control Suite to vision-based quadruped robots locomotion.\nWe provide rigorous theoretical analysis of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Marcin Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Deepali Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1\">Valerii Likhosherstov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1\">Anirban Santara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jie Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation. (arXiv:2102.12867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12867","description":"<p>Recent methods for long-tailed instance segmentation still struggle on rare\nobject classes with few training data. We propose a simple yet effective\nmethod, Feature Augmentation and Sampling Adaptation (FASA), that addresses the\ndata scarcity issue by augmenting the feature space especially for rare\nclasses. Both the Feature Augmentation (FA) and feature sampling components are\nadaptive to the actual training status -- FA is informed by the feature mean\nand variance of observed real samples from past iterations, and we sample the\ngenerated virtual features in a loss-adapted manner to avoid over-fitting. FASA\ndoes not require any elaborate loss design, and removes the need for\ninter-class transfer learning that often involves large cost and\nmanually-defined head/tail class groups. We show FASA is a fast, generic method\nthat can be easily plugged into standard or long-tailed segmentation\nframeworks, with consistent performance gains and little added cost. FASA is\nalso applicable to other tasks like long-tailed classification with\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1\">Yuhang Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Event-based Visuomotor Policies. (arXiv:2103.00806v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00806","description":"<p>Event-based cameras are dynamic vision sensors that provide asynchronous\nmeasurements of changes in per-pixel brightness at a microsecond level. This\nmakes them significantly faster than conventional frame-based cameras, and an\nappealing choice for high-speed navigation. While an interesting sensor\nmodality, this asynchronously streamed event data poses a challenge for machine\nlearning techniques that are more suited for frame-based data. In this paper,\nwe present an event variational autoencoder and show that it is feasible to\nlearn compact representations directly from asynchronous spatiotemporal event\ndata. Furthermore, we show that such pretrained representations can be used for\nevent-based reinforcement learning instead of end-to-end reward driven\nperception. We validate this framework of learning event-based visuomotor\npolicies by applying it to an obstacle avoidance scenario in simulation.\nCompared to techniques that treat event data as images, we show that\nrepresentations learnt from event streams result in faster policy training,\nadapt to different control capacities, and demonstrate a higher degree of\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_S/0/1/0/all/0/1\">Sami Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04537","description":"<p>We propose and demonstrate a representation learning approach by maximizing\nthe mutual information between local features of images and text. The goal of\nthis approach is to learn useful image representations by taking advantage of\nthe rich information contained in the free text that describes the findings in\nthe image. Our method trains image and text encoders by encouraging the\nresulting representations to exhibit high local mutual information. We make use\nof recent advances in mutual information estimation with neural network\ndiscriminators. We argue that the sum of local mutual information is typically\na lower bound on the global mutual information. Our experimental results in the\ndownstream image classification tasks demonstrate the advantages of using local\nfeatures for image-text representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruizhi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1\">Keegan Quigley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1\">Seth Berkowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Steven Horng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning. (arXiv:2103.11395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11395","description":"<p>In this paper, we address the problem of training deep neural networks in the\npresence of severe label noise. Our proposed training algorithm ScanMix,\ncombines semantic clustering with semi-supervised learning (SSL) to improve the\nfeature representations and enable an accurate identification of noisy samples,\neven in severe label noise scenarios. To be specific, ScanMix is designed based\non the expectation maximisation (EM) framework, where the E-step estimates the\nvalue of a latent variable to cluster the training images based on their\nappearance representations and classification results, and the M-step optimises\nthe SSL classification and learns effective feature representations via\nsemantic clustering. In our evaluations, we show state-of-the-art results on\nstandard benchmarks for symmetric, asymmetric and semantic label noise on\nCIFAR-10 and CIFAR-100, as well as large scale real label noise on WebVision.\nMost notably, for the benchmarks contaminated with large noise rates (80% and\nabove), our results are up to 27% better than the related work. The code is\navailable at https://github.com/ragavsachdeva/ScanMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Ragav Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1\">Filipe R Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Instance Segmentation with a Propose-Reduce Paradigm. (arXiv:2103.13746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13746","description":"<p>Video instance segmentation (VIS) aims to segment and associate all instances\nof predefined classes for each frame in videos. Prior methods usually obtain\nsegmentation for a frame or clip first, and merge the incomplete results by\ntracking or matching. These methods may cause error accumulation in the merging\nstep. Contrarily, we propose a new paradigm -- Propose-Reduce, to generate\ncomplete sequences for input videos by a single step. We further build a\nsequence propagation head on the existing image-level instance segmentation\nnetwork for long-term propagation. To ensure robustness and high recall of our\nproposed framework, multiple sequences are proposed where redundant sequences\nof the same instance are reduced. We achieve state-of-the-art performance on\ntwo representative benchmark datasets -- we obtain 47.6% in terms of AP on\nYouTube-VIS validation set and 70.4% for J&amp;F on DAVIS-UVOS validation set. Code\nis available at https://github.com/dvlab-research/ProposeReduce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huaijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruizheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution. (arXiv:2103.14006v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.14006","description":"<p>It is widely acknowledged that single image super-resolution (SISR) methods\nwould not perform well if the assumed degradation model deviates from those in\nreal images. Although several degradation models take additional factors into\nconsideration, such as blur, they are still not effective enough to cover the\ndiverse degradations of real images. To address this issue, this paper proposes\nto design a more complex but practical degradation model that consists of\nrandomly shuffled blur, downsampling and noise degradations. Specifically, the\nblur is approximated by two convolutions with isotropic and anisotropic\nGaussian kernels; the downsampling is randomly chosen from nearest, bilinear\nand bicubic interpolations; the noise is synthesized by adding Gaussian noise\nwith different noise levels, adopting JPEG compression with different quality\nfactors, and generating processed camera sensor noise via reverse-forward\ncamera image signal processing (ISP) pipeline model and RAW image noise model.\nTo verify the effectiveness of the new degradation model, we have trained a\ndeep blind ESRGAN super-resolver and then applied it to super-resolve both\nsynthetic and real images with diverse degradations. The experimental results\ndemonstrate that the new degradation model can help to significantly improve\nthe practicability of deep super-resolvers, thus providing a powerful\nalternative solution for real SISR applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch2Mesh: Reconstructing and Editing 3D Shapes from Sketches. (arXiv:2104.00482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00482","description":"<p>Reconstructing 3D shape from 2D sketches has long been an open problem\nbecause the sketches only provide very sparse and ambiguous information. In\nthis paper, we use an encoder/decoder architecture for the sketch to mesh\ntranslation. When integrated into a user interface that provides camera\nparameters for the sketches, this enables us to leverage its latent\nparametrization to represent and refine a 3D mesh so that its projections match\nthe external contours outlined in the sketch. We will show that this approach\nis easy to deploy, robust to style changes, and effective. Furthermore, it can\nbe used for shape refinement given only single pen strokes. We compare our\napproach to state-of-the-art methods on sketches -- both hand-drawn and\nsynthesized -- and demonstrate that we outperform them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1\">Benoit Guillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1\">Edoardo Remelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvernay_P/0/1/0/all/0/1\">Pierre Yvernay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13840","description":"<p>Very recently, a variety of vision transformer architectures for dense\nprediction tasks have been proposed and they show that the design of spatial\nattention is critical to their success in these tasks. In this work, we revisit\nthe design of the spatial attention and demonstrate that a carefully-devised\nyet simple spatial attention mechanism performs favourably against the\nstate-of-the-art schemes. As a result, we propose two vision transformer\narchitectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures\nare highly-efficient and easy to implement, only involving matrix\nmultiplications that are highly optimized in modern deep learning frameworks.\nMore importantly, the proposed architectures achieve excellent performance on a\nwide range of visual tasks, including image level classification as well as\ndense detection and segmentation. The simplicity and strong performance suggest\nthat our proposed architectures may serve as stronger backbones for many vision\ntasks. Our code is released at https://github.com/Meituan-AutoML/Twins .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huaxia Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOLQ: Segmenting Objects by Learning Queries. (arXiv:2106.02351v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02351","description":"<p>In this paper, we propose an end-to-end framework for instance segmentation.\nBased on the recently introduced DETR [1], our method, termed SOLQ, segments\nobjects by learning unified queries. In SOLQ, each query represents one object\nand has multiple representations: class, location and mask. The object queries\nlearned perform classification, box regression and mask encoding simultaneously\nin an unified vector form. During training phase, the mask vectors encoded are\nsupervised by the compression coding of raw spatial masks. In inference time,\nmask vectors produced can be directly transformed to spatial masks by the\ninverse process of compression coding. Experimental results show that SOLQ can\nachieve state-of-the-art performance, surpassing most of existing approaches.\nMoreover, the joint learning of unified query representation can greatly\nimprove the detection performance of DETR. We hope our SOLQ can serve as a\nstrong baseline for the Transformer-based instance segmentation. Code is\navailable at https://github.com/megvii-research/SOLQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fangao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yichen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic Videos. (arXiv:2107.11629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11629","description":"<p>Exploring to what humans pay attention in dynamic panoramic scenes is useful\nfor many fundamental applications, including augmented reality (AR) in retail,\nAR-powered recruitment, and visual language navigation. With this goal in mind,\nwe propose PV-SOD, a new task that aims to segment salient objects from\npanoramic videos. In contrast to existing fixation-/object-level saliency\ndetection tasks, we focus on audio-induced salient object detection (SOD),\nwhere the salient objects are labeled with the guidance of audio-induced eye\nmovements. To support this task, we collect the first large-scale dataset,\nnamed ASOD60K, which contains 4K-resolution video frames annotated with a\nsix-level hierarchy, thus distinguishing itself with richness, diversity and\nquality. Specifically, each sequence is marked with both its super-/sub-class,\nwith objects of each sub-class being further annotated with human eye\nfixations, bounding boxes, object-/instance-level masks, and associated\nattributes (e.g., geometrical distortion). These coarse-to-fine annotations\nenable detailed analysis for PV-SOD modelling, e.g., determining the major\nchallenges for existing SOD models, and predicting scanpaths to study the\nlong-term eye fixation behaviors of humans. We systematically benchmark 11\nrepresentative approaches on ASOD60K and derive several interesting findings.\nWe hope this study could serve as a good starting point for advancing SOD\nresearch towards panoramic videos. The dataset and benchmark will be made\npublicly available at https://github.com/PanoAsh/ASOD60K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fang-Yi Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation and CNN Classification For Automatic COVID-19 Diagnosis From CT-Scan Images On Small Dataset. (arXiv:2108.07148v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07148","description":"<p>We present an automatic COVID1-19 diagnosis framework from lung CT images.\nThe focus is on signal processing and classification on small datasets with\nefforts putting into exploring data preparation and augmentation to improve the\ngeneralization capability of the 2D CNN classification models. We propose a\nunique and effective data augmentation method using multiple Hounsfield Unit\n(HU) normalization windows. In addition, the original slice image is cropped to\nexclude background, and a filter is applied to filter out closed-lung images.\nFor the classification network, we choose to use 2D Densenet and Xception with\nthe feature pyramid network (FPN). To further improve the classification\naccuracy, an ensemble of multiple CNN models and HU windows is used. On the\ntraining/validation dataset, we achieve a patient classification accuracy of\n93.39%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hongwei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Robust Mitotic Figure Detection with Style Transfer. (arXiv:2109.01124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01124","description":"<p>We propose a new training scheme for domain generalization in mitotic figure\ndetection. Mitotic figures show different characteristics for each scanner. We\nconsider each scanner as a 'domain' and the image distribution specified for\neach domain as 'style'. The goal is to train our network to be robust on\nscanner types by using various 'style' images. To expand the style variance, we\ntransfer a style of the training image into arbitrary styles, by defining a\nmodule based on StarGAN. Our model with the proposed training scheme shows\npositive performance on MIDOG Preliminary Test-Set containing scanners never\nseen before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Youjin Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jihoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinah Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous Driving. (arXiv:2109.07648v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07648","description":"<p>We present a new and complex traffic dataset, METEOR, which captures traffic\npatterns in unstructured scenarios in India. METEOR consists of more than 1000\none-minute video clips, over 2 million annotated frames with ego-vehicle\ntrajectories, and more than 13 million bounding boxes for surrounding vehicles\nor traffic agents. METEOR is a unique dataset in terms of capturing the\nheterogeneity of microscopic and macroscopic traffic characteristics.\nFurthermore, we provide annotations for rare and interesting driving behaviors\nsuch as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane\nchanging, running traffic signals, driving in the wrong lanes, taking wrong\nturns, lack of right-of-way rules at intersections, etc. We also present\ndiverse traffic scenarios corresponding to rainy weather, nighttime driving,\ndriving in rural areas with unmarked roads, and high-density traffic scenarios.\nWe use our novel dataset to evaluate the performance of object detection and\nbehavior prediction algorithms. We show that state-of-the-art object detectors\nfail in these challenging conditions and also propose a new benchmark test:\naction-behavior prediction with a baseline mAP score of 70.74.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_M/0/1/0/all/0/1\">Mridul Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kala_R/0/1/0/all/0/1\">Rahul Kala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palugulla_R/0/1/0/all/0/1\">Rishitha Palugulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_C/0/1/0/all/0/1\">Chandrababu Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Alok Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Gesture Recognition. (arXiv:2109.09396v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09396","description":"<p>The Human-Machine Interaction (HMI) research field is an important topic in\nmachine learning that has been deeply investigated thanks to the rise of\ncomputing power in the last years. The first time, it is possible to use\nmachine learning to classify images and/or videos instead of the traditional\ncomputer vision algorithms. The aim of this paper is to build a symbiosis\nbetween a convolutional neural network (CNN) and a recurrent neural network\n(RNN) to recognize cultural/anthropological Italian sign language gestures from\nvideos. The CNN extracts important features that later are used by the RNN.\nWith RNNs we are able to store temporal information inside the model to provide\ncontextual information from previous frames to enhance the prediction accuracy.\nOur novel approach uses different data augmentation techniques and\nregularization methods from only RGB frames to avoid overfitting and provide a\nsmall generalization error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1\">Costanza Maria Improta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Vision for Space Mining. (arXiv:2109.12109v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.12109","description":"<p>Future Moon bases will likely be constructed using resources mined from the\nsurface of the Moon. The difficulty of maintaining a human workforce on the\nMoon and communications lag with Earth means that mining will need to be\nconducted using collaborative robots with a high degree of autonomy. In this\npaper, we explore the utility of robotic vision towards addressing several\nmajor challenges in autonomous mining in the lunar environment: lack of\nsatellite positioning systems, navigation in hazardous terrain, and delicate\nrobot interactions. Specifically, we describe and report the results of robotic\nvision algorithms that we developed for Phase 2 of the NASA Space Robotics\nChallenge, which was framed in the context of autonomous collaborative robots\nfor mining on the Moon. The competition provided a simulated lunar environment\nthat exhibits the complexities alluded to above. We show how machine\nlearning-enabled vision could help alleviate the challenges posed by the lunar\nenvironment. A robust multi-robot coordinator was also developed to achieve\nlong-term operation and effective collaboration between robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Ragav Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_R/0/1/0/all/0/1\">Ravi Hammond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bockman_J/0/1/0/all/0/1\">James Bockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_A/0/1/0/all/0/1\">Alec Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smart_B/0/1/0/all/0/1\">Brandon Smart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craggs_D/0/1/0/all/0/1\">Dustin Craggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowntree_T/0/1/0/all/0/1\">Thomas Rowntree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutz_E/0/1/0/all/0/1\">Elijah Schutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_A/0/1/0/all/0/1\">Adrian Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Andy Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreeNet: A lightweight One-Shot Aggregation Convolutional Network. (arXiv:2109.12342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12342","description":"<p>The architecture of deep convolutional networks (CNNs) has evolved for years,\nbecoming more accurate and faster. However, it is still challenging to design\nreasonable network structures that aim at obtaining the best accuracy under a\nlimited computational budget. In this paper, we propose a Tree block, named\nafter its appearance, which extends the One-Shot Aggregation (OSA) module while\nbeing more lightweight and flexible. Specifically, the Tree block replaces each\nof the $3\\times3$ Conv layers in OSA into a stack of shallow residual block\n(SRB) and $1\\times1$ Conv layer. The $1\\times1$ Conv layer is responsible for\ndimension increasing and the SRB is fed into the next step. By doing this, when\naggregating the same number of subsequent feature maps, the Tree block has a\ndeeper network structure while having less model complexity. In addition,\nresidual connection and efficient channel attention(ECA) is added to the Tree\nblock to further improve the performance of the network. Based on the Tree\nblock, we build efficient backbone models calling TreeNets. TreeNet has a\nsimilar network architecture to ResNet, making it flexible to replace ResNet in\nvarious computer vision frameworks. We comprehensively evaluate TreeNet on\ncommon-used benchmarks, including ImageNet-1k for classification, MS COCO for\nobject detection, and instance segmentation. Experimental results demonstrate\nthat TreeNet is more efficient and performs favorably against the current\nstate-of-the-art backbone methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1\">Lu Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detailed Region-Adaptive Normalization for Heavy Makeup Transfer. (arXiv:2109.14525v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14525","description":"<p>In recent years, facial makeup transfer has attracted growing attention due\nto its efficiency and flexibility in transferring makeup styles between\ndifferent faces. Although recent works have achieved realistic results, most of\nthem fail to handle heavy makeup styles with multiple colors and subtle\ndetails. Hence we propose a novel GAN model to handle heavy makeup transfer,\nwhile maintaining the robustness to different poses and expressions. Firstly, a\nMakeup Multi-Extraction Network is introduced to learn region-wise makeup\nfeatures from multiple layers. Then, a key transferring module called Detailed\nRegion-Adaptive Normalization is proposed to fuse different levels of makeup\nstyles in an adaptive way, making great improvement to the quality of heavy\nmakeup transfer. With the outputs from the two components, Makeup Transfer\nNetwork is used to perform makeup transfer. To evaluate the efficacy of our\nproposed method, we collected a new makeup dataset containing a wide range of\nheavy styles. Experiments show that our method achieves state-of-the-art\nresults both on light and heavy makeup styles, and is robust to different poses\nand expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}