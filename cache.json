{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it. (arXiv:2205.03472v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03472","description":"<p>Understanding longer narratives or participating in conversations requires\ntracking of discourse entities that have been mentioned. Indefinite noun\nphrases (NPs), such as 'a dog', frequently introduce discourse entities but\nthis behavior is modulated by sentential operators such as negation. For\nexample, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse\nentity due to the presence of negation. In this work, we adapt the\npsycholinguistic assessment of language models paradigm to higher-level\nlinguistic phenomena and introduce an English evaluation suite that targets the\nknowledge of the interactions between sentential operators and indefinite NPs.\nWe use this evaluation suite for a fine-grained investigation of the entity\ntracking abilities of the Transformer-based models GPT-2 and GPT-3. We find\nthat while the models are to a certain extent sensitive to the interactions we\ninvestigate, they are all challenged by the presence of multiple NPs and their\nbehavior is not systematic, which suggests that even models at the scale of\nGPT-3 do not fully acquire basic entity tracking abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_S/0/1/0/all/0/1\">Sebastian Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Intent Classification in the Legal Domain. (arXiv:2205.03509v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03509","description":"<p>A law practitioner has to go through a lot of long legal case proceedings. To\nunderstand the motivation behind the actions of different parties/individuals\nin a legal case, it is essential that the parts of the document that express an\nintent corresponding to the case be clearly understood. In this paper, we\nintroduce a dataset of 93 legal documents, belonging to the case categories of\neither Murder, Land Dispute, Robbery, or Corruption, where phrases expressing\nintent same as the category of the document are annotated. Also, we annotate\nfine-grained intents for each such phrase to enable a deeper understanding of\nthe case for a reader. Finally, we analyze the performance of several\ntransformer-based models in automating the process of extracting intent phrases\n(both at a coarse and a fine-grained level), and classifying a document into\none of the possible 4 categories, and observe that, our dataset is challenging,\nespecially in the case of fine-grained intent classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadnis_M/0/1/0/all/0/1\">Manav Nitin Kapadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghav_R/0/1/0/all/0/1\">R Raghav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORWA: A Citation-Oriented Related Work Annotation Dataset. (arXiv:2205.03512v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03512","description":"<p>Academic research is an exploratory activity to discover new solutions to\nproblems. By this nature, academic research works perform literature reviews to\ndistinguish their novelties from prior work. In natural language processing,\nthis literature review is usually conducted under the \"Related Work\" section.\nThe task of related work generation aims to automatically generate the related\nwork section given the rest of the research paper and a list of papers to cite.\nPrior work on this task has focused on the sentence as the basic unit of\ngeneration, neglecting the fact that related work sections consist of variable\nlength text fragments derived from different information sources. As a first\nstep toward a linguistically-motivated related work generation framework, we\npresent a Citation Oriented Related Work Annotation (CORWA) dataset that labels\ndifferent types of citation text fragments from different information sources.\nWe train a strong baseline model that automatically tags the CORWA labels on\nmassive unlabeled related work section texts. We further suggest a novel\nframework for human-in-the-loop, iterative, abstractive related work\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_B/0/1/0/all/0/1\">Biswadip Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jessica Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (arXiv:2205.03521v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03521","description":"<p>Multimodal named entity recognition and relation extraction (MNER and MRE) is\na fundamental and crucial branch in information extraction. However, existing\napproaches for MNER and MRE usually suffer from error sensitivity when\nirrelevant object images incorporated in texts. To deal with these issues, we\npropose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for\nvisual-enhanced entity and relation extraction, aiming to achieve more\neffective and robust performance. Specifically, we regard visual representation\nas pluggable visual prefix to guide the textual representation for error\ninsensitive forecasting decision. We further propose a dynamic gated\naggregation strategy to achieve hierarchical multi-scaled visual features as\nvisual prefix for fusion. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our method, and achieve state-of-the-art\nperformance. Code is available in https://github.com/zjunlp/HVPNeT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information. (arXiv:2205.03534v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03534","description":"<p>Recently, online shopping has gradually become a common way of shopping for\npeople all over the world. Wonderful merchandise advertisements often attract\nmore people to buy. These advertisements properly integrate multimodal\nmulti-structured information of commodities, such as visual spatial information\nand fine-grained structure information. However, traditional multimodal text\ngeneration focuses on the conventional description of what existed and\nhappened, which does not match the requirement of advertisement copywriting in\nthe real world. Because advertisement copywriting has a vivid language style\nand higher requirements of faithfulness. Unfortunately, there is a lack of\nreusable evaluation frameworks and a scarcity of datasets. Therefore, we\npresent a dataset, E-MMAD (e-commercial multimodal multi-structured\nadvertisement copywriting), which requires, and supports much more detailed\ninformation in text generation. Noticeably, it is one of the largest video\ncaptioning datasets in this field. Accordingly, we propose a baseline method\nand faithfulness evaluation metric on the strength of structured information\nreasoning to solve the demand in reality on this dataset. It surpasses the\nprevious methods by a large margin on all metrics. The dataset and method are\ncoming soon on \\url{https://e-mmad.github.io/e-mmad.net/index.html}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinglin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogIntAc: Modeling the Relationships between Intention, Emotion and Action in Interactive Process from Cognitive Perspective. (arXiv:2205.03540v1 [cs.HC])","link":"http://arxiv.org/abs/2205.03540","description":"<p>Intention, emotion and action are important psychological factors in human\nactivities, which play an important role in the interaction between\nindividuals. How to model the interaction process between individuals by\nanalyzing the relationship of their intentions, emotions, and actions at the\ncognitive level is challenging. In this paper, we propose a novel cognitive\nframework of individual interaction. The core of the framework is that\nindividuals achieve interaction through external action driven by their inner\nintention. Based on this idea, the interactions between individuals can be\nconstructed by establishing relationships between the intention, emotion and\naction. Furthermore, we conduct analysis on the interaction between individuals\nand give a reasonable explanation for the predicting results. To verify the\neffectiveness of the framework, we reconstruct a dataset and propose three\ntasks as well as the corresponding baseline models, including action abduction,\nemotion prediction and action generation. The novel framework shows an\ninteresting perspective on mimicking the mental state of human beings in\ncognitive science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SubGraph Networks based Entity Alignment for Cross-lingual Knowledge Graph. (arXiv:2205.03557v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03557","description":"<p>Entity alignment is the task of finding entities representing the same\nreal-world object in two knowledge graphs(KGs). Cross-lingual knowledge graph\nentity alignment aims to discover the cross-lingual links in the multi-language\nKGs, which is of great significance to the NLP applications and multi-language\nKGs fusion. In the task of aligning cross-language knowledge graphs, the\nstructures of the two graphs are very similar, and the equivalent entities\noften have the same subgraph structure characteristics. The traditional GCN\nmethod neglects to obtain structural features through representative parts of\nthe original graph and the use of adjacency matrix is not enough to effectively\nrepresent the structural features of the graph. In this paper, we introduce the\nsubgraph network (SGN) method into the GCN-based cross-lingual KG entity\nalignment method. In the method, we extracted the first-order subgraphs of the\nKGs to expand the structural features of the original graph to enhance the\nrepresentation ability of the entity embedding and improve the alignment\naccuracy. Experiments show that the proposed method outperforms the\nstate-of-the-art GCN-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shanqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiajun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaojuan Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Number Entity Recognition. (arXiv:2205.03559v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03559","description":"<p>Numbers are essential components of text, like any other word tokens, from\nwhich natural language processing (NLP) models are built and deployed. Though\nnumbers are typically not accounted for distinctly in most NLP tasks, there is\nstill an underlying amount of numeracy already exhibited by NLP models. In this\nwork, we attempt to tap this potential of state-of-the-art NLP models and\ntransfer their ability to boost performance in related tasks. Our proposed\nclassification of numbers into entities helps NLP models perform well on\nseveral tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on\nquestion answering using joint embeddings, outperforming the BERT and RoBERTa\nbaseline classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_D/0/1/0/all/0/1\">Dhanasekar Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Vivek Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Textual Representations via Statistical Measures of Similarity. (arXiv:2205.03589v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03589","description":"<p>When working with textual data, a natural application of disentangled\nrepresentations is fair classification where the goal is to make predictions\nwithout being biased (or influenced) by sensitive attributes that may be\npresent in the data (e.g., age, gender or race). Dominant approaches to\ndisentangle a sensitive attribute from textual representations rely on learning\nsimultaneously a penalization term that involves either an adversarial loss\n(e.g., a discriminator) or an information measure (e.g., mutual information).\nHowever, these methods require the training of a deep neural network with\nseveral parameter updates for each update of the representation model. As a\nmatter of fact, the resulting nested optimization loop is both time consuming,\nadding complexity to the optimization dynamic, and requires a fine\nhyperparameter selection (e.g., learning rates, architecture). In this work, we\nintroduce a family of regularizers for learning disentangled representations\nthat do not require training. These regularizers are based on statistical\nmeasures of similarity between the conditional probability distributions with\nrespect to the sensitive attributes. Our novel regularizers do not require\nadditional training, are faster and do not involve additional tuning while\nachieving better results both when combined with pretrained and randomly\ninitialized text encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Computationally Feasible Deep Active Learning. (arXiv:2205.03598v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03598","description":"<p>Active learning (AL) is a prominent technique for reducing the annotation\neffort required for training machine learning models. Deep learning offers a\nsolution for several essential obstacles to deploying AL in practice but\nintroduces many others. One of such problems is the excessive computational\nresources required to train an acquisition model and estimate its uncertainty\non instances in the unlabeled pool. We propose two techniques that tackle this\nissue for text classification and tagging tasks, offering a substantial\nreduction of AL iteration duration and the computational overhead introduced by\ndeep acquisition models in AL. We also demonstrate that our algorithm that\nleverages pseudo-labeling and distilled models overcomes one of the essential\nobstacles revealed previously in the literature. Namely, it was shown that due\nto differences between an acquisition model used to select instances during AL\nand a successor model trained on the labeled data, the benefits of AL can\ndiminish. We show that our algorithm, despite using a smaller and faster\nacquisition model, is capable of training a more expressive successor model\nwith higher performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsvigun_A/0/1/0/all/0/1\">Akim Tsvigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelmanov_A/0/1/0/all/0/1\">Artem Shelmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_G/0/1/0/all/0/1\">Gleb Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanochkin_L/0/1/0/all/0/1\">Leonid Sanochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larionov_D/0/1/0/all/0/1\">Daniil Larionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusev_G/0/1/0/all/0/1\">Gleb Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_L/0/1/0/all/0/1\">Leonid Zhukov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniMorph 4.0: Universal Morphology. (arXiv:2205.03608v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03608","description":"<p>The Universal Morphology (UniMorph) project is a collaborative effort\nproviding broad-coverage instantiated normalized morphological inflection\ntables for hundreds of diverse world languages. The project comprises two major\nthrusts: a language-independent feature schema for rich morphological\nannotation and a type-level resource of annotated data in diverse languages\nrealizing that schema. This paper presents the expansions and improvements made\non several fronts over the last couple of years (since McCarthy et al. (2020)).\nCollaborative efforts by numerous linguists have added 67 new languages,\nincluding 30 endangered languages. We have implemented several improvements to\nthe extraction pipeline to tackle some issues, e.g. missing gender and macron\ninformation. We have also amended the schema to use a hierarchical structure\nthat is needed for morphological phenomena like multiple-argument agreement and\ncase stacking, while adding some missing morphological features to make the\nschema more inclusive. In light of the last UniMorph release, we also augmented\nthe database with morpheme segmentation for 16 languages. Lastly, this new\nrelease makes a push towards inclusion of derivational morphology in UniMorph\nby enriching the data and annotation schema with instances representing\nderivational processes from MorphyNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieras_W/0/1/0/all/0/1\">Witold Kiera&#x15b;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_B/0/1/0/all/0/1\">Brian Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ate_Y/0/1/0/all/0/1\">Yustinus Ghanggo Ate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1\">Sabrina J. Mielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budianskaya_E/0/1/0/all/0/1\">Elena Budianskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khaissi_C/0/1/0/all/0/1\">Charbel El-Khaissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasser_M/0/1/0/all/0/1\">Michael Gasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_W/0/1/0/all/0/1\">William Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_M/0/1/0/all/0/1\">Mohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coler_M/0/1/0/all/0/1\">Matt Coler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samame_J/0/1/0/all/0/1\">Jaime Rafael Montoya Samame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camaiteri_D/0/1/0/all/0/1\">Delio Siticonatzi Camaiteri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_E/0/1/0/all/0/1\">Esa&#xfa; Zumaeta Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_D/0/1/0/all/0/1\">Didier L&#xf3;pez Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_J/0/1/0/all/0/1\">Juan L&#xf3;pez Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_G/0/1/0/all/0/1\">Gema Celeste Silva Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirix_P/0/1/0/all/0/1\">Peter Dirix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardy_J/0/1/0/all/0/1\">Jean-Philippe Bernardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherbakov_A/0/1/0/all/0/1\">Andrey Scherbakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayyr_ool_A/0/1/0/all/0/1\">Aziyana Bayyr-ool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariquiey_R/0/1/0/all/0/1\">Roberto Zariquiey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheifer_K/0/1/0/all/0/1\">Karina Sheifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganieva_S/0/1/0/all/0/1\">Sofya Ganieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_H/0/1/0/all/0/1\">Hilaria Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karahoga_R/0/1/0/all/0/1\">Ritv&#xe1;n Karah&#xf3;&#x1e7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markantonatou_S/0/1/0/all/0/1\">Stella Markantonatou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlidis_G/0/1/0/all/0/1\">George Pavlidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plugaryov_M/0/1/0/all/0/1\">Matvey Plugaryov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyachko_E/0/1/0/all/0/1\">Elena Klyachko</a>, et al. (52 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding. (arXiv:2205.03656v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03656","description":"<p>Although spoken language understanding (SLU) has achieved great success in\nhigh-resource languages, such as English, it remains challenging in\nlow-resource languages mainly due to the lack of high quality training data.\nThe recent multilingual code-switching approach samples some words in an input\nutterance and replaces them by expressions in some other languages of the same\nmeaning. The multilingual code-switching approach achieves better alignments of\nrepresentations across languages in zero-shot cross-lingual SLU. Surprisingly,\nall existing multilingual code-switching methods disregard the inherent\nsemantic structure in SLU, i.e., most utterances contain one or more slots, and\neach slot consists of one or more words. In this paper, we propose to exploit\nthe \"utterance-slot-word\" structure of SLU and systematically model this\nstructure by a multi-level contrastive learning framework at the utterance,\nslot, and word levels. We develop novel code-switching schemes to generate hard\nnegative examples for contrastive learning at all levels. Furthermore, we\ndevelop a label-aware joint model to leverage label semantics for cross-lingual\nknowledge transfer. Our experimental results show that our proposed methods\nsignificantly improve the performance compared with the strong baselines on two\nzero-shot cross-lingual SLU benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xianglin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Representations of Idioms in Conversational Systems. (arXiv:2205.03666v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03666","description":"<p>We demonstrate, in this study, that an open-domain conversational system\ntrained on idioms or figurative language generates more fitting responses to\nprompts containing idioms. Idioms are part of everyday speech in many\nlanguages, across many cultures, but they pose a great challenge for many\nNatural Language Processing (NLP) systems that involve tasks such as\nInformation Retrieval (IR) and Machine Translation (MT), besides conversational\nAI. We utilize the Potential Idiomatic Expression (PIE)-English idioms corpus\nfor the two tasks that we investigate: classification and conversation\ngeneration. We achieve state-of-the-art (SoTA) result of 98% macro F1 score on\nthe classification task by using the SoTA T5 model. We experiment with three\ninstances of the SoTA dialogue model, Dialogue Generative Pre-trained\nTransformer (DialoGPT), for conversation generation. Their performances are\nevaluated using the automatic metric perplexity and human evaluation. The\nresults show that the model trained on the idiom corpus generates more fitting\nresponses to prompts containing idioms 71.9% of the time, compared to a similar\nmodel not trained on the idioms corpus. We contribute the model checkpoint/demo\nand code on the HuggingFace hub for public access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Response Generation with State Management. (arXiv:2205.03676v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03676","description":"<p>The goal of empathetic response generation is to enhance the ability of\ndialogue systems to perceive and express emotions in conversations. Current\napproaches to this task mainly focus on improving the response generation model\nby recognizing the emotion of the user or predicting a target emotion to guide\nthe generation of responses. Such models only exploit partial information (the\nuser's emotion or the target emotion used as a guiding signal) and do not\nconsider multiple information together. In addition to the emotional style of\nthe response, the intent of the response is also very important for empathetic\nresponding. Thus, we propose a novel empathetic response generation model that\ncan consider multiple state information including emotions and intents\nsimultaneously. Specifically, we introduce a state management method to\ndynamically update the dialogue states, in which the user's emotion is first\nrecognized, then the target emotion and intent are obtained via predefined\nshift patterns with the user's emotion as input. The obtained information is\nused to control the response generation. Experimental results show that\ndynamically managing different information can help the model generate more\nempathetic responses compared with several baselines under both automatic and\nhuman evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiachen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lanjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Retrieval May Not Lead to Better Question Answering. (arXiv:2205.03685v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03685","description":"<p>Considerable progress has been made recently in open-domain question\nanswering (QA) problems, which require Information Retrieval (IR) and Reading\nComprehension (RC). A popular approach to improve the system's performance is\nto improve the quality of the retrieved context from the IR stage. In this work\nwe show that for StrategyQA, a challenging open-domain QA dataset that requires\nmulti-hop reasoning, this common approach is surprisingly ineffective --\nimproving the quality of the retrieved context hardly improves the system's\nperformance. We further analyze the system's behavior to identify potential\nreasons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengzhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethard_S/0/1/0/all/0/1\">Steven Bethard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Progression-Aware Autonomous Dialogue Agent. (arXiv:2205.03692v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03692","description":"<p>Recent advances in large-scale language modeling and generation have enabled\nthe creation of dialogue agents that exhibit human-like responses in a wide\nrange of conversational scenarios spanning a diverse set of tasks, from general\nchit-chat to focused goal-oriented discourse. While these agents excel at\ngenerating high-quality responses that are relevant to prior context, they\nsuffer from a lack of awareness of the overall direction in which the\nconversation is headed, and the likelihood of task success inherent therein.\nThus, we propose a framework in which dialogue agents can evaluate the\nprogression of a conversation toward or away from desired outcomes, and use\nthis signal to inform planning for subsequent responses. Our framework is\ncomposed of three key elements: (1) the notion of a \"global\" dialogue state\n(GDS) space, (2) a task-specific progression function (PF) computed in terms of\na conversation's trajectory through this space, and (3) a planning mechanism\nbased on dialogue rollouts by which an agent may use progression signals to\nselect its next response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanders_A/0/1/0/all/0/1\">Abraham Sanders</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Strzalkowski_T/0/1/0/all/0/1\">Tomek Strzalkowski</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Si_M/0/1/0/all/0/1\">Mei Si</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Albert Chang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Deepanshu Dey</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Braasch_J/0/1/0/all/0/1\">Jonas Braasch</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a> (2) ((1) Rensselaer Polytechnic Institute, Troy, NY, USA, (2) IBM Research, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury. (arXiv:2205.03695v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03695","description":"<p>Acute kidney injury (AKI) is a common clinical syndrome characterized by a\nsudden episode of kidney failure or kidney damage within a few hours or a few\ndays. Accurate early prediction of AKI for patients in ICU who are more likely\nthan others to have AKI can enable timely interventions, and reduce the\ncomplications of AKI. Much of the clinical information relevant to AKI is\ncaptured in clinical notes that are largely unstructured text and requires\nadvanced natural language processing (NLP) for useful information extraction.\nOn the other hand, pre-trained contextual language models such as Bidirectional\nEncoder Representations from Transformers (BERT) have improved performances for\nmany NLP tasks in general domain recently. However, few have explored BERT on\ndisease-specific medical domain tasks such as AKI early prediction. In this\npaper, we try to apply BERT to specific diseases and present an AKI\ndomain-specific pre-trained language model based on BERT (AKI-BERT) that could\nbe used to mine the clinical notes for early prediction of AKI. AKI-BERT is a\nBERT model pre-trained on the clinical notes of patients having risks for AKI.\nOur experiments on Medical Information Mart for Intensive Care III (MIMIC-III)\ndataset demonstrate that AKI-BERT can yield performance improvements for early\nAKI prediction, thus expanding the utility of the BERT model from general\nclinical domain to disease-specific domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengsheng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Liang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. (arXiv:2205.03720v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03720","description":"<p>The massive amount of trainable parameters in the pre-trained language models\n(PLMs) makes them hard to be deployed to multiple downstream tasks. To address\nthis issue, parameter-efficient transfer learning methods have been proposed to\ntune only a few parameters during fine-tuning while freezing the rest. This\npaper looks at existing methods along this line through the \\textit{kernel\nlens}. Motivated by the connection between self-attention in transformer-based\nPLMs and kernel learning, we propose \\textit{kernel-wise adapters}, namely\n\\textit{Kernel-mix}, that utilize the kernel structure in self-attention to\nguide the assignment of the tunable parameters. These adapters use guidelines\nfound in classical kernel learning and enable separate parameter tuning for\neach attention head. Our empirical results, over a diverse set of natural\nlanguage generation and understanding tasks, show that our proposed adapters\ncan attain or improve the strong performance of existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DxFormer: A Decoupled Automatic Diagnostic System Based on Decoder-Encoder Transformer with Dense Symptom Representations. (arXiv:2205.03755v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03755","description":"<p>Diagnosis-oriented dialogue system queries the patient's health condition and\nmakes predictions about possible diseases through continuous interaction with\nthe patient. A few studies use reinforcement learning (RL) to learn the optimal\npolicy from the joint action space of symptoms and diseases. However, existing\nRL (or Non-RL) methods cannot achieve sufficiently good prediction accuracy,\nstill far from its upper limit. To address the problem, we propose a decoupled\nautomatic diagnostic framework DxFormer, which divides the diagnosis process\ninto two steps: symptom inquiry and disease diagnosis, where the transition\nfrom symptom inquiry to disease diagnosis is explicitly determined by the\nstopping criteria. In DxFormer, we treat each symptom as a token, and formalize\nthe symptom inquiry and disease diagnosis to a language generation model and a\nsequence classification model respectively. We use the inverted version of\nTransformer, i.e., the decoder-encoder structure, to learn the representation\nof symptoms by jointly optimizing the reinforce reward and cross entropy loss.\nExtensive experiments on three public real-world datasets prove that our\nproposed model can effectively learn doctors' clinical experience and achieve\nthe state-of-the-art results in terms of symptom recall and diagnostic\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiajie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Multi-task Learning for Neural Chat Translation. (arXiv:2205.03766v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03766","description":"<p>Neural Chat Translation (NCT) aims to translate conversational text into\ndifferent languages. Existing methods mainly focus on modeling the bilingual\ndialogue characteristics (e.g., coherence) to improve chat translation via\nmulti-task learning on small-scale chat translation data. Although the NCT\nmodels have achieved impressive success, it is still far from satisfactory due\nto insufficient chat translation data and simple joint training manners. To\naddress the above issues, we propose a scheduled multi-task learning framework\nfor NCT. Specifically, we devise a three-stage training framework to\nincorporate the large-scale in-domain chat translation data into training by\nadding a second pre-training stage between the original pre-training and\nfine-tuning stages. Further, we investigate where and how to schedule the\ndialogue-related auxiliary tasks in multiple training stages to effectively\nenhance the main chat translation task. Extensive experiments in four language\ndirections (English-Chinese and English-German) verify the effectiveness and\nsuperiority of the proposed approach. Additionally, we have made the\nlarge-scale in-domain paired bilingual dialogue dataset publicly available to\nthe research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03767","description":"<p>Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70 of phrases with abbreviation length up to 10, leading to\nan effective keystroke saving rate of up to about 77 on these exact expansions.\nIncluding a small amount of context in the form of a single conversation turn\nmore than doubles abbreviation expansion accuracies compared to having no\ncontext, an effect that is more pronounced for longer phrases. Additionally,\nthe robustness of models against typo noise can be enhanced through fine-tuning\non noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shanqing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Ajit Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith R. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math-KG: Construction and Applications of Mathematical Knowledge Graph. (arXiv:2205.03772v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03772","description":"<p>Recently, the explosion of online education platforms makes a success in\nencouraging us to easily access online education resources. However, most of\nthem ignore the integration of massive unstructured information, which\ninevitably brings the problem of \\textit{information overload} and\n\\textit{knowledge trek}. In this paper, we proposed a mathematical knowledge\ngraph named Math-KG, which automatically constructed by the pipeline method\nwith the natural language processing technology to integrate the resources of\nthe mathematics. It is built from the corpora of Baidu Baike, Wikipedia. We\nimplement a simple application system to validate the proposed Math-KG can make\ncontributions on a series of scenes, including faults analysis and semantic\nsearch. The system is publicly available at GitHub\n\\footnote{\\url{https://github.com/wjn1996/Mathematical-Knowledge-Entity-Recognition}.}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis. (arXiv:2205.03784v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03784","description":"<p>Recent literature focuses on utilizing the entity information in the\nsentence-level relation extraction (RE), but this risks leaking superficial and\nspurious clues of relations. As a result, RE still suffers from unintended\nentity bias, i.e., the spurious correlation between entity mentions (names) and\nrelations. Entity bias can mislead the RE models to extract the relations that\ndo not exist in the text. To combat this issue, some previous work masks the\nentity mentions to prevent the RE models from overfitting entity mentions.\nHowever, this strategy degrades the RE performance because it loses the\nsemantic information of entities. In this paper, we propose the CORE\n(Counterfactual Analysis based Relation Extraction) debiasing method that\nguides the RE models to focus on the main effects of textual context without\nlosing the entity information. We first construct a causal graph for RE, which\nmodels the dependencies between variables in RE models. Then, we propose to\nconduct counterfactual analysis on our causal graph to distill and mitigate the\nentity bias, that captures the causal effects of specific entity mentions in\neach instance. Note that our CORE method is model-agnostic to debias existing\nRE systems during inference without changing their training processes.\nExtensive experimental results demonstrate that our CORE yields significant\ngains on both effectiveness and generalization for RE. The source code is\nprovided at: https://github.com/vanoracai/CoRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juncheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAPHCACHE: Message Passing as Caching for Sentence-Level Relation Extraction. (arXiv:2205.03786v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03786","description":"<p>Entity types and textual context are essential properties for sentence-level\nrelation extraction (RE). Existing work only encodes these properties within\nindividual instances, which limits the performance of RE given the insufficient\nfeatures in a single sentence. In contrast, we model these properties from the\nwhole dataset and use the dataset-level information to enrich the semantics of\nevery instance. We propose the GRAPHCACHE (Graph Neural Network as Caching)\nmodule, that propagates the features across sentences to learn better\nrepresentations for RE. GRAPHCACHE aggregates the features from sentences in\nthe whole dataset to learn global representations of properties, and use them\nto augment the local features within individual sentences. The global property\nfeatures act as dataset-level prior knowledge for RE, and a complement to the\nsentence-level features. Inspired by the classical caching technique in\ncomputer systems, we develop GRAPHCACHE to update the property representations\nin an online manner. Overall, GRAPHCACHE yields significant effectiveness gains\non RE and enables efficient message passing across all sentences in the\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Targeted Sentiment Analysis. (arXiv:2205.03804v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03804","description":"<p>Targeted Sentiment Analysis (TSA) is a central task for generating insights\nfrom consumer reviews. Such content is extremely diverse, with sites like\nAmazon or Yelp containing reviews on products and businesses from many\ndifferent domains. A real-world TSA system should gracefully handle that\ndiversity. This can be achieved by a multi-domain model -- one that is robust\nto the domain of the analyzed texts, and performs well on various domains. To\naddress this scenario, we present a multi-domain TSA system based on augmenting\na given training set with diverse weak labels from assorted domains. These are\nobtained through self-training on the Yelp reviews corpus. Extensive\nexperiments with our approach on three evaluation datasets across different\ndomains demonstrate the effectiveness of our solution. We further analyze how\nrestrictions imposed on the available labeled data affect the performance, and\ncompare the proposed method to the costly alternative of manually gathering\ndiverse TSA labeled data. Our results and analysis show that our approach is a\npromising step towards a practical domain-robust TSA system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence. (arXiv:2205.03815v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03815","description":"<p>The logical negation property (LNP), which implies generating different\npredictions for semantically opposite inputs, is an important property that a\ntrustworthy language model must satisfy. However, much recent evidence shows\nthat large-size pre-trained language models (PLMs) do not satisfy this\nproperty. In this paper, we perform experiments using probing tasks to assess\nPLM's LNP understanding. Unlike previous studies that only examined negation\nexpressions, we expand the boundary of the investigation to lexical semantics.\nThrough experiments, we observe that PLMs violate the LNP frequently. To\nalleviate the issue, we propose a novel intermediate training task, names\nmeaning-matching, designed to directly learn a meaning-text correspondence,\ninstead of relying on the distributional hypothesis. Through multiple\nexperiments, we find that the task enables PLMs to learn lexical semantic\ninformation. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm\nthat it is a safe intermediate task that guarantees a similar or better\nperformance of downstream tasks. Finally, we observe that our proposed approach\noutperforms our previous counterparts despite its time and resource efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mtumbuka_F/0/1/0/all/0/1\">Frank Mtumbuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation. (arXiv:2205.03835v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03835","description":"<p>In recent years, pre-trained models have become dominant in most natural\nlanguage processing (NLP) tasks. However, in the area of Automated Essay\nScoring (AES), pre-trained models such as BERT have not been properly used to\noutperform other deep learning models such as LSTM. In this paper, we introduce\na novel multi-scale essay representation for BERT that can be jointly learned.\nWe also employ multiple losses and transfer learning from out-of-domain essays\nto further improve the performance. Experiment results show that our approach\nderives much benefit from joint learning of multi-scale essay representation\nand obtains almost the state-of-the-art result among all deep learning models\nin the ASAP task. Our multi-scale essay representation also generalizes well to\nCommonLit Readability Prize data set, which suggests that the novel text\nrepresentation proposed in this paper may be a new and effective choice for\nlong-text tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruobing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assigning Species Information to Corresponding Genes by a Sequence Labeling Framework. (arXiv:2205.03853v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03853","description":"<p>The automatic assignment of species information to the corresponding genes in\na research article is a critically important step in the gene normalization\ntask, whereby a gene mention is normalized and linked to a database record or\nidentifier by a text-mining algorithm. Existing methods typically rely on\nheuristic rules based on gene and species co-occurrence in the article, but\ntheir accuracy is suboptimal. We therefore developed a high-performance method,\nusing a novel deep learning-based framework, to classify whether there is a\nrelation between a gene and a species. Instead of the traditional binary\nclassification framework in which all possible pairs of genes and species in\nthe same article are evaluated, we treat the problem as a sequence-labeling\ntask such that only a fraction of the pairs needs to be considered. Our\nbenchmarking results show that our approach obtains significantly higher\nperformance compared to that of the rule-based baseline method for the species\nassignment task (from 65.8% to 81.3% in accuracy). The source code and data for\nspecies assignment are freely available at\nhttps://github.com/ncbi/SpeciesAssignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_R/0/1/0/all/0/1\">Rezarta Islamaj Do&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's the Same Old Story! Enriching Event-Centric Knowledge Graphs by Narrative Aspects. (arXiv:2205.03876v1 [cs.IR])","link":"http://arxiv.org/abs/2205.03876","description":"<p>Our lives are ruled by events of varying importance ranging from simple\neveryday occurrences to incidents of societal dimension. And a lot of effort is\ntaken to exchange information and discuss about such events: generally\nspeaking, stringent narratives are formed to reduce complexity. But when\nconsidering complex events like the current conflict between Russia and Ukraine\nit is easy to see that those events cannot be grasped by objective facts alone,\nlike the start of the conflict or respective troop sizes. There are different\nviewpoints and assessments to consider, a different understanding of the roles\ntaken by individual participants, etc. So how can such subjective and\nviewpoint-dependent information be effectively represented together with all\nobjective information? Recently event-centric knowledge graphs have been\nproposed for objective event representation in the otherwise primarily\nentity-centric domain of knowledge graphs. In this paper we introduce a novel\nand lightweight structure for event-centric knowledge graphs, which for the\nfirst time allows for queries incorporating viewpoint-dependent and narrative\naspects. Our experiments prove the effective incorporation of subjective\nattributions for event participants and show the benefits of specifically\ntailored indexes for narrative query processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plotzky_F/0/1/0/all/0/1\">Florian Pl&#xf6;tzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balke_W/0/1/0/all/0/1\">Wolf-Tilo Balke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi. (arXiv:2205.03955v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03955","description":"<p>We present a completed, publicly available corpus of annotated semantic\nrelations of adpositions and case markers in Hindi. We used the multilingual\nSNACS annotation scheme, which has been applied to a variety of typologically\ndiverse languages. Building on past work examining linguistic problems in SNACS\nannotation, we use language models to attempt automatic labelling of SNACS\nsupersenses in Hindi and achieve results competitive with past work on English.\nWe look towards upstream applications in semantic role labelling and extension\nto related languages such as Gujarati.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkateswaran_N/0/1/0/all/0/1\">Nitin Venkateswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chart Question Answering: State of the Art and Future Directions. (arXiv:2205.03966v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03966","description":"<p>Information visualizations such as bar charts and line charts are very common\nfor analyzing data and discovering critical insights. Often people analyze\ncharts to answer questions that they have in mind. Answering such questions can\nbe challenging as they often require a significant amount of perceptual and\ncognitive effort. Chart Question Answering (CQA) systems typically take a chart\nand a natural language question as input and automatically generate the answer\nto facilitate visual data analysis. Over the last few years, there has been a\ngrowing body of literature on the task of CQA. In this survey, we\nsystematically review the current state-of-the-art research focusing on the\nproblem of chart question answering. We provide a taxonomy by identifying\nseveral important dimensions of the problem domain including possible inputs\nand outputs of the task and discuss the advantages and limitations of proposed\nsolutions. We then summarize various evaluation techniques used in the surveyed\npapers. Finally, we outline the open challenges and future research\nopportunities related to chart question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">E. Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehzadeh_P/0/1/0/all/0/1\">P. Kavehzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">A. Masry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning. (arXiv:2205.03972v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03972","description":"<p>Controlled table-to-text generation seeks to generate natural language\ndescriptions for highlighted subparts of a table. Previous SOTA systems still\nemploy a sequence-to-sequence generation method, which merely captures the\ntable as a linear structure and is brittle when table layouts change. We seek\nto go beyond this paradigm by (1) effectively expressing the relations of\ncontent pieces in the table, and (2) making our model robust to\ncontent-invariant structural transformations. Accordingly, we propose an\nequivariance learning framework, which encodes tables with a structure-aware\nself-attention mechanism. This prunes the full self-attention structure into an\norder-invariant graph attention that captures the connected graph structure of\ncells belonging to the same row or column, and it differentiates between\nrelevant cells and irrelevant cells from the structural perspective. Our\nframework also modifies the positional encoding mechanism to preserve the\nrelative position of tokens in the same cell but enforce position invariance\namong different cells. Our technology is free to be plugged into existing\ntable-to-text generation models, and has improved T5-based models to offer\nbetter performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,\nwe preserve promising performance, while previous SOTA systems, even with\ntransformation-based data augmentation, have seen significant performance\ndrops. Our code is available at https://github.com/luka-group/Lattice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhewei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Structured Span Selector. (arXiv:2205.03977v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03977","description":"<p>Many natural language processing tasks, e.g., coreference resolution and\nsemantic role labeling, require selecting text spans and making decisions about\nthem. A typical approach to such tasks is to score all possible spans and\ngreedily select spans for task-specific downstream processing. This approach,\nhowever, does not incorporate any inductive bias about what sort of spans ought\nto be selected, e.g., that selected spans tend to be syntactic constituents. In\nthis paper, we propose a novel grammar-based structured span selection model\nwhich learns to make use of the partial span-level annotation provided for such\nproblems. Compared to previous approaches, our approach gets rid of the\nheuristic greedy span selection scheme, allowing us to model the downstream\ntask on an optimal set of spans. We evaluate our model on two popular span\nprediction tasks: coreference resolution and semantic role labeling; and show\nimprovements on both.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACM -- Attribute Conditioning for Abstractive Multi Document Summarization. (arXiv:2205.03978v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03978","description":"<p>Abstractive multi document summarization has evolved as a task through the\nbasic sequence to sequence approaches to transformer and graph based\ntechniques. Each of these approaches has primarily focused on the issues of\nmulti document information synthesis and attention based approaches to extract\nsalient information. A challenge that arises with multi document summarization\nwhich is not prevalent in single document summarization is the need to\neffectively summarize multiple documents that might have conflicting polarity,\nsentiment or subjective information about a given topic. In this paper we\npropose ACM, attribute conditioned multi document summarization,a model that\nincorporates attribute conditioning modules in order to decouple conflicting\ninformation by conditioning for a certain attribute in the output summary. This\napproach shows strong gains in ROUGE score over baseline multi document\nsummarization approaches and shows gains in fluency, informativeness and\nreduction in repetitiveness as shown through a human annotation analysis study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sankar_A/0/1/0/all/0/1\">Aiswarya Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Ankit Chadha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03983","description":"<p>In this paper we share findings from our effort to build practical machine\ntranslation (MT) systems capable of translating across over one thousand\nlanguages. We describe results in three research domains: (i) Building clean,\nweb-mined datasets for 1500+ languages by leveraging semi-supervised\npre-training for language identification and developing data-driven filtering\ntechniques; (ii) Developing practical MT models for under-served languages by\nleveraging massively multilingual models trained with supervised parallel data\nfor over 100 high-resource languages and monolingual datasets for an additional\n1000+ languages; and (iii) Studying the limitations of evaluation metrics for\nthese languages and conducting qualitative analysis of the outputs from our MT\nmodels, highlighting several frequent error modes of these types of models. We\nhope that our work provides useful insights to practitioners working towards\nbuilding MT systems for currently understudied languages, and highlights\nresearch directions that can complement the weaknesses of massively\nmultilingual models in data-sparse settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Mengmeng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baljekar_P/0/1/0/all/0/1\">Pallavi Baljekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_K/0/1/0/all/0/1\">Klaus Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Apurva Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Macduff Hughes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System. (arXiv:2205.04006v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04006","description":"<p>Contextually aware intelligent agents are often required to understand the\nusers and their surroundings in real-time. Our goal is to build Artificial\nIntelligence (AI) systems that can assist children in their learning process.\nWithin such complex frameworks, Spoken Dialogue Systems (SDS) are crucial\nbuilding blocks to handle efficient task-oriented communication with children\nin game-based learning settings. We are working towards a multimodal dialogue\nsystem for younger kids learning basic math concepts. Our focus is on improving\nthe Natural Language Understanding (NLU) module of the task-oriented SDS\npipeline with limited datasets. This work explores the potential benefits of\ndata augmentation with paraphrase generation for the NLU models trained on\nsmall task-specific datasets. We also investigate the effects of extracting\nentities for conceivably further data expansion. We have shown that\nparaphrasing with model-in-the-loop (MITL) strategies using small seed data is\na promising approach yielding improved performance results for the Intent\nRecognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okur_E/0/1/0/all/0/1\">Eda Okur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving negation detection with negation-focused pre-training. (arXiv:2205.04012v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04012","description":"<p>Negation is a common linguistic feature that is crucial in many language\nunderstanding tasks, yet it remains a hard problem due to diversity in its\nexpression in different types of text. Recent work has shown that\nstate-of-the-art NLP models underperform on samples containing negation in\nvarious tasks, and that negation detection models do not transfer well across\ndomains. We propose a new negation-focused pre-training strategy, involving\ntargeted data augmentation and negation masking, to better incorporate negation\ninformation into language models. Extensive experiments on common benchmarks\nshow that our proposed approach improves negation detection performance and\ngeneralizability over the strong baseline NegBERT (Khandewal and Sawant, 2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thinh Hung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality. (arXiv:2205.04022v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04022","description":"<p>The machine translation (MT) task is typically formulated as that of\nreturning a single translation for an input segment. However, in many cases,\nmultiple different translations are valid and the appropriate translation may\ndepend on the intended target audience, characteristics of the speaker, or even\nthe relationship between speakers. Specific problems arise when dealing with\nhonorifics, particularly translating from English into languages with formality\nmarkers. For example, the sentence \"Are you sure?\" can be translated in German\nas \"Sind Sie sich sicher?\" (formal register) or \"Bist du dir sicher?\"\n(informal). Using wrong or inconsistent tone may be perceived as inappropriate\nor jarring for users of certain cultures and demographics. This work addresses\nthe problem of learning to control target language attributes, in this case\nformality, from a small amount of labeled contrastive data. We introduce an\nannotated dataset (CoCoA-MT) and an associated evaluation metric for training\nand evaluating formality-controlled MT models for six diverse target languages.\nWe show that we can train formality-controlled models by fine-tuning on labeled\ncontrastive data, achieving high accuracy (82% in-domain and 73% out-of-domain)\nwhile maintaining overall quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria N&#x103;dejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Benjamin Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProQA: Structural Prompt-based Pre-training for Unified Question Answering. (arXiv:2205.04040v1 [cs.CL])","link":"http://arxiv.org/abs/2205.04040","description":"<p>Question Answering (QA) is a longstanding challenge in natural language\nprocessing. Existing QA works mostly focus on specific question types,\nknowledge domains, or reasoning skills. The specialty in QA research hinders\nsystems from modeling commonalities between tasks and generalization for wider\napplications. To address this issue, we present ProQA, a unified QA paradigm\nthat solves various tasks through a single model. ProQA takes a unified\nstructural prompt as the bridge and improves the QA-centric ability by\nstructural prompt-based pre-training. Through a structurally designed\nprompt-based input schema, ProQA concurrently models the knowledge\ngeneralization for all QA tasks while keeping the knowledge customization for\nevery specific QA task. Furthermore, ProQA is pre-trained with structural\nprompt-formatted large-scale synthesized corpus, which empowers the model with\nthe commonly-required QA ability. Experimental results on 11 QA benchmarks\ndemonstrate that ProQA consistently boosts performance on both full data\nfine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,\nProQA exhibits strong ability in both continual learning and transfer learning\nby taking the advantages of the structural prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Information Constraints for Monte-Carlo Objectives. (arXiv:2012.00708v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2012.00708","description":"<p>A common failure mode of density models trained as variational autoencoders\nis to model the data without relying on their latent variables, rendering these\nvariables useless. Two contributing factors, the underspecification of the\nmodel and the looseness of the variational lower bound, have been studied\nseparately in the literature. We weave these two strands of research together,\nspecifically the tighter bounds of Monte-Carlo objectives and constraints on\nthe mutual information between the observable and the latent variables.\nEstimating the mutual information as the average Kullback-Leibler divergence\nbetween the easily available variational posterior $q(z|x)$ and the prior does\nnot work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct\napproximation to the model's true posterior $p(z|x)$. Hence, we construct\nestimators of the Kullback-Leibler divergence of the true posterior from the\nprior by recycling samples used in the objective, with which we train models of\ncontinuous and discrete latents at much improved rate-distortion and no\nposterior collapse. While alleviated, the tradeoff between modelling the data\nand using the latents still remains, and we urge for evaluating inference\nmethods across a range of mutual information values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Melis_G/0/1/0/all/0/1\">G&#xe1;bor Melis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andr&#xe1;s Gy&#xf6;rgy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Stance Detection for Mis- and Disinformation Identification. (arXiv:2103.00242v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00242","description":"<p>Understanding attitudes expressed in texts, also known as stance detection,\nplays an important role in systems for detecting false information online, be\nit misinformation (unintentionally false) or disinformation (intentionally\nfalse information). Stance detection has been framed in different ways,\nincluding (a) as a component of fact-checking, rumour detection, and detecting\npreviously fact-checked claims, or (b) as a task in its own right. While there\nhave been prior efforts to contrast stance detection with other related tasks\nsuch as argumentation mining and sentiment analysis, there is no existing\nsurvey on examining the relationship between stance detection and mis- and\ndisinformation detection. Here, we aim to bridge this gap by reviewing and\nanalysing existing work in this area, with mis- and disinformation in focus,\nand discussing lessons learnt and future challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11878","description":"<p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT\nevaluation. They can neither distinguish document-level improvements in\ntranslation quality from sentence-level ones, nor identify the discourse\nphenomena that cause context-agnostic translations. This paper introduces a\nnovel automatic metric BlonDe to widen the scope of automatic MT evaluation\nfrom sentence to document level. BlonDe takes discourse coherence into\nconsideration by categorizing discourse-related spans and calculating the\nsimilarity-based F1 measure of categorized spans. We conduct extensive\ncomparisons on a newly constructed dataset BWB. The experimental results show\nthat BlonD possesses better selectivity and interpretability at the\ndocument-level, and is more sensitive to document-level nuances. In a\nlarge-scale human study, BlonD also achieves significantly higher Pearson's r\ncorrelation with human judgments compared to previous metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Context in Detecting Previously Fact-Checked Claims. (arXiv:2104.07423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07423","description":"<p>Recent years have seen the proliferation of disinformation and fake news\nonline. Traditional approaches to mitigate these issues is to use manual or\nautomatic fact-checking. Recently, another approach has emerged: checking\nwhether the input claim has previously been fact-checked, which can be done\nautomatically, and thus fast, while also offering credibility and\nexplainability, thanks to the human fact-checking and explanations in the\nassociated fact-checking article. Here, we focus on claims made in a political\ndebate and we study the impact of modeling the context of the claim: both on\nthe source side, i.e., in the debate, as well as on the target side, i.e., in\nthe fact-checking explanation document. We do this by modeling the local\ncontext, the global context, as well as by means of co-reference resolution,\nand multi-hop reasoning over the sentences of the document describing the\nfact-checked claim. The experimental results show that each of these represents\na valuable information source, but that modeling the source-side context is\nmost important, and can yield 10+ points of absolute improvement over a\nstate-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy (CE) loss, which encourages an\nexact token-by-token match between a target sequence with a generated sequence.\nSuch training objective is sub-optimal when the target sequence is not perfect,\ne.g., when the target sequence is corrupted with noises, or when only weak\nsequence supervision is available. To address the challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL is designed to\nbe robust to various noises and edits in the target sequences. Moreover, the\nEISL computation is essentially an approximate convolution operation with\ntarget n-grams as kernels, which is easy to implement and efficient to compute\nwith existing libraries. To demonstrate the effectiveness of EISL, we conduct\nexperiments on a wide range of tasks, including machine translation with noisy\ntarget sequences, unsupervised text style transfer with only weak training\nsignals, and non-autoregressive generation with non-predefined generation\norder. Experimental results show our method significantly outperforms the\ncommon CE loss and other strong baselines on all the tasks. EISL has a simple\nAPI that can be used as a drop-in replacement of the CE loss:\nhttps://github.com/guangyliu/EISL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09274","description":"<p>We present a simple and effective way to generate a variety of paraphrases\nand find a good quality paraphrase among them. As in previous studies, it is\ndifficult to ensure that one generation method always generates the best\nparaphrase in various domains. Therefore, we focus on finding the best\ncandidate from multiple candidates, rather than assuming that there is only one\ncombination of generative models and decoding options. Our approach shows that\nit is easy to apply in various domains and has sufficiently good performance\ncompared to previous methods. In addition, our approach can be used for data\naugmentation that extends the downstream corpus, showing that it can help\nimprove performance in English and Korean datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We present small-text, a simple and modular active learning library, which\noffers pool-based active learning for single- and multi-label text\nclassification in Python. It comes with various pre-implemented\nstate-of-the-art query strategies, including some that can leverage the GPU.\nClearly defined interfaces allow the combination of a multitude of classifiers,\nquery strategies, and stopping criteria, thereby facilitating a quick mix and\nmatch, and enabling a rapid development of both active learning experiments and\napplications. To make various classifiers accessible in a consistent way, it\nintegrates several well-known existing machine learning libraries, namely,\nscikit-learn, PyTorch, and huggingface transformers, where the latter\nintegrations are available as optionally installable extensions, making the\navailability of a GPU competely optional. The library is available under the\nMIT License at https://github.com/webis-de/small-text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00356","description":"<p>Masked language models (MLMs) are pre-trained with a denoising objective that\nis in a mismatch with the objective of downstream fine-tuning. We propose\npragmatic masking and surrogate fine-tuning as two complementing strategies\nthat exploit social cues to drive pre-trained representations toward a broad\nset of concepts useful for a wide class of social meaning tasks. We test our\nmodels on $15$ different Twitter datasets for social meaning detection. Our\nmethods achieve $2.34\\%$ $F_1$ over a competitive baseline, while outperforming\ndomain-specific language models pre-trained on large datasets. Our methods also\nexcel in few-shot learning: with only $5\\%$ of training data (severely\nfew-shot), our methods enable an impressive $68.54\\%$ average $F_1$. The\nmethods are also language agnostic, as we show in a zero-shot setting involving\nsix datasets from three different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05271","description":"<p>Dialogue systems research is traditionally focused on dialogues between two\ninterlocutors, largely ignoring group conversations. Moreover, most previous\nresearch is focused either on task-oriented dialogue (e.g.\\ restaurant\nbookings) or user engagement (chatbots), while research on systems for\ncollaborative dialogues is an under-explored area. To this end, we introduce\nthe first publicly available dataset containing collaborative conversations on\nsolving a cognitive task, consisting of 500 group dialogues and 14k utterances.\nFurthermore, we propose a novel annotation schema that captures deliberation\ncues and release 50 dialogues annotated with it. Finally, we demonstrate the\nusefulness of the annotated data in training classifiers to predict the\nconstructiveness of a conversation. The data collection platform, dataset and\nannotated corpus are publicly available at https://delibot.xyz\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafford_T/0/1/0/all/0/1\">Tom Stafford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00122","description":"<p>The sheer volume of financial statements makes it difficult for humans to\naccess and analyze a business's financials. Robust numerical reasoning likewise\nfaces unique challenges in this domain. In this work, we focus on answering\ndeep questions over financial data, aiming to automate the analysis of a large\ncorpus of financial documents. In contrast to existing tasks on general domain,\nthe finance domain includes complex numerical reasoning and understanding of\nheterogeneous representations. To facilitate analytical progress, we propose a\nnew large-scale dataset, FinQA, with Question-Answering pairs over Financial\nreports, written by financial experts. We also annotate the gold reasoning\nprograms to ensure full explainability. We further introduce baselines and\nconduct comprehensive experiments in our dataset. The results demonstrate that\npopular, large, pre-trained models fall far short of expert humans in acquiring\nfinance knowledge and in complex multi-step numerical reasoning on that\nknowledge. Our dataset -- the first of its kind -- should therefore enable\nsignificant, new community research into complex application domains. The\ndataset and code are publicly available\\url{https://github.com/czyssrs/FinQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borova_I/0/1/0/all/0/1\">Iana Borova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langdon_D/0/1/0/all/0/1\">Dylan Langdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_R/0/1/0/all/0/1\">Reema Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_M/0/1/0/all/0/1\">Matt Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Routledge_B/0/1/0/all/0/1\">Bryan Routledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartography Active Learning. (arXiv:2109.04282v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04282","description":"<p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07958","description":"<p>We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephanie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08818","description":"<p>Incorporating lexical knowledge into deep learning models has been proved to\nbe very effective for sequence labeling tasks. However, previous works commonly\nhave difficulty dealing with large-scale dynamic lexicons which often cause\nexcessive matching noise and problems of frequent updates. In this paper, we\npropose DyLex, a plug-in lexicon incorporation approach for BERT based sequence\nlabeling tasks. Instead of leveraging embeddings of words in the lexicon as in\nconventional methods, we adopt word-agnostic tag embeddings to avoid\nre-training the representation while updating the lexicon. Moreover, we employ\nan effective supervised lexical knowledge denoising method to smooth out\nmatching noise. Finally, we introduce a col-wise attention based knowledge\nfusion mechanism to guarantee the pluggability of the proposed framework.\nExperiments on ten datasets of three tasks show that the proposed framework\nachieves new SOTA, even with very large scale lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guang-Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v10 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socially Aware Bias Measurements for Hindi Language Representations. (arXiv:2110.07871v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07871","description":"<p>Language representations are efficient tools used across NLP applications,\nbut they are strife with encoded societal biases. These biases are studied\nextensively, but with a primary focus on English language representations and\nbiases common in the context of Western society. In this work, we investigate\nbiases present in Hindi language representations with focuses on caste and\nreligion-associated biases. We demonstrate how biases are unique to specific\nlanguage representations based on the history and culture of the region they\nare widely spoken in, and how the same societal bias (such as binary\ngender-associated biases) is encoded by different words and text spans across\nlanguages. The discoveries of our work highlight the necessity of culture\nawareness and linguistic artifacts when modeling language representations, in\norder to better understand the encoded biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_V/0/1/0/all/0/1\">Vijit Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishi_A/0/1/0/all/0/1\">Akihiro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine-in-the-Loop Rewriting for Creative Image Captioning. (arXiv:2111.04193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04193","description":"<p>Machine-in-the-loop writing aims to enable humans to collaborate with models\nto complete their writing tasks more effectively. Prior work has found that\nproviding humans a machine-written draft or sentence-level continuations has\nlimited success since the generated text tends to deviate from humans'\nintention. To allow the user to retain control over the content, we train a\nrewriting model that, when prompted, modifies specified spans of text within\nthe user's original draft to introduce descriptive and figurative elements\nlocally in the text. We evaluate the model on its ability to collaborate with\nhumans on the task of creative image captioning. On a user study through Amazon\nMechanical Turk, our model is rated to be more helpful than a baseline\ninfilling language model. In addition, third-party evaluation shows that users\nwrite more descriptive and figurative captions when collaborating with our\nmodel compared to completing the task alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is \"My Favorite New Movie\" My Favorite Movie? Probing the Understanding of Recursive Noun Phrases. (arXiv:2112.08326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08326","description":"<p>Recursive noun phrases (NPs) have interesting semantic properties. For\nexample, \"my favorite new movie\" is not necessarily my favorite movie, whereas\n\"my new favorite movie\" is. This is common sense to humans, yet it is unknown\nwhether language models have such knowledge. We introduce the Recursive Noun\nPhrase Challenge (RNPC), a dataset of three textual inference tasks involving\ntextual entailment and event plausibility comparison, precisely targeting the\nunderstanding of recursive NPs. When evaluated on RNPC, state-of-the-art\nTransformer models only perform around chance. Still, we show that such\nknowledge is learnable with appropriate data. We further probe the models for\nrelevant linguistic features that can be learned from our tasks, including\nmodifier semantic category and modifier scope. Finally, models trained on RNPC\nachieve strong zero-shot performance on an extrinsic Harm Detection evaluation\ntask, showing the usefulness of the understanding of recursive NPs in\ndownstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocAMR: Multi-Sentence AMR Representation and Evaluation. (arXiv:2112.08513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08513","description":"<p>Despite extensive research on parsing of English sentences into Abstraction\nMeaning Representation (AMR) graphs, which are compared to gold graphs via the\nSmatch metric, full-document parsing into a unified graph representation lacks\nwell-defined representation and evaluation. Taking advantage of a\nsuper-sentential level of coreference annotation from previous work, we\nintroduce a simple algorithm for deriving a unified graph representation,\navoiding the pitfalls of information loss from over-merging and lack of\ncoherence from under-merging. Next, we describe improvements to the Smatch\nmetric to make it tractable for comparing document-level graphs, and use it to\nre-evaluate the best published document-level AMR parser. We also present a\npipeline approach combining the top performing AMR parser and coreference\nresolution systems, providing a strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_A/0/1/0/all/0/1\">Austin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaravel_S/0/1/0/all/0/1\">Sadhana Kumaravel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ram&#xf3;n Fernandez Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning To Retrieve Prompts for In-Context Learning. (arXiv:2112.08633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08633","description":"<p>In-context learning is a recent paradigm in natural language understanding,\nwhere a large pre-trained language model (LM) observes a test instance and a\nfew training examples as its input, and directly decodes the output without any\nupdate to its parameters. However, performance has been shown to strongly\ndepend on the selected training examples (termed prompt). In this work, we\npropose an efficient method for retrieving prompts for in-context learning\nusing annotated data and a LM. Given an input-output pair, we estimate the\nprobability of the output given the input and a candidate training example as\nthe prompt, and label training examples as positive or negative based on this\nprobability. We then train an efficient dense retriever from this data, which\nis used to retrieve training examples as prompts at test time. We evaluate our\napproach on three sequence-to-sequence tasks where language utterances are\nmapped to meaning representations, and find that it substantially outperforms\nprior work and multiple baselines across the board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rubin_O/0/1/0/all/0/1\">Ohad Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics. (arXiv:2201.04275v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04275","description":"<p>In order for language models to aid physics research, they must first encode\nrepresentations of mathematical and natural language discourse which lead to\ncoherent explanations, with correct ordering and relevance of statements. We\npresent a collection of datasets developed to evaluate the performance of\nlanguage models in this regard, which measure capabilities with respect to\nsentence ordering, position, section prediction, and discourse coherence.\nAnalysis of the data reveals equations and sub-disciplines which are most\ncommon in physics discourse, as well as the sentence-level frequency of\nequations and expressions. We present baselines that demonstrate how\ncontemporary language models are challenged by coherence related tasks in\nphysics, even when trained on mathematical natural language objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meadows_J/0/1/0/all/0/1\">Jordan Meadows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Overlapping Problem of Open-Domain Dialogue Datasets. (arXiv:2201.06219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06219","description":"<p>Open-domain dialogue systems aim to converse with humans through text, and\ndialogue research has heavily relied on benchmark datasets. In this work, we\nobserve the overlapping problem in DailyDialog and OpenSubtitles, two popular\nopen-domain dialogue benchmark datasets. Our systematic analysis then shows\nthat such overlapping can be exploited to obtain fake state-of-the-art\nperformance. Finally, we address this issue by cleaning these datasets and\nsetting up a proper data processing procedure for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuqiao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.08712","description":"<p>To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nian_Y/0/1/0/all/0/1\">Yi Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingna Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Cui Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03312","description":"<p>Prevailing deep models are single-purpose and overspecialize at individual\ntasks. However, when being extended to new tasks, they typically forget\npreviously learned skills and learn from scratch. We address this issue by\nintroducing SkillNet-NLU, a general-purpose model that stitches together\nexisting skills to learn new tasks more effectively. The key feature of our\napproach is that it is sparsely activated guided by predefined skills.\nDifferent from traditional dense models that always activate all the model\nparameters, SkillNet-NLU only activates parts of the model parameters whose\nskills are relevant to the target task. When learning for a new task, our\napproach precisely activates required skills and also provides an option to add\nnew skills. We evaluate on natural language understandings tasks and have the\nfollowing findings. First, with only one model checkpoint, SkillNet-NLU\nperforms better than task-specific fine-tuning and two multi-task learning\nbaselines (i.e., dense model and Mixture-of-Experts model) on six tasks.\nSecond, sparsely activated pre-training further improves the overall\nperformance. Third, SkillNet-NLU significantly outperforms baseline systems\nwhen being extended to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang. (arXiv:2203.04651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04651","description":"<p>Languages are continuously undergoing changes, and the mechanisms that\nunderlie these changes are still a matter of debate. In this work, we approach\nlanguage evolution through the lens of causality in order to model not only how\nvarious distributional factors associate with language change, but how they\ncausally affect it. In particular, we study slang, which is an informal\nlanguage that is typically restricted to a specific group or social setting. We\nanalyze the semantic change and frequency shift of slang words and compare them\nto those of standard, nonslang words. With causal discovery and causal\ninference techniques, we measure the effect that word type (slang/nonslang) has\non both semantic change and frequency shift, as well as its relationship to\nfrequency, polysemy and part of speech. Our analysis provides some new insights\nin the study of language change, e.g., we show that slang words undergo less\nsemantic change but tend to have larger frequency shifts over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keidar_D/0/1/0/all/0/1\">Daphna Keidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opedal_A/0/1/0/all/0/1\">Andreas Opedal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05918","description":"<p>Recent progress on parse tree encoder for sentence representation learning is\nnotable. However, these works mainly encode tree structures recursively, which\nis not conducive to parallelization. On the other hand, these works rarely take\ninto account the labels of arcs in dependency trees. To address both issues, we\npropose Dependency-Transformer, which applies a relation-attention mechanism\nthat works in concert with the self-attention mechanism. This mechanism aims to\nencode the dependency and the spatial positional relations between nodes in the\ndependency tree of sentences. By a score-based method, we successfully inject\nthe syntax information without affecting Transformer's parallelizability. Our\nmodel outperforms or is comparable to the state-of-the-art methods on four\ntasks for sentence representation and has obvious advantages in computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Junhua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiajun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decay No More: A Persistent Twitter Dataset for Learning Social Meaning. (arXiv:2204.04611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04611","description":"<p>With the proliferation of social media, many studies resort to social media\nto construct datasets for developing social meaning understanding systems. For\nthe popular case of Twitter, most researchers distribute tweet IDs without the\nactual text contents due to the data distribution policy of the platform. One\nissue is that the posts become increasingly inaccessible over time, which leads\nto unfair comparisons and a temporal bias in social media research. To\nalleviate this challenge of data decay, we leverage a paraphrase model to\npropose a new persistent English Twitter dataset for social meaning (PTSM).\nPTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We\nexperiment with two SOTA pre-trained language models and show that our PTSM can\nsubstitute the actual tweets with paraphrases with marginal performance loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Classify Open Intent via Soft Labeling and Manifold Mixup. (arXiv:2204.07804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07804","description":"<p>Open intent classification is a practical yet challenging task in dialogue\nsystems. Its objective is to accurately classify samples of known intents while\nat the same time detecting those of open (unknown) intents. Existing methods\nusually use outlier detection algorithms combined with K-class classifier to\ndetect open intents, where K represents the class number of known intents.\nDifferent from them, in this paper, we consider another way without using\noutlier detection algorithms. Specifically, we directly train a (K+1)-class\nclassifier for open intent classification, where the (K+1)-th class represents\nopen intents. To address the challenge that training a (K+1)-class classifier\nwith training samples of only K classes, we propose a deep model based on Soft\nLabeling and Manifold Mixup (SLMM). In our method, soft labeling is used to\nreshape the label distribution of the known intent samples, aiming at reducing\nmodel's overconfident on known intents. Manifold mixup is used to generate\npseudo samples for open intents, aiming at well optimizing the decision\nboundary of open intents. Experiments on four benchmark datasets demonstrate\nthat our method outperforms previous methods and achieves state-of-the-art\nperformance. All the code and data of this work can be obtained at\nhttps://github.com/zifengcheng/SLMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zifeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yafeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qing Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph. (arXiv:2204.10194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10194","description":"<p>Building query graphs from natural language questions is an important step in\ncomplex question answering over knowledge graph (Complex KGQA). In general, a\nquestion can be correctly answered if its query graph is built correctly and\nthe right answer is then retrieved by issuing the query graph against the KG.\nTherefore, this paper focuses on query graph generation from natural language\nquestions. Existing approaches for query graph generation ignore the semantic\nstructure of a question, resulting in a large number of noisy query graph\ncandidates that undermine prediction accuracies. In this paper, we define six\nsemantic structures from common questions in KGQA and develop a novel\nStructure-BERT to predict the semantic structure of a question. By doing so, we\ncan first filter out noisy candidate query graphs by the predicted semantic\nstructures, and then rank the remaining candidates with a BERT-based ranking\nmodel. Extensive experiments on two popular benchmarks MetaQA and\nWebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model. (arXiv:2204.13509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13509","description":"<p>Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seongjin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Hwijeen Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyoungSeok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gichang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life is not Always Depressing: Exploring the Happy Moments of People Diagnosed with Depression. (arXiv:2204.13569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13569","description":"<p>In this work, we explore the relationship between depression and\nmanifestations of happiness in social media. While the majority of works\nsurrounding depression focus on symptoms, psychological research shows that\nthere is a strong link between seeking happiness and being diagnosed with\ndepression. We make use of Positive-Unlabeled learning paradigm to\nautomatically extract happy moments from social media posts of both controls\nand users diagnosed with depression, and qualitatively analyze them with\nlinguistic tools such as LIWC and keyness information. We show that the life of\ndepressed individuals is not always bleak, with positive events related to\nfriends and family being more noteworthy to their lives compared to the more\nmundane happy events reported by control users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge. (arXiv:2205.00759v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00759","description":"<p>Conversational Causal Emotion Entailment aims to detect causal utterances for\na non-neutral targeted utterance from a conversation. In this work, we build\nconversations as graphs to overcome implicit contextual modelling of the\noriginal entailment style. Following the previous work, we further introduce\nthe emotion information into graphs. Emotion information can markedly promote\nthe detection of causal utterances whose emotion is the same as the targeted\nutterance. However, it is still hard to detect causal utterances with different\nemotions, especially neutral ones. The reason is that models are limited in\nreasoning causal clues and passing them between utterances. To alleviate this\nproblem, we introduce social commonsense knowledge (CSK) and propose a\nKnowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two\nutterances. As not all CSK is emotionally suitable for utterances, we therefore\npropose a sentiment-realized knowledge selecting strategy to filter CSK. To\nprocess KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph\nnetworks. Experimental results show that our method outperforms baselines and\ninfers more causes with different emotions from the targeted utterance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02023","description":"<p>The success of multilingual pre-trained models is underpinned by their\nability to learn representations shared by multiple languages even in absence\nof any explicit supervision. However, it remains unclear how these models learn\nto generalise across languages. In this work, we conjecture that multilingual\npre-trained models can derive language-universal abstractions about grammar. In\nparticular, we investigate whether morphosyntactic information is encoded in\nthe same subset of neurons in different languages. We conduct the first\nlarge-scale empirical study over 43 languages and 14 morphosyntactic categories\nwith a state-of-the-art neuron-level probe. Our findings show that the\ncross-lingual overlap between neurons is significant, but its extent may vary\nacross categories and depends on language proximity and pre-training data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Few-Shot Fine-Tuning for Opinion Summarization. (arXiv:2205.02170v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02170","description":"<p>Abstractive summarization models are typically pre-trained on large amounts\nof generic texts, then fine-tuned on tens or hundreds of thousands of annotated\nsamples. However, in opinion summarization, large annotated datasets of reviews\npaired with reference summaries are not available and would be expensive to\ncreate. This calls for fine-tuning methods robust to overfitting on small\ndatasets. In addition, generically pre-trained models are often not accustomed\nto the specifics of customer reviews and, after fine-tuning, yield summaries\nwith disfluencies and semantic mistakes. To address these problems, we utilize\nan efficient few-shot method based on adapters which, as we show, can easily\nstore in-domain knowledge. Instead of fine-tuning the entire model, we add\nadapters and pre-train them in a task-specific way on a large corpus of\nunannotated customer reviews, using held-out reviews as pseudo summaries. Then,\nfine-tune the adapters on the small available human-annotated dataset. We show\nthat this self-supervised adapter pre-training improves summary quality over\nstandard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp\ndatasets, respectively. Finally, for summary personalization, we condition on\naspect keyword queries, automatically created from generic datasets. In the\nsame vein, we pre-train the adapters in a query-based manner on customer\nreviews and then fine-tune them on annotated datasets. This results in\nbetter-organized summary content reflected in improved coherence and fewer\nredundancies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collective Relevance Labeling for Passage Retrieval. (arXiv:2205.03273v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.03273","description":"<p>Deep learning for Information Retrieval (IR) requires a large amount of\nhigh-quality query-document relevance labels, but such labels are inherently\nsparse. Label smoothing redistributes some observed probability mass over\nunobserved instances, often uniformly, uninformed of the true distribution. In\ncontrast, we propose knowledge distillation for informed labeling, without\nincurring high computation overheads at evaluation time. Our contribution is\ndesigning a simple but efficient teacher model which utilizes collective\nknowledge, to outperform state-of-the-arts distilled from a more complex\nteacher model. Specifically, we train up to x8 faster than the state-of-the-art\nteacher, while distilling the rankings better. Our code is publicly available\nat https://github.com/jihyukkim-nlp/CollectiveKD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A High-Resolution Chest CT-Scan Image Dataset for COVID-19 Diagnosis and Differentiation. (arXiv:2205.03408v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03408","description":"<p>During the COVID-19 pandemic, computed tomography (CT) is a good way to\ndiagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a\nform of computed tomography that uses advanced methods to improve image\nresolution. Publicly accessible COVID-19 CT image datasets are very difficult\nto come by due to privacy concerns, which impedes the study and development of\nAI-powered COVID-19 diagnostic algorithms based on CT images. To address this\nproblem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution\nchest CT Scan image dataset that includes not only COVID-19 cases of Ground\nGlass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT\nimages of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which\nincludes slice-level, and patient-level labels, has the potential to aid\nCOVID-19 research, especially for diagnosis and differentiation using\nartificial intelligence algorithms, machine learning and deep learning methods.\nThis dataset is accessible through web at: <a href=\"http://databiox.com\">this http URL</a> and includes\n181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy\nPaving, Air Space Consolidation and Negative.\n</p>\n<p>Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging,\nChest Image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abedi_I/0/1/0/all/0/1\">Iraj Abedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vali_M/0/1/0/all/0/1\">Mahsa Vali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shahreza_B/0/1/0/all/0/1\">Bentolhoda Otroshi Shahreza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bolhasani_H/0/1/0/all/0/1\">Hamidreza Bolhasani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution. (arXiv:2205.03409v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03409","description":"<p>Most of the existing video face super-resolution (VFSR) methods are trained\nand evaluated on VoxCeleb1, which is designed specifically for speaker\nidentification and the frames in this dataset are of low quality. As a\nconsequence, the VFSR models trained on this dataset can not output\nvisual-pleasing results. In this paper, we develop an automatic and scalable\npipeline to collect a high-quality video face dataset (VFHQ), which contains\nover $16,000$ high-fidelity clips of diverse interview scenarios. To verify the\nnecessity of VFHQ, we further conduct experiments and demonstrate that VFSR\nmodels trained on our VFHQ dataset can generate results with sharper edges and\nfiner textures than those trained on VoxCeleb1. In addition, we show that the\ntemporal information plays a pivotal role in eliminating video consistency\nissues as well as further improving visual performance. Based on VFHQ, by\nanalyzing the benchmarking study of several state-of-the-art algorithms under\nbicubic and blind settings. See our project page:\nhttps://liangbinxie.github.io/projects/vfhq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liangbin Xie. Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Honglun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers. (arXiv:2205.03436v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03436","description":"<p>Self-attention based models such as vision transformers (ViTs) have emerged\nas a very competitive architecture alternative to convolutional neural networks\n(CNNs) in computer vision. Despite increasingly stronger variants with\never-higher recognition accuracies, due to the quadratic complexity of\nself-attention, existing ViTs are typically demanding in computation and model\nsize. Although several successful design choices (e.g., the convolutions and\nhierarchical multi-stage structure) of prior CNNs have been reintroduced into\nrecent ViTs, they are still not sufficient to meet the limited resource\nrequirements of mobile devices. This motivates a very recent attempt to develop\nlight ViTs based on the state-of-the-art MobileNet-v2, but still leaves a\nperformance gap behind. In this work, pushing further along this under-studied\ndirection we introduce EdgeViTs, a new family of light-weight ViTs that, for\nthe first time, enable attention-based vision models to compete with the best\nlight-weight CNNs in the tradeoff between accuracy and on-device efficiency.\nThis is realized by introducing a highly cost-effective local-global-local\n(LGL) information exchange bottleneck based on optimal integration of\nself-attention and convolutions. For device-dedicated evaluation, rather than\nrelying on inaccurate proxies like the number of FLOPs or parameters, we adopt\na practical approach of focusing directly on on-device latency and, for the\nfirst time, energy efficiency. Specifically, we show that our models are\nPareto-optimal when both accuracy-latency and accuracy-energy trade-offs are\nconsidered, achieving strict dominance over other ViTs in almost all cases and\ncompeting with the most efficient CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1\">Lukasz Dudziak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Multi-modal 2D/3D Registration via Local Descriptors Learning. (arXiv:2205.03439v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03439","description":"<p>Multi-modal registration is a required step for many image-guided procedures,\nespecially ultrasound-guided interventions that require anatomical context.\nWhile a number of such registration algorithms are already available, they all\nrequire a good initialization to succeed due to the challenging appearance of\nultrasound images and the arbitrary coordinate system they are acquired in. In\nthis paper, we present a novel approach to solve the problem of registration of\nan ultrasound sweep to a pre-operative image. We learn dense keypoint\ndescriptors from which we then estimate the registration. We show that our\nmethod overcomes the challenges inherent to registration tasks with freehand\nultrasound sweeps, namely, the multi-modality and multidimensionality of the\ndata in addition to lack of precise ground truth and low amounts of training\nexamples. We derive a registration method that is fast, generic, fully\nautomatic, does not require any initialization and can naturally generate\nvisualizations aiding interpretability and explainability. Our approach is\nevaluated on a clinical dataset of paired MR volumes and ultrasound sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Markova_V/0/1/0/all/0/1\">Viktoria Markova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronchetti_M/0/1/0/all/0/1\">Matteo Ronchetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wein_W/0/1/0/all/0/1\">Wolfgang Wein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zettinig_O/0/1/0/all/0/1\">Oliver Zettinig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prevost_R/0/1/0/all/0/1\">Raphael Prevost</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentKeypointGAN: Controlling Images via Latent Keypoints -- Extended Abstract. (arXiv:2205.03448v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03448","description":"<p>Generative adversarial networks (GANs) can now generate photo-realistic\nimages. However, how to best control the image content remains an open\nchallenge. We introduce LatentKeypointGAN, a two-stage GAN internally\nconditioned on a set of keypoints and associated appearance embeddings\nproviding control of the position and style of the generated objects and their\nrespective parts. A major difficulty that we address is disentangling the image\ninto spatial and appearance factors with little domain knowledge and\nsupervision signals. We demonstrate in a user study and quantitative\nexperiments that LatentKeypointGAN provides an interpretable latent space that\ncan be used to re-arrange the generated images by re-positioning and exchanging\nkeypoint embeddings, such as generating portraits by combining the eyes, and\nmouth from different images. Notably, our method does not require labels as it\nis self-supervised and thereby applies to diverse application domains, such as\nediting portraits, indoor rooms, and full-body human poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Analysis of Non-Blind Deblurring Methods for Noisy Blurred Images. (arXiv:2205.03464v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03464","description":"<p>Image blurring refers to the degradation of an image wherein the image's\noverall sharpness decreases. Image blurring is caused by several factors.\nAdditionally, during the image acquisition process, noise may get added to the\nimage. Such a noisy and blurred image can be represented as the image resulting\nfrom the convolution of the original image with the associated point spread\nfunction, along with additive noise. However, the blurred image often contains\ninadequate information to uniquely determine the plausible original image.\nBased on the availability of blurring information, image deblurring methods can\nbe classified as blind and non-blind. In non-blind image deblurring, some prior\ninformation is known regarding the corresponding point spread function and the\nadded noise. The objective of this study is to determine the effectiveness of\nnon-blind image deblurring methods with respect to the identification and\nelimination of noise present in blurred images. In this study, three non-blind\nimage deblurring methods, namely Wiener deconvolution, Lucy-Richardson\ndeconvolution, and regularized deconvolution were comparatively analyzed for\nnoisy images featuring salt-and-pepper noise. Two types of blurring effects\nwere simulated, namely motion blurring and Gaussian blurring. The said three\nnon-blind deblurring methods were applied under two scenarios: direct\ndeblurring of noisy blurred images and deblurring of images after denoising\nthrough the application of the adaptive median filter. The obtained results\nwere then compared for each scenario to determine the best approach for\ndeblurring noisy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1\">Poorna Banerjee Dasgupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVIMO2: An Event Camera Dataset for Motion Segmentation, Optical Flow, Structure from Motion, and Visual Inertial Odometry in Indoor Scenes with Monocular or Stereo Algorithms. (arXiv:2205.03467v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03467","description":"<p>A new event camera dataset, EVIMO2, is introduced that improves on the\npopular EVIMO dataset by providing more data, from better cameras, in more\ncomplex scenarios. As with its predecessor, EVIMO2 provides labels in the form\nof per-pixel ground truth depth and segmentation as well as camera and object\nposes. All sequences use data from physical cameras and many sequences feature\nmultiple independently moving objects. Typically, such labeled data is\nunavailable in physical event camera datasets. Thus, EVIMO2 will serve as a\nchallenging benchmark for existing algorithms and rich training set for the\ndevelopment of new algorithms. In particular, EVIMO2 is suited for supporting\nresearch in motion and object segmentation, optical flow, structure from\nmotion, and visual (inertial) odometry in both monocular or stereo\nconfigurations.\n</p>\n<p>EVIMO2 consists of 41 minutes of data from three 640$\\times$480 event\ncameras, one 2080$\\times$1552 classical color camera, inertial measurements\nfrom two six axis inertial measurement units, and millimeter accurate object\nposes from a Vicon motion capture system. The dataset's 173 sequences are\narranged into three categories. 3.75 minutes of independently moving household\nobjects, 22.55 minutes of static scenes, and 14.85 minutes of basic motions in\nshallow scenes. Some sequences were recorded in low-light conditions where\nconventional cameras fail. Depth and segmentation are provided at 60 Hz for the\nevent cameras and 30 Hz for the classical camera. The masks can be regenerated\nusing open-source code up to rates as high as 200 Hz.\n</p>\n<p>This technical report briefly describes EVIMO2. The full documentation is\navailable online. Videos of individual sequences can be sampled on the download\npage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burner_L/0/1/0/all/0/1\">Levi Burner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrokhin_A/0/1/0/all/0/1\">Anton Mitrokhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Norm-Scaling for Out-of-Distribution Detection. (arXiv:2205.03493v1 [cs.LG])","link":"http://arxiv.org/abs/2205.03493","description":"<p>Out-of-Distribution (OoD) inputs are examples that do not belong to the true\nunderlying distribution of the dataset. Research has shown that deep neural\nnets make confident mispredictions on OoD inputs. Therefore, it is critical to\nidentify OoD inputs for safe and reliable deployment of deep neural nets. Often\na threshold is applied on a similarity score to detect OoD inputs. One such\nsimilarity is angular similarity which is the dot product of latent\nrepresentation with the mean class representation. Angular similarity encodes\nuncertainty, for example, if the angular similarity is less, it is less certain\nthat the input belongs to that class. However, we observe that, different\nclasses have different distributions of angular similarity. Therefore, applying\na single threshold for all classes is not ideal since the same similarity score\nrepresents different uncertainties for different classes. In this paper, we\npropose norm-scaling which normalizes the logits separately for each class.\nThis ensures that a single value consistently represents similar uncertainty\nfor various classes. We show that norm-scaling, when used with maximum softmax\nprobability detector, achieves 9.78% improvement in AUROC, 5.99% improvement in\nAUPR and 33.19% reduction in FPR95 metrics over previous state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_D/0/1/0/all/0/1\">Deepak Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03519","description":"<p>Deep learning methods have been successfully used in various computer vision\ntasks. Inspired by that success, deep learning has been explored in magnetic\nresonance imaging (MRI) reconstruction. In particular, integrating deep\nlearning and model-based optimization methods has shown considerable\nadvantages. However, a large amount of labeled training data is typically\nneeded for high reconstruction quality, which is challenging for some MRI\napplications. In this paper, we propose a novel reconstruction method, named\nDURED-Net, that enables interpretable unsupervised learning for MR image\nreconstruction by combining an unsupervised denoising network and a\nplug-and-play method. We aim to boost the reconstruction performance of\nunsupervised learning by adding an explicit prior that utilizes imaging\nphysics. Specifically, the leverage of a denoising network for MRI\nreconstruction is achieved using Regularization by Denoising (RED). Experiment\nresults demonstrate that the proposed method requires a reduced amount of\ntraining data to achieve high reconstruction quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1\">Peizhou Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoliang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaojuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_L/0/1/0/all/0/1\">Liang Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1\">Leslie Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (arXiv:2205.03521v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03521","description":"<p>Multimodal named entity recognition and relation extraction (MNER and MRE) is\na fundamental and crucial branch in information extraction. However, existing\napproaches for MNER and MRE usually suffer from error sensitivity when\nirrelevant object images incorporated in texts. To deal with these issues, we\npropose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for\nvisual-enhanced entity and relation extraction, aiming to achieve more\neffective and robust performance. Specifically, we regard visual representation\nas pluggable visual prefix to guide the textual representation for error\ninsensitive forecasting decision. We further propose a dynamic gated\naggregation strategy to achieve hierarchical multi-scaled visual features as\nvisual prefix for fusion. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our method, and achieve state-of-the-art\nperformance. Code is available in https://github.com/zjunlp/HVPNeT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Terrain Mapping and Landing Site Detection for Autonomous UAVs. (arXiv:2205.03522v1 [cs.RO])","link":"http://arxiv.org/abs/2205.03522","description":"<p>The next generation of Mars rotorcrafts requires on-board autonomous hazard\navoidance landing. To this end, this work proposes a system that performs\ncontinuous multi-resolution height map reconstruction and safe landing spot\ndetection. Structure-from-Motion measurements are aggregated in a pyramid\nstructure using a novel Optimal Mixture of Gaussians formulation that provides\na comprehensive uncertainty model. Our multiresolution pyramid is built more\nefficiently and accurately than past work by decoupling pyramid filling from\nthe measurement updates of different resolutions. To detect the safest landing\nlocation, after an optimized hazard segmentation, we use a mean shift algorithm\non multiple distance transform peaks to account for terrain roughness and\nuncertainty. The benefits of our contributions are evaluated on real and\nsynthetic flight data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Proenca_P/0/1/0/all/0/1\">Pedro F. Proen&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaune_J/0/1/0/all/0/1\">Jeff Delaune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockers_R/0/1/0/all/0/1\">Roland Brockers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution. (arXiv:2205.03524v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03524","description":"<p>Due to the sophisticated imaging process, an identical scene captured by\ndifferent cameras could exhibit distinct imaging patterns, introducing distinct\nproficiency among the super-resolution (SR) models trained on images from\ndifferent devices. In this paper, we investigate a novel and practical task\ncoded cross-device SR, which strives to adapt a real-world SR model trained on\nthe paired images captured by one camera to low-resolution (LR) images captured\nby arbitrary target devices. The proposed task is highly challenging due to the\nabsence of paired data from various imaging devices. To address this issue, we\npropose an unsupervised domain adaptation mechanism for real-world SR, named\nDual ADversarial Adaptation (DADA), which only requires LR images in the target\ndomain with available real paired data from a source camera. DADA employs the\nDomain-Invariant Attention (DIA) module to establish the basis of target model\ntraining even without HR supervision. Furthermore, the dual framework of DADA\nfacilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for\ntwo LR input images from two domains, and an Intra-domain Adversarial\nAdaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA\ntogether improve the model transferability from the source domain to the\ntarget. We empirically conduct experiments under six Real to Real adaptation\nsettings among three different cameras, and achieve superior performance\ncompared with existing state-of-the-art approaches. We also evaluate the\nproposed DADA to address the adaptation to the video camera, which presents a\npromising research topic to promote the wide applications of real-world\nsuper-resolution. Our source code is publicly available at\nhttps://github.com/lonelyhope/DADA.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoqian Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weikai Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_M/0/1/0/all/0/1\">Mingzhi Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic segmentation of meniscus based on MAE self-supervision and point-line weak supervision paradigm. (arXiv:2205.03525v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03525","description":"<p>Medical image segmentation based on deep learning is often faced with the\nproblems of insufficient datasets and long time-consuming labeling. In this\npaper, we introduce the self-supervised method MAE(Masked Autoencoders) into\nknee joint images to provide a good initial weight for the segmentation model\nand improve the adaptability of the model to small datasets. Secondly, we\npropose a weakly supervised paradigm for meniscus segmentation based on the\ncombination of point and line to reduce the time of labeling. Based on the weak\nlabel ,we design a region growing algorithm to generate pseudo-label. Finally\nwe train the segmentation network based on pseudo-labels with weight transfer\nfrom self-supervision. Sufficient experimental results show that our proposed\nmethod combining self-supervision and weak supervision can almost approach the\nperformance of purely fully supervised models while greatly reducing the\nrequired labeling time and dataset size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuhan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kexin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaolong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Changzhen Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information. (arXiv:2205.03534v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03534","description":"<p>Recently, online shopping has gradually become a common way of shopping for\npeople all over the world. Wonderful merchandise advertisements often attract\nmore people to buy. These advertisements properly integrate multimodal\nmulti-structured information of commodities, such as visual spatial information\nand fine-grained structure information. However, traditional multimodal text\ngeneration focuses on the conventional description of what existed and\nhappened, which does not match the requirement of advertisement copywriting in\nthe real world. Because advertisement copywriting has a vivid language style\nand higher requirements of faithfulness. Unfortunately, there is a lack of\nreusable evaluation frameworks and a scarcity of datasets. Therefore, we\npresent a dataset, E-MMAD (e-commercial multimodal multi-structured\nadvertisement copywriting), which requires, and supports much more detailed\ninformation in text generation. Noticeably, it is one of the largest video\ncaptioning datasets in this field. Accordingly, we propose a baseline method\nand faithfulness evaluation metric on the strength of structured information\nreasoning to solve the demand in reality on this dataset. It surpasses the\nprevious methods by a large margin on all metrics. The dataset and method are\ncoming soon on \\url{https://e-mmad.github.io/e-mmad.net/index.html}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinglin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiCo-Net: Regress Globally, Match Locally for Robust 6D Pose Estimation. (arXiv:2205.03536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03536","description":"<p>The challenges of learning a robust 6D pose function lie in 1) severe\nocclusion and 2) systematic noises in depth images. Inspired by the success of\npoint-pair features, the goal of this paper is to recover the 6D pose of an\nobject instance segmented from RGB-D images by locally matching pairs of\noriented points between the model and camera space. To this end, we propose a\nnovel Bi-directional Correspondence Mapping Network (BiCo-Net) to first\ngenerate point clouds guided by a typical pose regression, which can thus\nincorporate pose-sensitive information to optimize generation of local\ncoordinates and their normal vectors. As pose predictions via geometric\ncomputation only rely on one single pair of local oriented points, our BiCo-Net\ncan achieve robustness against sparse and occluded point clouds. An ensemble of\nredundant pose predictions from locally matching and direct pose regression\nfurther refines final pose output against noisy observations. Experimental\nresults on three popularly benchmarking datasets can verify that our method can\nachieve state-of-the-art performance, especially for the more challenging\nsevere occluded scenes. Source codes are available at\nhttps://github.com/Gorilla-Lab-SCUT/BiCo-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zelin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees. (arXiv:2205.03546v1 [cs.LG])","link":"http://arxiv.org/abs/2205.03546","description":"<p>Graph neural networks (GNNs) have achieved state-of-the-art performance in\nmany graph-based tasks such as node classification and graph classification.\nHowever, many recent works have demonstrated that an attacker can mislead GNN\nmodels by slightly perturbing the graph structure. Existing attacks to GNNs are\neither under the less practical threat model where the attacker is assumed to\naccess the GNN model parameters, or under the practical black-box threat model\nbut consider perturbing node features that are shown to be not enough\neffective. In this paper, we aim to bridge this gap and consider black-box\nattacks to GNNs with structure perturbation as well as with theoretical\nguarantees. We propose to address this challenge through bandit techniques.\nSpecifically, we formulate our attack as an online optimization with bandit\nfeedback. This original problem is essentially NP-hard due to the fact that\nperturbing the graph structure is a binary optimization problem. We then\npropose an online attack based on bandit optimization which is proven to be\n{sublinear} to the query number $T$, i.e., $\\mathcal{O}(\\sqrt{N}T^{3/4})$ where\n$N$ is the number of nodes in the graph. Finally, we evaluate our proposed\nattack by conducting experiments over multiple datasets and GNN models. The\nexperimental results on various citation graphs and image graphs show that our\nattack is both effective and efficient. Source code is available\nat~\\url{https://github.com/Metaoblivion/Bandit_GNN_Attack}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-enabled Detection and Classification of Bacterial Colonies using a Thin Film Transistor (TFT) Image Sensor. (arXiv:2205.03549v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03549","description":"<p>Early detection and identification of pathogenic bacteria such as Escherichia\ncoli (E. coli) is an essential task for public health. The conventional\nculture-based methods for bacterial colony detection usually take &gt;24 hours to\nget the final read-out. Here, we demonstrate a bacterial colony-forming-unit\n(CFU) detection system exploiting a thin-film-transistor (TFT)-based image\nsensor array that saves ~12 hours compared to the Environmental Protection\nAgency (EPA)-approved methods. To demonstrate the efficacy of this CFU\ndetection system, a lensfree imaging modality was built using the TFT image\nsensor with a sample field-of-view of ~10 cm^2. Time-lapse images of bacterial\ncolonies cultured on chromogenic agar plates were automatically collected at\n5-minute intervals. Two deep neural networks were used to detect and count the\ngrowing colonies and identify their species. When blindly tested with 265\ncolonies of E. coli and other coliform bacteria (i.e., Citrobacter and\nKlebsiella pneumoniae), our system reached an average CFU detection rate of\n97.3% at 9 hours of incubation and an average recovery rate of 91.6% at ~12\nhours. This TFT-based sensor can be applied to various microbiological\ndetection methods. Due to the large scalability, ultra-large field-of-view, and\nlow cost of the TFT-based image sensors, this platform can be integrated with\neach agar plate to be tested and disposed of after the automated CFU count. The\nimaging field-of-view of this platform can be cost-effectively increased to\n&gt;100 cm^2 to provide a massive throughput for CFU detection using, e.g.,\nroll-to-roll manufacturing of TFTs as used in the flexible display industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koydemir_H/0/1/0/all/0/1\">Hatice Ceylan Koydemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORiordan_K/0/1/0/all/0/1\">Keelan O&#x27;Riordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haga_Y/0/1/0/all/0/1\">Yuta Haga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobashi_J/0/1/0/all/0/1\">Junji Kobashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hitoshi Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaru_T/0/1/0/all/0/1\">Takaya Tamaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1\">Kazunori Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Heavy Rain Removal to Detail Restoration: A Faster and Better Network. (arXiv:2205.03553v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03553","description":"<p>The dense rain accumulation in heavy rain can significantly wash out images\nand thus destroy the background details of images. Although existing deep rain\nremoval models lead to improved performance for heavy rain removal, we find\nthat most of them ignore the detail reconstruction accuracy of rain-free\nimages. In this paper, we propose a dual-stage progressive enhancement network\n(DPENet) to achieve effective deraining with structure-accurate rain-free\nimages. Two main modules are included in our framework, namely a rain streaks\nremoval network (R$^2$Net) and a detail reconstruction network (DRNet). The\nformer aims to achieve accurate rain removal, and the latter is designed to\nrecover the details of rain-free images. We introduce two main strategies\nwithin our networks to achieve trade-off between the effectiveness of deraining\nand the detail restoration of rain-free images. Firstly, a dilated dense\nresidual block (DDRB) within the rain streaks removal network is presented to\naggregate high/low level features of heavy rain. Secondly, an enhanced residual\npixel-wise attention block (ERPAB) within the detail reconstruction network is\ndesigned for context information aggregation. We also propose a comprehensive\nloss function to highlight the marginal and regional accuracy of rain-free\nimages. Extensive experiments on benchmark public datasets show both efficiency\nand effectiveness of the proposed method in achieving structure-preserving\nrain-free images for heavy rain removal. The source code and pre-trained models\ncan be found at \\url{https://github.com/wybchd/DPENet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuanbo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Target Active Object Tracking with Monte Carlo Tree Search and Target Motion Modeling. (arXiv:2205.03555v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03555","description":"<p>In this work, we are dedicated to multi-target active object tracking (AOT),\nwhere there are multiple targets as well as multiple cameras in the\nenvironment. The goal is maximize the overall target coverage of all cameras.\nPrevious work makes a strong assumption that each camera is fixed in a location\nand only allowed to rotate, which limits its application. In this work, we\nrelax the setting by allowing all cameras to both move along the boundary lines\nand rotate. In our setting, the action space becomes much larger, which leads\nto much higher computational complexity to identify the optimal action. To this\nend, we propose to leverage the action selection from multi-agent reinforcement\nlearning (MARL) network to prune the search tree of Monte Carlo Tree Search\n(MCTS) method, so as to find the optimal action more efficiently. Besides, we\nmodel the motion of the targets to predict the future position of the targets,\nwhich makes a better estimation of the future environment state in the MCTS\nprocess. We establish a multi-target 2D environment to simulate the sports\ngames, and experimental results demonstrate that our method can effectively\nimprove the target coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Fusion Network for Multi-Oriented Object Detection. (arXiv:2205.03562v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03562","description":"<p>In object detection, non-maximum suppression (NMS) methods are extensively\nadopted to remove horizontal duplicates of detected dense boxes for generating\nfinal object instances. However, due to the degraded quality of dense detection\nboxes and not explicit exploration of the context information, existing NMS\nmethods via simple intersection-over-union (IoU) metrics tend to underperform\non multi-oriented and long-size objects detection. Distinguishing with general\nNMS methods via duplicate removal, we propose a novel graph fusion network,\nnamed GFNet, for multi-oriented object detection. Our GFNet is extensible and\nadaptively fuse dense detection boxes to detect more accurate and holistic\nmulti-oriented object instances. Specifically, we first adopt a locality-aware\nclustering algorithm to group dense detection boxes into different clusters. We\nwill construct an instance sub-graph for the detection boxes belonging to one\ncluster. Then, we propose a graph-based fusion network via Graph Convolutional\nNetwork (GCN) to learn to reason and fuse the detection boxes for generating\nfinal instance boxes. Extensive experiments both on public available\nmulti-oriented text datasets (including MSRA-TD500, ICDAR2015, ICDAR2017-MLT)\nand multi-oriented object datasets (DOTA) verify the effectiveness and\nrobustness of our method against general NMS methods in multi-oriented object\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jie-Bo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03569","description":"<p>Compressed video action recognition has recently drawn growing attention,\nsince it remarkably reduces the storage and computational cost via replacing\nraw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,\nmotion vectors and residuals). However, this task severely suffers from the\ncoarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB\nand motion modalities. To address the two issues above, this paper proposes a\nnovel framework, namely Attentive Cross-modal Interaction Network with Motion\nEnhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for\nthe RGB modality and the other for the motion modality. Particularly, the\nmotion stream employs a multi-scale block embedded with a denoising module to\nenhance representation learning. The interaction between the two streams is\nthen strengthened by introducing the Selective Motion Complement (SMC) and\nCross-Modality Augment (CMA) modules, where SMC complements the RGB modality\nwith spatio-temporally attentive local motion features and CMA further combines\nthe two modalities with selective feature augmentation. Extensive experiments\non the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the\neffectiveness and efficiency of MEACI-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xiuguo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utility-Oriented Underwater Image Quality Assessment Based on Transfer Learning. (arXiv:2205.03574v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03574","description":"<p>The widespread image applications have greatly promoted the vision-based\ntasks, in which the Image Quality Assessment (IQA) technique has become an\nincreasingly significant issue. For user enjoyment in multimedia systems, the\nIQA exploits image fidelity and aesthetics to characterize user experience;\nwhile for other tasks such as popular object recognition, there exists a low\ncorrelation between utilities and perceptions. In such cases, the\nfidelity-based and aesthetics-based IQA methods cannot be directly applied. To\naddress this issue, this paper proposes a utility-oriented IQA in object\nrecognition. In particular, we initialize our research in the scenario of\nunderwater fish detection, which is a critical task that has not yet been\nperfectly addressed. Based on this task, we build an Underwater Image Utility\nDatabase (UIUD) and a learning-based Underwater Image Utility Measure (UIUM).\nInspired by the top-down design of fidelity-based IQA, we exploit the deep\nmodels of object recognition and transfer their features to our UIUM.\nExperiments validate that the proposed transfer-learning-based UIUM achieves\npromising performance in the recognition task. We envision our research\nprovides insights to bridge the researches of IQA and computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongfu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Honggang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1\">Ke Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callet_P/0/1/0/all/0/1\">Patrick Le Callet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Chinese License Plate Detection and Recognition with High Efficiency. (arXiv:2205.03582v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03582","description":"<p>Recently, deep learning-based methods have reached an excellent performance\non License Plate (LP) detection and recognition tasks. However, it is still\nchallenging to build a robust model for Chinese LPs since there are not enough\nlarge and representative datasets. In this work, we propose a new dataset named\nChinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP\nimages as a supplement to the existing public benchmarks. The images are mainly\ncaptured with electronic monitoring systems with detailed annotations. To our\nknowledge, CRPD is the largest public multi-objective Chinese LP dataset with\nannotations of vertices. With CRPD, a unified detection and recognition network\nwith high efficiency is presented as the baseline. The network is end-to-end\ntrainable with totally real-time inference efficiency (30 fps with 640p). The\nexperiments on several public benchmarks demonstrate that our method has\nreached competitive performance. The code and dataset will be publicly\navailable at https://github.com/yxgong0/CRPD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yanxiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Linjie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shuai Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinchen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPQE: Structure-and-Perception-Based Quality Evaluation for Image Super-Resolution. (arXiv:2205.03584v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03584","description":"<p>The image Super-Resolution (SR) technique has greatly improved the visual\nquality of images by enhancing their resolutions. It also calls for an\nefficient SR Image Quality Assessment (SR-IQA) to evaluate those algorithms or\ntheir generated images. In this paper, we focus on the SR-IQA under deep\nlearning and propose a Structure-and-Perception-based Quality Evaluation\n(SPQE). In emerging deep-learning-based SR, a generated high-quality, visually\npleasing image may have different structures from its corresponding low-quality\nimage. In such case, how to balance the quality scores between no-reference\nperceptual quality and referenced structural similarity is a critical issue. To\nhelp ease this problem, we give a theoretical analysis on this tradeoff and\nfurther calculate adaptive weights for the two types of quality scores. We also\npropose two deep-learning-based regressors to model the no-reference and\nreferenced scores. By combining the quality scores and their weights, we\npropose a unified SPQE metric for SR-IQA. Experimental results demonstrate that\nthe proposed method outperforms the state-of-the-arts in different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Keke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuzhen Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jinsong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient VVC Intra Prediction Based on Deep Feature Fusion and Probability Estimation. (arXiv:2205.03587v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03587","description":"<p>The ever-growing multimedia traffic has underscored the importance of\neffective multimedia codecs. Among them, the up-to-date lossy video coding\nstandard, Versatile Video Coding (VVC), has been attracting attentions of video\ncoding community. However, the gain of VVC is achieved at the cost of\nsignificant encoding complexity, which brings the need to realize fast encoder\nwith comparable Rate Distortion (RD) performance. In this paper, we propose to\noptimize the VVC complexity at intra-frame prediction, with a two-stage\nframework of deep feature fusion and probability estimation. At the first\nstage, we employ the deep convolutional network to extract the spatialtemporal\nneighboring coding features. Then we fuse all reference features obtained by\ndifferent convolutional kernels to determine an optimal intra coding depth. At\nthe second stage, we employ a probability-based model and the spatial-temporal\ncoherence to select the candidate partition modes within the optimal coding\ndepth. Finally, these selected depths and partitions are executed whilst\nunnecessary computations are excluded. Experimental results on standard\ndatabase demonstrate the superiority of proposed method, especially for High\nDefinition (HD) and Ultra-HD (UHD) video sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Weize Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Video Coding with GAN Latent Learning. (arXiv:2205.03599v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03599","description":"<p>The introduction of multiple viewpoints inevitably increases the bitrates to\nstore and transmit video scenes. To reduce the compressed bitrates, researchers\nhave developed to skip intermediate viewpoints during compression and delivery,\nand finally reconstruct them with Side Information (SI). Generally, the depth\nmaps can be utilized to construct SI; however, it shows inferior performance\nwith inaccurate reconstruction or high bitrates. In this paper, we propose a\nmulti-view video coding based on SI of Generative Adversarial Network (GAN). At\nthe encoder, we construct a spatio-temporal Epipolar Plane Image (EPI) and\nfurther utilize convolutional network to extract the latent code of GAN as SI;\nwhile at the decoder side, we combine the SI and adjacent viewpoints to\nreconstruct intermediate views by the generator of GAN. In particular, we set a\njoint encoder constraint of reconstruction cost and SI entropy, in order to\nachieve an optimal tradeoff between reconstruction quality and bitrate\noverhead. Experiments show a significantly improved Rate-Distortion (RD)\nperformance compared with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lan_C/0/1/0/all/0/1\">Chengdong Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_H/0/1/0/all/0/1\">Hao Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Block-wise Pruning with Auxiliary Gating Structures for Deep Convolutional Neural Networks. (arXiv:2205.03602v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03602","description":"<p>Convolutional neural networks are prevailing in deep learning tasks. However,\nthey suffer from massive cost issues when working on mobile devices. Network\npruning is an effective method of model compression to handle such problems.\nThis paper presents a novel structured network pruning method with auxiliary\ngating structures which assigns importance marks to blocks in backbone network\nas a criterion when pruning. Block-wise pruning is then realized by proposed\nvoting strategy, which is different from prevailing methods who prune a model\nin small granularity like channel-wise. We further develop a three-stage\ntraining scheduling for the proposed architecture incorporating knowledge\ndistillation for better performance. Our experiments demonstrate that our\nmethod can achieve state-of-the-arts compression performance for the\nclassification tasks. In addition, our approach can integrate synergistically\nwith other pruning methods by providing pretrained models, thus achieving a\nbetter performance than the unpruned model with over 93\\% FLOPs reduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zhaofeng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Honggang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoyu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Regularized Correlation Filter for UAV Object Tracking with adaptive Contextual Learning and Keyfilter Selection. (arXiv:2205.03627v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03627","description":"<p>Recently, correlation filter has been widely applied in unmanned aerial\nvehicle (UAV) tracking due to its high frame rates, robustness and low\ncalculation resources. However, it is fragile because of two inherent defects,\ni.e, boundary effect and filter corruption. Some methods by enlarging the\nsearch area can mitigate the boundary effect, yet introducing the undesired\nbackground distractors. Another approaches can alleviate the temporal\ndegeneration of learned filters by introducing the temporal regularizer, which\ndepends on the assumption that the filers between consecutive frames should be\ncoherent. In fact, sometimes the filers at the ($t-1$)th frame is vulnerable to\nheavy occlusion from backgrounds, which causes that the assumption does not\nhold. To handle them, in this work, we propose a novel $\\ell_{1}$\nregularization correlation filter with adaptive contextual learning and\nkeyfilter selection for UAV tracking. Firstly, we adaptively detect the\npositions of effective contextual distractors by the aid of the distribution of\nlocal maximum values on the response map of current frame which is generated by\nusing the previous correlation filter model. Next, we eliminate inconsistent\nlabels for the tracked target by removing one on each distractor and develop a\nnew score scheme for each distractor. Then, we can select the keyfilter from\nthe filters pool by finding the maximal similarity between the target at the\ncurrent frame and the target template corresponding to each filter in the\nfilters pool. Finally, quantitative and qualitative experiments on three\nauthoritative UAV datasets show that the proposed method is superior to the\nstate-of-the-art tracking methods based on correlation filter framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhangjian Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuhua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiye Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Quality Assessment of Compressed Videos: A Subjective and Objective Study. (arXiv:2205.03630v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03630","description":"<p>In the video coding process, the perceived quality of a compressed video is\nevaluated by full-reference quality evaluation metrics. However, it is\ndifficult to obtain reference videos with perfect quality. To solve this\nproblem, it is critical to design no-reference compressed video quality\nassessment algorithms, which assists in measuring the quality of experience on\nthe server side and resource allocation on the network side. Convolutional\nNeural Network (CNN) has shown its advantage in Video Quality Assessment (VQA)\nwith promising successes in recent years. A large-scale quality database is\nvery important for learning accurate and powerful compressed video quality\nmetrics. In this work, a semi-automatic labeling method is adopted to build a\nlarge-scale compressed video quality database, which allows us to label a large\nnumber of compressed videos with manageable human workload. The resulting\nCompressed Video quality database with Semi-Automatic Ratings (CVSAR), so far\nthe largest of compressed video quality database. We train a no-reference\ncompressed video quality assessment model with a 3D CNN for SpatioTemporal\nFeature Extraction and Evaluation (STFEE). Experimental results demonstrate\nthat the proposed method outperforms state-of-the-art metrics and achieves\npromising generalization performance in cross-database tests. The CVSAR\ndatabase and STFEE model will be made publicly available to facilitate\nreproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Liqun Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jiachen He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weiling Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison Knowledge Translation for Generalizable Image Classification. (arXiv:2205.03633v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03633","description":"<p>Deep learning has recently achieved remarkable performance in image\nclassification tasks, which depends heavily on massive annotation. However, the\nclassification mechanism of existing deep learning models seems to contrast to\nhumans' recognition mechanism. With only a glance at an image of the object\neven unknown type, humans can quickly and precisely find other same category\nobjects from massive images, which benefits from daily recognition of various\nobjects. In this paper, we attempt to build a generalizable framework that\nemulates the humans' recognition mechanism in the image classification task,\nhoping to improve the classification performance on unseen categories with the\nsupport of annotations of other categories. Specifically, we investigate a new\ntask termed Comparison Knowledge Translation (CKT). Given a set of fully\nlabeled categories, CKT aims to translate the comparison knowledge learned from\nthe labeled categories to a set of novel categories. To this end, we put\nforward a Comparison Classification Translation Network (CCT-Net), which\ncomprises a comparison classifier and a matching discriminator. The comparison\nclassifier is devised to classify whether two images belong to the same\ncategory or not, while the matching discriminator works together in an\nadversarial manner to ensure whether classified results match the truth.\nExhaustive experiments show that CCT-Net achieves surprising generalization\nability on unseen categories and SOTA performance on target categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1\">Tian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaotuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zengliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiqiong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra-fast image categorization in vivo and in silico. (arXiv:2205.03635v1 [q-bio.NC])","link":"http://arxiv.org/abs/2205.03635","description":"<p>Humans are able to robustly categorize images and can, for instance, detect\nthe presence of an animal in a briefly flashed image in as little as 120 ms.\nInitially inspired by neuroscience, deep-learning algorithms literally bloomed\nup in the last decade such that the accuracy of machines is at present superior\nto humans for visual recognition tasks. However, these artificial networks are\nusually trained and evaluated on very specific tasks, for instance on the 1000\nseparate categories of ImageNet. In that regard, biological visual systems are\nmore flexible and efficient compared to artificial systems on generic\necological tasks. In order to deepen this comparison, we re-trained the\nstandard VGG Convolutional Neural Network (CNN) on two independent tasks which\nare ecologically relevant for humans: one task defined as detecting the\npresence of an animal and the other as detecting the presence of an artifact.\nWe show that retraining the network achieves human-like performance level which\nis reported in psychophysical tasks. We also compare the accuracy of the\ndetection on an image-by-image basis. This showed in particular that the two\nmodels perform better when combining their outputs. Indeed, animals (e.g.\nlions) tend to be less present in photographs containing artifacts (e.g.\nbuildings). These re-trained models could reproduce some unexpected behavioral\nobservations from humans psychophysics such as the robustness to rotations\n(e.g. upside-down or slanted image) or to a grayscale transformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Jeremie_J/0/1/0/all/0/1\">Jean-Nicolas J&#xe9;r&#xe9;mie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Perrinet_L/0/1/0/all/0/1\">Laurent U Perrinet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation. (arXiv:2205.03644v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03644","description":"<p>Segmentation of 3D knee MR images is important for the assessment of\nosteoarthritis. Like other medical data, the volume-wise labeling of knee MR\nimages is expertise-demanded and time-consuming; hence semi-supervised learning\n(SSL), particularly barely-supervised learning, is highly desirable for\ntraining with insufficient labeled data. We observed that the class imbalance\nproblem is severe in the knee MR images as the cartilages only occupy 6% of\nforeground volumes, and the situation becomes worse without sufficient labeled\ndata. To address the above problem, we present a novel framework for\nbarely-supervised knee segmentation with noisy and imbalanced labels. Our\nframework leverages label distribution to encourage the network to put more\neffort into learning cartilage parts. Specifically, we utilize 1.) label\nquantity distribution for modifying the objective loss function to a\nclass-aware weighted form and 2.) label position distribution for constructing\na cropping probability mask to crop more sub-volumes in cartilage areas from\nboth labeled and unlabeled inputs. In addition, we design dual\nuncertainty-aware sampling supervision to enhance the supervision of\nlow-confident categories for efficient unsupervised learning. Experiments show\nthat our proposed framework brings significant improvements by incorporating\nthe unlabeled data and alleviating the problem of class imbalance. More\nimportantly, our method outperforms the state-of-the-art SSL methods,\ndemonstrating the potential of our framework for the more challenging SSL\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huifeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zezhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoyan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Velocity Picking Using a Multi-Information Fusion Deep Semantic Segmentation Network. (arXiv:2205.03645v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03645","description":"<p>Velocity picking, a critical step in seismic data processing, has been\nstudied for decades. Although manual picking can produce accurate normal\nmoveout (NMO) velocities from the velocity spectra of prestack gathers, it is\ntime-consuming and becomes infeasible with the emergence of large amount of\nseismic data. Numerous automatic velocity picking methods have thus been\ndeveloped. In recent years, deep learning (DL) methods have produced good\nresults on the seismic data with medium and high signal-to-noise ratios (SNR).\nUnfortunately, it still lacks a picking method to automatically generate\naccurate velocities in the situations of low SNR. In this paper, we propose a\nmulti-information fusion network (MIFN) to estimate stacking velocity from the\nfusion information of velocity spectra and stack gather segments (SGS). In\nparticular, we transform the velocity picking problem into a semantic\nsegmentation problem based on the velocity spectrum images. Meanwhile, the\ninformation provided by SGS is used as a prior in the network to assist\nsegmentation. The experimental results on two field datasets show that the\npicking results of MIFN are stable and accurate for the scenarios with medium\nand high SNR, and it also performs well in low SNR scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">H.T.Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">J.S.Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Z.X.Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">C.X.Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">L.Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Z.Y.Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1\">W.F.Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Adversarial Learning for Skeleton-level to Pixel-level Adjustable Vessel Segmentation. (arXiv:2205.03646v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03646","description":"<p>You can have your cake and eat it too. Microvessel segmentation in optical\ncoherence tomography angiography (OCTA) images remains challenging.\nSkeleton-level segmentation shows clear topology but without diameter\ninformation, while pixel-level segmentation shows a clear caliber but low\ntopology. To close this gap, we propose a novel label adversarial learning\n(LAL) for skeleton-level to pixel-level adjustable vessel segmentation. LAL\nmainly consists of two designs: a label adversarial loss and an embeddable\nadjustment layer. The label adversarial loss establishes an adversarial\nrelationship between the two label supervisions, while the adjustment layer\nadjusts the network parameters to match the different adversarial weights. Such\na design can efficiently capture the variation between the two supervisions,\nmaking the segmentation continuous and tunable. This continuous process allows\nus to recommend high-quality vessel segmentation with clear caliber and\ntopology. Experimental results show that our results outperform manual\nannotations of current public datasets and conventional filtering effects.\nFurthermore, such a continuous process can also be used to generate an\nuncertainty map representing weak vessel boundaries and noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mingchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zetian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Inter-Class Distance for Semantic Segmentation. (arXiv:2205.03650v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03650","description":"<p>Knowledge distillation is widely adopted in semantic segmentation to reduce\nthe computation cost.The previous knowledge distillation methods for semantic\nsegmentation focus on pixel-wise feature alignment and intra-class feature\nvariation distillation, neglecting to transfer the knowledge of the inter-class\ndistance in the feature space, which is important for semantic segmentation. To\naddress this issue, we propose an Inter-class Distance Distillation (IDD)\nmethod to transfer the inter-class distance in the feature space from the\nteacher network to the student network. Furthermore, semantic segmentation is a\nposition-dependent task,thus we exploit a position information distillation\nmodule to help the student network encode more position information. Extensive\nexperiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show\nthat our method is helpful to improve the accuracy of semantic segmentation\nmodels and achieves the state-of-the-art performance. E.g. it boosts the\nbenchmark model(\"PSPNet+ResNet18\") by 7.50% in accuracy on the Cityscapes\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunluan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust 3D Object Recognition with Dense-to-Sparse Deep Domain Adaptation. (arXiv:2205.03654v1 [cs.RO])","link":"http://arxiv.org/abs/2205.03654","description":"<p>Three-dimensional (3D) object recognition is crucial for intelligent\nautonomous agents such as autonomous vehicles and robots alike to operate\neffectively in unstructured environments. Most state-of-art approaches rely on\nrelatively dense point clouds and performance drops significantly for sparse\npoint clouds. Unsupervised domain adaption allows to minimise the discrepancy\nbetween dense and sparse point clouds with minimal unlabelled sparse point\nclouds, thereby saving additional sparse data collection, annotation and\nretraining costs. In this work, we propose a novel method for point cloud based\nobject recognition with competitive performance with state-of-art methods on\ndense and sparse point clouds while being trained only with dense point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murali_P/0/1/0/all/0/1\">Prajval Kumar Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_R/0/1/0/all/0/1\">Ravinder Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaboli_M/0/1/0/all/0/1\">Mohsen Kaboli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arrhythmia Classifier using Binarized Convolutional Neural Network for Resource-Constrained Devices. (arXiv:2205.03661v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03661","description":"<p>Monitoring electrocardiogram signals is of great significance for the\ndiagnosis of arrhythmias. In recent years, deep learning and convolutional\nneural networks have been widely used in the classification of cardiac\narrhythmias. However, the existing neural network applied to ECG signal\ndetection usually requires a lot of computing resources, which is not friendlyF\nto resource-constrained equipment, and it is difficult to realize real-time\nmonitoring. In this paper, a binarized convolutional neural network suitable\nfor ECG monitoring is proposed, which is hardware-friendly and more suitable\nfor use in resource-constrained wearable devices. Targeting the MIT-BIH\narrhythmia database, the classifier based on this network reached an accuracy\nof 95.67% in the five-class test. Compared with the proposed baseline\nfull-precision network with an accuracy of 96.45%, it is only 0.78% lower.\nImportantly, it achieves 12.65 times the computing speedup, 24.8 times the\nstorage compression ratio, and only requires a quarter of the memory overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanshi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_N/0/1/0/all/0/1\">Ninghao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging. (arXiv:2205.03663v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03663","description":"<p>Single-pixel imaging (SPI) is a novel optical imaging technique by replacing\na two-dimensional pixelated sensor with a single-pixel detector and pattern\nilluminations. SPI have been extensively used for various tasks related to\nimage acquisition and processing. In this work, a novel non-image-based task of\nplaying Tic-Tac-Toe games interactively is merged into the framework of SPI. An\noptoelectronic artificial intelligent (AI) player with minimal digital\ncomputation can detect the game states, generate optimal moves and display\noutput results mainly by pattern illumination and single-pixel detection.\nSimulated and experimental results demonstrate the feasibility of proposed\nscheme and its unbeatable performance against human players.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_S/0/1/0/all/0/1\">Shuming Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zibang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block Modulating Video Compression: An Ultra Low Complexity Image Compression Encoder for Resource Limited Platforms. (arXiv:2205.03677v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03677","description":"<p>We consider the image and video compression on resource limited platforms. An\nultra low-cost image encoder, named Block Modulating Video Compression (BMVC)\nwith an encoding complexity ${\\cal O}(1)$ is proposed to be implemented on\nmobile platforms with low consumption of power and computation resources. We\nalso develop two types of BMVC decoders, implemented by deep neural networks.\nThe first BMVC decoder is based on the Plug-and-Play (PnP) algorithm, which is\nflexible to different compression ratios. And the second decoder is a memory\nefficient end-to-end convolutional neural network, which aims for real-time\ndecoding. Extensive results on the high definition images and videos\ndemonstrate the superior performance of the proposed codec and the robustness\nagainst bit quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yujia Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Siming Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tahir_W/0/1/0/all/0/1\">Waleed Tahir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengjue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lei Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenISP: Neural ISP for Low-Light Machine Cognition. (arXiv:2205.03688v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03688","description":"<p>Object detection in low-light conditions remains a challenging but important\nproblem with many practical implications. Some recent works show that, in\nlow-light conditions, object detectors using raw image data are more robust\nthan detectors using image data processed by a traditional ISP pipeline. To\nimprove detection performance in low-light conditions, one can fine-tune the\ndetector to use raw image data or use a dedicated low-light neural pipeline\ntrained with paired low- and normal-light data to restore and enhance the\nimage. However, different camera sensors have different spectral sensitivity\nand learning-based models using raw images process data in the sensor-specific\ncolor space. Thus, once trained, they do not guarantee generalization to other\ncamera sensors. We propose to improve generalization to unseen camera sensors\nby implementing a minimal neural ISP pipeline for machine cognition, named\nGenISP, that explicitly incorporates Color Space Transformation to a\ndevice-independent color space. We also propose a two-stage color processing\nimplemented by two image-to-parameter modules that take down-sized image as\ninput and regress global color correction parameters. Moreover, we propose to\ntrain our proposed GenISP under the guidance of a pre-trained object detector\nand avoid making assumptions about perceptual quality of the image, but rather\noptimize the image representation for machine cognition. At the inference\nstage, GenISP can be paired with any object detector. We perform extensive\nexperiments to compare our method to other low-light image restoration and\nenhancement methods in an extrinsic task-based evaluation and validate that\nGenISP can generalize to unseen sensors and object detectors. Finally, we\ncontribute a low-light dataset of 7K raw images annotated with 46K bounding\nboxes for task-based benchmarking of future low-light image restoration and\nobject detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morawski_I/0/1/0/all/0/1\">Igor Morawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dangi_S/0/1/0/all/0/1\">Shusil Dangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keratoconus Classifier for Smartphone-based Corneal Topographer. (arXiv:2205.03702v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03702","description":"<p>Keratoconus is a severe eye disease that leads to deformation of the cornea.\nIt impacts people aged 10-25 years and is the leading cause of blindness in\nthat demography. Corneal topography is the gold standard for keratoconus\ndiagnosis. It is a non-invasive process performed using expensive and bulky\nmedical devices called corneal topographers. This makes it inaccessible to\nlarge populations, especially in the Global South. Low-cost smartphone-based\ncorneal topographers, such as SmartKC, have been proposed to make keratoconus\ndiagnosis accessible. Similar to medical-grade topographers, SmartKC outputs\ncurvature heatmaps and quantitative metrics that need to be evaluated by\ndoctors for keratoconus diagnosis. An automatic scheme for evaluation of these\nheatmaps and quantitative values can play a crucial role in screening\nkeratoconus in areas where doctors are not available. In this work, we propose\na dual-head convolutional neural network (CNN) for classifying keratoconus on\nthe heatmaps generated by SmartKC. Since SmartKC is a new device and only had a\nsmall dataset (114 samples), we developed a 2-stage transfer learning strategy\n-- using historical data collected from a medical-grade topographer and a\nsubset of SmartKC data -- to satisfactorily train our network. This, combined\nwith our domain-specific data augmentations, achieved a sensitivity of 91.3%\nand a specificity of 94.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gairola_S/0/1/0/all/0/1\">Siddhartha Gairola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_P/0/1/0/all/0/1\">Pallavi Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Anand Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murali_K/0/1/0/all/0/1\">Kaushik Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwatra_N/0/1/0/all/0/1\">Nipun Kwatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Viewpoints and Path-planning for UAV-based 3D Reconstruction. (arXiv:2205.03716v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03716","description":"<p>Unmanned aerial vehicles (UAVs) are widely used platforms to carry data\ncapturing sensors for various applications. The reason for this success can be\nfound in many aspects: the high maneuverability of the UAVs, the capability of\nperforming autonomous data acquisition, flying at different heights, and the\npossibility to reach almost any vantage point. The selection of appropriate\nviewpoints and planning the optimum trajectories of UAVs is an emerging topic\nthat aims at increasing the automation, efficiency and reliability of the data\ncapturing process to achieve a dataset with desired quality. On the other hand,\n3D reconstruction using the data captured by UAVs is also attracting attention\nin research and industry. This review paper investigates a wide range of\nmodel-free and model-based algorithms for viewpoint and path planning for 3D\nreconstruction of large-scale objects. The analyzed approaches are limited to\nthose that employ a single-UAV as a data capturing platform for outdoor 3D\nreconstruction purposes. In addition to discussing the evaluation strategies,\nthis paper also highlights the innovations and limitations of the investigated\napproaches. It concludes with a critical analysis of the existing challenges\nand future research perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maboudi_M/0/1/0/all/0/1\">Mehdi Maboudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homaei_M/0/1/0/all/0/1\">MohammadReza Homaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Soohwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malihi_S/0/1/0/all/0/1\">Shirin Malihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatseresht_M/0/1/0/all/0/1\">Mohammad Saadatseresht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerke_M/0/1/0/all/0/1\">Markus Gerke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Independent Articulated Object Tracking with Factor Graphs. (arXiv:2205.03721v1 [cs.RO])","link":"http://arxiv.org/abs/2205.03721","description":"<p>Robots deployed in human-centric environments may need to manipulate a\ndiverse range of articulated objects, such as doors, dishwashers, and cabinets.\nArticulated objects often come with unexpected articulation mechanisms that are\ninconsistent with categorical priors: for example, a drawer might rotate about\na hinge joint instead of sliding open. We propose a category-independent\nframework for predicting the articulation models of unknown objects from\nsequences of RGB-D images. The prediction is performed by a two-step process:\nfirst, a visual perception module tracks object part poses from raw images, and\nsecond, a factor graph takes these poses and infers the articulation model\nincluding the current configuration between the parts as a 6D twist. We also\npropose a manipulation-oriented metric to evaluate predicted joint twists in\nterms of how well a compliant robot controller would be able to manipulate the\narticulated object given the predicted twist. We demonstrate that our visual\nperception and factor graph modules outperform baselines on simulated data and\nshow the applicability of our factor graph on real world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heppert_N/0/1/0/all/0/1\">Nick Heppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migimatsu_T/0/1/0/all/0/1\">Toki Migimatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Brent Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Claire Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Point Cloud Generation for Class Segmentation Applications. (arXiv:2205.03738v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03738","description":"<p>Maintenance of industrial facilities is a growing hazard due to the\ncumbersome process needed to identify infrastructure degradation. Digital Twins\nhave the potential to improve maintenance by monitoring the continuous digital\nrepresentation of infrastructure. However, the time needed to map the existing\ngeometry makes their use prohibitive. We previously developed class\nsegmentation algorithms to automate digital twinning, however a vast amount of\nannotated point clouds is needed. Currently, synthetic data generation for\nautomated segmentation is non-existent. We used Helios++ to automatically\nsegment point clouds from 3D models. Our research has the potential to pave the\nground for efficient industrial class segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanelli_M/0/1/0/all/0/1\">Maria Gonzalez Stefanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Avi Rajesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalui_S/0/1/0/all/0/1\">Sandeep Kamal Jalui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapaki_D/0/1/0/all/0/1\">Dr. Eva Agapaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled-and-Coupled Networks: Self-Supervised Hyperspectral Image Super-Resolution with Subpixel Fusion. (arXiv:2205.03742v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03742","description":"<p>Enormous efforts have been recently made to super-resolve hyperspectral (HS)\nimages with the aid of high spatial resolution multispectral (MS) images. Most\nprior works usually perform the fusion task by means of multifarious\npixel-level priors. Yet the intrinsic effects of a large distribution gap\nbetween HS-MS data due to differences in the spatial and spectral resolution\nare less investigated. The gap might be caused by unknown sensor-specific\nproperties or highly-mixed spectral information within one pixel (due to low\nspatial resolution). To this end, we propose a subpixel-level HS\nsuper-resolution framework by devising a novel decoupled-and-coupled network,\ncalled DC-Net, to progressively fuse HS-MS information from the pixel- to\nsubpixel-level, from the image- to feature-level. As the name suggests, DC-Net\nfirst decouples the input into common (or cross-sensor) and sensor-specific\ncomponents to eliminate the gap between HS-MS images before further fusion, and\nthen fully blends them by a model-guided coupled spectral unmixing (CSU) net.\nMore significantly, we append a self-supervised learning module behind the CSU\nnet by guaranteeing the material consistency to enhance the detailed\nappearances of the restored HS product. Extensive experimental results show the\nsuperiority of our method both visually and quantitatively and achieve a\nsignificant improvement in comparison with the state-of-the-arts. Furthermore,\nthe codes and datasets will be available at\nhttps://sites.google.com/view/danfeng-hong for the sake of reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Rubbing Restoration Using Generative Adversarial Networks. (arXiv:2205.03743v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03743","description":"<p>Rubbing restorations are significant for preserving world cultural history.\nIn this paper, we propose the RubbingGAN model for restoring incomplete rubbing\ncharacters. Specifically, we collect characters from the Zhang Menglong Bei and\nbuild up the first rubbing restoration dataset. We design the first generative\nadversarial network for rubbing restoration. Based on the dataset we collect,\nwe apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore\nthe characters. The results of experiments show that RubbingGAN can repair both\nslightly and severely incomplete rubbing characters fast and effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gongbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zijie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Select and Calibrate the Low-confidence: Dual-Channel Consistency based Graph Convolutional Networks. (arXiv:2205.03753v1 [cs.LG])","link":"http://arxiv.org/abs/2205.03753","description":"<p>The Graph Convolutional Networks (GCNs) have achieved excellent results in\nnode classification tasks, but the model's performance at low label rates is\nstill unsatisfactory. Previous studies in Semi-Supervised Learning (SSL) for\ngraph have focused on using network predictions to generate soft pseudo-labels\nor instructing message propagation, which inevitably contains the incorrect\nprediction due to the over-confident in the predictions. Our proposed\nDual-Channel Consistency based Graph Convolutional Networks (DCC-GCN) uses\ndual-channel to extract embeddings from node features and topological\nstructures, and then achieves reliable low-confidence and high-confidence\nsamples selection based on dual-channel consistency. We further confirmed that\nthe low-confidence samples obtained based on dual-channel consistency were low\nin accuracy, constraining the model's performance. Unlike previous studies\nignoring low-confidence samples, we calibrate the feature embeddings of the\nlow-confidence samples by using the neighborhood's high-confidence samples. Our\nexperiments have shown that the DCC-GCN can more accurately distinguish between\nlow-confidence and high-confidence samples, and can also significantly improve\nthe accuracy of low-confidence samples. We conducted extensive experiments on\nthe benchmark datasets and demonstrated that DCC-GCN is significantly better\nthan state-of-the-art baselines at different label rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Dynamic Embedding for Video Object Segmentation. (arXiv:2205.03761v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03761","description":"<p>Space-time memory (STM) based video object segmentation (VOS) networks\nusually keep increasing memory bank every several frames, which shows excellent\nperformance. However, 1) the hardware cannot withstand the ever-increasing\nmemory requirements as the video length increases. 2) Storing lots of\ninformation inevitably introduces lots of noise, which is not conducive to\nreading the most important information from the memory bank. In this paper, we\npropose a Recurrent Dynamic Embedding (RDE) to build a memory bank of constant\nsize. Specifically, we explicitly generate and update RDE by the proposed\nSpatio-temporal Aggregation Module (SAM), which exploits the cue of historical\ninformation. To avoid error accumulation owing to the recurrent usage of SAM,\nwe propose an unbiased guidance loss during the training stage, which makes SAM\nmore robust in long videos. Moreover, the predicted masks in the memory bank\nare inaccurate due to the inaccurate network inference, which affects the\nsegmentation of the query frame. To address this problem, we design a novel\nself-correction strategy so that the network can repair the embeddings of masks\nwith different qualities in the memory bank. Extensive experiments show our\nmethod achieves the best tradeoff between performance and speed. Code is\navailable at https://github.com/Limingxing00/RDE-VOS-CVPR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Li Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1\">Pan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoViST:Learning Robust Metrics for Visual Storytelling. (arXiv:2205.03774v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03774","description":"<p>Visual storytelling (VST) is the task of generating a story paragraph that\ndescribes a given image sequence. Most existing storytelling approaches have\nevaluated their models using traditional natural language generation metrics\nlike BLEU or CIDEr. However, such metrics based on n-gram matching tend to have\npoor correlation with human evaluation scores and do not explicitly consider\nother criteria necessary for storytelling such as sentence structure or topic\ncoherence. Moreover, a single score is not enough to assess a story as it does\nnot inform us about what specific errors were made by the model. In this paper,\nwe propose 3 evaluation metrics sets that analyses which aspects we would look\nfor in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy.\nWe measure the reliability of our metric sets by analysing its correlation with\nhuman judgement scores on a sample of machine stories obtained from 4\nstate-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our\nmetric sets outperforms other metrics on human correlation, and could be served\nas a learning based evaluation metric set that is complementary to existing\nrule-based metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Eileen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseTT: Visual Tracking with Sparse Transformers. (arXiv:2205.03776v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03776","description":"<p>Transformers have been successfully applied to the visual tracking task and\nsignificantly promote tracking performance. The self-attention mechanism\ndesigned to model long-range dependencies is the key to the success of\nTransformers. However, self-attention lacks focusing on the most relevant\ninformation in the search regions, making it easy to be distracted by\nbackground. In this paper, we relieve this issue with a sparse attention\nmechanism by focusing the most relevant information in the search regions,\nwhich enables a much accurate tracking. Furthermore, we introduce a double-head\npredictor to boost the accuracy of foreground-background classification and\nregression of target bounding boxes, which further improve the tracking\nperformance. Extensive experiments show that, without bells and whistles, our\nmethod significantly outperforms the state-of-the-art approaches on LaSOT,\nGOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the\ntraining time of our method is reduced by 75% compared to that of TransT. The\nsource code and models are available at https://github.com/fzh0917/SparseTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhihong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wenrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Cycled Generative Adversarial Networks for Real-World Face Super-Resolution. (arXiv:2205.03777v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03777","description":"<p>Real-world face super-resolution (SR) is a highly ill-posed image restoration\ntask. The fully-cycled Cycle-GAN architecture is widely employed to achieve\npromising performance on face SR, but prone to produce artifacts upon\nchallenging cases in real-world scenarios, since joint participation in the\nsame degradation branch will impact final performance due to huge domain gap\nbetween real-world and synthetic LR ones obtained by generators. To better\nexploit the powerful generative capability of GAN for real-world face SR, in\nthis paper, we establish two independent degradation branches in the forward\nand backward cycle-consistent reconstruction processes, respectively, while the\ntwo processes share the same restoration branch. Our Semi-Cycled Generative\nAdversarial Networks (SCGAN) is able to alleviate the adverse effects of the\ndomain gap between the real-world LR face images and the synthetic LR ones, and\nto achieve accurate and robust face SR performance by the shared restoration\nbranch regularized by both the forward and backward cycle-consistent learning\nprocesses. Experiments on two synthetic and two real-world datasets demonstrate\nthat, our SCGAN outperforms the state-of-the-art methods on recovering the face\nstructures/details and quantitative metrics for real-world face SR. The code\nwill be publicly released at https://github.com/HaoHou-98/SCGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1\">Hao Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaotao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yingkun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Benzheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo. (arXiv:2205.03783v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03783","description":"<p>Recent cost volume pyramid based deep neural networks have unlocked the\npotential of efficiently leveraging high-resolution images for depth inference\nfrom multi-view stereo. In general, those approaches assume that the depth of\neach pixel follows a unimodal distribution. Boundary pixels usually follow a\nmulti-modal distribution as they represent different depths; Therefore, the\nassumption results in an erroneous depth prediction at the coarser level of the\ncost volume pyramid and can not be corrected in the refinement levels leading\nto wrong depth predictions. In contrast, we propose constructing the cost\nvolume by non-parametric depth distribution modeling to handle pixels with\nunimodal and multi-modal distributions. Our approach outputs multiple depth\nhypotheses at the coarser level to avoid errors in the early stage. As we\nperform local search around these multiple hypotheses in subsequent levels, our\napproach does not maintain the rigid depth spatial ordering and, therefore, we\nintroduce a sparse cost aggregation network to derive information within each\nvolume. We evaluate our approach extensively on two benchmark datasets: DTU and\nTanks &amp; Temples. Our experimental results show that our model outperforms\nexisting methods by a large margin and achieves superior performance on\nboundary regions. Code is available at https://github.com/NVlabs/NP-CVP-MVSNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiayu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Class Knowledge Distillation for Face Presentation Attack Detection. (arXiv:2205.03792v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03792","description":"<p>Face presentation attack detection (PAD) has been extensively studied by\nresearch communities to enhance the security of face recognition systems.\nAlthough existing methods have achieved good performance on testing data with\nsimilar distribution as the training data, their performance degrades severely\nin application scenarios with data of unseen distributions. In situations where\nthe training and testing data are drawn from different domains, a typical\napproach is to apply domain adaptation techniques to improve face PAD\nperformance with the help of target domain data. However, it has always been a\nnon-trivial challenge to collect sufficient data samples in the target domain,\nespecially for attack samples. This paper introduces a teacher-student\nframework to improve the cross-domain performance of face PAD with one-class\ndomain adaptation. In addition to the source domain data, the framework\nutilizes only a few genuine face samples of the target domain. Under this\nframework, a teacher network is trained with source domain samples to provide\ndiscriminative feature representations for face PAD. Student networks are\ntrained to mimic the teacher network and learn similar representations for\ngenuine face samples of the target domain. In the test phase, the similarity\nscore between the representations of the teacher and student networks is used\nto distinguish attacks from genuine ones. To evaluate the proposed framework\nunder one-class domain adaptation settings, we devised two new protocols and\nconducted extensive experiments. The experimental results show that our method\noutperforms baselines under one-class domain adaptation settings and even\nstate-of-the-art methods with unsupervised domain adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Rizhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kwok-Yan Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongjian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Structured Block-Term Tensor Decomposition For Hyperspectral Unmixing. (arXiv:2205.03798v1 [eess.SP])","link":"http://arxiv.org/abs/2205.03798","description":"<p>The block-term tensor decomposition model with multilinear rank-$(L_r,L_r,1)$\nterms (or, the \"LL1 tensor decomposition\" in short) offers a valuable\nalternative for hyperspectral unmixing (HU) under the linear mixture model.\nParticularly, the LL1 decomposition ensures the endmember/abundance\nidentifiability in scenarios where such guarantees are not supported by the\nclassic matrix factorization (MF) approaches. However, existing LL1-based HU\nalgorithms use a three-factor parameterization of the tensor (i.e., the\nhyperspectral image cube), which leads to a number of challenges including high\nper-iteration complexity, slow convergence, and difficulties in incorporating\nstructural prior information. This work puts forth an LL1 tensor\ndecomposition-based HU algorithm that uses a constrained two-factor\nre-parameterization of the tensor data. As a consequence, a two-block\nalternating gradient projection (GP)-based LL1 algorithm is proposed for HU.\nWith carefully designed projection solvers, the GP algorithm enjoys a\nrelatively low per-iteration complexity. Like in MF-based HU, the factors under\nour parameterization correspond to the endmembers and abundances. Thus, the\nproposed framework is natural to incorporate physics-motivated priors that\narise in HU. The proposed algorithm often attains orders-of-magnitude speedup\nand substantial HU performance gains compared to the existing three-factor\nparameterization-based HU algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xi-Le Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Past and Future Motion Guided Network for Audio Visual Event Localization. (arXiv:2205.03802v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03802","description":"<p>In recent years, audio-visual event localization has attracted much\nattention. It's purpose is to detect the segment containing audio-visual events\nand recognize the event category from untrimmed videos. Existing methods use\naudio-guided visual attention to lead the model pay attention to the spatial\narea of the ongoing event, devoting to the correlation between audio and visual\ninformation but ignoring the correlation between audio and spatial motion. We\npropose a past and future motion extraction (pf-ME) module to mine the visual\nmotion from videos ,embedded into the past and future motion guided network\n(PFAGN), and motion guided audio attention (MGAA) module to achieve focusing on\nthe information related to interesting events in audio modality through the\npast and future visual motion. We choose AVE as the experimental verification\ndataset and the experiments show that our method outperforms the\nstate-of-the-arts in both supervised and weakly-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tingxiu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Few-shot Image Generation. (arXiv:2205.03805v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03805","description":"<p>Modern GANs excel at generating high quality and diverse images. However,\nwhen transferring the pretrained GANs on small target data (e.g., 10-shot), the\ngenerator tends to replicate the training samples.\n</p>\n<p>Several methods have been proposed to address this few-shot image generation\ntask, but there is a lack of effort to analyze them under a unified framework.\n</p>\n<p>As our first contribution, we propose a framework to analyze existing methods\nduring the adaptation. Our analysis discovers that while some methods have\ndisproportionate focus on diversity preserving which impede quality\nimprovement, all methods achieve similar quality after convergence.\n</p>\n<p>Therefore, the better methods are those that can slow down diversity\ndegradation. Furthermore, our analysis reveals that there is still plenty of\nroom to further slow down diversity degradation.\n</p>\n<p>Informed by our analysis and to slow down the diversity degradation of the\ntarget generator during adaptation, our second contribution proposes to apply\nmutual information (MI) maximization to retain the source domain's rich\nmulti-level diversity information in the target domain generator.\n</p>\n<p>We propose to perform MI maximization by contrastive loss (CL), leverage the\ngenerator and discriminator as two feature encoders to extract different\nmulti-level features for computing CL. We refer to our method as Dual\nContrastive Learning (DCL).\n</p>\n<p>Extensive experiments on several public datasets show that, while leading to\na slower diversity-degrading generator during adaptation, our proposed DCL\nbrings visually pleasant quality and state-of-the-art quantitative performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Houjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Tracking with Cyclic Shifting Window Attention. (arXiv:2205.03806v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03806","description":"<p>Transformer architecture has been showing its great strength in visual object\ntracking, for its effective attention mechanism. Existing transformer-based\napproaches adopt the pixel-to-pixel attention strategy on flattened image\nfeatures and unavoidably ignore the integrity of objects. In this paper, we\npropose a new transformer architecture with multi-scale cyclic shifting window\nattention for visual object tracking, elevating the attention from pixel to\nwindow level. The cross-window multi-scale attention has the advantage of\naggregating attention at different scales and generates the best fine-scale\nmatch for the target object. Furthermore, the cyclic shifting strategy brings\ngreater accuracy by expanding the window samples with positional information,\nand at the same time saves huge amounts of computational power by removing\nredundant calculations. Extensive experiments demonstrate the superior\nperformance of our method, which also sets the new state-of-the-art records on\nfive challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet,\nand GOT-10k benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zikai Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ping Phoebe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fingerprint Template Invertibility: Minutiae vs. Deep Templates. (arXiv:2205.03809v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03809","description":"<p>Much of the success of fingerprint recognition is attributed to\nminutiae-based fingerprint representation. It was believed that minutiae\ntemplates could not be inverted to obtain a high fidelity fingerprint image,\nbut this assumption has been shown to be false. The success of deep learning\nhas resulted in alternative fingerprint representations (embeddings), in the\nhope that they might offer better recognition accuracy as well as\nnon-invertibility of deep network-based templates. We evaluate whether deep\nfingerprint templates suffer from the same reconstruction attacks as the\nminutiae templates. We show that while a deep template can be inverted to\nproduce a fingerprint image that could be matched to its source image, deep\ntemplates are more resistant to reconstruction attacks than minutiae templates.\nIn particular, reconstructed fingerprint images from minutiae templates yield a\nTAR of about 100.0% (98.3%) @ FAR of 0.01% for type-I (type-II) attacks using a\nstate-of-the-art commercial fingerprint matcher, when tested on NIST SD4. The\ncorresponding attack performance for reconstructed fingerprint images from deep\ntemplates using the same commercial matcher yields a TAR of less than 1% for\nboth type-I and type-II attacks; however, when the reconstructed images are\nmatched using the same deep network, they achieve a TAR of 85.95% (68.10%) for\ntype-I (type-II) attacks. Furthermore, what is missing from previous\nfingerprint template inversion studies is an evaluation of the black-box attack\nperformance, which we perform using 3 different state-of-the-art fingerprint\nmatchers. We conclude that fingerprint images generated by inverting minutiae\ntemplates are highly susceptible to both white-box and black-box attack\nevaluations, while fingerprint images generated by deep templates are resistant\nto black-box evaluations and comparatively less susceptible to white-box\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijewardena_K/0/1/0/all/0/1\">Kanishka P. Wijewardena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kai Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGADA: Perturbation-Guided Adversarial Alignment for Few-shot Learning Under the Support-Query Shift. (arXiv:2205.03817v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03817","description":"<p>Few-shot learning methods aim to embed the data to a low-dimensional\nembedding space and then classify the unseen query data to the seen support\nset. While these works assume that the support set and the query set lie in the\nsame embedding space, a distribution shift usually occurs between the support\nset and the query set, i.e., the Support-Query Shift, in the real world. Though\noptimal transportation has shown convincing results in aligning different\ndistributions, we find that the small perturbations in the images would\nsignificantly misguide the optimal transportation and thus degrade the model\nperformance. To relieve the misalignment, we first propose a novel adversarial\ndata augmentation method, namely Perturbation-Guided Adversarial Alignment\n(PGADA), which generates the hard examples in a self-supervised manner. In\naddition, we introduce Regularized Optimal Transportation to derive a smooth\noptimal transportation plan. Extensive experiments on three benchmark datasets\nmanifest that our framework significantly outperforms the eleven\nstate-of-the-art methods on three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsi-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming-Syan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Homography Estimation with Coplanarity-Aware GAN. (arXiv:2205.03821v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03821","description":"<p>Estimating homography from an image pair is a fundamental problem in image\nalignment. Unsupervised learning methods have received increasing attention in\nthis field due to their promising performance and label-free training. However,\nexisting methods do not explicitly consider the problem of plane-induced\nparallax, which will make the predicted homography compromised on multiple\nplanes. In this work, we propose a novel method HomoGAN to guide unsupervised\nhomography estimation to focus on the dominant plane. First, a multi-scale\ntransformer network is designed to predict homography from the feature pyramids\nof input images in a coarse-to-fine fashion. Moreover, we propose an\nunsupervised GAN to impose coplanarity constraint on the predicted homography,\nwhich is realized by using a generator to predict a mask of aligned regions,\nand then a discriminator to check if two masked feature maps are induced by a\nsingle homography. To validate the effectiveness of HomoGAN and its components,\nwe conduct extensive experiments on a large-scale dataset, and the results show\nthat our matching error is 22% lower than the previous SOTA method. Code is\navailable at https://github.com/megvii-research/HomoGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingbo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Geometry-Aware Cross Guidance Network for Stereo Image Inpainting. (arXiv:2205.03825v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03825","description":"<p>Currently, single image inpainting has achieved promising results based on\ndeep convolutional neural networks. However, inpainting on stereo images with\nmissing regions has not been explored thoroughly, which is also a significant\nbut different problem. One crucial requirement for stereo image inpainting is\nstereo consistency. To achieve it, we propose an Iterative Geometry-Aware Cross\nGuidance Network (IGGNet). The IGGNet contains two key ingredients, i.e., a\nGeometry-Aware Attention (GAA) module and an Iterative Cross Guidance (ICG)\nstrategy. The GAA module relies on the epipolar geometry cues and learns the\ngeometry-aware guidance from one view to another, which is beneficial to make\nthe corresponding regions in two views consistent. However, learning guidance\nfrom co-existing missing regions is challenging. To address this issue, the ICG\nstrategy is proposed, which can alternately narrow down the missing regions of\nthe two views in an iterative manner. Experimental results demonstrate that our\nproposed network outperforms the latest stereo image inpainting model and\nstate-of-the-art single image inpainting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Automated Binary Pattern Extraction For Finger Vein Identification using Double Optimization Stages-Based Unsupervised Learning Approach. (arXiv:2205.03840v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03840","description":"<p>Today, finger vein identification is gaining popularity as a potential\nbiometric identification framework solution. Machine learning-based\nunsupervised, supervised, and deep learning algorithms have had a significant\ninfluence on finger vein detection and recognition at the moment. Deep\nlearning, on the other hand, necessitates a large number of training datasets\nthat must be manually produced and labeled. In this research, we offer a\ncompletely automated unsupervised learning strategy for training dataset\ncreation. Our method is intended to extract and build a decent binary mask\ntraining dataset completely automated. In this technique, two optimization\nsteps are devised and employed. The initial stage of optimization is to create\na completely automated unsupervised image clustering based on finger vein image\nlocalization. Worldwide finger vein pattern orientation estimation is employed\nin the second optimization to optimize the retrieved finger vein lines.\nFinally, the proposed system achieves 99.6 - percent pattern extraction\naccuracy, which is significantly higher than other common unsupervised learning\nmethods like k-means and Fuzzy C-Means (FCM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hameed_A/0/1/0/all/0/1\">Ali Salah Hameed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Azzawi_A/0/1/0/all/0/1\">Adil Al-Azzawi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models. (arXiv:2205.03859v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03859","description":"<p>Conditional image generation has paved the way for several breakthroughs in\nimage editing, generating stock photos and 3-D object generation. This\ncontinues to be a significant area of interest with the rise of new\nstate-of-the-art methods that are based on diffusion models. However, diffusion\nmodels provide very little control over the generated image, which led to\nsubsequent works exploring techniques like classifier guidance, that provides a\nway to trade off diversity with fidelity. In this work, we explore techniques\nto condition diffusion models with carefully crafted input noise artifacts.\nThis allows generation of images conditioned on semantic attributes. This is\ndifferent from existing approaches that input Gaussian noise and further\nintroduce conditioning at the diffusion model's inference step. Our experiments\nover several examples and conditional settings show the potential of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vedant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandial_S/0/1/0/all/0/1\">Surgan Jandial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_S/0/1/0/all/0/1\">Siddharth Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N. Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03860","description":"<p>Vision-language pre-training (VLP) relying on large-scale pre-training\ndatasets has shown premier performance on various downstream tasks. In this\nsense, a complete and fair benchmark (i.e., including large-scale pre-training\ndatasets and a variety of downstream datasets) is essential for VLP. But how to\nconstruct such a benchmark in Chinese remains a critical problem. To this end,\nwe develop a large-scale Chinese cross-modal benchmark called Zero for AI\nresearchers to fairly compare VLP models. We release two pre-training datasets\nand five fine-tuning datasets for downstream tasks. Furthermore, we propose a\nnovel pre-training framework of pre-Ranking + Ranking for cross-modal learning.\nSpecifically, we apply global contrastive pre-ranking to learn the individual\nrepresentations of images and Chinese texts, respectively. We then fuse the\nrepresentations in a fine-grained ranking manner via an image-text cross\nencoder and a text-image cross encoder. To further enhance the capability of\nthe model, we propose a two-way distillation strategy consisting of\ntarget-guided Distillation and feature-guided Distillation. For simplicity, we\ncall our model R2D2. We achieve state-of-the-art performance on four public\ncross-modal datasets and our five downstream datasets. The datasets, models and\ncodes will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chunyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Heng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fanjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1\">Henrique Morimitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1\">Dawei Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Learning of Hard Positives for Place Recognition. (arXiv:2205.03871v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03871","description":"<p>Image retrieval methods for place recognition learn global image descriptors\nthat are used for fetching geo-tagged images at inference time. Recent works\nhave suggested employing weak and self-supervision for mining hard positives\nand hard negatives in order to improve localization accuracy and robustness to\nvisibility changes (e.g. in illumination or view point). However, generating\nhard positives, which is essential for obtaining robustness, is still limited\nto hard-coded or global augmentations. In this work we propose an adversarial\nmethod to guide the creation of hard positives for training image retrieval\nnetworks. Our method learns local and global augmentation policies which will\nincrease the training loss, while the image retrieval network is forced to\nlearn more powerful features for discriminating increasingly difficult\nexamples. This approach allows the image retrieval network to generalize beyond\nthe hard examples presented in the data and learn features that are robust to a\nwide range of variations. Our method achieves state-of-the-art recalls on the\nPitts250 and Tokyo 24/7 benchmarks and outperforms recent image retrieval\nmethods on the rOxford and rParis datasets by a noticeable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wenxuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1\">Yoli Shavit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wensen Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Semi-Supervised Learning for Text Recognition. (arXiv:2205.03873v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03873","description":"<p>Until recently, the number of public real-world text images was insufficient\nfor training scene text recognizers. Therefore, most modern training methods\nrely on synthetic data and operate in a fully supervised manner. Nevertheless,\nthe amount of public real-world text images has increased significantly lately,\nincluding a great deal of unlabeled data. Leveraging these resources requires\nsemi-supervised approaches; however, the few existing methods do not account\nfor vision-language multimodality structure and therefore suboptimal for\nstate-of-the-art multimodal architectures. To bridge this gap, we present\nsemi-supervised learning for multimodal text recognizers (SemiMTR) that\nleverages unlabeled data at each modality training phase. Notably, our method\nrefrains from extra training stages and maintains the current three-stage\nmultimodal training procedure. Our algorithm starts by pretraining the vision\nmodel through a single-stage training that unifies self-supervised learning\nwith supervised training. More specifically, we extend an existing visual\nrepresentation learning algorithm and propose the first contrastive-based\nmethod for scene text recognition. After pretraining the language model on a\ntext corpus, we fine-tune the entire network via a sequential, character-level,\nconsistency regularization between weakly and strongly augmented views of text\nimages. In a novel setup, consistency is enforced on each modality separately.\nExtensive experiments validate that our method outperforms the current training\nschemes and achieves state-of-the-art results on multiple scene text\nrecognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aberdam_A/0/1/0/all/0/1\">Aviad Aberdam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazor_S/0/1/0/all/0/1\">Shai Mazor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_R/0/1/0/all/0/1\">Ron Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction. (arXiv:2205.03883v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03883","description":"<p>Parallel Imaging (PI) is one of the most im-portant and successful\ndevelopments in accelerating magnetic resonance imaging (MRI). Recently deep\nlearning PI has emerged as an effective technique to accelerate MRI.\nNevertheless, most approaches have so far been based image domain. In this\nwork, we propose to explore the k-space domain via robust generative modeling\nfor flexible PI reconstruction, coined weight-k-space generative model (WKGM).\nSpecifically, WKGM is a generalized k-space domain model, where the k-space\nweighting technology and high-dimensional space strategy are efficiently\nincorporated for score-based generative model training, resulting in good and\nrobust reconstruction. In addition, WKGM is flexible and thus can\nsynergistically combine various traditional k-space PI models, generating\nlearning-based priors to produce high-fidelity reconstructions. Experimental\nresults on datasets with varying sampling patterns and acceleration factors\ndemonstrate that WKGM can attain state-of-the-art reconstruction results under\nthe well-learned k-space generative prior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Die Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Adaptation for Recipe Retrieval with Mixup. (arXiv:2205.03891v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03891","description":"<p>Cross-modal recipe retrieval has attracted research attention in recent\nyears, thanks to the availability of large-scale paired data for training.\nNevertheless, obtaining adequate recipe-image pairs covering the majority of\ncuisines for supervised learning is difficult if not impossible. By\ntransferring knowledge learnt from a data-rich cuisine to a data-scarce\ncuisine, domain adaptation sheds light on this practical problem. Nevertheless,\nexisting works assume recipes in source and target domains are mostly\noriginated from the same cuisine and written in the same language. This paper\nstudies unsupervised domain adaptation for image-to-recipe retrieval, where\nrecipes in source and target domains are in different languages. Moreover, only\nrecipes are available for training in the target domain. A novel recipe mixup\nmethod is proposed to learn transferable embedding features between the two\ndomains. Specifically, recipe mixup produces mixed recipes to form an\nintermediate domain by discretely exchanging the section(s) between source and\ntarget recipes. To bridge the domain gap, recipe mixup loss is proposed to\nenforce the intermediate domain to locate in the shortest geodesic path between\nsource and target domains in the recipe embedding space. By using Recipe 1M\ndataset as source domain (English) and Vireo-FoodTransfer dataset as target\ndomain (Chinese), empirical experiments verify the effectiveness of recipe\nmixup for cross-lingual adaptation in the context of image-to-recipe retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">Wing-Kwong Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvMAE: Masked Convolution Meets Masked Autoencoders. (arXiv:2205.03892v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03892","description":"<p>Vision Transformers (ViT) become widely-adopted architectures for various\nvision tasks. Masked auto-encoding for feature pretraining and multi-scale\nhybrid convolution-transformer architectures can further unleash the potentials\nof ViT, leading to state-of-the-art performances on image classification,\ndetection and semantic segmentation. In this paper, our ConvMAE framework\ndemonstrates that multi-scale hybrid convolution-transformer can learn more\ndiscriminative representations via the mask auto-encoding scheme. However,\ndirectly using the original masking strategy leads to the heavy computational\ncost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the\nmasked convolution to prevent information leakage in the convolution blocks. A\nsimple block-wise masking strategy is proposed to ensure computational\nefficiency. We also propose to more directly supervise the multi-scale features\nof the encoder to boost multi-scale features. Based on our pretrained ConvMAE\nmodels, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared\nwith MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs\nsurpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP\nrespectively. Code and pretrained models are available at\nhttps://github.com/Alpha-VL/ConvMAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preservation of High Frequency Content for Deep Learning-Based Medical Image Classification. (arXiv:2205.03898v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03898","description":"<p>Chest radiographs are used for the diagnosis of multiple critical illnesses\n(e.g., Pneumonia, heart failure, lung cancer), for this reason, systems for the\nautomatic or semi-automatic analysis of these data are of particular interest.\nAn efficient analysis of large amounts of chest radiographs can aid physicians\nand radiologists, ultimately allowing for better medical care of lung-, heart-\nand chest-related conditions. We propose a novel Discrete Wavelet Transform\n(DWT)-based method for the efficient identification and encoding of visual\ninformation that is typically lost in the down-sampling of high-resolution\nradiographs, a common step in computer-aided diagnostic pipelines. Our proposed\napproach requires only slight modifications to the input of existing\nstate-of-the-art Convolutional Neural Networks (CNNs), making it easily\napplicable to existing image classification frameworks. We show that the extra\nhigh-frequency components offered by our method increased the classification\nperformance of several CNNs in benchmarks employing the NIH Chest-8 and\nImageNet-2017 datasets. Based on our results we hypothesize that providing\nfrequency-specific coefficients allows the CNNs to specialize in the\nidentification of structures that are particular to a frequency band,\nultimately increasing classification performance, without an increase in\ncomputational load. The implementation of our work is available at\ngithub.com/DeclanMcIntosh/LeGallCuda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+McIntosh_D/0/1/0/all/0/1\">Declan McIntosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marques_T/0/1/0/all/0/1\">Tunai Porto Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Albu_A/0/1/0/all/0/1\">Alexandra Branzan Albu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftPool++: An Encoder-Decoder Network for Point Cloud Completion. (arXiv:2205.03899v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03899","description":"<p>We propose a novel convolutional operator for the task of point cloud\ncompletion. One striking characteristic of our approach is that, conversely to\nrelated work it does not require any max-pooling or voxelization operation.\nInstead, the proposed operator used to learn the point cloud embedding in the\nencoder extracts permutation-invariant features from the point cloud via a\nsoft-pooling of feature activations, which are able to preserve fine-grained\ngeometric details. These features are then passed on to a decoder architecture.\nDue to the compression in the encoder, a typical limitation of this type of\narchitectures is that they tend to lose parts of the input shape structure. We\npropose to overcome this limitation by using skip connections specifically\ndevised for point clouds, where links between corresponding layers in the\nencoder and the decoder are established. As part of these connections, we\nintroduce a transformation matrix that projects the features from the encoder\nto the decoder and vice-versa. The quantitative and qualitative results on the\ntask of object completion from partial scans on the ShapeNet dataset show that\nincorporating our approach achieves state-of-the-art performance in shape\ncompletion both at low and high resolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">David Joseph Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03923","description":"<p>Neural scene representations, both continuous and discrete, have recently\nemerged as a powerful new paradigm for 3D scene understanding. Recent efforts\nhave tackled unsupervised discovery of object-centric neural scene\nrepresentations. However, the high cost of ray-marching, exacerbated by the\nfact that each object representation has to be ray-marched separately, leads to\ninsufficiently sampled radiance fields and thus, noisy renderings, poor\nframerates, and high memory and time complexity during training and rendering.\nHere, we propose to represent objects in an object-centric, compositional scene\nrepresentation as light fields. We propose a novel light field compositor\nmodule that enables reconstructing the global light field from a set of\nobject-centric light fields. Dubbed Compositional Object Light Fields (COLF),\nour method enables unsupervised learning of object-centric neural scene\nrepresentations, state-of-the-art reconstruction and novel view synthesis\nperformance on standard datasets, and rendering and training speeds at orders\nof magnitude faster than existing 3D approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1\">Cameron Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution UAV Image Generation for Sorghum Panicle Detection. (arXiv:2205.03947v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03947","description":"<p>The number of panicles (or heads) of Sorghum plants is an important\nphenotypic trait for plant development and grain yield estimation. The use of\nUnmanned Aerial Vehicles (UAVs) enables the capability of collecting and\nanalyzing Sorghum images on a large scale. Deep learning can provide methods\nfor estimating phenotypic traits from UAV images but requires a large amount of\nlabeled data. The lack of training data due to the labor-intensive ground\ntruthing of UAV images causes a major bottleneck in developing methods for\nSorghum panicle detection and counting. In this paper, we present an approach\nthat uses synthetic training images from generative adversarial networks (GANs)\nfor data augmentation to enhance the performance of Sorghum panicle detection\nand counting. Our method can generate synthetic high-resolution UAV RGB images\nwith panicle labels by using image-to-image translation GANs with a limited\nground truth dataset of real UAV RGB images. The results show the improvements\nin panicle detection and counting using our data augmentation approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_E/0/1/0/all/0/1\">Enyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhankun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation. (arXiv:2205.03962v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03962","description":"<p>Virtual facial avatars will play an increasingly important role in immersive\ncommunication, games and the metaverse, and it is therefore critical that they\nbe inclusive. This requires accurate recovery of the appearance, represented by\nalbedo, regardless of age, sex, or ethnicity. While significant progress has\nbeen made on estimating 3D facial geometry, albedo estimation has received less\nattention. The task is fundamentally ambiguous because the observed color is a\nfunction of albedo and lighting, both of which are unknown. We find that\ncurrent methods are biased towards light skin tones due to (1) strongly biased\npriors that prefer lighter pigmentation and (2) algorithmic solutions that\ndisregard the light/albedo ambiguity. To address this, we propose a new\nevaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation\nand, hence, fairness. Specifically, we create the first facial albedo\nevaluation benchmark where subjects are balanced in terms of skin color, and\nmeasure accuracy using the Individual Typology Angle (ITA) metric. We then\naddress the light/albedo ambiguity by building on a key observation: the image\nof the full scene -- as opposed to a cropped image of the face -- contains\nimportant information about lighting that can be used for disambiguation. TRUST\nregresses facial albedo by conditioning both on the face region and a global\nillumination signal obtained from the scene image. Our experimental results\nshow significant improvement compared to state-of-the-art methods on albedo\nestimation, both in terms of accuracy and fairness. The evaluation benchmark\nand code will be made available for research purposes at\nhttps://trust.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Haiwen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesch_J/0/1/0/all/0/1\">Joachim Tesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1\">Victoria Abrevaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Private Eye: On the Limits of Textual Screen Peeking via Eyeglass Reflections in Video Conferencing. (arXiv:2205.03971v1 [cs.CR])","link":"http://arxiv.org/abs/2205.03971","description":"<p>Personal video conferencing has become the new norm after COVID-19 caused a\nseismic shift from in-person meetings and phone calls to video conferencing for\ndaily communications and sensitive business. Video leaks participants'\non-screen information because eyeglasses and other reflective objects\nunwittingly expose partial screen contents. Using mathematical modeling and\nhuman subjects experiments, this research explores the extent to which emerging\nwebcams might leak recognizable textual information gleamed from eyeglass\nreflections captured by webcams. The primary goal of our work is to measure,\ncompute, and predict the factors, limits, and thresholds of recognizability as\nwebcam technology evolves in the future. Our work explores and characterizes\nthe viable threat models based on optical attacks using multi-frame super\nresolution techniques on sequences of video frames. Our experimental results\nand models show it is possible to reconstruct and recognize on-screen text with\na height as small as 10 mm with a 720p webcam. We further apply this threat\nmodel to web textual content with varying attacker capabilities to find\nthresholds at which text becomes recognizable. Our user study with 20\nparticipants suggests present-day 720p webcams are sufficient for adversaries\nto reconstruct textual content on big-font websites. Our models further show\nthat the evolution toward 4K cameras will tip the threshold of text leakage to\nreconstruction of most header texts on popular websites. Our research proposes\nnear-term mitigations, and justifies the importance of following the principle\nof least privilege for long-term defense against this attack. For\nprivacy-sensitive scenarios, it's further recommended to develop technologies\nthat blur all objects by default, then only unblur what is absolutely necessary\nto facilitate natural-looking conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Shivan Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kevin Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Nonlocal Graph-PDE and Higher-Order Geometric Integration for Image Labeling. (arXiv:2205.03991v1 [math.OC])","link":"http://arxiv.org/abs/2205.03991","description":"<p>This paper introduces a novel nonlocal partial difference equation (PDE) for\nlabeling metric data on graphs. The PDE is derived as nonlocal\nreparametrization of the assignment flow approach that was introduced in\n\\textit{J.~Math.~Imaging \\&amp; Vision} 58(2), 2017. Due to this parameterization,\nsolving the PDE numerically is shown to be equivalent to computing the\nRiemannian gradient flow with respect to a nonconvex potential. We devise an\nentropy-regularized difference-of-convex-functions (DC) decomposition of this\npotential and show that the basic geometric Euler scheme for integrating the\nassignment flow is equivalent to solving the PDE by an established DC\nprogramming scheme. Moreover, the viewpoint of geometric integration reveals a\nbasic way to exploit higher-order information of the vector field that drives\nthe assignment flow, in order to devise a novel accelerated DC programming\nscheme. A detailed convergence analysis of both numerical schemes is provided\nand illustrated by numerical experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Sitenko_D/0/1/0/all/0/1\">Dmitrij Sitenko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Boll_B/0/1/0/all/0/1\">Bastian Boll</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware-Robust In-RRAM-Computing for Object Detection. (arXiv:2205.03996v1 [cs.AR])","link":"http://arxiv.org/abs/2205.03996","description":"<p>In-memory computing is becoming a popular architecture for deep-learning\nhardware accelerators recently due to its highly parallel computing, low power,\nand low area cost. However, in-RRAM computing (IRC) suffered from large device\nvariation and numerous nonideal effects in hardware. Although previous\napproaches including these effects in model training successfully improved\nvariation tolerance, they only considered part of the nonideal effects and\nrelatively simple classification tasks. This paper proposes a joint hardware\nand software optimization strategy to design a hardware-robust IRC macro for\nobject detection. We lower the cell current by using a low word-line voltage to\nenable a complete convolution calculation in one operation that minimizes the\nimpact of nonlinear addition. We also implement ternary weight mapping and\nremove batch normalization for better tolerance against device variation, sense\namplifier variation, and IR drop problem. An extra bias is included to overcome\nthe limitation of the current sensing range. The proposed approach has been\nsuccessfully applied to a complex object detection task with only 3.85\\% mAP\ndrop, whereas a naive design suffers catastrophic failure under these nonideal\neffects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yu-Hsiang Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_C/0/1/0/all/0/1\">Cheng En Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1\">Tuo-Hung Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tian-Sheuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jou_S/0/1/0/all/0/1\">Shyh Jye Jou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Row-wise Accelerator for Vision Transformer. (arXiv:2205.03998v1 [cs.AR])","link":"http://arxiv.org/abs/2205.03998","description":"<p>Following the success of the natural language processing, the transformer for\nvision applications has attracted significant attention in recent years due to\nits excellent performance. However, existing deep learning hardware\naccelerators for vision cannot execute this structure efficiently due to\nsignificant model architecture differences. As a result, this paper proposes\nthe hardware accelerator for vision transformers with row-wise scheduling,\nwhich decomposes major operations in vision transformers as a single dot\nproduct primitive for a unified and efficient execution. Furthermore, by\nsharing weights in columns, we can reuse the data and reduce the usage of\nmemory. The implementation with TSMC 40nm CMOS technology only requires 262K\ngate count and 149KB SRAM buffer for 403.2 GOPS throughput at 600MHz clock\nfrequency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tian-Sheuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photo-to-Shape Material Transfer for Diverse Structures. (arXiv:2205.04018v1 [cs.GR])","link":"http://arxiv.org/abs/2205.04018","description":"<p>We introduce a method for assigning photorealistic relightable materials to\n3D shapes in an automatic manner. Our method takes as input a photo exemplar of\na real object and a 3D object with segmentation, and uses the exemplar to guide\nthe assignment of materials to the parts of the shape, so that the appearance\nof the resulting shape is as similar as possible to the exemplar. To accomplish\nthis goal, our method combines an image translation neural network with a\nmaterial assignment neural network. The image translation network translates\nthe color from the exemplar to a projection of the 3D shape and the part\nsegmentation from the projection to the exemplar. Then, the material prediction\nnetwork assigns materials from a collection of realistic materials to the\nprojected parts, based on the translated images and perceptual similarity of\nthe materials. One key idea of our method is to use the translation network to\nestablish a correspondence between the exemplar and shape projection, which\nallows us to transfer materials between objects with diverse structures.\nAnother key idea of our method is to use the two pairs of (color, segmentation)\nimages provided by the image translation to guide the material assignment,\nwhich enables us to ensure the consistency in the assignment. We demonstrate\nthat our method allows us to assign materials to shapes so that their\nappearances better resemble the input exemplars, improving the quality of the\nresults over the state-of-the-art method, and allowing us to automatically\ncreate thousands of shapes with high-quality photorealistic materials. Code and\ndata for this paper are available at https://github.com/XiangyuSu611/TMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiangyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaick_O/0/1/0/all/0/1\">Oliver Van Kaick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches. (arXiv:2205.04026v1 [cs.RO])","link":"http://arxiv.org/abs/2205.04026","description":"<p>In this paper, we are interested in the problem of generating target grasps\nby understanding freehand sketches. The sketch is useful for the persons who\ncannot formulate language and the cases where a textual description is not\navailable on the fly. However, very few works are aware of the usability of\nthis novel interactive way between humans and robots. To this end, we propose a\nmethod to generate a potential grasp configuration relevant to the\nsketch-depicted objects. Due to the inherent ambiguity of sketches with\nabstract details, we take the advantage of the graph by incorporating the\nstructure of the sketch to enhance the representation ability. This\ngraph-represented sketch is further validated to improve the generalization of\nthe network, capable of learning the sketch-queried grasp detection by using a\nsmall collection (around 100 samples) of hand-drawn sketches. Additionally, our\nmodel is trained and tested in an end-to-end manner which is easy to be\nimplemented in real-world applications. Experiments on the multi-object VMRD\nand GraspNet-1Billion datasets demonstrate the good generalization of the\nproposed method. The physical robot experiments confirm the utility of our\nmethod in object-cluttered scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 6-DoF Object Poses to Grasp Category-level Objects by Language Instructions. (arXiv:2205.04028v1 [cs.RO])","link":"http://arxiv.org/abs/2205.04028","description":"<p>This paper studies the task of any objects grasping from the known categories\nby free-form language instructions. This task demands the technique in computer\nvision, natural language processing, and robotics. We bring these disciplines\ntogether on this open challenge, which is essential to human-robot interaction.\nCritically, the key challenge lies in inferring the category of objects from\nlinguistic instructions and accurately estimating the 6-DoF information of\nunseen objects from the known classes. In contrast, previous works focus on\ninferring the pose of object candidates at the instance level. This\nsignificantly limits its applications in real-world scenarios.In this paper, we\npropose a language-guided 6-DoF category-level object localization model to\nachieve robotic grasping by comprehending human intention. To this end, we\npropose a novel two-stage method. Particularly, the first stage grounds the\ntarget in the RGB image through language description of names, attributes, and\nspatial relations of objects. The second stage extracts and segments point\nclouds from the cropped depth image and estimates the full 6-DoF object pose at\ncategory-level. Under such a manner, our approach can locate the specific\nobject by following human instructions, and estimate the full 6-DoF pose of a\ncategory-known but unseen instance which is not utilized for training the\nmodel. Extensive experimental results show that our method is competitive with\nthe state-of-the-art language-conditioned grasp method. Importantly, we deploy\nour approach on a physical robot to validate the usability of our framework in\nreal-world applications. Please refer to the supplementary for the demo videos\nof our robot experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning. (arXiv:2205.04042v1 [cs.CV])","link":"http://arxiv.org/abs/2205.04042","description":"<p>Incremental few-shot object detection aims at detecting novel classes without\nforgetting knowledge of the base classes with only a few labeled training data\nfrom the novel classes. Most related prior works are on incremental object\ndetection that rely on the availability of abundant training samples per novel\nclass that substantially limits the scalability to real-world setting where\nnovel data can be scarce. In this paper, we propose the Incremental-DETR that\ndoes incremental few-shot object detection via fine-tuning and self-supervised\nlearning on the DETR object detector. To alleviate severe over-fitting with few\nnovel class data, we first fine-tune the class-specific components of DETR with\nself-supervision from additional object proposals generated using Selective\nSearch as pseudo labels. We further introduce a incremental few-shot\nfine-tuning strategy with knowledge distillation on the class-specific\ncomponents of DETR to encourage the network in detecting novel classes without\ncatastrophic forgetting. Extensive experiments conducted on standard\nincremental object detection and incremental few-shot object detection settings\nshow that our approach significantly outperforms state-of-the-art methods by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Na Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Co-attentional Transformer reconstructs 100x ultra-fast/low-dose whole-body PET from longitudinal images and anatomically guided MRI. (arXiv:2205.04044v1 [eess.IV])","link":"http://arxiv.org/abs/2205.04044","description":"<p>Despite its tremendous value for the diagnosis, treatment monitoring and\nsurveillance of children with cancer, whole body staging with positron emission\ntomography (PET) is time consuming and associated with considerable radiation\nexposure. 100x (1% of the standard clinical dosage) ultra-low-dose/ultra-fast\nwhole-body PET reconstruction has the potential for cancer imaging with\nunprecedented speed and improved safety, but it cannot be achieved by the naive\nuse of machine learning techniques. In this study, we utilize the global\nsimilarity between baseline and follow-up PET and magnetic resonance (MR)\nimages to develop Masked-LMCTrans, a longitudinal multi-modality co-attentional\nCNN-Transformer that provides interaction and joint reasoning between serial\nPET/MRs of the same patient. We mask the tumor area in the referenced baseline\nPET and reconstruct the follow-up PET scans. In this manner, Masked-LMCTrans\nreconstructs 100x almost-zero radio-exposure whole-body PET that was not\npossible before. The technique also opens a new pathway for longitudinal\nradiology imaging reconstruction, a significantly under-explored area to date.\nOur model was trained and tested with Stanford PET/MRI scans of pediatric\nlymphoma patients and evaluated externally on PET/MRI images from T\\\"ubingen\nUniversity. The high image quality of the reconstructed 100x whole-body PET\nimages resulting from the application of Masked-LMCTrans will substantially\nadvance the development of safer imaging approaches and shorter exam-durations\nfor pediatric patients, as well as expand the possibilities for frequent\nlongitudinal monitoring of these patients by PET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan-Ran/0/1/0/all/0/1\">Yan-Ran</a> (Joyce) <a href=\"http://arxiv.org/find/eess/1/au:+Wang/0/1/0/all/0/1\">Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheybani_N/0/1/0/all/0/1\">Natasha Diba Sheybani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiaolong Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiangshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hawk_K/0/1/0/all/0/1\">Kristina Elizabeth Hawk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theruvath_A/0/1/0/all/0/1\">Ashok Joseph Theruvath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xuerong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pribnow_A/0/1/0/all/0/1\">Allison Pribnow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daldrup_Link_H/0/1/0/all/0/1\">Heike E. Daldrup-Link</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carving out the low surface brightness universe with NoiseChisel. (arXiv:1909.11230v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/1909.11230","description":"<p>NoiseChisel is a program to detect very low signal-to-noise ratio (S/N)\nfeatures with minimal assumptions on their morphology. It was introduced in\n2015 and released within a collection of data analysis programs and libraries\nknown as GNU Astronomy Utilities (Gnuastro). The 10th stable version of\nGnuastro was released in August 2019 and NoiseChisel has significantly\nimproved: detecting even fainter signal, enabling better user control over its\ninner workings, and many bug fixes. The most important change until version\n0.10 is that NoiseChisel's segmentation features have been moved into a new\nprogram called Segment. Another major change is the final growth strategy of\nits true detections, for example NoiseChisel is able to detect the outer wings\nof M51 down to S/N of 0.25, or 25.97 mag/arcsec2 on a single-exposure SDSS\nimage (r-band). Segment is also able to detect the localized HII regions as\n\"clumps\" much more successfully. For a detailed list of improvements after\nversion 0.10, see the most recent manual. Finally, to orchestrate a controlled\nanalysis, the concept of reproducibility is discussed: this paper itself is\nexactly reproducible (commit 751467d).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Akhlaghi_M/0/1/0/all/0/1\">Mohammad Akhlaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11945","description":"<p>Supervised learning (SL) has achieved remarkable success in numerous\nartificial intelligence (AI) applications. In the current literature, by\nreferring to the properties of the ground-truth labels prepared for the\ntraining data set, SL is roughly categorized as fully supervised learning (FSL)\nand weakly supervised learning (WSL). FSL concerns the situation where the\ntraining data set is assigned with ideal ground-truth labels, while WSL\nconcerns the situation where the training data set is assigned with non-ideal\nground-truth labels. However, solutions for various FSL tasks have shown that\nthe given ground-truth labels are not always learnable, and the target\ntransformation from the given ground-truth labels to learnable targets can\nsignificantly affect the performance of the final FSL solutions. The roughness\nof the FSL category conceals some details that are critical to building the\nappropriate solutions for some specific FSL tasks. In this paper, taking into\nconsideration the properties of the target transformation from the given\nground-truth labels to learnable targets, we firstly categorize FSL into three\nnarrower sub-types and then focus on the sub-type moderately supervised\nlearning (MSL) that concerns the situation where the given ground-truth labels\nare ideal, but due to the simplicity in annotation of the given ground-truth\nlabels, careful designs are required to transform the given ground-truth labels\ninto learnable targets. From the perspectives of the definition, framework and\ngenerality, we comprehensively illustrate MSL to reveal what details are\nconcealed by the roughness of the FSL category. At the meantime, via presenting\nthe definition, framework and generality of MSL, this paper as well establishes\na tutorial for AI application engineers to refer to viewing a problem to be\nsolved from the mathematicians' vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset. (arXiv:2009.02899v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.02899","description":"<p>Post-hoc analysis is a popular category in eXplainable artificial\nintelligence (XAI) study. In particular, methods that generate heatmaps have\nbeen used to explain the deep neural network (DNN), a black-box model. Heatmaps\ncan be appealing due to the intuitive and visual ways to understand them but\nassessing their qualities might not be straightforward. Different ways to\nassess heatmaps' quality have their own merits and shortcomings. This paper\nintroduces a synthetic dataset that can be generated adhoc along with the\nground-truth heatmaps for more objective quantitative assessment. Each sample\ndata is an image of a cell with easily recognized features that are\ndistinguished from localization ground-truth mask, hence facilitating a more\ntransparent assessment of different XAI methods. Comparison and recommendations\nare made, shortcomings are clarified along with suggestions for future research\ndirections to handle the finer details of select post-hoc analysis methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjoa_E/0/1/0/all/0/1\">Erico Tjoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResNet-LDDMM: Advancing the LDDMM Framework using Deep Residual Networks. (arXiv:2102.07951v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2102.07951","description":"<p>In deformable registration, the geometric framework - large deformation\ndiffeomorphic metric mapping or LDDMM, in short - has inspired numerous\ntechniques for comparing, deforming, averaging and analyzing shapes or images.\nGrounded in flows, which are akin to the equations of motion used in fluid\ndynamics, LDDMM algorithms solve the flow equation in the space of plausible\ndeformations, i.e. diffeomorphisms. In this work, we make use of deep residual\nneural networks to solve the non-stationary ODE (flow equation) based on a\nEuler's discretization scheme. The central idea is to represent time-dependent\nvelocity fields as fully connected ReLU neural networks (building blocks) and\nderive optimal weights by minimizing a regularized loss function. Computing\nminimizing paths between deformations, thus between shapes, turns to find\noptimal network parameters by back-propagating over the intermediate building\nblocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal\npartition of the space into multiple polytopes, and then computes optimal\nvelocity vectors as affine transformations on each of these polytopes. As a\nresult, different parts of the shape, even if they are close (such as two\nfingers of a hand), can be made to belong to different polytopes, and therefore\nbe moved in different directions without costing too much energy. Importantly,\nwe show how diffeomorphic transformations, or more precisely bilipshitz\ntransformations, are predicted by our algorithm. We illustrate these ideas on\ndiverse registration problems of 3D shapes under complex topology-preserving\ntransformations. We thus provide essential foundations for more advanced shape\nvariability analysis under a novel joint geometric-neural networks\nRiemannian-like framework, i.e. ResNet-LDDMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arguillere_S/0/1/0/all/0/1\">Sylvain Arguill&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Contrastive Optimization of Siamese Networks for Place Recognition. (arXiv:2103.06638v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06638","description":"<p>Visual place recognition is a challenging task in computer vision and a key\ncomponent of camera-based localization and navigation systems. Recently,\nConvolutional Neural Networks (CNNs) achieved high results and good\ngeneralization capabilities. They are usually trained using pairs or triplets\nof images labeled as either similar or dissimilar, in a binary fashion. In\npractice, the similarity between two images is not binary, but continuous.\nFurthermore, training these CNNs is computationally complex and involves costly\npair and triplet mining strategies.\n</p>\n<p>We propose a Generalized Contrastive loss (GCL) function that relies on image\nsimilarity as a continuous measure, and use it to train a siamese CNN.\nFurthermore, we present three techniques for automatic annotation of image\npairs with labels indicating their degree of similarity, and deploy them to\nre-annotate the MSLS, TB-Places, and 7Scenes datasets.\n</p>\n<p>We demonstrate that siamese CNNs trained using the GCL function and the\nimproved annotations consistently outperform their binary counterparts. Our\nmodels trained on MSLS outperform the state-of-the-art methods, including\nNetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the\nPittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons\ndatasets. Furthermore, training a siamese network using the GCL function does\nnot require complex pair mining. We release the source code at\nhttps://github.com/marialeyvallina/generalized_contrastive_loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leyva_Vallina_M/0/1/0/all/0/1\">Mar&#xed;a Leyva-Vallina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1\">Nicola Strisciuglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkov_N/0/1/0/all/0/1\">Nicolai Petkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections. (arXiv:2103.17229v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17229","description":"<p>Graph matching aims to establish correspondences between vertices of graphs\nsuch that both the node and edge attributes agree. Various learning-based\nmethods were recently proposed for finding correspondences between image key\npoints based on deep graph matching formulations. While these approaches mainly\nfocus on learning node and edge attributes, they completely ignore the 3D\ngeometry of the underlying 3D objects depicted in the 2D images. We fill this\ngap by proposing a trainable framework that takes advantage of graph neural\nnetworks for learning a deformable 3D geometry model from inhomogeneous image\ncollections, i.e.,~a set of images that depict different instances of objects\nfrom the same category. Experimentally, we demonstrate that our method\noutperforms recent learning-based approaches for graph matching considering\nboth accuracy and cycle-consistency error, while we in addition obtain the\nunderlying 3D geometry of the objects depicted in the 2D images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenzhang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenamandra_T/0/1/0/all/0/1\">Tarun Yenamandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07961","description":"<p>Mitochondria instance segmentation from electron microscopy (EM) images has\nseen notable progress since the introduction of deep learning methods. In this\npaper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,\nfor 3D mitochondria instance segmentation from Rat and Human samples.\nSpecifically, we design a simple yet effective anisotropic convolution block\nand deploy a multi-scale training strategy, which together boost the\nsegmentation performance. Moreover, we enhance the generalizability of the\ntrained models on the test set by adding a denoising operation as\npre-processing. In the Large-scale 3D Mitochondria Instance Segmentation\nChallenge at ISBI 2021, our method ranks the 1st place. Code is available at\nhttps://github.com/Limingxing00/MitoEM2021-Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting-GNN: Boosting Algorithm for Graph Networks on Imbalanced Node Classification. (arXiv:2105.11625v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.11625","description":"<p>The Graph Neural Network (GNN) has been widely used for graph data\nrepresentation. However, the existing researches only consider the ideal\nbalanced dataset, and the imbalanced dataset is rarely considered. Traditional\nmethods such as resampling, reweighting, and synthetic samples that deal with\nimbalanced datasets are no longer applicable in GNN. This paper proposes an\nensemble model called Boosting-GNN, which uses GNNs as the base classifiers\nduring boosting. In Boosting-GNN, higher weights are set for the training\nsamples that are not correctly classified by the previous classifier, thus\nachieving higher classification accuracy and better reliability. Besides,\ntransfer learning is used to reduce computational cost and increase fitting\nability. Experimental results indicate that the proposed Boosting-GNN model\nachieves better performance than GCN, GraphSAGE, GAT, SGC, N-GCN, and most\nadvanced reweighting and resampling methods on synthetic imbalanced datasets,\nwith an average performance improvement of 4.5%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">S. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">L. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey and Taxonomy on Image Dehazing Based on Deep Learning. (arXiv:2106.03323v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03323","description":"<p>With the development of convolutional neural networks, hundreds of deep\nlearning based dehazing methods have been proposed. In this paper, we provide a\ncomprehensive survey on supervised, semi-supervised, and unsupervised dehazing.\nWe first discuss the physical model, datasets, network modules, loss functions,\nand evaluation metrics that are commonly used. Then, the main contributions of\nvarious dehazing algorithms are categorized and summarized. Further,\nquantitative and qualitative experiments of various baseline methods are\ncarried out. Finally, the unsolved issues and challenges that can inspire the\nfuture research are pointed out. A collection of useful dehazing materials is\navailable at https://github.com/Xiaofeng-life/AwesomeDehazing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_J/0/1/0/all/0/1\">Jie Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xiaofeng Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiuxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salvage of Supervision in Weakly Supervised Object Detection. (arXiv:2106.04073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04073","description":"<p>Weakly supervised object detection~(WSOD) has recently attracted much\nattention. However, the lack of bounding-box supervision makes its accuracy\nmuch lower than fully supervised object detection (FSOD), and currently modern\nFSOD techniques cannot be applied to WSOD. To bridge the performance and\ntechnical gaps between WSOD and FSOD, this paper proposes a new framework,\nSalvage of Supervision (SoS), with the key idea being to harness every\npotentially useful supervisory signal in WSOD: the weak image-level labels, the\npseudo-labels, and the power of semi-supervised object detection. This paper\nproposes new approaches to utilize these weak and noisy signals effectively,\nand shows that each type of supervisory signal brings in notable improvements,\noutperforms existing WSOD methods (which mainly use only the weak labels) by\nlarge margins. The proposed SoS-WSOD method also has the ability to freely use\nmodern FSOD techniques. SoS-WSOD achieves 64.4 $m\\text{AP}_{50}$ on VOC2007,\n61.9 $m\\text{AP}_{50}$ on VOC2012 and 16.6 $m\\text{AP}_{50:95}$ on MS-COCO, and\nalso has fast inference speed. Ablations and visualization further verify the\neffectiveness of SoS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_L/0/1/0/all/0/1\">Lin Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen-Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2106.05528v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05528","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na fully-labeled source domain to a different unlabeled target domain. Most\nexisting UDA methods learn domain-invariant feature representations by\nminimizing feature distances across domains. In this work, we build upon\ncontrastive self-supervised learning to align features so as to reduce the\ndomain discrepancy between training and testing sets. Exploring the same set of\ncategories shared by both domains, we introduce a simple yet effective\nframework CDCL, for domain alignment. In particular, given an anchor image from\none domain, we minimize its distances to cross-domain samples from the same\nclass relative to those from different categories. Since target labels are\nunavailable, we use a clustering-based approach with carefully initialized\ncenters to produce pseudo labels. In addition, we demonstrate that CDCL is a\ngeneral framework and can be adapted to the data-free setting, where the source\ndata are unavailable during training, with minimal modification. We conduct\nexperiments on two widely used domain adaptation benchmarks, i.e., Office-31\nand VisDA-2017, for image classification tasks, and demonstrate that CDCL\nachieves state-of-the-art performance on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06959","description":"<p>The discovery of the disentanglement properties of the latent space in GANs\nmotivated a lot of research to find the semantically meaningful directions on\nit. In this paper, we suggest that the disentanglement property is closely\nrelated to the geometry of the latent space. In this regard, we propose an\nunsupervised method for finding the semantic-factorizing directions on the\nintermediate latent space of GANs based on the local geometry. Intuitively, our\nproposed method, called Local Basis, finds the principal variation of the\nlatent space in the neighborhood of the base latent variable. Experimental\nresults show that the local principal variation corresponds to the semantic\nfactorization and traversing along it provides strong robustness to image\ntraversal. Moreover, we suggest an explanation for the limited success in\nfinding the global traversal directions in the latent space, especially W-space\nof StyleGAN2. We show that W-space is warped globally by comparing the local\ngeometry, discovered from Local Basis, through the metric on Grassmannian\nManifold. The global warpage implies that the latent space is not well-aligned\nglobally and therefore the global traversal directions are bound to show\nlimited success on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.07806","description":"<p>Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification. (arXiv:2106.07846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07846","description":"<p>Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Guang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07873","description":"<p>State-of-the-art (SOTA) Generative Models (GMs) can synthesize\nphoto-realistic images that are hard for humans to distinguish from genuine\nphotos. Identifying and understanding manipulated media are crucial to mitigate\nthe social concerns on the potential misuse of GMs. We propose to perform\nreverse engineering of GMs to infer model hyperparameters from the images\ngenerated by these models. We define a novel problem, \"model parsing\", as\nestimating GM network architectures and training loss functions by examining\ntheir generated images - a task seemingly impossible for human beings. To\ntackle this problem, we propose a framework with two components: a Fingerprint\nEstimation Network (FEN), which estimates a GM fingerprint from a generated\nimage by training with four constraints to encourage the fingerprint to have\ndesired properties, and a Parsing Network (PN), which predicts network\narchitecture and loss functions from the estimated fingerprints. To evaluate\nour approach, we collect a fake image dataset with 100K images generated by 116\ndifferent GMs. Extensive experiments show encouraging results in parsing the\nhyperparameters of the unseen models. Finally, our fingerprint estimation can\nbe leveraged for deepfake detection and image attribution, as we show by\nreporting SOTA results on both the deepfake detection (Celeb-DF) and image\nattribution benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1\">Vishal Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. (arXiv:2107.10433v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10433","description":"<p>Many RGB-T trackers attempt to attain robust feature representation by\nutilizing an adaptive weighting scheme (or attention mechanism). Different from\nthese works, we propose a new dynamic modality-aware filter generation module\n(named MFGNet) to boost the message communication between visible and thermal\ndata by adaptively adjusting the convolutional kernels for various input images\nin practical tracking. Given the image pairs as input, we first encode their\nfeatures with the backbone network. Then, we concatenate these feature maps and\ngenerate dynamic modality-aware filters with two independent networks. The\nvisible and thermal filters will be used to conduct a dynamic convolutional\noperation on their corresponding input feature maps respectively. Inspired by\nresidual connection, both the generated visible and thermal feature maps will\nbe summarized with input feature maps. The augmented feature maps will be fed\ninto the RoI align module to generate instance-level features for subsequent\nclassification. To address issues caused by heavy occlusion, fast motion and\nout-of-view, we propose to conduct a joint local and global search by\nexploiting a new direction-aware target driven attention mechanism. The spatial\nand temporal recurrent neural network is used to capture the direction-aware\ncontext for accurate global attention prediction. Extensive experiments on\nthree large-scale RGB-T tracking benchmark datasets validated the effectiveness\nof our proposed algorithm. The source code of this paper is available at\n\\textcolor{magenta}{\\url{https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12056","description":"<p>Existing machines are functionally specific tools that were made for easy\nprediction and control. Tomorrow's machines may be closer to biological systems\nin their mutability, resilience, and autonomy. But first they must be capable\nof sequentially learning, and retaining, new information without being exposed\nto it arbitrarily often. Past efforts to engineer such systems have sought to\nbuild or regulate artificial neural networks using disjoint sets of weights\nthat are uniquely sensitive to specific tasks or inputs. This has not yet\nenabled continual learning over long sequences of previously unseen data\nwithout corrupting existing knowledge: a problem known as catastrophic\nforgetting. In this paper, we introduce a system that can learn sequentially\nover previously unseen datasets (ImageNet, CIFAR-100) with little forgetting\nover time. This is done by controlling the activity of weights in a\nconvolutional neural network on the basis of inputs using top-down regulation\ngenerated by a second feed-forward neural network. We find that our method\nlearns continually under domain transfer with sparse bursts of activity in\nweights that are recycled across tasks, rather than by maintaining\ntask-specific modules. Sparse synaptic bursting is found to balance activity\nand suppression such that new functions can be learned without corrupting\nextant knowledge, thus mirroring the balance of order and disorder in systems\nat the edge of chaos. This behavior emerges during a prior pre-training (or\n'meta-learning') phase in which regulated synapses are selectively\ndisinhibited, or grown, from an initial state of uniform suppression through\nprediction error minimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">Shawn L. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1\">Nick Cheney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LDC-VAE: A Latent Distribution Consistency Approach to Variational AutoEncoders. (arXiv:2109.10640v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10640","description":"<p>Variational autoencoders (VAEs), as an important aspect of generative models,\nhave received a lot of research interests and reached many successful\napplications. However, it is always a challenge to achieve the consistency\nbetween the learned latent distribution and the prior latent distribution when\noptimizing the evidence lower bound (ELBO), and finally leads to an\nunsatisfactory performance in data generation. In this paper, we propose a\nlatent distribution consistency approach to avoid such substantial\ninconsistency between the posterior and prior latent distributions in ELBO\noptimizing. We name our method as latent distribution consistency VAE\n(LDC-VAE). We achieve this purpose by assuming the real posterior distribution\nin latent space as a Gibbs form, and approximating it by using our encoder.\nHowever, there is no analytical solution for such Gibbs posterior in\napproximation, and traditional approximation ways are time consuming, such as\nusing the iterative sampling-based MCMC. To address this problem, we use the\nStein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior.\nMeanwhile, we use the SVGD to train a sampler net which can obtain efficient\nsamples from the Gibbs posterior. Comparative studies on the popular image\ngeneration datasets show that our method has achieved comparable or even better\nperformance than several powerful improvements of VAEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification. (arXiv:2109.12333v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12333","description":"<p>Unsupervised person re-identification (Re-ID) is a promising and very\nchallenging research problem in computer vision. Learning robust and\ndiscriminative features with unlabeled data is of central importance to Re-ID.\nRecently, more attention has been paid to unsupervised Re-ID algorithms based\non clustered pseudo-label. However, the previous approaches did not fully\nexploit information of hard samples, simply using cluster centroid or all\ninstances for contrastive learning. In this paper, we propose a Hard-sample\nGuided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss\nwith instance-level loss for unsupervised person Re-ID. Our approach applies\ncluster centroid contrastive loss to ensure that the network is updated in a\nmore stable way. Meanwhile, introduction of a hard instance contrastive loss\nfurther mines the discriminative information. Extensive experiments on two\npopular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms\nprevious state-of-the-art methods and significantly improves the performance of\nunsupervised person Re-ID. The code of our work is available soon at\nhttps://github.com/bupt-ai-cz/HHCL-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based Hand Gesture Recognition. (arXiv:2109.13879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13879","description":"<p>Estimating the 3D pose of a hand from a 2D image is a well-studied problem\nand a requirement for several real-life applications such as virtual reality,\naugmented reality, and hand gesture recognition. Currently, reasonable\nestimations can be computed from single RGB images, especially when a\nmulti-task learning approach is used to force the system to consider the shape\nof the hand when its pose is determined. However, depending on the method used\nto represent the hand, the performance can drop considerably in real-life\ntasks, suggesting that stable descriptions are required to achieve satisfactory\nresults. In this paper, we present a keypoint-based end-to-end framework for 3D\nhand and pose estimation and successfully apply it to the task of hand gesture\nrecognition as a study case. Specifically, after a pre-processing step in which\nthe images are normalized, the proposed pipeline uses a multi-task semantic\nfeature extractor generating 2D heatmaps and hand silhouettes from RGB images,\na viewpoint encoder to predict the hand and camera view parameters, a stable\nhand estimator to produce the 3D hand pose and shape, and a loss function to\nguide all of the components jointly during the learning phase. Tests were\nperformed on a 3D pose and shape estimation benchmark dataset to assess the\nproposed framework, which obtained state-of-the-art performance. Our system was\nalso evaluated on two hand-gesture recognition benchmark datasets and\nsignificantly outperformed other keypoint-based approaches, indicating that it\nis an effective solution that is able to generate stable 3D estimates for hand\npose and shape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragomeni_A/0/1/0/all/0/1\">Adriano Fragomeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannone_D/0/1/0/all/0/1\">Daniele Pannone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Adaptation for Cross-Domain Object Detection. (arXiv:2110.02578v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02578","description":"<p>Cross-domain object detection is more challenging than object classification\nsince multiple objects exist in an image and the location of each object is\nunknown in the unlabeled target domain. As a result, when we adapt features of\ndifferent objects to enhance the transferability of the detector, the features\nof the foreground and the background are easy to be confused, which may hurt\nthe discriminability of the detector. Besides, previous methods focused on\ncategory adaptation but ignored another important part for object detection,\ni.e., the adaptation on bounding box regression. To this end, we propose\nD-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation\nand the training of the detector. Besides, we fill the blank of regression\ndomain adaptation in object detection by introducing a bounding box adaptor.\nExperiments show that D-adapt achieves state-of-the-art results on four\ncross-domain object detection tasks and yields 17% and 21% relative improvement\non benchmark datasets Clipart1k and Comic2k in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05706","description":"<p>This paper unifies the multi-focus images fusion (MFIF) and blind super\nresolution (SR) problems as the multi-focus image super resolution fusion\n(MFISRF) task, and proposes a novel unified dataset-free unsupervised framework\nnamed deep fusion prior (DFP) to address such MFISRF task. DFP consists of\nSKIPnet network, DoubleReblur focus measurement tactic, decision embedding\nmodule and loss functions. In particular, DFP can obtain MFISRF only from two\nlow-resolution inputs without any extent dataset; SKIPnet implementing\nunsupervised learning via deep image prior is an end-to-end generated network\nacting as the engine of DFP; DoubleReblur is used to determine the primary\ndecision map without learning but based on estimated PSF and Gaussian kernels\nconvolution; decision embedding module optimizes the decision map via learning;\nand DFP losses composed of content loss, joint gradient loss and gradient limit\nloss can obtain high-quality MFISRF results robustly. Experiments have proved\nthat our proposed DFP approaches and even outperforms those state-of-art MFIF\nand SR method combinations. Additionally, DFP is a general framework, thus its\nnetworks and focus measurement tactics can be continuously updated to further\nimprove the MFISRF performance. DFP codes are open source and will be available\nsoon at <a href=\"http://github.com/GuYuanjie/DeepFusionPrior.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reducing Aleatoric Uncertainty for Medical Imaging Tasks. (arXiv:2110.11012v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.11012","description":"<p>In safety-critical applications like medical diagnosis, certainty associated\nwith a model's prediction is just as important as its accuracy. Consequently,\nuncertainty estimation and reduction play a crucial role. Uncertainty in\npredictions can be attributed to noise or randomness in data (aleatoric) and\nincorrect model inferences (epistemic). While model uncertainty can be reduced\nwith more data or bigger models, aleatoric uncertainty is more intricate. This\nwork proposes a novel approach that interprets data uncertainty estimated from\na self-supervised task as noise inherent to the data and utilizes it to reduce\naleatoric uncertainty in another task related to the same dataset via data\naugmentation. The proposed method was evaluated on a benchmark medical imaging\ndataset with image reconstruction as the self-supervised task and segmentation\nas the image analysis task. Our findings demonstrate the effectiveness of the\nproposed approach in significantly reducing the aleatoric uncertainty in the\nimage segmentation task while achieving better or on-par performance compared\nto the standard augmentation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sambyal_A/0/1/0/all/0/1\">Abhishek Singh Sambyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Magnification Network for Vessel Segmentation in OCTA Images. (arXiv:2110.13428v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.13428","description":"<p>Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging modality that allows micron-level resolution to visualize the retinal\nmicrovasculature. The retinal vessel segmentation in OCTA images is still an\nopen problem, and especially the thin and dense structure of the capillary\nplexus is an important challenge of this problem. In this work, we propose a\nnovel image magnification network (IMN) for vessel segmentation in OCTA images.\nContrary to the U-Net structure with a down-sampling encoder and up-sampling\ndecoder, the proposed IMN adopts the design of up-sampling encoding and then\ndown-sampling decoding. This design is to capture more low-level image details\nto reduce the omission of small structures. The experimental results on three\nopen OCTA datasets show that the proposed IMN with an average dice score of\n90.2% achieves the best performance in vessel segmentation of OCTA images.\nBesides, we also demonstrate the superior performance of IMN in cross-field\nimage vessel segmentation and vessel skeleton extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mingchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yerui Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weiwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation. (arXiv:2111.02682v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02682","description":"<p>The recent developments of deep learning models that capture complex temporal\npatterns of crop phenology have greatly advanced crop classification from\nSatellite Image Time Series (SITS). However, when applied to target regions\nspatially different from the training region, these models perform poorly\nwithout any target labels due to the temporal shift of crop phenology between\nregions. Although various unsupervised domain adaptation techniques have been\nproposed in recent years, no method explicitly learns the temporal shift of\nSITS and thus provides only limited benefits for crop classification. To\naddress this, we propose TimeMatch, which explicitly accounts for the temporal\nshift for improved SITS-based domain adaptation. In TimeMatch, we first\nestimate the temporal shift from the target to the source region using the\npredictions of a source-trained model. Then, we re-train the model for the\ntarget region by an iterative algorithm where the estimated shift is used to\ngenerate accurate target pseudo-labels. Additionally, we introduce an\nopen-access dataset for cross-region adaptation from SITS in four different\nregions in Europe. On our dataset, we demonstrate that TimeMatch outperforms\nall competing methods by 11% in average F1-score across five different\nadaptation scenarios, setting a new state-of-the-art in cross-region\nadaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1\">Joachim Nyborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1\">Charlotte Pelletier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1\">Ira Assent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Detection on Chest X-Ray Images: A comparison of CNN architectures and ensembles. (arXiv:2111.09972v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.09972","description":"<p>COVID-19 quickly became a global pandemic after only four months of its first\ndetection. It is crucial to detect this disease as soon as possible to decrease\nits spread. The use of chest X-ray (CXR) images became an effective screening\nstrategy, complementary to the reverse transcription-polymerase chain reaction\n(RT-PCR). Convolutional neural networks (CNNs) are often used for automatic\nimage classification and they can be very useful in CXR diagnostics. In this\npaper, 21 different CNN architectures are tested and compared in the task of\nidentifying COVID-19 in CXR images. They were applied to the COVIDx8B dataset,\na large COVID-19 dataset with 16,352 CXR images coming from patients of at\nleast 51 countries. Ensembles of CNNs were also employed and they showed better\nefficacy than individual instances. The best individual CNN instance results\nwere achieved by DenseNet169, with an accuracy of 98.15% and an F1 score of\n98.12%. These were further increased to 99.25% and 99.24%, respectively,\nthrough an ensemble with five instances of DenseNet169. These results are\nhigher than those obtained in recent works using the same dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breve_F/0/1/0/all/0/1\">Fabricio Breve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11397","description":"<p>Estimating solar rooftop potential at a national level is a fundamental\nbuilding block for every country to utilize solar power efficiently. Solar\nrooftop potential assessment relies on several features such as building\ngeometry, location, and surrounding facilities. Hence, national-level\napproximations that do not take these factors into deep consideration are often\ninaccurate. This paper introduces Lebanon's first comprehensive footprint and\nsolar rooftop potential maps using deep learning-based instance segmentation to\nextract buildings' footprints from satellite images. A photovoltaic panels\nplacement algorithm that considers the morphology of each roof is proposed. We\nshow that the average rooftop's solar potential can fulfill the yearly electric\nneeds of a single-family residence while using only 5% of the roof surface. The\nusage of 50% of a residential apartment rooftop area would achieve energy\nsecurity for up to 8 households. We also compute the average and total solar\nrooftop potential per district to localize regions corresponding to the highest\nand lowest solar rooftop potential yield. Factors such as size, ground coverage\nratio and PV_out are carefully investigated for each district. Baalbeck\ndistrict yielded the highest total solar rooftop potential despite its low\nbuilt-up area. While, Beirut capital city has the highest average solar rooftop\npotential due to its extremely populated urban nature. Reported results and\nanalysis reveal solar rooftop potential urban patterns and provides\npolicymakers and key stakeholders with tangible insights. Lebanon's total solar\nrooftop potential is about 28.1 TWh/year, two times larger than the national\nenergy consumption in 2019.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1\">Hasan Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samhat_A/0/1/0/all/0/1\">Abed Ellatif Samhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1\">Ghaleb Faour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1\">Ali J. Ghandour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset-free Self-supervised Disentangled Learning Method for Adaptive Infrared and Visible Images Super-resolution Fusion. (arXiv:2112.02869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02869","description":"<p>This study proposes a novel general dataset-free self-supervised learning\nframework based-on physical model named self-supervised disentangled learning\n(SDL), and proposes a novel method named Deep Retinex fusion (DRF) which\napplies SDL framework with generative networks and Retinex theory in infrared\nand visible images super-resolution fusion. Meanwhile, a generative dual-path\nfusion network ZipperNet and adaptive fusion loss function Retinex loss are\ndesigned for effectively high-quality fusion. The core idea of DRF (based-on\nSDL) consists of two parts: one is generating components which are disentangled\nfrom physical model using generative networks; the other is loss functions\nwhich are designed based-on physical relation, and generated components are\ncombined by loss functions in training phase. Furthermore, in order to verify\nthe effectiveness of our proposed DRF, qualitative and quantitative comparisons\ncompared with six state-of-the-art methods are performed on three different\ninfrared and visible datasets. Our code will be open source available soon at\nhttps://github.com/GuYuanjie/Deep-Retinex-fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation. (arXiv:2112.06180v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06180","description":"<p>We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solarte_B/0/1/0/all/0/1\">Bolivar Solarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chin-Hsuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triangle Attack: A Query-efficient Decision-based Adversarial Attack. (arXiv:2112.06569v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06569","description":"<p>Decision-based attack poses a severe threat to real-world applications since\nit regards the target model as a black box and only accesses the hard\nprediction label. Great efforts have been made recently to decrease the number\nof queries; however, existing decision-based attacks still require thousands of\nqueries in order to generate good quality adversarial examples. In this work,\nwe find that a benign sample, the current and the next adversarial examples\ncould naturally construct a triangle in a subspace for any iterative attacks.\nBased on the law of sines, we propose a novel Triangle Attack (TA) to optimize\nthe perturbation by utilizing the geometric information that the longer side is\nalways opposite the larger angle in any triangle. However, directly applying\nsuch information on the input image is ineffective because it cannot thoroughly\nexplore the neighborhood of the input sample in the high dimensional space. To\naddress this issue, TA optimizes the perturbation in the low frequency space\nfor effective dimensionality reduction owing to the generality of such\ngeometric property. Extensive evaluations on the ImageNet dataset demonstrate\nthat TA achieves a much higher attack success rate within 1,000 queries and\nneeds a much less number of queries to achieve the same attack success rate\nunder various perturbation budgets than existing decision-based attacks. With\nsuch high efficiency, we further demonstrate the applicability of TA on\nreal-world API, i.e., Tencent Cloud API.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_K/0/1/0/all/0/1\">Kangheng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving neural implicit surfaces geometry with patch warping. (arXiv:2112.09648v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09648","description":"<p>Neural implicit surfaces have become an important technique for multi-view 3D\nreconstruction but their accuracy remains limited. In this paper, we argue that\nthis comes from the difficulty to learn and render high frequency textures with\nneural networks. We thus propose to add to the standard neural rendering\noptimization a direct photo-consistency term across the different views.\nIntuitively, we optimize the implicit geometry so that it warps views on each\nother in a consistent way. We demonstrate that two elements are key to the\nsuccess of such an approach: (i) warping entire patches, using the predicted\noccupancy and normals of the 3D points along each ray, and measuring their\nsimilarity with a robust structural similarity (SSIM); (ii) handling visibility\nand occlusion in such a way that incorrect warps are not given too much\nimportance while encouraging a reconstruction as complete as possible. We\nevaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL\nbenchmarks and show it outperforms state of the art unsupervised implicit\nsurfaces reconstructions by over 20% on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darmon_F/0/1/0/all/0/1\">Fran&#xe7;ois Darmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bascle_B/0/1/0/all/0/1\">B&#xe9;n&#xe9;dicte Bascle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaux_J/0/1/0/all/0/1\">Jean-Cl&#xe9;ment Devaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monasse_P/0/1/0/all/0/1\">Pascal Monasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks. (arXiv:2112.09791v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09791","description":"<p>Few-shot object detection (FSOD) aims to detect never-seen objects using few\nexamples. This field sees recent improvement owing to the meta-learning\ntechniques by learning how to match between the query image and few-shot class\nexamples, such that the learned model can generalize to few-shot novel classes.\nHowever, currently, most of the meta-learning-based methods perform pairwise\nmatching between query image regions (usually proposals) and novel classes\nseparately, therefore failing to take into account multiple relationships among\nthem. In this paper, we propose a novel FSOD model using heterogeneous graph\nconvolutional networks. Through efficient message passing among all the\nproposal and class nodes with three different types of edges, we could obtain\ncontext-aware proposal features and query-adaptive, multiclass-enhanced\nprototype representations for each class, which could help promote the pairwise\nmatching and improve final FSOD accuracy. Extensive experimental results show\nthat our proposed model, denoted as QA-FewDet, outperforms the current\nstate-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under\ndifferent shots and evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10482","description":"<p>We propose a new 3D spatial understanding task of 3D Question Answering\n(3D-QA). In the 3D-QA task, models receive visual information from the entire\n3D scene of the rich RGB-D indoor scan and answer the given textual questions\nabout the 3D scene. Unlike the 2D-question answering of VQA, the conventional\n2D-QA models suffer from problems with spatial understanding of object\nalignment and directions and fail the object identification from the textual\nquestions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,\nwhere the model learns a fused descriptor from 3D object proposals and encoded\nsentence embeddings. This learned descriptor correlates the language\nexpressions with the underlying geometric features of the 3D scan and\nfacilitates the regression of 3D bounding boxes to determine described objects\nin textual questions and outputs correct answers. We collected human-edited\nquestion-answer pairs with free-form answers that are grounded to 3D objects in\neach 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs\nfrom the 800 indoor scenes drawn from the ScanNet dataset. To the best of our\nknowledge, the proposed 3D-QA task is the first large-scale effort to perform\nobject-grounded question-answering in 3D environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azuma_D/0/1/0/all/0/1\">Daichi Azuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyanishi_T/0/1/0/all/0/1\">Taiki Miyanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurita_S/0/1/0/all/0/1\">Shuhei Kurita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1\">Motoaki Kawanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review. (arXiv:2201.08550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08550","description":"<p>In the past ten years, the computing power of machine vision (MV) has been\ncontinuously improved, and image analysis algorithms have developed rapidly. At\nthe same time, histopathological slices can be stored as digital images.\nTherefore, MV algorithms can provide doctors with diagnostic references. In\nparticular, the continuous improvement of deep learning algorithms has further\nimproved the accuracy of MV in disease detection and diagnosis. This paper\nreviews the applications of image processing technology based on MV in lymphoma\nhistopathological images in recent years, including segmentation,\nclassification and detection. Finally, the current methods are analyzed, some\nmore potential methods are proposed, and further prospects are made.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks. (arXiv:2201.10162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10162","description":"<p>Traditional media coding schemes typically encode image/video into a\nsemantic-unknown binary stream, which fails to directly support downstream\nintelligent tasks at the bitstream level. Semantically Structured Image Coding\n(SSIC) framework makes the first attempt to enable decoding-free or\npartial-decoding image intelligent task analysis via a Semantically Structured\nBitstream (SSB). However, the SSIC only considers image coding and its\ngenerated SSB only contains the static object information. In this paper, we\nextend the idea of semantically structured coding from video coding perspective\nand propose an advanced Semantically Structured Video Coding (SSVC) framework\nto support heterogeneous intelligent applications. Video signals contain more\nrich dynamic motion information and exist more redundancy due to the similarity\nbetween adjacent frames. Thus, we present a reformulation of semantically\nstructured bitstream (SSB) in SSVC which contains both static object\ncharacteristics and dynamic motion clues. Specifically, we introduce optical\nflow to encode continuous motion information and reduce cross-frame redundancy\nvia a predictive coding architecture, then the optical flow and residual\ninformation are reorganized into SSB, which enables the proposed SSVC could\nbetter adaptively support video-based downstream intelligent applications.\nExtensive experiments demonstrate that the proposed SSVC framework could\ndirectly support multiple intelligent tasks just depending on a partially\ndecoded bitstream. This avoids the full bitstream decompression and thus\nsignificantly saves bitrate/bandwidth consumption for intelligent analytics. We\nverify this point on the tasks of image object detection, pose estimation,\nvideo action recognition, video object segmentation, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression. (arXiv:2202.06028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06028","description":"<p>In point cloud compression, sufficient contexts are significant for modeling\nthe point cloud distribution. However, the contexts gathered by the previous\nvoxel-based methods decrease when handling sparse point clouds. To address this\nproblem, we propose a multiple-contexts deep learning framework called\nOctAttention employing the octree structure, a memory-efficient representation\nfor point clouds. Our approach encodes octree symbol sequences in a lossless\nway by gathering the information of sibling and ancestor nodes. Expressly, we\nfirst represent point clouds with octree to reduce spatial redundancy, which is\nrobust for point clouds with different resolutions. We then design a\nconditional entropy model with a large receptive field that models the sibling\nand ancestor contexts to exploit the strong dependency among the neighboring\nnodes and employ an attention mechanism to emphasize the correlated nodes in\nthe context. Furthermore, we introduce a mask operation during training and\ntesting to make a trade-off between encoding time and performance. Compared to\nthe previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate\ngain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset\n(e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based\nbaseline. The code is available at https://github.com/zb12138/OctAttention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chunyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALGAN: Anomaly Detection by Generating Pseudo Anomalous Data via Latent Variables. (arXiv:2202.10281v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.10281","description":"<p>In many anomaly detection tasks, where anomalous data rarely appear and are\ndifficult to collect, training using only normal data is important. Although it\nis possible to manually create anomalous data using prior knowledge, they may\nbe subject to user bias. In this paper, we propose an Anomalous Latent variable\nGenerative Adversarial Network (ALGAN) in which the GAN generator produces\npseudo-anomalous data as well as fake-normal data, whereas the discriminator is\ntrained to distinguish between normal and pseudo-anomalous data. This differs\nfrom the standard GAN discriminator, which specializes in classifying two\nsimilar classes. The training dataset contains only normal data; the latent\nvariables are introduced in anomalous states and are input into the generator\nto produce diverse pseudo-anomalous data. We compared the performance of ALGAN\nwith other existing methods on the MVTec-AD, Magnetic Tile Defects, and\nCOIL-100 datasets. The experimental results showed that ALGAN exhibited an\nAUROC comparable to those of state-of-the-art methods while achieving a much\nfaster prediction time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1\">Hironori Murase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukumizu_K/0/1/0/all/0/1\">Kenji Fukumizu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12613","description":"<p>Visual-inertial-odometry has attracted extensive attention in the field of\nautonomous driving and robotics. The size of Field of View (FoV) plays an\nimportant role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a\nlarge FoV enables to perceive a wide range of surrounding scene elements and\nfeatures. However, when the field of the camera reaches the negative half\nplane, one cannot simply use [u,v,1]^T to represent the image feature points\nanymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for\ncameras with extremely large FoV. We leverage a three-dimensional vector with\nunit length to represent feature points, and design a series of algorithms to\novercome this challenge. To address the scarcity of panoramic visual odometry\ndatasets with ground-truth location and pose, we present the PALVIO dataset,\ncollected with a Panoramic Annular Lens (PAL) system with an entire FoV of\n360x(40-120) degrees and an IMU sensor. With a comprehensive variety of\nexperiments, the proposed LF-VIO is verified on both the established PALVIO\nbenchmark and a public fisheye camera dataset with a FoV of 360x(0-93.5)\ndegrees. LF-VIO outperforms state-of-the-art visual-inertial-odometry methods.\nOur dataset and code are made publicly available at\nhttps://github.com/flysoaryun/LF-VIO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01252","description":"<p>3D point cloud understanding is an important component in autonomous driving\nand robotics. In this paper, we present a novel Embedding-Querying paradigm\n(EQ- Paradigm) for 3D understanding tasks including detection, segmentation,\nand classification. EQ-Paradigm is a unified paradigm that enables the\ncombination of any existing 3D backbone architectures with different task\nheads. Under the EQ-Paradigm, the input is firstly encoded in the embedding\nstage with an arbitrary feature extraction architecture, which is independent\nof tasks and heads. Then, the querying stage enables the encoded features to be\napplicable for diverse task heads. This is achieved by introducing an\nintermediate representation, i.e., Q-representation, in the querying stage to\nserve as a bridge between the embedding stage and task heads. We design a novel\nQ- Net as the querying stage network. Extensive experimental results on various\n3D tasks, including object detection, semantic segmentation and shape\nclassification, show that EQ-Paradigm in tandem with Q-Net is a general and\neffective pipeline, which enables a flexible collaboration of backbones and\nheads, and further boosts the performance of the state-of-the-art methods.\nCodes and models are available at\nhttps://github.com/dvlab-research/DeepVision3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zetong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03622","description":"<p>A stroke occurs when an artery in the brain ruptures and bleeds or when the\nblood supply to the brain is cut off. Blood and oxygen cannot reach the brain's\ntissues due to the rupture or obstruction resulting in tissue death. The Middle\ncerebral artery (MCA) is the largest cerebral artery and the most commonly\ndamaged vessel in stroke. The quick onset of a focused neurological deficit\ncaused by interruption of blood flow in the territory supplied by the MCA is\nknown as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is\nused to estimate the extent of early ischemic changes in patients with MCA\nstroke. This study proposes a deep learning-based method to score the CT scan\nfor ASPECTS. Our work has three highlights. First, we propose a novel method\nfor medical image segmentation for stroke detection. Second, we show the\neffectiveness of AI solution for fully-automated ASPECT scoring with reduced\ndiagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a\ndice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72\nfor the infarcts segmentation. Lastly, we show that our model's performance is\ninline with inter-reader variability between radiologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Ujjwal Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranjan_M/0/1/0/all/0/1\">Mukul Ranjan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golla_S/0/1/0/all/0/1\">Satish Golla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanamala_S/0/1/0/all/0/1\">Swetha Tanamala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sreenivas_P/0/1/0/all/0/1\">Preetham Sreenivas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chilamkurthy_S/0/1/0/all/0/1\">Sasank Chilamkurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandian_J/0/1/0/all/0/1\">Jeyaraj Pandian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarpley_J/0/1/0/all/0/1\">Jason Tarpley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos. (arXiv:2203.03796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03796","description":"<p>Activity detection in surveillance videos is a challenging task caused by\nsmall objects, complex activity categories, its untrimmed nature, etc. Existing\nmethods are generally limited in performance due to inaccurate proposals, poor\nclassifiers or inadequate post-processing method. In this work, we propose a\ncomprehensive and effective activity detection system in untrimmed surveillance\nvideos for person-centered and vehicle-centered activities. It consists of four\nmodules, i.e., object localizer, proposal filter, activity classifier and\nactivity refiner. For person-centered activities, a novel part-attention\nmechanism is proposed to explore detailed features in different body parts. As\nfor vehicle-centered activities, we propose a localization masking method to\njointly encode motion and foreground attention features. We conduct experiments\non the large-scale activity detection datasets VIRAT, and achieve the best\nresults for both groups of activities. Furthermore, our team won the 1st place\nin the TRECVID 2021 ActEV challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yunhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhihang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junfeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.08368","description":"<p>The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kai Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation. (arXiv:2203.08667v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08667","description":"<p>With the development of deep convolutional neural networks, medical image\nsegmentation has achieved a series of breakthroughs in recent years. However,\nthe higher-performance convolutional neural networks always mean numerous\nparameters and high computation costs, which will hinder the applications in\nclinical scenarios. Meanwhile, the scarceness of large-scale annotated medical\nimage datasets further impedes the application of high-performance networks. To\ntackle these problems, we propose Graph Flow, a comprehensive knowledge\ndistillation framework, for both network-efficiency and annotation-efficiency\nmedical image segmentation. Specifically, our core Graph Flow Distillation\ntransfer the essence of cross-layer variations from a well-trained cumbersome\nteacher network to a non-trained compact student network. In addition, an\nunsupervised Paraphraser Module is designed to purify the knowledge of the\nteacher network, which is also beneficial for the stabilization of training\nprocedure. Furthermore, we build a unified distillation framework by\nintegrating the adversarial distillation and the vanilla logits distillation,\nwhich can further refine the final predictions of the compact network.\nExtensive experiments conducted on Gastric Cancer Segmentation Dataset and\nSynapse Multi-organ Segmentation Dataset demonstrate the prominent ability of\nour method which achieves state-of-the-art performance on these\ndifferent-modality and multi-category medical image datasets. Moreover, we\ndemonstrate the effectiveness of our Graph Flow through a new semi-supervised\nparadigm for dual efficient medical image segmentation. Our code will be\navailable at Graph Flow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09494","description":"<p>We present a general-purpose framework for image modelling and vision tasks\nbased on probabilistic frame prediction. Our approach unifies a broad range of\ntasks, from image segmentation, to novel view synthesis and video\ninterpolation. We pair this framework with an architecture we term Transframer,\nwhich uses U-Net and Transformer components to condition on annotated context\nframes, and outputs sequences of sparse, compressed image features. Transframer\nis the state-of-the-art on a variety of video generation benchmarks, is\ncompetitive with the strongest models on few-shot view synthesis, and can\ngenerate coherent 30 second videos from a single image without any explicit\ngeometric information. A single generalist Transframer simultaneously produces\npromising results on 8 tasks, including semantic segmentation, image\nclassification and optical flow prediction with no task-specific architectural\ncomponents, demonstrating that multi-task computer vision can be tackled using\nprobabilistic image models. Our approach can in principle be applied to a wide\nrange of applications that require learning the conditional structure of\nannotated image-formatted data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1\">Charlie Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">Jacob Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1\">Iain Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-space and Image Domain Collaborative Energy based Model for Parallel MRI Reconstruction. (arXiv:2203.10776v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10776","description":"<p>Decreasing magnetic resonance (MR) image acquisition times can potentially\nmake MR examinations more accessible. Prior arts including the deep learning\nmodels have been devoted to solving the problem of long MRI imaging time.\nRecently, deep generative models have exhibited great potentials in algorithm\nrobustness and usage flexibility. Nevertheless, no existing such schemes that\ncan be learned or employed directly to the k-space measurement. Furthermore,\nhow do the deep generative models work well in hybrid domain is also worth to\nbe investigated. In this work, by taking advantage of the deep en-ergy-based\nmodels, we propose a k-space and image domain collaborative generative model to\ncomprehensively estimate the MR data from under-sampled measurement.\nExperimental comparisons with the state-of-the-arts demonstrated that the\nproposed hybrid method has less error in reconstruction and is more stable\nunder different acceleration factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jijun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors. (arXiv:2203.13467v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.13467","description":"<p>DPICT is the first learning-based image codec supporting fine granular\nscalability. In this paper, we describe how to implement two key components of\nDPICT efficiently: trit-plane slicing and rate-distortion-optimized\n(RD-optimized) coding. In DPICT, we transform an image into a latent tensor,\nrepresent the tensor in ternary digits (trits), and encode the trits in the\ndecreasing order of significance. For entropy encoding, it is necessary to\ncompute the probability of each trit, which demands high time complexity in\nboth the encoder and the decoder. To reduce the complexity, we develop a\nparallel computing scheme for the probabilities, which is described in detail\nwith pseudo-codes. Moreover, we compare the trit-plane slicing in DPICT with\nthe alternative bit-plane slicing. Experimental results show that the time\ncomplexity is reduced significantly by the parallel computing and that the\ntrit-plane slicing provides better RD performances than the bit-plane slicing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_S/0/1/0/all/0/1\">Seungmin Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Han Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expanding Low-Density Latent Regions for Open-Set Object Detection. (arXiv:2203.14911v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14911","description":"<p>Modern object detectors have achieved impressive progress under the close-set\nsetup. However, open-set object detection (OSOD) remains challenging since\nobjects of unknown categories are often misclassified to existing known\nclasses. In this work, we propose to identify unknown objects by separating\nhigh/low-density regions in the latent space, based on the consensus that\nunknown objects are usually distributed in low-density latent regions. As\ntraditional threshold-based methods only maintain limited low-density regions,\nwhich cannot cover all unknown objects, we present a novel Open-set Detector\n(OpenDet) with expanded low-density regions. To this aim, we equip OpenDet with\ntwo learners, Contrastive Feature Learner (CFL) and Unknown Probability Learner\n(UPL). CFL performs instance-level contrastive learning to encourage compact\nfeatures of known classes, leaving more low-density regions for unknown\nclasses; UPL optimizes unknown probability based on the uncertainty of\npredictions, which further divides more low-density regions around the cluster\nof known classes. Thus, unknown objects in low-density regions can be easily\nidentified with the learned unknown probability. Extensive experiments\ndemonstrate that our method can significantly improve the OSOD performance,\ne.g., OpenDet reduces the Absolute Open-Set Errors by 25%-35% on six OSOD\nbenchmarks. Code is available at: https://github.com/csuhan/opendet2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepViT: Separable Vision Transformer. (arXiv:2203.15380v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15380","description":"<p>Vision Transformers have witnessed prevailing success in a series of vision\ntasks. However, they often require enormous amount of computations to achieve\nhigh performance, which is burdensome to deploy on resource-constrained\ndevices. To address these issues, we draw lessons from depthwise separable\nconvolution and imitate its ideology to design the Separable Vision\nTransformer, abbreviated as SepViT. SepViT helps to carry out the information\ninteraction within and among the windows via a depthwise separable\nself-attention. The novel window token embedding and grouped self-attention are\nemployed to model the attention relationship among windows with negligible\ncomputational cost and capture a long-range visual dependencies of multiple\nwindows, respectively. Extensive experiments on various benchmark tasks\ndemonstrate SepViT can achieve state-of-the-art results in terms of trade-off\nbetween accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy\non ImageNet-1K classification while decreasing the latency by 40%, compared to\nthe ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream\nvision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic\nsegmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box\nAP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v2 [cond-mat.mtrl-sci] UPDATED)","link":"http://arxiv.org/abs/2204.00629","description":"<p>Technological advances are in part enabled by the development of novel\nmanufacturing processes that give rise to new materials or material property\nimprovements. Development and evaluation of new manufacturing methodologies is\nlabor-, time-, and resource-intensive expensive due to complex, poorly defined\nrelationships between advanced manufacturing process parameters and the\nresulting microstructures. In this work, we present a topological\nrepresentation of temper (heat-treatment) dependent material micro-structure,\nas captured by scanning electron microscopy, called TopTemp. We show that this\ntopological representation is able to support temper classification of\nmicrostructures in a data limited setting, generalizes well to previously\nunseen samples, is robust to image perturbations, and captures domain\ninterpretable features. The presented work outperforms conventional deep\nlearning baselines and is a first step towards improving understanding of\nprocess parameters and resulting material properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Kassab_L/0/1/0/all/0/1\">Lara Kassab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Sahithi Kappagantula</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan Emerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02980","description":"<p>Image colorization aims to add color information to a grayscale image in a\nrealistic way. Recent methods mostly rely on deep learning strategies. While\nlearning to automatically colorize an image, one can define well-suited\nobjective functions related to the desired color output. Some of them are based\non a specific type of error between the predicted image and ground truth one,\nwhile other losses rely on the comparison of perceptual properties. But, is the\nchoice of the objective function that crucial, i.e., does it play an important\nrole in the results? In this chapter, we aim to answer this question by\nanalyzing the impact of the loss function on the estimated colorization\nresults. To that goal, we review the different losses and evaluation metrics\nthat are used in the literature. We then train a baseline network with several\nof the reviewed objective functions: classic L1 and L2 losses, as well as more\ncomplex combinations such as Wasserstein GAN and VGG-based LPIPS loss.\nQuantitative results show that the models trained with VGG-based LPIPS provide\noverall slightly better results for most evaluation metrics. Qualitative\nresults exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or\nagain with the VGG-based LPIPS. Finally, the convenience of quantitative user\nstudies is also discussed to overcome the difficulty of properly assessing on\ncolorized images, notably for the case of old archive photographs where no\nground truth is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1\">Coloma Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugeau_A/0/1/0/all/0/1\">Aur&#xe9;lie Bugeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_H/0/1/0/all/0/1\">Hernan Carrillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_R/0/1/0/all/0/1\">R&#xe9;mi Giraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raad_L/0/1/0/all/0/1\">Lara Raad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitoria_P/0/1/0/all/0/1\">Patricia Vitoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Aware Transformer Network for Better Human-Object Interaction Detection. (arXiv:2204.04911v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04911","description":"<p>Human-Object Interactions (HOI) detection, which aims to localize a human and\na relevant object while recognizing their interaction, is crucial for\nunderstanding a still image. Recently, transformer-based models have\nsignificantly advanced the progress of HOI detection. However, the capability\nof these models has not been fully explored since the Object Query of the model\nis always simply initialized as just zeros, which would affect the performance.\nIn this paper, we try to study the issue of promoting transformer-based HOI\ndetectors by initializing the Object Query with category-aware semantic\ninformation. To this end, we innovatively propose the Category-Aware\nTransformer Network (CATN). Specifically, the Object Query would be initialized\nvia category priors represented by an external object detection model to yield\nbetter performance. Moreover, such category priors can be further used for\nenhancing the representation ability of features via the attention mechanism.\nWe have firstly verified our idea via the Oracle experiment by initializing the\nObject Query with the groundtruth category information. And then extensive\nexperiments have been conducted to show that a HOI detection model equipped\nwith our idea outperforms the baseline by a large margin to achieve a new\nstate-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Leizhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kunlun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Luxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xu Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atmospheric Turbulence Removal with Complex-Valued Convolutional Neural Network. (arXiv:2204.06989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06989","description":"<p>Atmospheric turbulence distorts visual imagery and is always problematic for\ninformation interpretation by both human and machine. Most well-developed\napproaches to remove atmospheric turbulence distortion are model-based.\nHowever, these methods require high computation and large memory making\nreal-time operation infeasible. Deep learning-based approaches have hence\ngained more attention but currently work efficiently only on static scenes.\nThis paper presents a novel learning-based framework offering short temporal\nspanning to support dynamic scenes. We exploit complex-valued convolutions as\nphase information, altered by atmospheric turbulence, is captured better than\nusing ordinary real-valued convolutions. Two concatenated modules are proposed.\nThe first module aims to remove geometric distortions and, if enough memory,\nthe second module is applied to refine micro details of the videos.\nExperimental results show that our proposed framework efficiently mitigates the\natmospheric turbulence distortion and significantly outperforms existing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OMG: Observe Multiple Granularities for Natural Language-Based Vehicle Retrieval. (arXiv:2204.08209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08209","description":"<p>Retrieving tracked-vehicles by natural language descriptions plays a critical\nrole in smart city construction. It aims to find the best match for the given\ntexts from a set of tracked vehicles in surveillance videos. Existing works\ngenerally solve it by a dual-stream framework, which consists of a text\nencoder, a visual encoder and a cross-modal loss function. Although some\nprogress has been made, they failed to fully exploit the information at various\nlevels of granularity. To tackle this issue, we propose a novel framework for\nthe natural language-based vehicle retrieval task, OMG, which Observes Multiple\nGranularities with respect to visual representation, textual representation and\nobjective functions. For the visual representation, target features, context\nfeatures and motion features are encoded separately. For the textual\nrepresentation, one global embedding, three local embeddings and a color-type\nprompt embedding are extracted to represent various granularities of semantic\nfeatures. Finally, the overall framework is optimized by a cross-modal\nmulti-granularity contrastive loss function. Experiments demonstrate the\neffectiveness of our method. Our OMG significantly outperforms all previous\nmethods and ranks the 9th on the 6th AI City Challenge Track2. The codes are\navailable at https://github.com/dyhBUPT/OMG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yunhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiangning Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1\">Fei Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infographics Wizard: Flexible Infographics Authoring and Design Exploration. (arXiv:2204.09904v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2204.09904","description":"<p>Infographics are an aesthetic visual representation of information following\nspecific design principles of human perception. Designing infographics can be a\ntedious process for non-experts and time-consuming, even for professional\ndesigners. With the help of designers, we propose a semi-automated infographic\nframework for general structured and flow-based infographic design generation.\nFor novice designers, our framework automatically creates and ranks infographic\ndesigns for a user-provided text with no requirement for design input. However,\nexpert designers can still provide custom design inputs to customize the\ninfographics. We will also contribute an individual visual group (VG) designs\ndataset (in SVG), along with a 1k complete infographic image dataset with\nsegmented VGs in this work. Evaluation results confirm that by using our\nframework, designers from all expertise levels can generate generic infographic\ndesigns faster than existing methods while maintaining the same quality as\nhand-designed infographics templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Anjul Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pushkar Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Swasti Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Representational Shift for Continual Fine-tuning. (arXiv:2204.10535v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10535","description":"<p>We study a practical setting of continual learning: fine-tuning on a\npre-trained model continually. Previous work has found that, when training on\nnew tasks, the features (penultimate layer representations) of previous data\nwill change, called representational shift. Besides the shift of features, we\nreveal that the intermediate layers' representational shift (IRS) also matters\nsince it disrupts batch normalization, which is another crucial cause of\ncatastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning\nmethod incorporating two components, cross-convolution batch normalization\n(Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution\nrunning means instead of post-convolution, and recovers post-convolution ones\nbefore testing, which corrects the inaccurate estimates of means under IRS.\nHierarchical fine-tuning leverages a multi-stage strategy to fine-tune the\npre-trained network, preventing massive changes in Conv layers and thus\nalleviating IRS. Experimental results on four datasets show that our method\nremarkably outperforms several state-of-the-art methods with lower storage\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_S/0/1/0/all/0/1\">Shibo Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12037","description":"<p>Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks an unified guidance and analysis about\nwhy modern visual representation learning methods are easily collapse into data\nbias and have limited generalization and cognitive abilities. Inspired by the\nstrong inference ability of human-level agents, recent years have therefore\nwitnessed great effort in developing causal reasoning paradigms to realize\nrobust representation and model learning with good cognitive ability. In this\npaper, we conduct a comprehensive review of existing causal reasoning methods\nfor visual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yushen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transferability for Domain Adaptive Detection Transformers. (arXiv:2204.14195v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14195","description":"<p>DETR-style detectors stand out amongst in-domain scenarios, but their\nproperties in domain shift settings are under-explored. This paper aims to\nbuild a simple but effective baseline with a DETR-style detector on domain\nshift settings based on two findings. For one, mitigating the domain shift on\nthe backbone and the decoder output features excels in getting favorable\nresults. For another, advanced domain alignment methods in both parts further\nenhance the performance. Thus, we propose the Object-Aware Alignment (OAA)\nmodule and the Optimal Transport based Alignment (OTA) module to achieve\ncomprehensive domain alignment on the outputs of the backbone and the detector.\nThe OAA module aligns the foreground regions identified by pseudo-labels in the\nbackbone outputs, leading to domain-invariant based features. The OTA module\nutilizes sliced Wasserstein distance to maximize the retention of location\ninformation while minimizing the domain gap in the decoder outputs. We\nimplement the findings and the alignment modules into our adaptation method,\nand it benchmarks the DETR-style detector on the domain shift settings.\nExperiments on various domain adaptive scenarios validate the effectiveness of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Kaixiong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shugang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator. (arXiv:2205.00146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00146","description":"<p>Automatic font generation remains a challenging research issue due to the\nlarge amounts of characters with complicated structures. Typically, only a few\nsamples can serve as the style/content reference (termed few-shot learning),\nwhich further increases the difficulty to preserve local style patterns or\ndetailed glyph structures. We investigate the drawbacks of previous studies and\nfind that a coarse-grained discriminator is insufficient for supervising a font\ngenerator. To this end, we propose a novel Component-Aware Module (CAM), which\nsupervises the generator to decouple content and style at a more fine-grained\nlevel, i.e., the component level. Different from previous studies struggling to\nincrease the complexity of generators, we aim to perform more effective\nsupervision for a relatively simple generator to achieve its full potential,\nwhich is a brand new perspective for font generation. The whole framework\nachieves remarkable results by coupling component-level supervision with\nadversarial learning, hence we call it Component-Guided GAN, shortly CG-GAN.\nExtensive experiments show that our approach outperforms state-of-the-art\none-shot font generation methods. Furthermore, it can be applied to handwritten\nword synthesis and scene text image editing, suggesting the generalization of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yuxin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01550","description":"<p>Point clouds have the characteristics of disorder, unstructured and\nsparseness.Aiming at the problem of the non-structural nature of point clouds,\nthanks to the excellent performance of convolutional neural networks in image\nprocessing, one of the solutions is to extract features from point clouds based\non two-dimensional convolutional neural networks. The three-dimensional\ninformation carried in the point cloud can be converted to two-dimensional, and\nthen processed by a two-dimensional convolutional neural network, and finally\nback-projected to three-dimensional.In the process of projecting 3D information\nto 2D and back-projection, certain information loss will inevitably be caused\nto the point cloud and category inconsistency will be introduced in the\nback-projection stage;Another solution is the voxel-based point cloud\nsegmentation method, which divides the point cloud into small grids one by\none.However, the point cloud is sparse, and the direct use of 3D convolutional\nneural network inevitably wastes computing resources. In this paper, we propose\na feature extraction module based on multi-scale ultra-sparse convolution and a\nfeature selection module based on channel attention, and build a point cloud\nsegmentation network framework based on this.By introducing multi-scale sparse\nconvolution, network could capture richer feature information based on\nconvolution kernels of different sizes, improving the segmentation result of\npoint cloud segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01920","description":"<p>Multi-modal aerial view object classification (MAVOC) in Automatic target\nrecognition (ATR), although an important and challenging problem, has been\nunder studied. This paper firstly finds that fine-grained data, class imbalance\nand various shooting conditions preclude the representational ability of\ngeneral image classification. Moreover, the MAVOC dataset has scene aggregation\ncharacteristics. By exploiting these properties, we propose Scene Clustering\nBased Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to\nemploy in post-processing. The SCP-Label brings greater accuracy by assigning\nthe same label to objects within the same scene while also mitigating bias and\nconfusion with model ensembles. Its performance surpasses the official baseline\nby a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on\nTrack 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.\nFinally, we win the championship both on Track1 and Track2 in the CVPR 2022\nPerception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our\ncode is available at https://github.com/HowieChangchn/SCP-Label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shenshen Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANUBIS: Skeleton Action Recognition Dataset, Review, and Benchmark. (arXiv:2205.02071v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02071","description":"<p>Skeleton-based action recognition, as a subarea of action recognition, is\nswiftly accumulating attention and popularity. The task is to recognize actions\nperformed by human articulation points. Compared with other data modalities, 3D\nhuman skeleton representations have extensive unique desirable characteristics,\nincluding succinctness, robustness, racial-impartiality, and many more. We aim\nto provide a roadmap for new and existing researchers a on the landscapes of\nskeleton-based action recognition for new and existing researchers. To this\nend, we present a review in the form of a taxonomy on existing works of\nskeleton-based action recognition. We partition them into four major\ncategories: (1) datasets; (2) extracting spatial features; (3) capturing\ntemporal patterns; (4) improving signal quality. For each method, we provide\nconcise yet informatively-sufficient descriptions. To promote more fair and\ncomprehensive evaluation on existing approaches of skeleton-based action\nrecognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared\nwith previously collected dataset, ANUBIS are advantageous in the following\nfour aspects: (1) employing more recently released sensors; (2) containing\nnovel back view; (3) encouraging high enthusiasm of subjects; (4) including\nactions of the COVID pandemic era. Using ANUBIS, we comparably benchmark\nperformance of current skeleton-based action recognizers. At the end of this\npaper, we outlook future development of skeleton-based action recognition by\nlisting several new technical problems. We believe they are valuable to solve\nin order to commercialize skeleton-based action recognition in the near future.\nThe dataset of ANUBIS is available at:\n<a href=\"http://hcc-workshop.anu.edu.au/webs/anu101/home.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Madhawa Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnrealNAS: Can We Search Neural Architectures with Unreal Data?. (arXiv:2205.02162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02162","description":"<p>Neural architecture search (NAS) has shown great success in the automatic\ndesign of deep neural networks (DNNs). However, the best way to use data to\nsearch network architectures is still unclear and under exploration. Previous\nwork has analyzed the necessity of having ground-truth labels in NAS and\ninspired broad interest. In this work, we take a further step to question\nwhether real data is necessary for NAS to be effective. The answer to this\nquestion is important for applications with limited amount of accessible data,\nand can help people improve NAS by leveraging the extra flexibility of data\ngeneration. To explore if NAS needs real data, we construct three types of\nunreal datasets using: 1) randomly labeled real images; 2) generated images and\nlabels; and 3) generated Gaussian noise with random labels. These datasets\nfacilitate to analyze the generalization and expressivity of the searched\narchitectures. We study the performance of architectures searched on these\nconstructed datasets using popular differentiable NAS methods. Extensive\nexperiments on CIFAR, ImageNet and CheXpert show that the searched\narchitectures can achieve promising results compared with those derived from\nthe conventional NAS pipeline with real labeled data, suggesting the\nfeasibility of performing NAS with unreal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaicheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image. (arXiv:1909.01106v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1909.01106","description":"<p>We propose a novel model for 3D semantic completion from a single depth\nimage, based on a single encoder and three separate generators used to\nreconstruct different geometric and semantic representations of the original\nand completed scene, all sharing the same latent space. To transfer information\nbetween the geometric and semantic branches of the network, we introduce paths\nbetween them concatenating features at corresponding network layers. Motivated\nby the limited amount of training samples from real scenes, an interesting\nattribute of our architecture is the capacity to supplement the existing\ndataset by generating a new training dataset with high quality, realistic\nscenes that even includes occlusion and real noise. We build the new dataset by\nsampling the features directly from latent space which generates a pair of\npartial volumetric surface and completed volumetric semantic surface. Moreover,\nwe utilize multiple discriminators to increase the accuracy and realism of the\nreconstructions. We demonstrate the benefits of our approach on standard\nbenchmarks for the two most common completion tasks: semantic 3D scene\ncompletion and 3D object completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">David Joseph Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}