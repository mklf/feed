{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How do lexical semantics affect translation? An empirical study. (arXiv:2201.00075v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00075","description":"<p>Neural machine translation (NMT) systems aim to map text from one language\ninto another. While there are a wide variety of applications of NMT, one of the\nmost important is translation of natural language. A distinguishing factor of\nnatural language is that words are typically ordered according to the rules of\nthe grammar of a given language. Although many advances have been made in\ndeveloping NMT systems for translating natural language, little research has\nbeen done on understanding how the word ordering of and lexical similarity\nbetween the source and target language affect translation performance. Here, we\ninvestigate these relationships on a variety of low-resource language pairs\nfrom the OpenSubtitles2016 database, where the source language is English, and\nfind that the more similar the target language is to English, the greater the\ntranslation performance. In addition, we study the impact of providing NMT\nmodels with part of speech of words (POS) in the English sequence and find\nthat, for Transformer-based models, the more dissimilar the target language is\nfrom English, the greater the benefit provided by POS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Vivek Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_D/0/1/0/all/0/1\">Dhanasekar Sundararaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Fake News Detection using cross-checking with reliable sources. (arXiv:2201.00083v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00083","description":"<p>Over the past decade, fake news and misinformation have turned into a major\nproblem that has impacted different aspects of our lives, including politics\nand public health. Inspired by natural human behavior, we present an approach\nthat automates the detection of fake news. Natural human behavior is to\ncross-check new information with reliable sources. We use Natural Language\nProcessing (NLP) and build a machine learning (ML) model that automates the\nprocess of cross-checking new information with a set of predefined reliable\nsources. We implement this for Twitter and build a model that flags fake\ntweets. Specifically, for a given tweet, we use its text to find relevant news\nfrom reliable news agencies. We then train a Random Forest model that checks if\nthe textual content of the tweet is aligned with the trusted news. If it is\nnot, the tweet is classified as fake. This approach can be generally applied to\nany kind of information and is not limited to a specific news story or a\ncategory of information. Our implementation of this approach gives a $70\\%$\naccuracy which outperforms other generic fake-news classification models. These\nresults pave the way towards a more sensible and natural approach to fake news\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_Z/0/1/0/all/0/1\">Zahra Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_M/0/1/0/all/0/1\">Milad Ranjbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanbarnejad_F/0/1/0/all/0/1\">Fakhteh Ghanbarnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raeisi_S/0/1/0/all/0/1\">Sadegh Raeisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Search for Large Scale Clinical Ontologies. (arXiv:2201.00118v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00118","description":"<p>Finding concepts in large clinical ontologies can be challenging when queries\nuse different vocabularies. A search algorithm that overcomes this problem is\nuseful in applications such as concept normalisation and ontology matching,\nwhere concepts can be referred to in different ways, using different synonyms.\nIn this paper, we present a deep learning based approach to build a semantic\nsearch system for large clinical ontologies. We propose a Triplet-BERT model\nand a method that generates training data directly from the ontologies. The\nmodel is evaluated using five real benchmark data sets and the results show\nthat our approach achieves high results on both free text to concept and\nconcept to concept searching tasks, and outperforms all baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Duy-Hoa Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_M/0/1/0/all/0/1\">Madonna Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truran_D/0/1/0/all/0/1\">Donna Truran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1\">Bevan Koopman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metke_Jimenez_A/0/1/0/all/0/1\">Alejandro Metke-Jimenez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization. (arXiv:2201.00136v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00136","description":"<p>Commonsense question answering (CQA) aims to test if models can answer\nquestions regarding commonsense knowledge that everyone knows. Prior works that\nincorporate external knowledge bases have shown promising results, but\nknowledge bases are expensive to construct and are often limited to a fixed set\nof relations. In this paper, we instead focus on better utilizing the\n\\textit{implicit knowledge} stored in pre-trained language models. While\nresearchers have found that the knowledge embedded in pre-trained language\nmodels can be extracted by having them fill in the blanks of carefully designed\nprompts for relation extraction and text classification, it remains unclear if\nwe can adopt this paradigm in CQA where the inputs and outputs take much more\nflexible forms. To this end, we investigate four translation methods that can\ntranslate natural questions into cloze-style sentences to better solicit\ncommonsense knowledge from language models, including a syntactic-based model,\nan unsupervised neural model, and two supervised neural models. In addition, to\ncombine the different translation methods, we propose to encourage consistency\namong model predictions on different translated questions with unlabeled data.\nWe demonstrate the effectiveness of our methods on three CQA datasets in\nzero-shot settings. We show that our methods are complementary to a knowledge\nbase improved model, and combining them can lead to state-of-the-art zero-shot\nperformance. Analyses also reveal distinct characteristics of the different\ncloze translation methods and provide insights on why combining them can lead\nto great improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges of sampling and how phylogenetic comparative methods help: With a case study of the Pama-Nyungan laminal contrast. (arXiv:2201.00195v1 [q-bio.PE])","link":"http://arxiv.org/abs/2201.00195","description":"<p>Phylogenetic comparative methods are new in our field and are shrouded, for\nmost linguists, in at least a little mystery. Yet the path that led to their\ndiscovery in comparative biology is so similar to the methodological history of\nbalanced sampling, that it is only an accident of history that they were not\ndiscovered by a typologist. Here we clarify the essential logic behind\nphylogenetic comparative methods and their fundamental relatedness to a deep\nintellectual tradition focussed on sampling. Then we introduce concepts,\nmethods and tools which will enable typologists to use these methods in\neveryday typological research. The key commonality of phylogenetic comparative\nmethods and balanced sampling is that they attempt to deal with statistical\nnon-independence due to genealogy. Whereas sampling can never achieve\nindependence and requires most comparative data to be discarded, phylogenetic\ncomparative methods achieve independence while retaining and using all data. We\ndiscuss the essential notions of phylogenetic signal; uncertainty about trees;\ntypological averages and proportions that are sensitive to genealogy;\ncomparison across language families; and the effects of areality. Extensive\nsupplementary materials illustrate computational tools for practical analysis\nand we illustrate the methods discussed with a typological case study of the\nlaminal contrast in Pama-Nyungan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Macklin_Cordes_J/0/1/0/all/0/1\">Jayden L. Macklin-Cordes</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Round_E/0/1/0/all/0/1\">Erich R. Round</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informed Multi-context Entity Alignment. (arXiv:2201.00304v1 [cs.AI])","link":"http://arxiv.org/abs/2201.00304","description":"<p>Entity alignment is a crucial step in integrating knowledge graphs (KGs) from\nmultiple sources. Previous attempts at entity alignment have explored different\nKG structures, such as neighborhood-based and path-based contexts, to learn\nentity embeddings, but they are limited in capturing the multi-context\nfeatures. Moreover, most approaches directly utilize the embedding similarity\nto determine entity alignment without considering the global interaction among\nentities and relations. In this work, we propose an Informed Multi-context\nEntity Alignment (IMEA) model to address these issues. In particular, we\nintroduce Transformer to flexibly capture the relation, path, and neighborhood\ncontexts, and design holistic reasoning to estimate alignment probabilities\nbased on both embedding similarity and the relation/entity functionality. The\nalignment evidence obtained from holistic reasoning is further injected back\ninto the Transformer via the proposed soft label editing to inform embedding\nlearning. Experimental results on several benchmark datasets demonstrate the\nsuperiority of our IMEA model compared with existing state-of-the-art entity\nalignment methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_K/0/1/0/all/0/1\">Kexuan Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaofang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (arXiv:2201.00318v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00318","description":"<p>Text classification is a fundamental Natural Language Processing task that\nhas a wide variety of applications, where deep learning approaches have\nproduced state-of-the-art results. While these models have been heavily\ncriticized for their black-box nature, their robustness to slight perturbations\nin input text has been a matter of concern. In this work, we carry out a\ndata-focused study evaluating the impact of systematic practical perturbations\non the performance of the deep learning based text classification models like\nCNN, LSTM, and BERT-based algorithms. The perturbations are induced by the\naddition and removal of unwanted tokens like punctuation and stop-words that\nare minimally associated with the final performance of the model. We show that\nthese deep learning approaches including BERT are sensitive to such legitimate\ninput perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,\nand tweet_eval. We observe that BERT is more susceptible to the removal of\ntokens as compared to the addition of tokens. Moreover, LSTM is slightly more\nsensitive to input perturbations as compared to CNN based model. The work also\nserves as a practical guide to assessing the impact of discrepancies in\ntrain-test conditions on the final performance of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miyajiwala_A/0/1/0/all/0/1\">Aamir Miyajiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladkat_A/0/1/0/all/0/1\">Arnav Ladkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadale_S/0/1/0/all/0/1\">Samiksha Jagadale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Establishing Strong Baselines for TripClick Health Retrieval. (arXiv:2201.00365v1 [cs.IR])","link":"http://arxiv.org/abs/2201.00365","description":"<p>We present strong Transformer-based re-ranking and dense retrieval baselines\nfor the recently released TripClick health ad-hoc retrieval collection. We\nimprove the - originally too noisy - training data with a simple negative\nsampling policy. We achieve large gains over BM25 in the re-ranking task of\nTripClick, which were not achieved with the original baselines. Furthermore, we\nstudy the impact of different domain-specific pre-trained models on TripClick.\nFinally, we show that dense retrieval outperforms BM25 by considerable margins,\neven with simple training procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1\">Sophia Althammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertkan_M/0/1/0/all/0/1\">Mete Sertkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topical Classification of Food Safety Publications with a Knowledge Base. (arXiv:2201.00374v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00374","description":"<p>The vast body of scientific publications presents an increasing challenge of\nfinding those that are relevant to a given research question, and making\ninformed decisions on their basis. This becomes extremely difficult without the\nuse of automated tools. Here, one possible area for improvement is automatic\nclassification of publication abstracts according to their topic. This work\nintroduces a novel, knowledge base-oriented publication classifier. The\nproposed method focuses on achieving scalability and easy adaptability to other\ndomains. Classification speed and accuracy are shown to be satisfactory, in the\nvery demanding field of food safety. Further development and evaluation of the\nmethod is needed, as the proposed approach shows much potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sowinski_P/0/1/0/all/0/1\">Piotr Sowi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasielewska_Michniewska_K/0/1/0/all/0/1\">Katarzyna Wasielewska-Michniewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actor-Critic Network for Q&A in an Adversarial Environment. (arXiv:2201.00455v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00455","description":"<p>Significant work has been placed in the Q&amp;A NLP space to build models that\nare more robust to adversarial attacks. Two key areas of focus are in\ngenerating adversarial data for the purposes of training against these\nsituations or modifying existing architectures to build robustness within. This\npaper introduces an approach that joins these two ideas together to train a\ncritic model for use in an almost reinforcement learning framework. Using the\nAdversarial SQuAD \"Add One Sent\" dataset we show that there are some promising\nsigns for this method in protecting against Adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_B/0/1/0/all/0/1\">Bejan Sadeghian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Latent Structures in Natural Language Processing: A Survey. (arXiv:2201.00490v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00490","description":"<p>While end-to-end learning with fully differentiable models has enabled\ntremendous success in natural language process (NLP) and machine learning,\nthere have been significant recent interests in learning with latent discrete\nstructures to incorporate better inductive biases for improved end-task\nperformance and better interpretability. This paradigm, however, is not\nstraightforwardly amenable to the mainstream gradient-based optimization\nmethods. This work surveys three main families of methods to learn such models:\nsurrogate gradients, continuous relaxation, and marginal likelihood\nmaximization via sampling. We conclude with a review of applications of these\nmethods and an inspection of the learned latent structure that they induce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models. (arXiv:2201.00558v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00558","description":"<p>We perform knowledge distillation (KD) benchmark from task-specific BERT-base\nteacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,\nand BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text\nclassification and sequence labeling in the Indonesian language. We also\ncompare various aspects of distillations including the usage of word embeddings\nand unlabeled data augmentation. Our experiments show that, despite the rising\npopularity of Transformer-based models, using BiLSTM and CNN student models\nprovide the best trade-off between performance and computational resource (CPU,\nRAM, and storage) compared to pruned BERT models. We further propose some quick\nwins on performing KD to produce small NLP models via efficient KD training\nmechanisms involving simple choices of loss functions, word embeddings, and\nunlabeled data preparation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1\">Made Nindyatama Nityasya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wibowo_H/0/1/0/all/0/1\">Haryo Akbarianto Wibowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevi_R/0/1/0/all/0/1\">Rendi Chevi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toxicity Detection for Indic Multilingual Social Media Content. (arXiv:2201.00598v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00598","description":"<p>Toxic content is one of the most critical issues for social media platforms\ntoday. India alone had 518 million social media users in 2020. In order to\nprovide a good experience to content creators and their audience, it is crucial\nto flag toxic comments and the users who post that. But the big challenge is\nidentifying toxicity in low resource Indic languages because of the presence of\nmultiple representations of the same text. Moreover, the posts/comments on\nsocial media do not adhere to a particular format, grammar or sentence\nstructure; this makes the task of abuse detection even more challenging for\nmultilingual social media platforms. This paper describes the system proposed\nby team 'Moj Masti' using the data provided by ShareChat/Moj in \\emph{IIIT-D\nMultilingual Abusive Comment Identification} challenge. We focus on how we can\nleverage multilingual transformer based pre-trained and fine-tuned models to\napproach code-mixed/code-switched classification tasks. Our best performing\nsystem was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score\nof 0.9 on the test data/leaderboard. We also observed an increase in the\nperformance by adding transliterated data. Furthermore, using weak metadata,\nensembling and some post-processing techniques boosted the performance of our\nsystem, thereby placing us 1st on the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhaveri_M/0/1/0/all/0/1\">Manan Jhaveri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaiya_D/0/1/0/all/0/1\">Devanshu Ramaiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Stance Detection of Tweets Via Distant Network Supervision. (arXiv:2201.00614v1 [cs.SI])","link":"http://arxiv.org/abs/2201.00614","description":"<p>Detecting and labeling stance in social media text is strongly motivated by\nhate speech detection, poll prediction, engagement forecasting, and concerted\npropaganda detection. Today's best neural stance detectors need large volumes\nof training data, which is difficult to curate given the fast-changing\nlandscape of social media text and issues on which users opine. Homophily\nproperties over the social network provide strong signal of coarse-grained\nuser-level stance. But semi-supervised approaches for tweet-level stance\ndetection fail to properly leverage homophily. In light of this, We present\nSANDS, a new semi-supervised stance detector. SANDS starts from very few\nlabeled tweets. It builds multiple deep feature views of tweets. It also uses a\ndistant supervision signal from the social network to provide a surrogate loss\nsignal to the component learners. We prepare two new tweet datasets comprising\nover 236,000 politically tinted tweets from two demographics (US and India)\nposted by over 87,000 users, their follower-followee graph, and over 8,000\ntweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)\non US (India)-based datasets, outperforming 17 baselines (including variants of\nSANDS) substantially, particularly for minority stance labels and noisy text.\nNumerous ablation experiments on SANDS disentangle the dynamics of textual and\nnetwork-propagated stance signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caur_S/0/1/0/all/0/1\">Samiya Caur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v1 [cs.IR])","link":"http://arxiv.org/abs/2201.00693","description":"<p>To enhance research on multimodal knowledge base and multimodal information\nprocessing, we propose a new task called multimodal entity tagging (MET) with a\nmultimodal knowledge base (MKB). We also develop a dataset for the problem\nusing an existing MKB. In an MKB, there are entities and their associated texts\nand images. In MET, given a text-image pair, one uses the information in the\nMKB to automatically identify the related entity in the text-image pair. We\nsolve the task by using the information retrieval paradigm and implement\nseveral baselines using state-of-the-art methods in NLP and CV. We conduct\nextensive experiments and make analyses on the experimental results. The\nresults show that the task is challenging, but current technologies can achieve\nrelatively high performance. We will release the dataset, code, and models for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chao Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions. (arXiv:2201.00768v1 [cs.CL])","link":"http://arxiv.org/abs/2201.00768","description":"<p>Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1\">Marwan Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Soohyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyang_D/0/1/0/all/0/1\">DaeHun Nyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohaisen_D/0/1/0/all/0/1\">David Mohaisen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods. (arXiv:1907.09358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.09358","description":"<p>Interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as machine\nlearning, computer vision, and natural language processing. Much of the growth\nin these fields has been made possible with deep learning, a sub-area of\nmachine learning that uses artificial neural networks. This has created\nsignificant interest in the integration of vision and language. In this survey,\nwe focus on ten prominent tasks that integrate language and vision by\ndiscussing their problem formulation, methods, existing datasets, evaluation\nmeasures, and compare the results obtained with corresponding state-of-the-art\nmethods. Our efforts go beyond earlier surveys which are either task-specific\nor concentrate only on one type of visual content, i.e., image or video.\nFurthermore, we also provide some potential future directions in this field of\nresearch with an anticipation that this survey stimulates innovative thoughts\nand ideas to address the existing challenges and build new applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mogadala_A/0/1/0/all/0/1\">Aditya Mogadala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalimuthu_M/0/1/0/all/0/1\">Marimuthu Kalimuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement. (arXiv:2003.13917v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2003.13917","description":"<p>Recent studies have highlighted adversarial examples as ubiquitous threats to\nthe deep neural network (DNN) based speech recognition systems. In this work,\nwe present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial\nspeech signals. Specifically, we evaluate the model performance by\ninterpretable speech recognition metrics and discuss the model performance by\nthe augmented adversarial training. Our experiments show that our proposed\nU-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from\n1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term\nobjective intelligibility (STOI) from 0.83 to 0.96 on the task of speech\nenhancement with adversarial speech examples. We conduct experiments on the\nautomatic speech recognition (ASR) task with adversarial audio attacks. We find\nthat (i) temporal features learned by the attention network are capable of\nenhancing the robustness of DNN based ASR models; (ii) the generalization power\nof DNN based ASR model could be enhanced by applying adversarial training with\nan additive adversarial data augmentation. The ASR metric on word-error-rates\n(WERs) shows that there is an absolute 2.22 $\\%$ decrease under gradient-based\nperturbation, and an absolute 2.03 $\\%$ decrease, under evolutionary-optimized\nperturbation, which suggests that our enhancement models with adversarial\ntraining can further secure a resilient ASR system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoli Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-Wise Multi-View Decoding for Improved Natural Language Generation. (arXiv:2005.08081v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.08081","description":"<p>In sequence-to-sequence learning, e.g., natural language generation, the\ndecoder relies on the attention mechanism to efficiently extract information\nfrom the encoder. While it is common practice to draw information from only the\nlast encoder layer, recent work has proposed to use representations from\ndifferent encoder layers for diversified levels of information. Nonetheless,\nthe decoder still obtains only a single view of the source sequences, which\nmight lead to insufficient training of the encoder layer stack due to the\nhierarchy bypassing problem. In this work, we propose layer-wise multi-view\ndecoding, where for each decoder layer, together with the representations from\nthe last encoder layer, which serve as a global view, those from other encoder\nlayers are supplemented for a stereoscopic view of the source sequences.\nSystematic experiments and analyses show that we successfully address the\nhierarchy bypassing problem, require almost negligible parameter increase, and\nsubstantially improve the performance of sequence-to-sequence learning with\ndeep representations on five diverse tasks, i.e., machine translation,\nabstractive summarization, image captioning, video captioning, and medical\nreport generation. In particular, our approach achieves new state-of-the-art\nresults on eight benchmark datasets, including a low-resource machine\ntranslation dataset and two low-resource medical report generation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Rediscovery Hypothesis: Language Models Need to Meet Linguistics. (arXiv:2103.01819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01819","description":"<p>There is an ongoing debate in the NLP community whether modern language\nmodels contain linguistic knowledge, recovered through so-called probes. In\nthis paper, we study whether linguistic knowledge is a necessary condition for\nthe good performance of modern language models, which we call the\n\\textit{rediscovery hypothesis}. In the first place, we show that language\nmodels that are significantly compressed but perform well on their pretraining\nobjectives retain good scores when probed for linguistic structures. This\nresult supports the rediscovery hypothesis and leads to the second contribution\nof our paper: an information-theoretic framework that relates language modeling\nobjectives with linguistic information. This framework also provides a metric\nto measure the impact of linguistic information on the word prediction task. We\nreinforce our analytical results with various experiments, both on synthetic\nand on real NLP tasks in English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1\">Maxat Tezekbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhakhmet_N/0/1/0/all/0/1\">Nuradil Kozhakhmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babazhanova_M/0/1/0/all/0/1\">Madina Babazhanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1\">Zhenisbek Assylbekov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03861","description":"<p>Identifying political perspectives in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\npolitical ideologies. Previous approaches focus on textual content and leave\nout the rich social and political context that is essential in the argument\nmining process. To address this limitation, we propose a political perspective\ndetection method that incorporates external domain knowledge. Specifically, we\nconstruct a political knowledge graph to serve as domain-specific external\nknowledge. We then leverage heterogeneous information networks to represent\nnews documents, which jointly model news text and external knowledge. Finally,\nwe adopt relational graph neural networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments demonstrate that\nour method consistently achieves the best performance on two real-world\nperspective detection benchmarks. Ablation studies further bear out the\nnecessity of external knowledge and the effectiveness of our graph-based\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legislator Representation Learning with Social Context and Expert Knowledge. (arXiv:2108.03881v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03881","description":"<p>Modeling the ideological perspectives of political actors is an essential\ntask in computational political science with applications in many downstream\ntasks. Existing approaches are generally limited to textual data and voting\nrecords, while they neglect the rich social context and valuable expert\nknowledge for holistic evaluation. In this paper, we propose a representation\nlearning framework of political actors that jointly leverages social context\nand expert knowledge. Specifically, we retrieve and extract factual statements\nabout legislators to leverage social context information. We then construct a\nheterogeneous information network to incorporate social context and use\nrelational graph neural networks to learn legislator representations. Finally,\nwe train our model with three objectives to align representation learning with\nexpert knowledge, model ideological stance consistency, and simulate the echo\nchamber phenomenon. Extensive experiments demonstrate that our learned\nrepresentations successfully advance the state-of-the-art in three downstream\ntasks. Further analysis proves the correlation between learned legislator\nrepresentations and various socio-political factors, as well as bearing out the\nnecessity of social context and expert knowledge in modeling political actors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v7 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique or doing this is to apply named entity\ndisambiguation (NED) systems to the question, and retrieve KB facts for the\ndisambiguated entities. This work presents CLOCQ, an efficient method that\nprunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses\na top-k query processor over score-ordered lists of KB items that combine\nsignals about lexical matching, relevance to the question, coherence among\ncandidate items, and connectivity in the KB graph. Experiments with two recent\nQA benchmarks for complex questions demonstrate the superiority of CLOCQ over\nstate-of-the-art baselines with respect to answer presence, size of the search\nspace, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese. (arXiv:2109.09701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09701","description":"<p>In this paper, we present BARTpho with two versions BARTpho-syllable and\nBARTpho-word, which are the first public large-scale monolingual\nsequence-to-sequence models pre-trained for Vietnamese. BARTpho uses the\n\"large\" architecture and the pre-training scheme of the sequence-to-sequence\ndenoising autoencoder BART, thus especially suitable for generative NLP tasks.\nWe conduct experiments to compare our BARTpho with its competitor mBART on a\ndownstream task of Vietnamese text summarization and show that: in both\nautomatic and human evaluations, BARTpho outperforms the strong baseline mBART\nand improves the state-of-the-art. We release BARTpho to facilitate future\nresearch and applications of generative Vietnamese NLP tasks. Our BARTpho\nmodels are publicly available at: https://github.com/VinAIResearch/BARTpho\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nguyen Luong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duong Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Vietnamese Conversations From Facebook Page To Build Training Dataset For Chatbot. (arXiv:2112.15338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15338","description":"<p>The biggest challenge of building chatbots is training data. The required\ndata must be realistic and large enough to train chatbots. We create a tool to\nget actual training data from Facebook messenger of a Facebook page. After text\npreprocessing steps, the newly obtained dataset generates FVnC and Sample\ndataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract\nfeatures of our text data. K-Means and DBSCAN clustering algorithms are used\nfor clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply\nV-measure score and Silhouette score to evaluate the performance of clustering\nalgorithms. We also demonstrate the efficiency of PhoBERT compared to other\nmodels in feature extraction on the Sample dataset and wiki dataset. A\nGridSearch algorithm that combines both clustering evaluations is also proposed\nto find optimal parameters. Thanks to clustering such a number of\nconversations, we save a lot of time and effort to build data and storylines\nfor training chatbot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Trieu Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thi-Kim-Ngoan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Thi-Hong-Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Quynh-Chau Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15417","description":"<p>Due to the exponentially increasing reach of social media, it is essential to\nfocus on its negative aspects as it can potentially divide society and incite\npeople into violence. In this paper, we present our system description of work\non the shared task ComMA@ICON, where we have to classify how aggressive the\nsentence is and if the sentence is gender-biased or communal biased. These\nthree could be the primary reasons to cause significant problems in society. As\nteam Hypers we have proposed an approach that utilizes different pretrained\nmodels with Attention and mean pooling methods. We were able to get Rank 3 with\n0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on\nMulti-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5\nwith 0.336 Instance F1 score on Hindi. The source code and the pretrained\nmodels of this work can be found here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Roshan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navaneethakrishnan_S/0/1/0/all/0/1\">Subalalitha Chinnaudayar Navaneethakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi6_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi6</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Using Gaze Behaviour for Natural Language Processing. (arXiv:2112.15471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15471","description":"<p>Gaze behaviour has been used as a way to gather cognitive information for a\nnumber of years. In this paper, we discuss the use of gaze behaviour in solving\ndifferent tasks in natural language processing (NLP) without having to record\nit at test time. This is because the collection of gaze behaviour is a costly\ntask, both in terms of time and money. Hence, in this paper, we focus on\nresearch done to alleviate the need for recording gaze behaviour at run time.\nWe also mention different eye tracking corpora in multiple languages, which are\ncurrently available and can be used in natural language processing. We conclude\nour paper by discussing applications in a domain - education - and how learning\ngaze behaviour can help in solving the tasks of complex word identification and\nautomatic essay grading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathias_S/0/1/0/all/0/1\">Sandeep Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhijit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multi-Dimensional Model Compression of Vision Transformer. (arXiv:2201.00043v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00043","description":"<p>Vision transformers (ViT) have recently attracted considerable attentions,\nbut the huge computational cost remains an issue for practical deployment.\nPrevious ViT pruning methods tend to prune the model along one dimension\nsolely, which may suffer from excessive reduction and lead to sub-optimal model\nquality. In contrast, we advocate a multi-dimensional ViT compression paradigm,\nand propose to harness the redundancy reduction from attention head, neuron and\nsequence dimensions jointly. We firstly propose a statistical dependence based\npruning criterion that is generalizable to different dimensions for identifying\ndeleterious components. Moreover, we cast the multi-dimensional compression as\nan optimization, learning the optimal pruning policy across the three\ndimensions that maximizes the compressed model's accuracy under a computational\nbudget. The problem is solved by our adapted Gaussian process search with\nexpected improvement. Experimental results show that our method effectively\nreduces the computational cost of various ViT models. For example, our method\nreduces 40\\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,\noutperforming previous state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iCaps: Iterative Category-level Object Pose and Shape Estimation. (arXiv:2201.00059v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00059","description":"<p>This paper proposes a category-level 6D object pose and shape estimation\napproach iCaps, which allows tracking 6D poses of unseen objects in a category\nand estimating their 3D shapes. We develop a category-level auto-encoder\nnetwork using depth images as input, where feature embeddings from the\nauto-encoder encode poses of objects in a category. The auto-encoder can be\nused in a particle filter framework to estimate and track 6D poses of objects\nin a category. By exploiting an implicit shape representation based on signed\ndistance functions, we build a LatentNet to estimate a latent representation of\nthe 3D shape given the estimated pose of an object. Then the estimated pose and\nshape can be used to update each other in an iterative way. Our category-level\n6D object pose and shape estimation pipeline only requires 2D detection and\nsegmentation for initialization. We evaluate our approach on a publicly\navailable dataset and demonstrate its effectiveness. In particular, our method\nachieves comparably high accuracy on shape estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xinke Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1\">Junyi Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bretl_T/0/1/0/all/0/1\">Timothy Bretl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems. (arXiv:2201.00063v1 [eess.SY])","link":"http://arxiv.org/abs/2201.00063","description":"<p>Emerging edge applications require both a fast response latency and complex\nprocessing. This is infeasible without expensive hardware that can process\ncomplex operations -- such as object detection -- within a short time. Many\napproach this problem by addressing the complexity of the models -- via model\ncompression, pruning and quantization -- or compressing the input. In this\npaper, we propose a different perspective when addressing the performance\nchallenges. Croesus is a multi-stage approach to edge-cloud systems that\nprovides the ability to find the balance between accuracy and performance.\nCroesus consists of two stages (that can be generalized to multiple stages): an\ninitial and a final stage. The initial stage performs the computation in\nreal-time using approximate/best-effort computation at the edge. The final\nstage performs the full computation at the cloud, and uses the results to\ncorrect any errors made at the initial stage. In this paper, we demonstrate the\nimplications of such an approach on a video analytics use-case and show how\nmulti-stage processing yields a better balance between accuracy and\nperformance. Moreover, we study the safety of multi-stage transactions via two\nproposals: multi-stage serializability (MS-SR) and multi-stage invariant\nconfluence with Apologies (MS-IA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gazzaz_S/0/1/0/all/0/1\">Samaa Gazzaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_V/0/1/0/all/0/1\">Vishal Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nawab_F/0/1/0/all/0/1\">Faisal Nawab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchTrack: Multiple Object Tracking Using Frame Patches. (arXiv:2201.00080v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00080","description":"<p>Object motion and object appearance are commonly used information in multiple\nobject tracking (MOT) applications, either for associating detections across\nframes in tracking-by-detection methods or direct track predictions for\njoint-detection-and-tracking methods. However, not only are these two types of\ninformation often considered separately, but also they do not help optimize the\nusage of visual information from the current frame of interest directly. In\nthis paper, we present PatchTrack, a Transformer-based\njoint-detection-and-tracking system that predicts tracks using patches of the\ncurrent frame of interest. We use the Kalman filter to predict the locations of\nexisting tracks in the current frame from the previous frame. Patches cropped\nfrom the predicted bounding boxes are sent to the Transformer decoder to infer\nnew tracks. By utilizing both object motion and object appearance information\nencoded in patches, the proposed method pays more attention to where new tracks\nare more likely to occur. We show the effectiveness of PatchTrack on recent MOT\nbenchmarks, including MOT16 (MOTA 73.71%, IDF1 65.77%) and MOT17 (MOTA 73.59%,\nIDF1 65.23%). The results are published on\nhttps://motchallenge.net/method/MOT=4725&amp;chl=10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranmanesh_S/0/1/0/all/0/1\">Seyed Mehdi Iranmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lien_K/0/1/0/all/0/1\">Kuo-Chin Lien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging. (arXiv:2201.00084v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00084","description":"<p>Endoscopic images typically contain several artifacts. The artifacts\nsignificantly impact image analysis result in computer-aided diagnosis.\nConvolutional neural networks (CNNs), a type of deep learning, can removes such\nartifacts. Various architectures have been proposed for the CNNs, and the\naccuracy of artifact removal varies depending on the choice of architecture.\nTherefore, it is necessary to determine the artifact removal accuracy,\ndepending on the selected architecture. In this study, we focus on endoscopic\nsurgical instruments as artifacts, and determine and discuss the artifact\nremoval accuracy using seven different CNN architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_T/0/1/0/all/0/1\">Taira Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanioka_K/0/1/0/all/0/1\">Kensuke Tanioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hiwa_S/0/1/0/all/0/1\">Satoru Hiwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hiroyasu_T/0/1/0/all/0/1\">Tomoyuki Hiroyasu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision Based Parking Optimization System. (arXiv:2201.00095v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00095","description":"<p>An improvement in technology is linearly related to time and time-relevant\nproblems. It has been seen that as time progresses, the number of problems\nhumans face also increases. However, technology to resolve these problems tends\nto improve as well. One of the earliest existing problems which started with\nthe invention of vehicles was parking. The ease of resolving this problem using\ntechnology has evolved over the years but the problem of parking still remains\nunsolved. The main reason behind this is that parking does not only involve one\nproblem but it consists of a set of problems within itself. One of these\nproblems is the occupancy detection of the parking slots in a distributed\nparking ecosystem. In a distributed system, users would find preferable parking\nspaces as opposed to random parking spaces. In this paper, we propose a\nweb-based application as a solution for parking space detection in different\nparking spaces. The solution is based on Computer Vision (CV) and is built\nusing the Django framework written in Python 3.0. The solution works to resolve\nthe occupancy detection problem along with providing the user the option to\ndetermine the block based on availability and his preference. The evaluation\nresults for our proposed system are promising and efficient. The proposed\nsystem can also be integrated with different systems and be used for solving\nother relevant parking problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Siddharth Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reginald_J/0/1/0/all/0/1\">Jeffrey Matthew Reginald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Ting Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalyPath360: Saliency and Scanpath Prediction Framework for Omnidirectional Images. (arXiv:2201.00096v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00096","description":"<p>This paper introduces a new framework to predict visual attention of\nomnidirectional images. The key setup of our architecture is the simultaneous\nprediction of the saliency map and a corresponding scanpath for a given\nstimulus. The framework implements a fully encoder-decoder convolutional neural\nnetwork augmented by an attention module to generate representative saliency\nmaps. In addition, an auxiliary network is employed to generate probable\nviewport center fixation points through the SoftArgMax function. The latter\nallows to derive fixation points from feature maps. To take advantage of the\nscanpath prediction, an adaptive joint probability distribution model is then\napplied to construct the final unbiased saliency map by leveraging the encoder\ndecoder-based saliency map and the scanpath-based saliency heatmap. The\nproposed framework was evaluated in terms of saliency and scanpath prediction,\nand the results were compared to state-of-the-art methods on Salient360!\ndataset. The results showed the relevance of our framework and the benefits of\nsuch architecture for further omnidirectional visual attention prediction\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1\">Mohamed Amine Kerkouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1\">Marouane Tliba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeh_M/0/1/0/all/0/1\">Mohamed Sayeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack via Dual-Stage Network Erosion. (arXiv:2201.00097v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00097","description":"<p>Deep neural networks are vulnerable to adversarial examples, which can fool\ndeep models by adding subtle perturbations. Although existing attacks have\nachieved promising results, it still leaves a long way to go for generating\ntransferable adversarial examples under the black-box setting. To this end,\nthis paper proposes to improve the transferability of adversarial examples, and\napplies dual-stage feature-level perturbations to an existing model to\nimplicitly create a set of diverse models. Then these models are fused by the\nlongitudinal ensemble during the iterations. The proposed method is termed\nDual-Stage Network Erosion (DSNE). We conduct comprehensive experiments both on\nnon-residual and residual networks, and obtain more transferable adversarial\nexamples with the computational cost similar to the state-of-the-art method. In\nparticular, for the residual networks, the transferability of the adversarial\nexamples can be significantly improved by biasing the residual block\ninformation to the skip connections. Our work provides new insights into the\narchitectural vulnerability of neural networks and presents new challenges to\nthe robustness of neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yexin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junhua Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhisong Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images. (arXiv:2201.00100v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00100","description":"<p>Training deep models for RGB-D salient object detection (SOD) often requires\na large number of labeled RGB-D images. However, RGB-D data is not easily\nacquired, which limits the development of RGB-D SOD techniques. To alleviate\nthis issue, we present a Dual-Semi RGB-D Salient Object Detection Network\n(DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency\ndetection. We first devise a depth decoupling convolutional neural network\n(DDCNN), which contains a depth estimation branch and a saliency detection\nbranch. The depth estimation branch is trained with RGB-D images and then used\nto estimate the pseudo depth maps for all unlabeled RGB images to form the\npaired data. The saliency detection branch is used to fuse the RGB feature and\ndepth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned\nas the backbone in a teacher-student framework for semi-supervised learning.\nMoreover, we also introduce a consistency loss on the intermediate attention\nand saliency maps for the unlabeled data, as well as a supervised depth and\nsaliency loss for labeled data. Experimental results on seven widely-used\nbenchmark datasets demonstrate that our DDCNN outperforms state-of-the-art\nmethods both quantitatively and qualitatively. We also demonstrate that our\nsemi-supervised DS-Net can further improve the performance, even when using an\nRGB image with the pseudo depth map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Region Feature Synthesizer for Zero-Shot Object Detection. (arXiv:2201.00103v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00103","description":"<p>Zero-shot object detection aims at incorporating class semantic vectors to\nrealize the detection of (both seen and) unseen classes given an unconstrained\ntest image. In this study, we reveal the core challenges in this research area:\nhow to synthesize robust region features (for unseen objects) that are as\nintra-class diverse and inter-class separable as the real samples, so that\nstrong unseen object detectors can be trained upon them. To address these\nchallenges, we build a novel zero-shot object detection framework that contains\nan Intra-class Semantic Diverging component and an Inter-class Structure\nPreserving component. The former is used to realize the one-to-more mapping to\nobtain diverse visual features from each class semantic vector, preventing\nmiss-classifying the real unseen objects as image backgrounds. While the latter\nis used to avoid the synthesized features too scattered to mix up the\ninter-class and foreground-background relationship. To demonstrate the\neffectiveness of the proposed approach, comprehensive experiments on PASCAL\nVOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the\nnew state-of-the-art performance on PASCAL VOC and COCO and it is the first\nstudy to carry out zero-shot object detection in remote sensing imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Peiliang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">De Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality-aware Part Models for Occluded Person Re-identification. (arXiv:2201.00107v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00107","description":"<p>Occlusion poses a major challenge for person re-identification (ReID).\nExisting approaches typically rely on outside tools to infer visible body\nparts, which may be suboptimal in terms of both computational efficiency and\nReID accuracy. In particular, they may fail when facing complex occlusions,\nsuch as those between pedestrians. Accordingly, in this paper, we propose a\nnovel method named Quality-aware Part Models (QPM) for occlusion-robust ReID.\nFirst, we propose to jointly learn part features and predict part quality\nscores. As no quality annotation is available, we introduce a strategy that\nautomatically assigns low scores to occluded body parts, thereby weakening the\nimpact of occluded body parts on ReID results. Second, based on the predicted\npart quality scores, we propose a novel identity-aware spatial attention (ISA)\nmodule. In this module, a coarse identity-aware feature is utilized to\nhighlight pixels of the target pedestrian, so as to handle the occlusion\nbetween pedestrians. Third, we design an adaptive and efficient approach for\ngenerating global features from common non-occluded regions with respect to\neach image pair. This design is crucial, but is often ignored by existing\nmethods. QPM has three key advantages: 1) it does not rely on any outside tools\nin either the training or inference stages; 2) it handles occlusions caused by\nboth objects and other pedestrians;3) it is highly computationally efficient.\nExperimental results on four popular databases for occluded ReID demonstrate\nthat QPM consistently outperforms state-of-the-art methods by significant\nmargins. The code of QPM will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators. (arXiv:2201.00112v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00112","description":"<p>Recent advances in deep generative models have led to immense progress in 3D\nshape synthesis. While existing models are able to synthesize shapes\nrepresented as voxels, point-clouds, or implicit functions, these methods only\nindirectly enforce the plausibility of the final 3D shape surface. Here we\npresent a 3D shape synthesis framework (SurfGen) that directly applies\nadversarial training to the object surface. Our approach uses a differentiable\nspherical projection layer to capture and represent the explicit zero\nisosurface of an implicit 3D generator as functions defined on the unit sphere.\nBy processing the spherical representation of 3D object surfaces with a\nspherical CNN in an adversarial setting, our generator can better learn the\nstatistics of natural shape surfaces. We evaluate our model on large-scale\nshape datasets, and demonstrate that the end-to-end trained model is capable of\ngenerating high fidelity 3D shapes with diverse topology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Andrew Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tai Sing Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAFL: A Self-Attention Scene Text Recognizer with Focal Loss. (arXiv:2201.00132v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00132","description":"<p>In the last decades, scene text recognition has gained worldwide attention\nfrom both the academic community and actual users due to its importance in a\nwide range of applications. Despite achievements in optical character\nrecognition, scene text recognition remains challenging due to inherent\nproblems such as distortions or irregular layout. Most of the existing\napproaches mainly leverage recurrence or convolution-based neural networks.\nHowever, while recurrent neural networks (RNNs) usually suffer from slow\ntraining speed due to sequential computation and encounter problems as\nvanishing gradient or bottleneck, CNN endures a trade-off between complexity\nand performance. In this paper, we introduce SAFL, a self-attention-based\nneural network model with the focal loss for scene text recognition, to\novercome the limitation of the existing approaches. The use of focal loss\ninstead of negative log-likelihood helps the model focus more on low-frequency\nsamples training. Moreover, to deal with the distortions and irregular texts,\nwe exploit Spatial TransformerNetwork (STN) to rectify text before passing to\nthe recognition network. We perform experiments to compare the performance of\nthe proposed model with seven benchmarks. The numerical results show that our\nmodel achieves the best performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_B/0/1/0/all/0/1\">Bao Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Cong_T/0/1/0/all/0/1\">Thanh Le-Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Manh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phi Le Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Feature Uncertainty in Stochastic Neural Networks for Adversarial Robustness. (arXiv:2201.00148v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00148","description":"<p>It is well-known that deep neural networks (DNNs) have shown remarkable\nsuccess in many fields. However, when adding an imperceptible magnitude\nperturbation on the model input, the model performance might get rapid\ndecrease. To address this issue, a randomness technique has been proposed\nrecently, named Stochastic Neural Networks (SNNs). Specifically, SNNs inject\nrandomness into the model to defend against unseen attacks and improve the\nadversarial robustness. However, existed studies on SNNs mainly focus on\ninjecting fixed or learnable noises to model weights/activations. In this\npaper, we find that the existed SNNs performances are largely bottlenecked by\nthe feature representation ability. Surprisingly, simply maximizing the\nvariance per dimension of the feature distribution leads to a considerable\nboost beyond all previous methods, which we named maximize feature distribution\nvariance stochastic neural network (MFDV-SNN). Extensive experiments on\nwell-known white- and black-box attacks show that MFDV-SNN achieves a\nsignificant improvement over existing methods, which indicates that it is a\nsimple but effective method to improve model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Single Image Deblurring. (arXiv:2201.00155v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00155","description":"<p>This paper tackles the problem of dynamic scene deblurring. Although\nend-to-end fully convolutional designs have recently advanced the\nstate-of-the-art in non-uniform motion deblurring, their performance-complexity\ntrade-off is still sub-optimal. Existing approaches achieve a large receptive\nfield by a simple increment in the number of generic convolution layers,\nkernel-size, which comes with the burden of the increase in model size and\ninference speed. In this work, we propose an efficient pixel adaptive and\nfeature attentive design for handling large blur variations within and across\ndifferent images. We also propose an effective content-aware global-local\nfiltering module that significantly improves the performance by considering not\nonly the global dependencies of the pixel but also dynamically using the\nneighboring pixels. We use a patch hierarchical attentive architecture composed\nof the above module that implicitly discover the spatial variations in the blur\npresent in the input image and in turn perform local and global modulation of\nintermediate features. Extensive qualitative and quantitative comparisons with\nprior art on deblurring benchmarks demonstrate the superiority of the proposed\nnetwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Diabetic Foot Ulcer Datasets: An Overview. (arXiv:2201.00163v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00163","description":"<p>This paper provides conceptual foundation and procedures used in the\ndevelopment of diabetic foot ulcer datasets over the past decade, with a\ntimeline to demonstrate progress. We conduct a survey on data capturing methods\nfor foot photographs, an overview of research in developing private and public\ndatasets, the related computer vision tasks (detection, segmentation and\nclassification), the diabetic foot ulcer challenges and the future direction of\nthe development of the datasets. We report the distribution of dataset users by\ncountry and year. Our aim is to share the technical challenges that we\nencountered together with good practices in dataset development, and provide\nmotivation for other researchers to participate in data sharing in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kendrick_C/0/1/0/all/0/1\">Connah Kendrick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reeves_N/0/1/0/all/0/1\">Neil D. Reeves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyal_M/0/1/0/all/0/1\">Manu Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pappachan_J/0/1/0/all/0/1\">Joseph M. Pappachan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cassidy_B/0/1/0/all/0/1\">Bill Cassidy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-attention Multi-view Representation Learning with Diversity-promoting Complementarity. (arXiv:2201.00168v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00168","description":"<p>Multi-view learning attempts to generate a model with a better performance by\nexploiting the consensus and/or complementarity among multi-view data. However,\nin terms of complementarity, most existing approaches only can find\nrepresentations with single complementarity rather than complementary\ninformation with diversity. In this paper, to utilize both complementarity and\nconsistency simultaneously, give free rein to the potential of deep learning in\ngrasping diversity-promoting complementarity for multi-view representation\nlearning, we propose a novel supervised multi-view representation learning\nalgorithm, called Self-Attention Multi-View network with Diversity-Promoting\nComplementarity (SAMVDPC), which exploits the consistency by a group of\nencoders, uses self-attention to find complementary information entailing\ndiversity. Extensive experiments conducted on eight real-world datasets have\ndemonstrated the effectiveness of our proposed method, and show its superiority\nover several baseline methods, which only consider single complementary\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xi-hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xionglin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Scene Video Deblurring using Non-Local Attention. (arXiv:2201.00169v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00169","description":"<p>This paper tackles the challenging problem of video deblurring. Most of the\nexisting works depend on implicit or explicit alignment for temporal\ninformation fusion which either increase the computational cost or result in\nsuboptimal performance due to wrong alignment. In this study, we propose a\nfactorized spatio-temporal attention to perform non-local operations across\nspace and time to fully utilize the available information without depending on\nalignment. It shows superior performance compared to existing fusion techniques\nwhile being much efficient. Extensive experiments on multiple datasets\ndemonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Subspace Adaptive Learning via Autoencoder and Attention. (arXiv:2201.00171v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00171","description":"<p>Multi-view learning can cover all features of data samples more\ncomprehensively, so multi-view learning has attracted widespread attention.\nTraditional subspace clustering methods, such as sparse subspace clustering\n(SSC) and low-ranking subspace clustering (LRSC), cluster the affinity matrix\nfor a single view, thus ignoring the problem of fusion between views. In our\narticle, we propose a new Multiview Subspace Adaptive Learning based on\nAttention and Autoencoder (MSALAA). This method combines a deep autoencoder and\na method for aligning the self-representations of various views in Multi-view\nLow-Rank Sparse Subspace Clustering (MLRSSC), which can not only increase the\ncapability to non-linearity fitting, but also can meets the principles of\nconsistency and complementarity of multi-view learning. We empirically observe\nsignificant improvement over existing baseline methods on six real-life\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hao-jie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiong-lin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Image Inpainting. (arXiv:2201.00177v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00177","description":"<p>Image inpainting methods have shown significant improvements by using deep\nneural networks recently. However, many of these techniques often create\ndistorted structures or blurry textures inconsistent with surrounding areas.\nThe problem is rooted in the encoder layers' ineffectiveness in building a\ncomplete and faithful embedding of the missing regions. To address this\nproblem, two-stage approaches deploy two separate networks for a coarse and\nfine estimate of the inpainted image. Some approaches utilize handcrafted\nfeatures like edges or contours to guide the reconstruction process. These\nmethods suffer from huge computational overheads owing to multiple generator\nnetworks, limited ability of handcrafted features, and sub-optimal utilization\nof the information present in the ground truth. Motivated by these\nobservations, we propose a distillation based approach for inpainting, where we\nprovide direct feature level supervision for the encoder layers in an adaptive\nmanner. We deploy cross and self distillation techniques and discuss the need\nfor a dedicated completion-block in encoder to achieve the distillation target.\nWe conduct extensive evaluations on multiple datasets to validate our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Restoration using Feature-guidance. (arXiv:2201.00187v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00187","description":"<p>Image restoration is the task of recovering a clean image from a degraded\nversion. In most cases, the degradation is spatially varying, and it requires\nthe restoration network to both localize and restore the affected regions. In\nthis paper, we present a new approach suitable for handling the image-specific\nand spatially-varying nature of degradation in images affected by practically\noccurring artifacts such as blur, rain-streaks. We decompose the restoration\ntask into two stages of degradation localization and degraded region-guided\nrestoration, unlike existing methods which directly learn a mapping between the\ndegraded and clean images. Our premise is to use the auxiliary task of\ndegradation mask prediction to guide the restoration process. We demonstrate\nthat the model trained for this auxiliary task contains vital region knowledge,\nwhich can be exploited to guide the restoration network's training using\nattentive knowledge distillation technique. Further, we propose mask-guided\nconvolution and global context aggregation module that focuses solely on\nrestoring the degraded regions. The proposed approach's effectiveness is\ndemonstrated by achieving significant improvement over strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turath-150K: Image Database of Arab Heritage. (arXiv:2201.00220v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00220","description":"<p>Large-scale image databases remain largely biased towards objects and\nactivities encountered in a select few cultures. This absence of\nculturally-diverse images, which we refer to as the hidden tail, limits the\napplicability of pre-trained neural networks and inadvertently excludes\nresearchers from under-represented regions. To begin remedying this issue, we\ncurate Turath-150K, a database of images of the Arab world that reflect\nobjects, activities, and scenarios commonly found there. In the process, we\nintroduce three benchmark databases, Turath Standard, Art, and UNESCO,\nspecialised subsets of the Turath dataset. After demonstrating the limitations\nof existing networks pre-trained on ImageNet when deployed on such benchmarks,\nwe train and evaluate several networks on the task of image classification. As\na consequence of Turath, we hope to engage machine learning researchers in\nunder-represented regions, and to inspire the release of additional\nculture-focused databases. The database can be accessed here:\ndanikiyasseh.github.io/Turath.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiyasseh_D/0/1/0/all/0/1\">Dani Kiyasseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Bouri_R/0/1/0/all/0/1\">Rasheed El-Bouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Applications for Lung Cancer Diagnosis: A systematic review. (arXiv:2201.00227v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00227","description":"<p>Lung cancer has been one of the most prevalent disease in recent years.\nAccording to the research of this field, more than 200,000 cases are identified\neach year in the US. Uncontrolled multiplication and growth of the lung cells\nresult in malignant tumour formation. Recently, deep learning algorithms,\nespecially Convolutional Neural Networks (CNN), have become a superior way to\nautomatically diagnose disease. The purpose of this article is to review\ndifferent models that lead to different accuracy and sensitivity in the\ndiagnosis of early-stage lung cancer and to help physicians and researchers in\nthis field. The main purpose of this work is to identify the challenges that\nexist in lung cancer based on deep learning. The survey is systematically\nwritten that combines regular mapping and literature review to review 32\nconference and journal articles in the field from 2016 to 2021. After analysing\nand reviewing the articles, the questions raised in the articles are being\nanswered. This research is superior to other review articles in this field due\nto the complete review of relevant articles and systematic write up.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_H/0/1/0/all/0/1\">Hesamoddin Hosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shadroo_S/0/1/0/all/0/1\">Shabnam Shadroo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SporeAgent: Reinforced Scene-level Plausibility for Object Pose Refinement. (arXiv:2201.00239v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00239","description":"<p>Observational noise, inaccurate segmentation and ambiguity due to symmetry\nand occlusion lead to inaccurate object pose estimates. While depth- and\nRGB-based pose refinement approaches increase the accuracy of the resulting\npose estimates, they are susceptible to ambiguity in the observation as they\nconsider visual alignment. We propose to leverage the fact that we often\nobserve static, rigid scenes. Thus, the objects therein need to be under\nphysically plausible poses. We show that considering plausibility reduces\nambiguity and, in consequence, allows poses to be more accurately predicted in\ncluttered environments. To this end, we extend a recent RL-based registration\napproach towards iterative refinement of object poses. Experiments on the\nLINEMOD and YCB-VIDEO datasets demonstrate the state-of-the-art performance of\nour depth-based refinement approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_D/0/1/0/all/0/1\">Dominik Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patten_T/0/1/0/all/0/1\">Timothy Patten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincze_M/0/1/0/all/0/1\">Markus Vincze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace modeling for fast and high-sensitivity X-ray chemical imaging. (arXiv:2201.00259v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00259","description":"<p>Resolving morphological chemical phase transformations at the nanoscale is of\nvital importance to many scientific and industrial applications across various\ndisciplines. The TXM-XANES imaging technique, by combining full field\ntransmission X-ray microscopy (TXM) and X-ray absorption near edge structure\n(XANES), has been an emerging tool which operates by acquiring a series of\nmicroscopy images with multi-energy X-rays and fitting to obtain the chemical\nmap. Its capability, however, is limited by the poor signal-to-noise ratios due\nto the system errors and low exposure illuminations for fast acquisition. In\nthis work, by exploiting the intrinsic properties and subspace modeling of the\nTXM-XANES imaging data, we introduce a simple and robust denoising approach to\nimprove the image quality, which enables fast and high-sensitivity chemical\nimaging. Extensive experiments on both synthetic and real datasets demonstrate\nthe superior performance of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jizhou Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zan_G/0/1/0/all/0/1\">Guibin Zan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_G/0/1/0/all/0/1\">Guannan Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pianetta_P/0/1/0/all/0/1\">Piero Pianetta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Cross-dataset Generalization for License Plate Recognition. (arXiv:2201.00267v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00267","description":"<p>Automatic License Plate Recognition (ALPR) systems have shown remarkable\nperformance on license plates (LPs) from multiple regions due to advances in\ndeep learning and the increasing availability of datasets. The evaluation of\ndeep ALPR systems is usually done within each dataset; therefore, it is\nquestionable if such results are a reliable indicator of generalization\nability. In this paper, we propose a traditional-split versus\nleave-one-dataset-out experimental setup to empirically assess the\ncross-dataset generalization of 12 Optical Character Recognition (OCR) models\napplied to LP recognition on nine publicly available datasets with a great\nvariety in several aspects (e.g., acquisition settings, image resolution, and\nLP layouts). We also introduce a public dataset for end-to-end ALPR that is the\nfirst to contain images of vehicles with Mercosur LPs and the one with the\nhighest number of motorcycle images. The experimental results shed light on the\nlimitations of the traditional-split protocol for evaluating approaches in the\nALPR context, as there are significant drops in performance for most datasets\nwhen training and testing the models in a leave-one-dataset-out fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_E/0/1/0/all/0/1\">Everton V. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucio_D/0/1/0/all/0/1\">Diego R. Lucio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1\">Valter Estevam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents. (arXiv:2201.00308v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00308","description":"<p>Diffusion Probabilistic models have been shown to generate state-of-the-art\nresults on several competitive image synthesis benchmarks but lack a\nlow-dimensional, interpretable latent space, and are slow at generation. On the\nother hand, Variational Autoencoders (VAEs) typically have access to a\nlow-dimensional latent space but exhibit poor sample quality. Despite recent\nadvances, VAEs usually require high-dimensional hierarchies of the latent codes\nto generate high-quality samples. We present DiffuseVAE, a novel generative\nframework that integrates VAE within a diffusion model framework, and leverage\nthis to design a novel conditional parameterization for diffusion models. We\nshow that the resulting model can improve upon the unconditional diffusion\nmodel in terms of sampling efficiency while also equipping diffusion models\nwith the low-dimensional VAE inferred latent code. Furthermore, we show that\nthe proposed model can generate high-resolution samples and exhibits synthesis\nquality comparable to state-of-the-art models on standard benchmarks. Lastly,\nwe show that the proposed method can be used for controllable image synthesis\nand also exhibits out-of-the-box capabilities for downstream tasks like image\nsuper-resolution and denoising. For reproducibility, our source code is\npublicly available at \\url{https://github.com/kpandey008/DiffuseVAE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1\">Kushagra Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Avideep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Feature Propagation and Edge Skip-Connections for Automatic Abdominal Organ Segmentation. (arXiv:2201.00317v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00317","description":"<p>Automatic segmentation of abdominal organs in computed tomography (CT) images\ncan support radiation therapy and image-guided surgery workflows. Developing of\nsuch automatic solutions remains challenging mainly owing to complex organ\ninteractions and blurry boundaries in CT images. To address these issues, we\nfocus on effective spatial context modeling and explicit edge segmentation\npriors. Accordingly, we propose a 3D network with four main components trained\nend-to-end including shared encoder, edge detector, decoder with edge\nskip-connections (ESCs) and recurrent feature propagation head (RFP-Head). To\ncapture wide-range spatial dependencies, the RFP-Head propagates and harvests\nlocal features through directed acyclic graphs (DAGs) formulated with recurrent\nconnections in an efficient slice-wise manner, with regard to spatial\narrangement of image units. To leverage edge information, the edge detector\nlearns edge prior knowledge specifically tuned for semantic segmentation by\nexploiting intermediate features from the encoder with the edge supervision.\nThe ESCs then aggregate the edge knowledge with multi-level decoder features to\nlearn a hierarchy of discriminative features explicitly modeling\ncomplementarity between organs' interiors and edges for segmentation. We\nconduct extensive experiments on two challenging abdominal CT datasets with\neight annotated organs. Experimental results show that the proposed network\noutperforms several state-of-the-art models, especially for the segmentation of\nsmall and complicated structures (gallbladder, esophagus, stomach, pancreas and\nduodenum). The code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zefan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-LinkNet: Learning Contextual Inpainting Across Latent Space of Generative Adversarial Network. (arXiv:2201.00323v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00323","description":"<p>Deep learning methods outperform traditional methods in image inpainting. In\norder to generate contextual textures, researchers are still working to improve\non existing methods and propose models that can extract, propagate, and\nreconstruct features similar to ground-truth regions. Furthermore, the lack of\na high-quality feature transfer mechanism in deeper layers contributes to\npersistent aberrations on generated inpainted regions. To address these\nlimitations, we propose the V-LinkNet cross-space learning strategy network. To\nimprove learning on contextualised features, we design a loss model that\nemploys both encoders. In addition, we propose a recursive residual transition\nlayer (RSTL). The RSTL extracts high-level semantic information and propagates\nit down layers. Finally, we compare inpainting performance on the same face\nwith different masks and on different faces with the same masks. To improve\nimage inpainting reproducibility, we propose a standard protocol to overcome\nbiases with various masks and images. We investigate the V-LinkNet components\nusing experimental methods. Our result surpasses the state of the art when\nevaluated on the CelebA-HQ with the standard protocol. In addition, our model\ncan generalise well when evaluated on Paris Street View, and Places2 datasets\nwith the standard protocol.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jam_J/0/1/0/all/0/1\">Jireh Jam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendrick_C/0/1/0/all/0/1\">Connah Kendrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouard_V/0/1/0/all/0/1\">Vincent Drouard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_K/0/1/0/all/0/1\">Kevin Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Riemannian Nearest-Regularized Subspace Classification for Polarimetric SAR images. (arXiv:2201.00337v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00337","description":"<p>As a representation learning method, nearest regularized subspace(NRS)\nalgorithm is an effective tool to obtain both accuracy and speed for PolSAR\nimage classification. However, existing NRS methods use the polarimetric\nfeature vector but the PolSAR original covariance matrix(known as Hermitian\npositive definite(HPD)matrix) as the input. Without considering the matrix\nstructure, existing NRS-based methods cannot learn correlation among channels.\nHow to utilize the original covariance matrix to NRS method is a key problem.\nTo address this limit, a Riemannian NRS method is proposed, which consider the\nHPD matrices endow in the Riemannian space. Firstly, to utilize the PolSAR\noriginal data, a Riemannian NRS method(RNRS) is proposed by constructing HPD\ndictionary and HPD distance metric. Secondly, a new Tikhonov regularization\nterm is designed to reduce the differences within the same class. Finally, the\noptimal method is developed and the first-order derivation is inferred. During\nthe experimental test, only T matrix is used in the proposed method, while\nmultiple of features are utilized for compared methods. Experimental results\ndemonstrate the proposed method can outperform the state-of-art algorithms even\nusing less features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Junfei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_H/0/1/0/all/0/1\">Haiyan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detail-Preserving Transformer for Light Field Image Super-Resolution. (arXiv:2201.00346v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00346","description":"<p>Recently, numerous algorithms have been developed to tackle the problem of\nlight field super-resolution (LFSR), i.e., super-resolving low-resolution light\nfields to gain high-resolution views. Despite delivering encouraging results,\nthese approaches are all convolution-based, and are naturally weak in global\nrelation modeling of sub-aperture images necessarily to characterize the\ninherent structure of light fields. In this paper, we put forth a novel\nformulation built upon Transformers, by treating LFSR as a sequence-to-sequence\nreconstruction task. In particular, our model regards sub-aperture images of\neach vertical or horizontal angular view as a sequence, and establishes\nlong-range geometric dependencies within each sequence via a spatial-angular\nlocally-enhanced self-attention layer, which maintains the locality of each\nsub-aperture image as well. Additionally, to better recover image details, we\npropose a detail-preserving Transformer (termed as DPT), by leveraging gradient\nmaps of light field to guide the sequence learning. DPT consists of two\nbranches, with each associated with a Transformer for learning from an original\nor gradient image sequence. The two branches are finally fused to obtain\ncomprehensive feature representations for reconstruction. Evaluations are\nconducted on a number of light field datasets, including real-world scenes and\nsynthetic data. The proposed method achieves superior performance comparing\nwith other state-of-the-art schemes. Our code is publicly available at:\nhttps://github.com/BITszwang/DPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shunzhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_H/0/1/0/all/0/1\">Huijun Di</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parkour Spot ID: Feature Matching in Satellite and Street view images using Deep Learning. (arXiv:2201.00377v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00377","description":"<p>How to find places that are not indexed by Google Maps? We propose an\nintuitive method and framework to locate places based on their distinctive\nspatial features. The method uses satellite and street view images in machine\nvision approaches to classify locations. If we can classify locations, we just\nneed to repeat for non-overlapping locations in our area of interest. We assess\nthe proposed system in finding Parkour spots in the campus of Arizona State\nUniversity. The results are very satisfactory, having found more than 25 new\nParkour spots, with a rate of true positives above 60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morais_J/0/1/0/all/0/1\">Jo&#xe3;o Morais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_K/0/1/0/all/0/1\">Kaushal Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_B/0/1/0/all/0/1\">Bhuvaneshwar Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_S/0/1/0/all/0/1\">Shantanu Rajesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and High-Quality Image Denoising via Malleable Convolutions. (arXiv:2201.00392v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00392","description":"<p>Many image processing networks apply a single set of static convolutional\nkernels across the entire input image, which is sub-optimal for natural images,\nas they often consist of heterogeneous visual patterns. Recent work in\nclassification, segmentation, and image restoration has demonstrated that\ndynamic kernels outperform static kernels at modeling local image statistics.\nHowever, these works often adopt per-pixel convolution kernels, which introduce\nhigh memory and computation costs. To achieve spatial-varying processing\nwithout significant overhead, we present \\textbf{Malle}able\n\\textbf{Conv}olution (\\textbf{MalleConv}), as an efficient variant of dynamic\nconvolution. The weights of \\ours are dynamically produced by an efficient\npredictor network capable of generating content-dependent outputs at specific\nspatial locations. Unlike previous works, \\ours generates a much smaller set of\nspatially-varying kernels from input, which enlarges the network's receptive\nfield and significantly reduces computational and memory costs. These kernels\nare then applied to a full-resolution feature map through an efficient\nslice-and-conv operator with minimum memory overhead. We further build a\nefficient denoising network using MalleConv, coined as \\textbf{MalleNet}. It\nachieves high quality results without very deep architecture, \\eg, it is\n8.91$\\times$ faster than the best performed denoising algorithms (SwinIR),\nwhile maintaining similar performance. We also show that a single \\ours added\nto a standard convolution-based backbones can contribute significantly reduce\nthe computational cost or boost image quality at similar cost. Project page:\nhttps://yifanjiang.net/MalleConv.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wronski_B/0/1/0/all/0/1\">Bart Wronski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jon Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianfan Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules. (arXiv:2201.00404v1 [q-bio.NC])","link":"http://arxiv.org/abs/2201.00404","description":"<p>Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder\n(ASD) by using network-based functional connectivity. It has been shown that\nASD is associated with brain regions and their inter-connections. However,\ndiscriminating based on connectivity patterns among imaging data of the control\npopulation and that of ASD patients' brains is a non-trivial task. In order to\ntackle said classification task, we propose a novel deep learning architecture\n(MHATC) consisting of multi-head attention and temporal consolidation modules\nfor classifying an individual as a patient of ASD. The devised architecture\nresults from an in-depth analysis of the limitations of current deep neural\nnetwork solutions for similar applications. Our approach is not only robust but\ncomputationally efficient, which can allow its adoption in a variety of other\nresearch and clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_R/0/1/0/all/0/1\">Ranjeet Ranjan Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bhardwaj_A/0/1/0/all/0/1\">Abhishek Bhardwaj</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garg_D/0/1/0/all/0/1\">Devin Garg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bhavsar_A/0/1/0/all/0/1\">Arnav Bhavsar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nigam_A/0/1/0/all/0/1\">Aditya Nigam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Introspective Agent: Interdependence of Strategy, Physiology, and Sensing for Embodied Agents. (arXiv:2201.00411v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00411","description":"<p>The last few years have witnessed substantial progress in the field of\nembodied AI where artificial agents, mirroring biological counterparts, are now\nable to learn from interaction to accomplish complex tasks. Despite this\nsuccess, biological organisms still hold one large advantage over these\nsimulated agents: adaptation. While both living and simulated agents make\ndecisions to achieve goals (strategy), biological organisms have evolved to\nunderstand their environment (sensing) and respond to it (physiology). The net\ngain of these factors depends on the environment, and organisms have adapted\naccordingly. For example, in a low vision aquatic environment some fish have\nevolved specific neurons which offer a predictable, but incredibly rapid,\nstrategy to escape from predators. Mammals have lost these reactive systems,\nbut they have a much larger fields of view and brain circuitry capable of\nunderstanding many future possibilities. While traditional embodied agents\nmanipulate an environment to best achieve a goal, we argue for an introspective\nagent, which considers its own abilities in the context of its environment. We\nshow that different environments yield vastly different optimal designs, and\nincreasing long-term planning is often far less beneficial than other\nimprovements, such as increased physical ability. We present these findings to\nbroaden the definition of improvement in embodied AI passed increasingly\ncomplex models. Just as in nature, we hope to reframe strategy as one tool,\namong many, to succeed in an environment. Code is available at:\nhttps://github.com/sarahpratt/introspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1\">Sarah Pratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUSeg: The Foot Ulcer Segmentation Challenge. (arXiv:2201.00414v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00414","description":"<p>Acute and chronic wounds with varying etiologies burden the healthcare\nsystems economically. The advanced wound care market is estimated to reach $22\nbillion by 2024. Wound care professionals provide proper diagnosis and\ntreatment with heavy reliance on images and image documentation. Segmentation\nof wound boundaries in images is a key component of the care and diagnosis\nprotocol since it is important to estimate the area of the wound and provide\nquantitative measurement for the treatment. Unfortunately, this process is very\ntime-consuming and requires a high level of expertise. Recently automatic wound\nsegmentation methods based on deep learning have shown promising performance\nbut require large datasets for training and it is unclear which methods perform\nbetter. To address these issues, we propose the Foot Ulcer Segmentation\nchallenge (FUSeg) organized in conjunction with the 2021 International\nConference on Medical Image Computing and Computer Assisted Intervention\n(MICCAI). We built a wound image dataset containing 1,210 foot ulcer images\ncollected over 2 years from 889 patients. It is pixel-wise annotated by wound\ncare experts and split into a training set with 1010 images and a testing set\nwith 200 images for evaluation. Teams around the world developed automated\nmethods to predict wound segmentations on the testing set of which annotations\nwere kept private. The predictions were evaluated and ranked based on the\naverage Dice coefficient. The FUSeg challenge remains an open challenge as a\nbenchmark for wound segmentation after the conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuanbo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1\">Amirreza Mahbod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1\">Isabella Ellinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Sandeep Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niezgoda_J/0/1/0/all/0/1\">Jeffrey Niezgoda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Splicing ViT Features for Semantic Appearance Transfer. (arXiv:2201.00424v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00424","description":"<p>We present a method for semantically transferring the visual appearance of\none natural image to another. Specifically, our goal is to generate an image in\nwhich objects in a source structure image are \"painted\" with the visual\nappearance of their semantically related objects in a target appearance image.\nOur method works by training a generator given only a single\nstructure/appearance image pair as input. To integrate semantic information\ninto our framework - a pivotal component in tackling this task - our key idea\nis to leverage a pre-trained and fixed Vision Transformer (ViT) model which\nserves as an external semantic prior. Specifically, we derive novel\nrepresentations of structure and appearance extracted from deep ViT features,\nuntwisting them from the learned self-attention modules. We then establish an\nobjective function that splices the desired structure and appearance\nrepresentations, interweaving them together in the space of ViT features. Our\nframework, which we term \"Splice\", does not involve adversarial training, nor\ndoes it require any additional input information such as semantic segmentation\nor correspondences, and can generate high-resolution results, e.g., work in HD.\nWe demonstrate high quality results on a variety of in-the-wild image pairs,\nunder significant variations in the number of objects, their pose and\nappearance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tumanyan_N/0/1/0/all/0/1\">Narek Tumanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1\">Omer Bar-Tal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1\">Shai Bagon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Denoising with Control over Deep Network Hallucination. (arXiv:2201.00429v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00429","description":"<p>Deep image denoisers achieve state-of-the-art results but with a hidden cost.\nAs witnessed in recent literature, these deep networks are capable of\noverfitting their training distributions, causing inaccurate hallucinations to\nbe added to the output and generalizing poorly to varying data. For better\ncontrol and interpretability over a deep denoiser, we propose a novel framework\nexploiting a denoising network. We call it controllable confidence-based image\ndenoising (CCID). In this framework, we exploit the outputs of a deep denoising\nnetwork alongside an image convolved with a reliable filter. Such a filter can\nbe a simple convolution kernel which does not risk adding hallucinated\ninformation. We propose to fuse the two components with a frequency-domain\napproach that takes into account the reliability of the deep network outputs.\nWith our framework, the user can control the fusion of the two components in\nthe frequency domain. We also provide a user-friendly map estimating spatially\nthe confidence in the output that potentially contains network hallucination.\nResults show that our CCID not only provides more interpretability and control,\nbut can even outperform both the quantitative performance of the deep denoiser\nand that of the reliable filter, especially when the test data diverge from the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiyuan Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cassayre_F/0/1/0/all/0/1\">Florian Cassayre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Owsianko_H/0/1/0/all/0/1\">Haley Owsianko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helou_M/0/1/0/all/0/1\">Majed El Helou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVNet: Temporal Voting Network for Action Localization. (arXiv:2201.00434v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00434","description":"<p>We propose a Temporal Voting Network (TVNet) for action localization in\nuntrimmed videos. This incorporates a novel Voting Evidence Module to locate\ntemporal boundaries, more accurately, where temporal contextual evidence is\naccumulated to predict frame-level probabilities of start and end action\nboundaries. Our action-independent evidence module is incorporated within a\npipeline to calculate confidence scores and action classes. We achieve an\naverage mAP of 34.6% on ActivityNet-1.3, particularly outperforming previous\nmethods with the highest IoU of 0.95. TVNet also achieves mAP of 56.0% when\ncombined with PGCN and 59.1% with MUSES at 0.5 IoU on THUMOS14 and outperforms\nprior work at all thresholds. Our code is available at\nhttps://github.com/hanielwang/TVNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrett_T/0/1/0/all/0/1\">Toby Perrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection by LTP Texture Characterization on Opposing Color Pairs under SLICO Superpixel Constraint. (arXiv:2201.00439v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00439","description":"<p>The effortless detection of salient objects by humans has been the subject of\nresearch in several fields, including computer vision as it has many\napplications. However, salient object detection remains a challenge for many\ncomputer models dealing with color and textured images. Herein, we propose a\nnovel and efficient strategy, through a simple model, almost without internal\nparameters, which generates a robust saliency map for a natural image. This\nstrategy consists of integrating color information into local textural patterns\nto characterize a color micro-texture. Most models in the literature that use\nthe color and texture features treat them separately. In our case, it is the\nsimple, yet powerful LTP (Local Ternary Patterns) texture descriptor applied to\nopposing color pairs of a color space that allows us to achieve this end. Each\ncolor micro-texture is represented by vector whose components are from a\nsuperpixel obtained by SLICO (Simple Linear Iterative Clustering with zero\nparameter) algorithm which is simple, fast and exhibits state-of-the-art\nboundary adherence. The degree of dissimilarity between each pair of color\nmicro-texture is computed by the FastMap method, a fast version of MDS\n(Multi-dimensional Scaling), that considers the color micro-textures\nnon-linearity while preserving their distances. These degrees of dissimilarity\ngive us an intermediate saliency map for each RGB, HSL, LUV and CMY color\nspaces. The final saliency map is their combination to take advantage of the\nstrength of each of them. The MAE (Mean Absolute Error) and F$_{\\beta}$\nmeasures of our saliency maps, on the complex ECSSD dataset show that our model\nis both simple and efficient, outperforming several state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ndayikengurukiye_D/0/1/0/all/0/1\">Didier Ndayikengurukiye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mignotte_M/0/1/0/all/0/1\">Max Mignotte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graph Generation: A Comprehensive Survey. (arXiv:2201.00443v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00443","description":"<p>Deep learning techniques have led to remarkable breakthroughs in the field of\ngeneric object detection and have spawned a lot of scene-understanding tasks in\nrecent years. Scene graph has been the focus of research because of its\npowerful semantic representation and applications to scene understanding. Scene\nGraph Generation (SGG) refers to the task of automatically mapping an image\ninto a semantic structural scene graph, which requires the correct labeling of\ndetected objects and their relationships. Although this is a challenging task,\nthe community has proposed a lot of SGG approaches and achieved good results.\nIn this paper, we provide a comprehensive survey of recent achievements in this\nfield brought about by deep learning techniques. We review 138 representative\nworks that cover different input modalities, and systematically summarize\nexisting methods of image-based SGG from the perspective of feature extraction\nand fusion. We attempt to connect and systematize the existing visual\nrelationship detection methods, to summarize, and interpret the mechanisms and\nthe strategies of SGG in a comprehensive way. Finally, we finish this survey\nwith deep discussions about current existing problems and future research\ndirections. This survey will help readers to develop a better understanding of\nthe current research status and ideas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Youliang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yixuan Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1\">Haoran Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiguang Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Guided Semantic Learning Network for Temporal Sentence Grounding. (arXiv:2201.00454v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00454","description":"<p>Temporal sentence grounding (TSG) is crucial and fundamental for video\nunderstanding. Although the existing methods train well-designed deep networks\nwith a large amount of data, we find that they can easily forget the rarely\nappeared cases in the training stage due to the off-balance data distribution,\nwhich influences the model generalization and leads to undesirable performance.\nTo tackle this issue, we propose a memory-augmented network, called\nMemory-Guided Semantic Learning Network (MGSL-Net), that learns and memorizes\nthe rarely appeared content in TSG tasks. Specifically, MGSL-Net consists of\nthree main parts: a cross-modal inter-action module, a memory augmentation\nmodule, and a heterogeneous attention module. We first align the given\nvideo-query pair by a cross-modal graph convolutional network, and then utilize\na memory module to record the cross-modal shared semantic features in the\ndomain-specific persistent memory. During training, the memory slots are\ndynamically associated with both common and rare cases, alleviating the\nforgetting issue. In testing, the rare cases can thus be enhanced by retrieving\nthe stored memories, resulting in better generalization. At last, the\nheterogeneous attention module is utilized to integrate the enhanced\nmulti-modal features in both video and query domains. Experimental results on\nthree benchmarks show the superiority of our method on both effectiveness and\nefficiency, which substantially improves the accuracy not only on the entire\ndataset but also on rare cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zichuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Motion and Appearance Information for Temporal Sentence Grounding. (arXiv:2201.00457v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00457","description":"<p>This paper addresses temporal sentence grounding. Previous works typically\nsolve this task by learning frame-level video features and align them with the\ntextual information. A major limitation of these works is that they fail to\ndistinguish ambiguous video frames with subtle appearance differences due to\nframe-level feature extraction. Recently, a few methods adopt Faster R-CNN to\nextract detailed object features in each frame to differentiate the\nfine-grained appearance similarities. However, the object-level features\nextracted by Faster R-CNN suffer from missing motion analysis since the object\ndetection model lacks temporal modeling. To solve this issue, we propose a\nnovel Motion-Appearance Reasoning Network (MARN), which incorporates both\nmotion-aware and appearance-aware object features to better reason object\nrelations for modeling the activity among successive frames. Specifically, we\nfirst introduce two individual video encoders to embed the video into\ncorresponding motion-oriented and appearance-aspect object representations.\nThen, we develop separate motion and appearance branches to learn motion-guided\nand appearance-guided object relations, respectively. At last, both motion and\nappearance information from two branches are associated to generate more\nrepresentative features for final grounding. Extensive experiments on two\nchallenging datasets (Charades-STA and TACoS) show that our proposed MARN\nsignificantly outperforms previous state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark. (arXiv:2201.00458v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00458","description":"<p>Lung cancer is one of the deadliest cancers, and in part its effective\ndiagnosis and treatment depend on the accurate delineation of the tumor.\nHuman-centered segmentation, which is currently the most common approach, is\nsubject to inter-observer variability, and is also time-consuming, considering\nthe fact that only experts are capable of providing annotations. Automatic and\nsemi-automatic tumor segmentation methods have recently shown promising\nresults. However, as different researchers have validated their algorithms\nusing various datasets and performance metrics, reliably evaluating these\nmethods is still an open challenge. The goal of the Lung-Originated Tumor\nSegmentation from Computed Tomography Scan (LOTUS) Benchmark created through\n2018 IEEE Video and Image Processing (VIP) Cup competition, is to provide a\nunique dataset and pre-defined metrics, so that different researchers can\ndevelop and evaluate their methods in a unified fashion. The 2018 VIP Cup\nstarted with a global engagement from 42 countries to access the competition\ndata. At the registration stage, there were 129 members clustered into 28 teams\nfrom 10 countries, out of which 9 teams made it to the final stage and 6 teams\nsuccessfully completed all the required tasks. In a nutshell, all the\nalgorithms proposed during the competition, are based on deep learning models\ncombined with a false positive reduction technique. Methods developed by the\nthree finalists show promising results in tumor segmentation, however, more\neffort should be put into reducing the false positive rate. This competition\nmanuscript presents an overview of the VIP-Cup challenge, along with the\nproposed algorithms and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirby_J/0/1/0/all/0/1\">Justin Kirby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1\">Amir Asif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haque_M/0/1/0/all/0/1\">Mohammad Ariful Haque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Shahruk Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamal_U/0/1/0/all/0/1\">Uday Kamal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jhih-Yuan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibtehaz_N/0/1/0/all/0/1\">Nabil Ibtehaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foisol_S/0/1/0/all/0/1\">Sh. M. Amir Foisol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_K/0/1/0/all/0/1\">Kin-Man Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guang_Z/0/1/0/all/0/1\">Zhong Guang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Channappayya_S/0/1/0/all/0/1\">Sumohana S. Channappayya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dev_C/0/1/0/all/0/1\">Chander Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometrics in the Time of Pandemic: 40% Masked Face Recognition Degradation can be Reduced to 2%. (arXiv:2201.00461v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00461","description":"<p>In this study of the face recognition on masked versus unmasked faces\ngenerated using Flickr-Faces-HQ and SpeakingFaces datasets, we report 36.78%\ndegradation of recognition performance caused by the mask-wearing at the time\nof pandemics, in particular, in border checkpoint scenarios. We have achieved\nbetter performance and reduced the degradation to 1.79% using advanced deep\nlearning approaches in the cross-spectral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Queiroz_L/0/1/0/all/0/1\">Leonardo Queiroz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kenneth Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1\">Svetlana Yanushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmerko_V/0/1/0/all/0/1\">Vlad Shmerko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation. (arXiv:2201.00462v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00462","description":"<p>Computer-aided medical image segmentation has been applied widely in\ndiagnosis and treatment to obtain clinically useful information of shapes and\nvolumes of target organs and tissues. In the past several years, convolutional\nneural network (CNN) based methods (e.g., U-Net) have dominated this area, but\nstill suffered from inadequate long-range information capturing. Hence, recent\nwork presented computer vision Transformer variants for medical image\nsegmentation tasks and obtained promising performances. Such Transformers model\nlong-range dependency by computing pair-wise patch relations. However, they\nincur prohibitive computational costs, especially on 3D medical images (e.g.,\nCT and MRI). In this paper, we propose a new method called Dilated Transformer,\nwhich conducts self-attention for pair-wise patch relations captured\nalternately in local and global scopes. Inspired by dilated convolution\nkernels, we conduct the global self-attention in a dilated manner, enlarging\nreceptive fields without increasing the patches involved and thus reducing\ncomputational costs. Based on this design of Dilated Transformer, we construct\na U-shaped encoder-decoder hierarchical architecture called D-Former for 3D\nmedical image segmentation. Experiments on the Synapse and ACDC datasets show\nthat our D-Former model, trained from scratch, outperforms various competitive\nCNN-based or Transformer-based segmentation models at a low computational cost\nwithout time-consuming per-training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yixuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kuanlun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jintai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Honghao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark. (arXiv:2201.00466v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00466","description":"<p>Ophthalmologists have used fundus images to screen and diagnose eye diseases.\nHowever, different equipments and ophthalmologists pose large variations to the\nquality of fundus images. Low-quality (LQ) degraded fundus images easily lead\nto uncertainty in clinical screening and generally increase the risk of\nmisdiagnosis. Thus, real fundus image restoration is worth studying.\nUnfortunately, real clinical benchmark has not been explored for this task so\nfar. In this paper, we investigate the real clinical fundus image restoration\nproblem. Firstly, We establish a clinical dataset, Real Fundus (RF), including\n120 low- and high-quality (HQ) image pairs. Then we propose a novel\nTransformer-based Generative Adversarial Network (RFormer) to restore the real\ndegradation of clinical fundus images. The key component in our network is the\nWindow-based Self-Attention Block (WSAB) which captures non-local\nself-similarity and long-range dependencies. To produce more visually pleasant\nresults, a Transformer-based discriminator is introduced. Extensive experiments\non our clinical benchmark show that the proposed RFormer significantly\noutperforms the state-of-the-art (SOTA) methods. In addition, experiments of\ndownstream tasks such as vessel segmentation and optic disc/cup detection\ndemonstrate that our proposed RFormer benefits clinical fundus image analysis\nand applications. The dataset, code, and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deng_Z/0/1/0/all/0/1\">Zhuo Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Q/0/1/0/all/0/1\">Qiqi Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xue Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_D/0/1/0/all/0/1\">Dong Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaochong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"maskGRU: Tracking Small Objects in the Presence of Large Background Motions. (arXiv:2201.00467v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00467","description":"<p>We propose a recurrent neural network-based spatio-temporal framework named\nmaskGRU for the detection and tracking of small objects in videos. While there\nhave been many developments in the area of object tracking in recent years,\ntracking a small moving object amid other moving objects and actors (such as a\nball amid moving players in sports footage) continues to be a difficult task.\nExisting spatio-temporal networks, such as convolutional Gated Recurrent Units\n(convGRUs), are difficult to train and have trouble accurately tracking small\nobjects under such conditions. To overcome these difficulties, we developed the\nmaskGRU framework that uses a weighted sum of the internal hidden state\nproduced by a convGRU and a 3-channel mask of the tracked object's predicted\nbounding box as the hidden state to be used at the next time step of the\nunderlying convGRU. We believe the technique of incorporating a mask into the\nhidden state through a weighted sum has two benefits: controlling the effect of\nexploding gradients and introducing an attention-like mechanism into the\nnetwork by indicating where in the previous video frame the object is located.\nOur experiments show that maskGRU outperforms convGRU at tracking objects that\nare small relative to the video resolution even in the presence of other moving\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roros_C/0/1/0/all/0/1\">Constantine J. Roros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Open World Object Detection. (arXiv:2201.00471v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00471","description":"<p>Open World Object Detection (OWOD), simulating the real dynamic world where\nknowledge grows continuously, attempts to detect both known and unknown classes\nand incrementally learn the identified unknown ones. We find that although the\nonly previous OWOD work constructively puts forward to the OWOD definition, the\nexperimental settings are unreasonable with the illogical benchmark, confusing\nmetric calculation, and inappropriate method. In this paper, we rethink the\nOWOD experimental setting and propose five fundamental benchmark principles to\nguide the OWOD benchmark construction. Moreover, we design two fair evaluation\nprotocols specific to the OWOD problem, filling the void of evaluating from the\nperspective of unknown classes. Furthermore, we introduce a novel and effective\nOWOD framework containing an auxiliary Proposal ADvisor (PAD) and a\nClass-specific Expelling Classifier (CEC). The non-parametric PAD could assist\nthe RPN in identifying accurate unknown proposals without supervision, while\nCEC calibrates the over-confident activation boundary and filters out confusing\npredictions through a class-specific expelling function. Comprehensive\nexperiments conducted on our fair benchmark demonstrate that our method\noutperforms other state-of-the-art object detection approaches in terms of both\nexisting and our new metrics.\\footnote{Our benchmark and code are available at\nhttps://github.com/RE-OWOD/RE-OWOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaowei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duorui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaFT: Clustering and Filter on Tokens of Transformer for Weakly Supervised Object Localization. (arXiv:2201.00475v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00475","description":"<p>Weakly supervised object localization (WSOL) is a challenging task to\nlocalize the object by only category labels. However, there is contradiction\nbetween classification and localization because accurate classification network\ntends to pay attention to discriminative region of objects rather than the\nentirety. We propose this discrimination is caused by handcraft threshold\nchoosing in CAM-based methods. Therefore, we propose Clustering and Filter of\nTokens (CaFT) with Vision Transformer (ViT) backbone to solve this problem in\nanother way. CaFT first sends the patch tokens of the image split to ViT and\ncluster the output tokens to generate initial mask of the object. Secondly,\nCaFT considers the initial mask as pseudo labels to train a shallow convolution\nhead (Attention Filter, AtF) following backbone to directly extract the mask\nfrom tokens. Then, CaFT splits the image into parts, outputs masks respectively\nand merges them into one refined mask. Finally, a new AtF is trained on the\nrefined masks and used to predict the box of object. Experiments verify that\nCaFT outperforms previous work and achieves 97.55\\% and 69.86\\% localization\naccuracy with ground-truth class on CUB-200 and ImageNet-1K respectively. CaFT\nprovides a fresh way to think about the WSOL task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language as Queries for Referring Video Object Segmentation. (arXiv:2201.00487v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00487","description":"<p>Referring video object segmentation (R-VOS) is an emerging cross-modal task\nthat aims to segment the target object referred by a language expression in all\nvideo frames. In this work, we propose a simple and unified framework built\nupon Transformer, termed ReferFormer. It views the language as queries and\ndirectly attends to the most relevant regions in the video frames. Concretely,\nwe introduce a small set of object queries conditioned on the language as the\ninput to the Transformer. In this manner, all the queries are obligated to find\nthe referred objects only. They are eventually transformed into dynamic kernels\nwhich capture the crucial object-level information, and play the role of\nconvolution filters to generate the segmentation masks from feature maps. The\nobject tracking is achieved naturally by linking the corresponding queries\nacross frames. This mechanism greatly simplifies the pipeline and the\nend-to-end framework is significantly different from the previous methods.\nExtensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and\nJHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS,\nRefer-Former achieves 55.6J&amp;F with a ResNet-50 backbone without bells and\nwhistles, which exceeds the previous state-of-the-art performance by 8.4\npoints. In addition, with the strong Swin-Large backbone, ReferFormer achieves\nthe best J&amp;F of 62.4 among all existing methods. The J&amp;F metric can be further\nboosted to 63.3 by adopting a simple post-process technique. Moreover, we show\nthe impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences\nandJHMDB-Sentences respectively, which significantly outperforms the previous\nmethods by a large margin. Code is publicly available at\nhttps://github.com/wjn922/ReferFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-Theta Local Neighborhood Pattern for Unconstrained Facial Image Recognition and Retrieval. (arXiv:2201.00504v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00504","description":"<p>In this paper R-Theta Local Neighborhood Pattern (RTLNP) is proposed for\nfacial image retrieval. RTLNP exploits relationships amongst the pixels in\nlocal neighborhood of the reference pixel at different angular and radial\nwidths. The proposed encoding scheme divides the local neighborhood into\nsectors of equal angular width. These sectors are again divided into subsectors\nof two radial widths. Average grayscales values of these two subsectors are\nencoded to generate the micropatterns. Performance of the proposed descriptor\nhas been evaluated and results are compared with the state of the art\ndescriptors e.g. LBP, LTP, CSLBP, CSLTP, Sobel-LBP, LTCoP, LMeP, LDP, LTrP,\nMBLBP, BRINT and SLBP. The most challenging facial constrained and\nunconstrained databases, namely; AT&amp;T, CARIA-Face-V5-Cropped, LFW, and Color\nFERET have been used for showing the efficiency of the proposed descriptor.\nProposed descriptor is also tested on near infrared (NIR) face databases; CASIA\nNIR-VIS 2.0 and PolyU-NIRFD to explore its potential with respect to NIR facial\nimages. Better retrieval rates of RTLNP as compared to the existing state of\nthe art descriptors show the effectiveness of the descriptor\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Gradient Hexa Pattern: A Descriptor for Face Recognition and Retrieval. (arXiv:2201.00509v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00509","description":"<p>Local descriptors used in face recognition are robust in a sense that these\ndescriptors perform well in varying pose, illumination and lighting conditions.\nAccuracy of these descriptors depends on the precision of mapping the\nrelationship that exists in the local neighborhood of a facial image into\nmicrostructures. In this paper a local gradient hexa pattern (LGHP) is proposed\nthat identifies the relationship amongst the reference pixel and its\nneighboring pixels at different distances across different derivative\ndirections. Discriminative information exists in the local neighborhood as well\nas in different derivative directions. Proposed descriptor effectively\ntransforms these relationships into binary micropatterns discriminating\ninterclass facial images with optimal precision. Recognition and retrieval\nperformance of the proposed descriptor has been compared with state-of-the-art\ndescriptors namely LDP and LVP over the most challenging and benchmark facial\nimage databases, i.e. Cropped Extended Yale-B, CMU-PIE, color-FERET, and LFW.\nThe proposed descriptor has better recognition as well as retrieval rates\ncompared to state-of-the-art descriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Centre Symmetric Quadruple Pattern: A Novel Descriptor for Facial Image Recognition and Retrieval. (arXiv:2201.00511v1 [cs.MM])","link":"http://arxiv.org/abs/2201.00511","description":"<p>Facial features are defined as the local relationships that exist amongst the\npixels of a facial image. Hand-crafted descriptors identify the relationships\nof the pixels in the local neighbourhood defined by the kernel. Kernel is a two\ndimensional matrix which is moved across the facial image. Distinctive\ninformation captured by the kernel with limited number of pixel achieves\nsatisfactory recognition and retrieval accuracies on facial images taken under\nconstrained environment (controlled variations in light, pose, expressions, and\nbackground). To achieve similar accuracies under unconstrained environment\nlocal neighbourhood has to be increased, in order to encode more pixels.\nIncreasing local neighbourhood also increases the feature length of the\ndescriptor. In this paper we propose a hand-crafted descriptor namely Centre\nSymmetric Quadruple Pattern (CSQP), which is structurally symmetric and encodes\nthe facial asymmetry in quadruple space. The proposed descriptor efficiently\nencodes larger neighbourhood with optimal number of binary bits. It has been\nshown using average entropy, computed over feature images encoded with the\nproposed descriptor, that the CSQP captures more meaningful information as\ncompared to state of the art descriptors. The retrieval and recognition\naccuracies of the proposed descriptor has been compared with state of the art\nhand-crafted descriptors (CSLBP, CSLTP, LDP, LBP, SLBP and LDGP) on bench mark\ndatabases namely; LFW, Colour-FERET, and CASIA-face-v5. Result analysis shows\nthat the proposed descriptor performs well under controlled as well as\nuncontrolled variations in pose, illumination, background and expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Asymmetric Local Pattern: A Novel Descriptor for Unconstrained Facial Image Recognition and Retrieval. (arXiv:2201.00518v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00518","description":"<p>Feature description is one of the most frequently studied areas in the expert\nsystems and machine learning. Effective encoding of the images is an essential\nrequirement for accurate matching. These encoding schemes play a significant\nrole in recognition and retrieval systems. Facial recognition systems should be\neffective enough to accurately recognize individuals under intrinsic and\nextrinsic variations of the system. The templates or descriptors used in these\nsystems encode spatial relationships of the pixels in the local neighbourhood\nof an image. Features encoded using these hand crafted descriptors should be\nrobust against variations such as; illumination, background, poses, and\nexpressions. In this paper a novel hand crafted cascaded asymmetric local\npattern (CALP) is proposed for retrieval and recognition facial image. The\nproposed descriptor uniquely encodes relationship amongst the neighbouring\npixels in horizontal and vertical directions. The proposed encoding scheme has\noptimum feature length and shows significant improvement in accuracy under\nenvironmental and physiological changes in a facial image. State of the art\nhand crafted descriptors namely; LBP, LDGP, CSLBP, SLBP and CSLTP are compared\nwith the proposed descriptor on most challenging datasets namely; Caltech-face,\nLFW, and CASIA-face-v5. Result analysis shows that, the proposed descriptor\noutperforms state of the art under uncontrolled variations in expressions,\nbackground, pose and illumination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Soumendu Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pavan Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer with Deformable Attention. (arXiv:2201.00520v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00520","description":"<p>Transformers have recently shown superior performances on various vision\ntasks. The large, sometimes even global, receptive field endows Transformer\nmodels with higher representation power over their CNN counterparts.\nNevertheless, simply enlarging receptive field also gives rise to several\nconcerns. On the one hand, using dense attention e.g., in ViT, leads to\nexcessive memory and computational cost, and features can be influenced by\nirrelevant parts which are beyond the region of interests. On the other hand,\nthe sparse attention adopted in PVT or Swin Transformer is data agnostic and\nmay limit the ability to model long range relations. To mitigate these issues,\nwe propose a novel deformable self-attention module, where the positions of key\nand value pairs in self-attention are selected in a data-dependent way. This\nflexible scheme enables the self-attention module to focus on relevant regions\nand capture more informative features. On this basis, we present Deformable\nAttention Transformer, a general backbone model with deformable attention for\nboth image classification and dense prediction tasks. Extensive experiments\nshow that our models achieve consistently improved results on comprehensive\nbenchmarks. Code is available at https://github.com/LeapLabTHU/DAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhuofan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novelty-based Generalization Evaluation for Traffic Light Detection. (arXiv:2201.00531v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00531","description":"<p>The advent of Convolutional Neural Networks (CNNs) has led to their\napplication in several domains. One noteworthy application is the perception\nsystem for autonomous driving that relies on the predictions from CNNs.\nPractitioners evaluate the generalization ability of such CNNs by calculating\nvarious metrics on an independent test dataset. A test dataset is often chosen\nbased on only one precondition, i.e., its elements are not a part of the\ntraining data. Such a dataset may contain objects that are both similar and\nnovel w.r.t. the training dataset. Nevertheless, existing works do not reckon\nthe novelty of the test samples and treat them all equally for evaluating\ngeneralization. Such novelty-based evaluations are of significance to validate\nthe fitness of a CNN in autonomous driving applications. Hence, we propose a\nCNN generalization scoring framework that considers novelty of objects in the\ntest dataset. We begin with the representation learning technique to reduce the\nimage data into a low-dimensional space. It is on this space we estimate the\nnovelty of the test samples. Finally, we calculate the generalization score as\na combination of the test data prediction performance and novelty. We perform\nan experimental study of the same for our traffic light detection application.\nIn addition, we systematically visualize the results for an interpretable\nnotion of novelty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekar_A/0/1/0/all/0/1\">Arvind Kumar Shekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_L/0/1/0/all/0/1\">Laureen Lake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1\">Liang Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Embeddings for Fuzzy Logic Verification of Deep Neural Networks in Perception Tasks. (arXiv:2201.00572v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00572","description":"<p>One major drawback of deep neural networks (DNNs) for use in sensitive\napplication domains is their black-box nature. This makes it hard to verify or\nmonitor complex, symbolic requirements. In this work, we present a simple, yet\neffective, approach to verify whether a trained convolutional neural network\n(CNN) respects specified symbolic background knowledge. The knowledge may\nconsist of any fuzzy predicate logic rules. For this, we utilize methods from\nexplainable artificial intelligence (XAI): First, using concept embedding\nanalysis, the output of a computer vision CNN is post-hoc enriched by concept\noutputs; second, logical rules from prior knowledge are fuzzified to serve as\ncontinuous-valued functions on the concept outputs. These can be evaluated with\nlittle computational overhead. We demonstrate three diverse use-cases of our\nmethod on stateof-the-art object detectors: Finding corner cases, utilizing the\nrules for detecting and localizing DNN misbehavior during runtime, and\ncomparing the logical consistency of DNNs. The latter is used to find related\ndifferences between EfficientDet D1 and Mask R-CNN object detectors. We show\nthat this approach benefits from fuzziness and calibrating the concept outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwalbe_G/0/1/0/all/0/1\">Gesina Schwalbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirth_C/0/1/0/all/0/1\">Christian Wirth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Grounded Visual Embeddings for Zero-Shot Learning. (arXiv:2201.00577v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00577","description":"<p>Zero-shot learning methods rely on fixed visual and semantic embeddings,\nextracted from independent vision and language models, both pre-trained for\nother large-scale tasks. This is a weakness of current zero-shot learning\nframeworks as such disjoint embeddings fail to adequately associate visual and\ntextual information to their shared semantic content. Therefore, we propose to\nlearn semantically grounded and enriched visual information by computing a\njoint image and text model with a two-stream network on a proxy task. To\nimprove this alignment between image and textual representations, provided by\nattributes, we leverage ancillary captions to provide grounded semantic\ninformation. Our method, dubbed joint embeddings for zero-shot learning is\nevaluated on several benchmark datasets, improving the performance of existing\nstate-of-the-art methods in both standard ($+1.6$\\% on aPY, $+2.6\\%$ on FLO)\nand generalized ($+2.1\\%$ on AWA$2$, $+2.2\\%$ on CUB) zero-shot recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_S/0/1/0/all/0/1\">Shah Nawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_J/0/1/0/all/0/1\">Jacopo Cavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR Point--to--point Correspondences for Rigorous Registration of Kinematic Scanning in Dynamic Networks. (arXiv:2201.00596v1 [cs.RO])","link":"http://arxiv.org/abs/2201.00596","description":"<p>With the objective of improving the registration of LiDAR point clouds\nproduced by kinematic scanning systems, we propose a novel trajectory\nadjustment procedure that leverages on the automated extraction of selected\nreliable 3D point--to--point correspondences between overlapping point clouds\nand their joint integration (adjustment) together with all raw inertial and\nGNSS observations. This is performed in a tightly coupled fashion using a\nDynamic Network approach that results in an optimally compensated trajectory\nthrough modeling of errors at the sensor, rather than the trajectory, level.\nThe 3D correspondences are formulated as static conditions within this network\nand the registered point cloud is generated with higher accuracy utilizing the\ncorrected trajectory and possibly other parameters determined within the\nadjustment. We first describe the method for selecting correspondences and how\nthey are inserted into the Dynamic Network as new observation models. We then\ndescribe the experiments conducted to evaluate the performance of the proposed\nframework in practical airborne laser scanning scenarios with low-cost MEMS\ninertial sensors. In the conducted experiments, the method proposed to\nestablish 3D correspondences is effective in determining point--to--point\nmatches across a wide range of geometries such as trees, buildings and cars.\nOur results demonstrate that the method improves the point cloud registration\naccuracy, that is otherwise strongly affected by errors in the determined\nplatform attitude or position (in nominal and emulated GNSS outage conditions),\nand possibly determine unknown boresight angles using only a fraction of the\ntotal number of 3D correspondences that are established.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brun_A/0/1/0/all/0/1\">Aur&#xe9;lien Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucci_D/0/1/0/all/0/1\">Davide Antonio Cucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skaloud_J/0/1/0/all/0/1\">Jan Skaloud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v1 [cs.LG])","link":"http://arxiv.org/abs/2201.00604","description":"<p>Most semi-supervised learning methods over-sample labeled data when\nconstructing training mini-batches. This paper studies whether this common\npractice improves learning and how. We compare it to an alternative setting\nwhere each mini-batch is uniformly sampled from all the training data, labeled\nor not, which greatly reduces direct supervision from true labels in typical\nlow-label regimes. However, this simpler setting can also be seen as more\ngeneral and even necessary in multi-task problems where over-sampling labeled\ndata would become intractable. Our experiments on semi-supervised CIFAR-10\nimage classification using FixMatch show a performance drop when using the\nuniform sampling approach which diminishes when the amount of labeled data or\nthe training time increases. Further, we analyse the training dynamics to\nunderstand how over-sampling of labeled data compares to uniform sampling. Our\nmain finding is that over-sampling is especially beneficial early in training\nbut gets less important in the later stages when more pseudo-labels become\ncorrect. Nevertheless, we also find that keeping some true labels remains\nimportant to avoid the accumulation of confirmation errors from incorrect\npseudo-labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabadan_M/0/1/0/all/0/1\">Miquel Mart&#xed; i Rabad&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bujwid_S/0/1/0/all/0/1\">Sebastian Bujwid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieropan_A/0/1/0/all/0/1\">Alessandro Pieropan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1\">Atsuto Maki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings. (arXiv:2201.00625v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00625","description":"<p>Spotting graphical symbols from the computer-aided design (CAD) drawings is\nessential to many industrial applications. Different from raster images, CAD\ndrawings are vector graphics consisting of geometric primitives such as\nsegments, arcs, and circles. By treating each CAD drawing as a graph, we\npropose a novel graph attention network GAT-CADNet to solve the panoptic symbol\nspotting problem: vertex features derived from the GAT branch are mapped to\nsemantic labels, while their attention scores are cascaded and mapped to\ninstance prediction. Our key contributions are three-fold: 1) the instance\nsymbol spotting task is formulated as a subgraph detection problem and solved\nby predicting the adjacency matrix; 2) a relative spatial encoding (RSE) module\nexplicitly encodes the relative positional and geometric relation among\nvertices to enhance the vertex attention; 3) a cascaded edge encoding (CEE)\nmodule extracts vertex attentions from multiple stages of GAT and treats them\nas edge encoding to predict the adjacency matrix. The proposed GAT-CADNet is\nintuitive yet effective and manages to solve the panoptic symbol spotting\nproblem in one consolidated network. Extensive experiments and ablation studies\non the public benchmark show that our graph-based approach surpasses existing\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianfang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Feature Extraction from Histopathological Images Through A Fine-tuning ImageNet Model. (arXiv:2201.00636v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00636","description":"<p>Due to lack of annotated pathological images, transfer learning has been the\npredominant approach in the field of digital pathology.Pre-trained neural\nnetworks based on ImageNet database are often used to extract \"off the shelf\"\nfeatures, achieving great success in predicting tissue types, molecular\nfeatures, and clinical outcomes, etc. We hypothesize that fine-tuning the\npre-trained models using histopathological images could further improve feature\nextraction, and downstream prediction performance.We used 100,000 annotated HE\nimage patches for colorectal cancer (CRC) to finetune a pretrained Xception\nmodel via a twostep approach.The features extracted from finetuned Xception\n(FTX2048) model and Imagepretrained (IMGNET2048) model were compared through:\n(1) tissue classification for HE images from CRC, same image type that was used\nfor finetuning; (2) prediction of immunerelated gene expression and (3) gene\nmutations for lung adenocarcinoma (LUAD).Fivefold cross validation was used for\nmodel performance evaluation. The extracted features from the finetuned FTX2048\nexhibited significantly higher accuracy for predicting tisue types of CRC\ncompared to the off the shelf feature directly from Xception based on ImageNet\ndatabase. Particularly, FTX2048 markedly improved the accuracy for stroma from\n87% to 94%. Similarly, features from FTX2048 boosted the prediction of\ntranscriptomic expression of immunerelated genesin LUAD. For the genes that had\nsignigicant relationships with image fetures, the features fgrom the finetuned\nmodel imprroved the prediction for the majority of the genes. Inaddition,\nfetures from FTX2048 improved prediction of mutation for 5 out of 9 most\nfrequently mutated genes in LUAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cen_M/0/1/0/all/0/1\">Min Cen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jinfeng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Steven Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression-Resistant Backdoor Attack against Deep Neural Networks. (arXiv:2201.00672v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00672","description":"<p>In recent years, many backdoor attacks based on training data poisoning have\nbeen proposed. However, in practice, those backdoor attacks are vulnerable to\nimage compressions. When backdoor instances are compressed, the feature of\nspecific backdoor trigger will be destroyed, which could result in the backdoor\nattack performance deteriorating. In this paper, we propose a\ncompression-resistant backdoor attack based on feature consistency training. To\nthe best of our knowledge, this is the first backdoor attack that is robust to\nimage compressions. First, both backdoor images and their compressed versions\nare input into the deep neural network (DNN) for training. Then, the feature of\neach image is extracted by internal layers of the DNN. Next, the feature\ndifference between backdoor images and their compressed versions are minimized.\nAs a result, the DNN treats the feature of compressed images as the feature of\nbackdoor images in feature space. After training, the backdoor attack against\nDNN is robust to image compression. Furthermore, we consider three different\nimage compressions (i.e., JPEG, JPEG2000, WEBP) in feature consistency\ntraining, so that the backdoor attack is robust to multiple image compression\nalgorithms. Experimental results demonstrate the effectiveness and robustness\nof the proposed backdoor attack. When the backdoor instances are compressed,\nthe attack success rate of common backdoor attack is lower than 10%, while the\nattack success rate of our compression-resistant backdoor is greater than 97%.\nThe compression-resistant attack is still robust even when the backdoor images\nare compressed with low compression quality. In addition, extensive experiments\nhave demonstrated that, our compression-resistant backdoor attack has the\ngeneralization ability to resist image compression which is not used in the\ntraining process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v1 [cs.IR])","link":"http://arxiv.org/abs/2201.00693","description":"<p>To enhance research on multimodal knowledge base and multimodal information\nprocessing, we propose a new task called multimodal entity tagging (MET) with a\nmultimodal knowledge base (MKB). We also develop a dataset for the problem\nusing an existing MKB. In an MKB, there are entities and their associated texts\nand images. In MET, given a text-image pair, one uses the information in the\nMKB to automatically identify the related entity in the text-image pair. We\nsolve the task by using the information retrieval paradigm and implement\nseveral baselines using state-of-the-art methods in NLP and CV. We conduct\nextensive experiments and make analyses on the experimental results. The\nresults show that the task is challenging, but current technologies can achieve\nrelatively high performance. We will release the dataset, code, and models for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chao Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview point cloud registration with anisotropic and space-varying localization noise. (arXiv:2201.00708v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00708","description":"<p>In this paper, we address the problem of registering multiple point clouds\ncorrupted with high anisotropic localization noise. Our approach follows the\nwidely used framework of Gaussian mixture model (GMM) reconstruction with an\nexpectation-maximization (EM) algorithm. Existing methods are based on an\nimplicit assumption of space-invariant isotropic Gaussian noise. However, this\nassumption is violated in practice in applications such as single molecule\nlocalization microscopy (SMLM). To address this issue, we propose to introduce\nan explicit localization noise model that decouples shape modeling with the GMM\nfrom noise handling. We design a stochastic EM algorithm that considers\nnoise-free data as a latent variable, with closed-form solutions at each EM\nstep. The first advantage of our approach is to handle space-variant and\nanisotropic Gaussian noise with arbitrary covariances. The second advantage is\nto leverage the explicit noise model to impose prior knowledge about the noise\nthat may be available from physical sensors. We show on various simulated data\nthat our noise handling strategy improves significantly the robustness to high\nlevels of anisotropic noise. We also demonstrate the performance of our method\non real SMLM data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fortun_D/0/1/0/all/0/1\">Denis Fortun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baudrier_E/0/1/0/all/0/1\">Etienne Baudrier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwettler_F/0/1/0/all/0/1\">Fabian Zwettler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_M/0/1/0/all/0/1\">Markus Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisan_S/0/1/0/all/0/1\">Sylvain Faisan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Data Classification with a Label-driven Auto-weighted Strategy. (arXiv:2201.00714v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00714","description":"<p>Distinguishing the importance of views has proven to be quite helpful for\nsemi-supervised multi-view learning models. However, existing strategies cannot\ntake advantage of semi-supervised information, only distinguishing the\nimportance of views from a data feature perspective, which is often influenced\nby low-quality views then leading to poor performance. In this paper, by\nestablishing a link between labeled data and the importance of different views,\nwe propose an auto-weighted strategy to evaluate the importance of views from a\nlabel perspective to avoid the negative impact of unimportant or low-quality\nviews. Based on this strategy, we propose a transductive semi-supervised\nauto-weighted multi-view classification model. The initialization of the\nproposed model can be effectively determined by labeled data, which is\npractical. The model is decoupled into three small-scale sub-problems that can\nefficiently be optimized with a local convergence guarantee. The experimental\nresults on classification tasks show that the proposed method achieves optimal\nor sub-optimal classification accuracy at the lowest computational cost\ncompared to other related methods, and the weight change experiments show that\nour proposed strategy can distinguish view importance more accurately than\nother related strategies on multi-view datasets with low-quality views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guoxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haonan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shengli Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qibin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BDG-Net: Boundary Distribution Guided Network for Accurate Polyp Segmentation. (arXiv:2201.00767v1 [eess.IV])","link":"http://arxiv.org/abs/2201.00767","description":"<p>Colorectal cancer (CRC) is one of the most common fatal cancer in the world.\nPolypectomy can effectively interrupt the progression of adenoma to\nadenocarcinoma, thus reducing the risk of CRC development. Colonoscopy is the\nprimary method to find colonic polyps. However, due to the different sizes of\npolyps and the unclear boundary between polyps and their surrounding mucosa, it\nis challenging to segment polyps accurately. To address this problem, we design\na Boundary Distribution Guided Network (BDG-Net) for accurate polyp\nsegmentation. Specifically, under the supervision of the ideal Boundary\nDistribution Map (BDM), we use Boundary Distribution Generate Module (BDGM) to\naggregate high-level features and generate BDM. Then, BDM is sent to the\nBoundary Distribution Guided Decoder (BDGD) as complementary spatial\ninformation to guide the polyp segmentation. Moreover, a multi-scale feature\ninteraction strategy is adopted in BDGD to improve the segmentation accuracy of\npolyps with different sizes. Extensive quantitative and qualitative evaluations\ndemonstrate the effectiveness of our model, which outperforms state-of-the-art\nmodels remarkably on five public polyp datasets while maintaining low\ncomputational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zihuan Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhichuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jie Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Linfeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceQgen: Semi-Supervised Deep Learning for Face Image Quality Assessment. (arXiv:2201.00770v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00770","description":"<p>In this paper we develop FaceQgen, a No-Reference Quality Assessment approach\nfor face images based on a Generative Adversarial Network that generates a\nscalar quality measure related with the face recognition accuracy. FaceQgen\ndoes not require labelled quality measures for training. It is trained from\nscratch using the SCface database. FaceQgen applies image restoration to a face\nimage of unknown quality, transforming it into a canonical high quality image,\ni.e., frontal pose, homogeneous background, etc. The quality estimation is\nbuilt as the similarity between the original and the restored images, since low\nquality images experience bigger changes due to restoration. We compare three\ndifferent numerical quality measures: a) the MSE between the original and the\nrestored images, b) their SSIM, and c) the output score of the Discriminator of\nthe GAN. The results demonstrate that FaceQgen's quality measures are good\nestimators of face recognition accuracy. Our experiments include a comparison\nwith other quality assessment methods designed for faces and for general\nimages, in order to position FaceQgen in the state of the art. This comparison\nshows that, even though FaceQgen does not surpass the best existing face\nquality assessment methods in terms of face recognition accuracy prediction, it\nachieves good enough results to demonstrate the potential of semi-supervised\nlearning approaches for quality estimation (in particular, data-driven learning\nbased on a single high quality image per subject), having the capacity to\nimprove its performance in the future with adequate refinement of the model and\nthe significant advantage over competing methods of not needing quality labels\nfor its development. This makes FaceQgen flexible and scalable without\nexpensive data curation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Ortega_J/0/1/0/all/0/1\">Javier Hernandez-Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Autoencoder for Point Cloud Self-supervised Representation Learning. (arXiv:2201.00785v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00785","description":"<p>Many 3D representations (e.g., point clouds) are discrete samples of the\nunderlying continuous 3D surface. This process inevitably introduces sampling\nvariations on the underlying 3D shapes. In learning 3D representation, the\nvariations should be disregarded while transferable knowledge of the underlying\n3D shape should be captured. This becomes a grand challenge in existing\nrepresentation learning paradigms. This paper studies autoencoding on point\nclouds. The standard autoencoding paradigm forces the encoder to capture such\nsampling variations as the decoder has to reconstruct the original point cloud\nthat has sampling variations. We introduce Implicit Autoencoder(IAE), a simple\nyet effective method that addresses this challenge by replacing the point cloud\ndecoder with an implicit decoder. The implicit decoder outputs a continuous\nrepresentation that is shared among different point cloud sampling of the same\nmodel. Reconstructing under the implicit representation can prioritize that the\nencoder discards sampling variations, introducing more space to learn useful\nfeatures. We theoretically justify this claim under a simple linear\nautoencoder. Moreover, the implicit decoder offers a rich space to design\nsuitable implicit representations for different tasks. We demonstrate the\nusefulness of IAE across various self-supervised learning tasks for both 3D\nobjects and 3D scenes. Experimental results show that IAE consistently\noutperforms the state-of-the-art in each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Li Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering. (arXiv:2201.00791v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00791","description":"<p>While recent advances in deep neural networks have made it possible to render\nhigh-quality images, generating photo-realistic and personalized talking head\nremains challenging. With given audio, the key to tackling this task is\nsynchronizing lip movement and simultaneously generating personalized\nattributes like head movement and eye blink. In this work, we observe that the\ninput audio is highly correlated to lip motion while less correlated to other\npersonalized attributes (e.g., head movements). Inspired by this, we propose a\nnovel framework based on neural radiance field to pursue high-fidelity and\npersonalized talking head generation. Specifically, neural radiance field takes\nlip movements features and personalized attributes as two disentangled\nconditions, where lip movements are directly predicted from the audio inputs to\nachieve lip-synchronized generation. In the meanwhile, personalized attributes\nare sampled from a probabilistic model, where we design a Transformer-based\nvariational autoencoder sampled from Gaussian Process to learn plausible and\nnatural-looking head pose and eye blink. Experiments on several benchmarks\ndemonstrate that our method achieves significantly better results than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">RuiZhe Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space. (arXiv:2201.00814v1 [cs.CV])","link":"http://arxiv.org/abs/2201.00814","description":"<p>This paper explores the feasibility of finding an optimal sub-model from a\nvision transformer and introduces a pure vision transformer slimming (ViT-Slim)\nframework that can search such a sub-structure from the original model\nend-to-end across multiple dimensions, including the input tokens, MHSA and MLP\nmodules with state-of-the-art performance. Our method is based on a learnable\nand unified l1 sparsity constraint with pre-defined factors to reflect the\nglobal importance in the continuous searching space of different dimensions.\nThe searching process is highly efficient through a single-shot training\nscheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for\nsearching process, and the searched structure is flexible with diverse\ndimensionalities in different modules. Then, a budget threshold is employed\naccording to the requirements of accuracy-FLOPs trade-off on running devices,\nand a re-training process is performed to obtain the final models. The\nextensive experiments show that our ViT-Slim can compress up to 40% of\nparameters and 40% FLOPs on various vision transformers while increasing the\naccuracy by ~0.6% on ImageNet. We also demonstrate the advantage of our\nsearched models on several downstream datasets. Our source code will be\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods. (arXiv:1907.09358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.09358","description":"<p>Interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as machine\nlearning, computer vision, and natural language processing. Much of the growth\nin these fields has been made possible with deep learning, a sub-area of\nmachine learning that uses artificial neural networks. This has created\nsignificant interest in the integration of vision and language. In this survey,\nwe focus on ten prominent tasks that integrate language and vision by\ndiscussing their problem formulation, methods, existing datasets, evaluation\nmeasures, and compare the results obtained with corresponding state-of-the-art\nmethods. Our efforts go beyond earlier surveys which are either task-specific\nor concentrate only on one type of visual content, i.e., image or video.\nFurthermore, we also provide some potential future directions in this field of\nresearch with an anticipation that this survey stimulates innovative thoughts\nand ideas to address the existing challenges and build new applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mogadala_A/0/1/0/all/0/1\">Aditya Mogadala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalimuthu_M/0/1/0/all/0/1\">Marimuthu Kalimuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.03781","description":"<p>Each woman living in the United States has about 1 in 8 chance of developing\ninvasive breast cancer. The mitotic cell count is one of the most common tests\nto assess the aggressiveness or grade of breast cancer. In this prognosis,\nhistopathology images must be examined by a pathologist using high-resolution\nmicroscopes to count the cells. Unfortunately, can be an exhaustive task with\npoor reproducibility, especially for non-experts. Deep learning networks have\nrecently been adapted to medical applications which are able to automatically\nlocalize these regions of interest. However, these region-based networks lack\nthe ability to take advantage of the segmentation features produced by a full\nimage CNN which are often used as a sole method of detection. Therefore, the\nproposed method leverages Faster RCNN for object detection while fusing\nsegmentation features generated by a UNet with RGB image features to achieve an\nF-score of 0.508 on the MITOS-ATYPIA 2014 mitosis counting challenge dataset,\noutperforming state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yancey_R/0/1/0/all/0/1\">Robin Elizabeth Yancey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedBoost: Federated Learning with Gradient Protected Boosting for Text Recognition. (arXiv:2007.07296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07296","description":"<p>Typical machine learning approaches require centralized data for model\ntraining, which may not be possible where restrictions on data sharing are in\nplace due to, for instance, privacy and gradient protection. The recently\nproposed Federated Learning (FL) framework allows learning a shared model\ncollaboratively without data being centralized or data sharing among data\nowners. However, we show in this paper that the generalization ability of the\njoint model is poor on Non-Independent and Non-Identically Distributed\n(Non-IID) data, particularly when the Federated Averaging (FedAvg) strategy is\nused due to the weight divergence phenomenon. We propose a novel boosting\nalgorithm for FL to address this generalization issue, as well as achieving a\nmuch faster convergence rate in gradient-based optimization. In addition, a\nsecure gradient sharing protocol using Homomorphic Encryption (HE) and\nDifferential Privacy (DP) is introduced to defend against gradient leakage\nattack. We demonstrate the proposed Federated Boosting (FedBoost) method\nachieves significant improvements in both prediction accuracy and run-time\nefficiency on text recognition task using public benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingjing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yichuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Boundary Based Out-of-Distribution Classifier for Generalized Zero-Shot Learning. (arXiv:2008.04872v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.04872","description":"<p>Generalized Zero-Shot Learning (GZSL) is a challenging topic that has\npromising prospects in many realistic scenarios. Using a gating mechanism that\ndiscriminates the unseen samples from the seen samples can decompose the GZSL\nproblem to a conventional Zero-Shot Learning (ZSL) problem and a supervised\nclassification problem. However, training the gate is usually challenging due\nto the lack of data in the unseen domain. To resolve this problem, in this\npaper, we propose a boundary based Out-of-Distribution (OOD) classifier which\nclassifies the unseen and seen domains by only using seen samples for training.\nFirst, we learn a shared latent space on a unit hyper-sphere where the latent\ndistributions of visual features and semantic attributes are aligned\nclass-wisely. Then we find the boundary and the center of the manifold for each\nclass. By leveraging the class centers and boundaries, the unseen samples can\nbe separated from the seen samples. After that, we use two experts to classify\nthe seen and unseen samples separately. We extensively validate our approach on\nfive popular benchmark datasets including AWA1, AWA2, CUB, FLO and SUN. The\nexperimental results demonstrate the advantages of our approach over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xuguang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs. (arXiv:2010.11703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.11703","description":"<p>In recent years, the robotics community has extensively examined methods\nconcerning the place recognition task within the scope of simultaneous\nlocalization and mapping applications.This article proposes an appearance-based\nloop closure detection pipeline named ``FILD++\" (Fast and Incremental Loop\nclosure Detection).First, the system is fed by consecutive images and, via\npassing them twice through a single convolutional neural network, global and\nlocal deep features are extracted.Subsequently, a hierarchical navigable\nsmall-world graph incrementally constructs a visual database representing the\nrobot's traversed path based on the computed global features.Finally, a query\nimage, grabbed each time step, is set to retrieve similar locations on the\ntraversed route.An image-to-image pairing follows, which exploits local\nfeatures to evaluate the spatial information. Thus, in the proposed article, we\npropose a single network for global and local feature extraction in contrast to\nour previous work (FILD), while an exhaustive search for the verification\nprocess is adopted over the generated deep local features avoiding the\nutilization of hash codes. Exhaustive experiments on eleven publicly available\ndatasets exhibit the system's high performance (achieving the highest recall\nscore on eight of them) and low execution times (22.05 ms on average in New\nCollege, which is the largest one containing 52480 images) compared to other\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haogang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsintotas_K/0/1/0/all/0/1\">Konstantinos A. Tsintotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasteratos_A/0/1/0/all/0/1\">Antonios Gasteratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Contrastive Self-Supervised Learning with False Negative Cancellation. (arXiv:2011.11765v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11765","description":"<p>Self-supervised representation learning has made significant leaps fueled by\nprogress in contrastive learning, which seeks to learn transformations that\nembed positive input pairs nearby, while pushing negative pairs far apart.\nWhile positive pairs can be generated reliably (e.g., as different views of the\nsame image), it is difficult to accurately establish negative pairs, defined as\nsamples from different images regardless of their semantic content or visual\nfeatures. A fundamental problem in contrastive learning is mitigating the\neffects of false negatives. Contrasting false negatives induces two critical\nissues in representation learning: discarding semantic information and slow\nconvergence. In this paper, we propose novel approaches to identify false\nnegatives, as well as two strategies to mitigate their effect, i.e. false\nnegative elimination and attraction, while systematically performing rigorous\nevaluations to study this problem in detail. Our method exhibits consistent\nimprovements over existing contrastive learning-based methods. Without labels,\nwe identify false negatives with 40% accuracy among 1000 semantic classes on\nImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the\nprevious state-of-the-art when finetuning with 1% labels. Our code is available\nat https://github.com/google-research/fnc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tri Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1\">Michael Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_M/0/1/0/all/0/1\">Maryam Khademi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Open-World Learning and Steps Toward Open-World Learning Without Labels. (arXiv:2011.12906v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12906","description":"<p>In open-world learning, an agent starts with a set of known classes, detects,\nand manages things that it does not know, and learns them over time from a\nnon-stationary stream of data. Open-world learning is related to but also\ndistinct from a multitude of other learning problems and this paper briefly\nanalyzes the key differences between a wide range of problems including\nincremental learning, generalized novelty discovery, and generalized zero-shot\nlearning. This paper formalizes various open-world learning problems including\nopen-world learning without labels. These open-world problems can be addressed\nwith modifications to known elements, we present a new framework that enables\nagents to combine various modules for novelty-detection,\nnovelty-characterization, incremental learning, and instance management to\nlearn new classes from a stream of unlabeled data in an unsupervised manner,\nsurvey how to adapt a few state-of-the-art techniques to fit the framework and\nuse them to define seven baselines for performance on the open-world learning\nwithout labels problem. We then discuss open-world learning quality and analyze\nhow that can improve instance management. We also discuss some of the general\nambiguity issues that occur in open-world learning without labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafarzadeh_M/0/1/0/all/0/1\">Mohsen Jafarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamija_A/0/1/0/all/0/1\">Akshay Raj Dhamija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Touqeer Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boult_T/0/1/0/all/0/1\">Terrance E. Boult</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Descent Aggregation Framework for Gradient-based Bi-level Optimization. (arXiv:2102.07976v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07976","description":"<p>In recent years, a variety of gradient-based methods have been developed to\nsolve Bi-Level Optimization (BLO) problems in machine learning and computer\nvision areas. However, the theoretical correctness and practical effectiveness\nof these existing approaches always rely on some restrictive conditions (e.g.,\nLower-Level Singleton, LLS), which could hardly be satisfied in real-world\napplications. Moreover, previous literature only proves theoretical results\nbased on their specific iteration strategies, thus lack a general recipe to\nuniformly analyze the convergence behaviors of different gradient-based BLOs.\nIn this work, we formulate BLOs from an optimistic bi-level viewpoint and\nestablish a new gradient-based algorithmic framework, named Bi-level Descent\nAggregation (BDA), to partially address the above issues. Specifically, BDA\nprovides a modularized structure to hierarchically aggregate both the upper-\nand lower-level subproblems to generate our bi-level iterative dynamics.\nTheoretically, we establish a general convergence analysis template and derive\na new proof recipe to investigate the essential theoretical properties of\ngradient-based BLO methods. Furthermore, this work systematically explores the\nconvergence behavior of BDA in different optimization scenarios, i.e.,\nconsidering various solution qualities (i.e., global/local/stationary solution)\nreturned from solving approximation subproblems. Extensive experiments justify\nour theoretical results and demonstrate the superiority of the proposed\nalgorithm for hyper-parameter optimization and meta-learning tasks. Source code\nis available at https://github.com/vis-opt-group/BDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1\">Pan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from Volunteers and Deep Learning for 314,000 Galaxies. (arXiv:2102.08414v2 [astro-ph.GA] UPDATED)","link":"http://arxiv.org/abs/2102.08414","description":"<p>We present Galaxy Zoo DECaLS: detailed visual morphological classifications\nfor Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8\nfootprint. Deeper DECaLS images (r=23.6 vs. r=22.2 from SDSS) reveal spiral\narms, weak bars, and tidal features not previously visible in SDSS imaging. To\nbest exploit the greater depth of DECaLS images, volunteers select from a new\nset of answers designed to improve our sensitivity to mergers and bars. Galaxy\nZoo volunteers provide 7.5 million individual classifications over 314,000\ngalaxies. 140,000 galaxies receive at least 30 classifications, sufficient to\naccurately measure detailed morphology like bars, and the remainder receive\napproximately 5. All classifications are used to train an ensemble of Bayesian\nconvolutional neural networks (a state-of-the-art deep learning method) to\npredict posteriors for the detailed morphology of all 314,000 galaxies. When\nmeasured against confident volunteer classifications, the networks are\napproximately 99% accurate on every question. Morphology is a fundamental\nfeature of every galaxy; our human and machine classifications are an accurate\nand detailed resource for understanding how galaxies evolve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lintott_C/0/1/0/all/0/1\">Chris Lintott</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Geron_T/0/1/0/all/0/1\">Tobias Geron</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kruk_S/0/1/0/all/0/1\">Sandor Kruk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Krawczyk_C/0/1/0/all/0/1\">Coleman Krawczyk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Willett_K/0/1/0/all/0/1\">Kyle W. Willett</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bamford_S/0/1/0/all/0/1\">Steven Bamford</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kelvin_L/0/1/0/all/0/1\">Lee S. Kelvin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fortson_L/0/1/0/all/0/1\">Lucy Fortson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Keel_W/0/1/0/all/0/1\">William Keel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Masters_K/0/1/0/all/0/1\">Karen L. Masters</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mehta_V/0/1/0/all/0/1\">Vihang Mehta</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Simmons_B/0/1/0/all/0/1\">Brooke D. Simmons</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Smethurst_R/0/1/0/all/0/1\">Rebecca Smethurst</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Smith_L/0/1/0/all/0/1\">Lewis Smith</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Baeten_E/0/1/0/all/0/1\">Elisabeth M. Baeten</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Macmillan_C/0/1/0/all/0/1\">Christine Macmillan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elsa: Energy-based learning for semi-supervised anomaly detection. (arXiv:2103.15296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15296","description":"<p>Anomaly detection aims at identifying deviant instances from the normal data\ndistribution. Many advances have been made in the field, including the\ninnovative use of unsupervised contrastive learning. However, existing methods\ngenerally assume clean training data and are limited when the data contain\nunknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly\ndetection approach that unifies the concept of energy-based models with\nunsupervised contrastive learning. Elsa instills robustness against any data\ncontamination by a carefully designed fine-tuning step based on the new energy\nfunction that forces the normal data to be divided into classes of prototypes.\nExperiments on multiple contamination scenarios show the proposed model\nachieves SOTA performance. Extensive analyses also verify the contribution of\neach component in the proposed model. Beyond the experiments, we also offer a\ntheoretical interpretation of why contrastive learning alone cannot detect\nanomalies under data contamination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sungwon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hyeonho Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Vibration Tomography: Estimating Interior Material Properties from Monocular Video. (arXiv:2104.02735v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02735","description":"<p>An object's interior material properties, while invisible to the human eye,\ndetermine motion observed on its surface. We propose an approach that estimates\nheterogeneous material properties of an object from a monocular video of its\nsurface vibrations. Specifically, we show how to estimate Young's modulus and\ndensity throughout a 3D object with known geometry. Knowledge of how these\nvalues change across the object is useful for simulating its motion and\ncharacterizing any defects. Traditional non-destructive testing approaches,\nwhich often require expensive instruments, generally estimate only homogenized\nmaterial properties or simply identify the presence of defects. In contrast,\nour approach leverages monocular video to (1) identify image-space modes from\nan object's sub-pixel motion, and (2) directly infer spatially-varying Young's\nmodulus and density values from the observed modes. We demonstrate our approach\non both simulated and real videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Berthy T. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogren_A/0/1/0/all/0/1\">Alexander C. Ogren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daraio_C/0/1/0/all/0/1\">Chiara Daraio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouman_K/0/1/0/all/0/1\">Katherine L. Bouman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BM-NAS: Bilevel Multimodal Neural Architecture Search. (arXiv:2104.09379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09379","description":"<p>Deep neural networks (DNNs) have shown superior performances on various\nmultimodal learning problems. However, it often requires huge efforts to adapt\nDNNs to individual multimodal tasks by manually engineering unimodal features\nand designing multimodal feature fusion strategies. This paper proposes Bilevel\nMultimodal Neural Architecture Search (BM-NAS) framework, which makes the\narchitecture of multimodal fusion models fully searchable via a bilevel\nsearching scheme. At the upper level, BM-NAS selects the inter/intra-modal\nfeature pairs from the pretrained unimodal backbones. At the lower level,\nBM-NAS learns the fusion strategy for each feature pair, which is a combination\nof predefined primitive operations. The primitive operations are elaborately\ndesigned and they can be flexibly combined to accommodate various effective\nfeature fusion modules such as multi-head attention (Transformer) and Attention\non Attention (AoA). Experimental results on three multimodal tasks demonstrate\nthe effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS\nachieves competitive performances with much less search time and fewer model\nparameters in comparison with the existing generalized multimodal NAS methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yihang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07059","description":"<p>Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, manually annotating\nmedical data is often laborious, and most existing learning-based approaches\nfail to accurately delineate object boundaries without effective geometric\nconstraints. Contrastive learning, a sub-area of self-supervised learning, has\nrecently been noted as a promising direction in multiple application fields. In\nthis work, we present a novel Contrastive Voxel-wise Representation\nDistillation (CVRD) method with geometric constraints to learn global-local\nvisual representations for volumetric medical image segmentation with limited\nannotations. Our framework can effectively learn global and local features by\ncapturing 3D spatial context and rich anatomical information. Specifically, we\nintroduce a voxel-to-volume contrastive algorithm to learn global information\nfrom 3D images, and propose to perform local voxel-to-voxel distillation to\nexplicitly make use of local cues in the embedding space. Moreover, we\nintegrate an elastic interaction-based active contour model as a geometric\nregularization term to enable fast and reliable object delineations in an\nend-to-end learning manner. Results on the Atrial Segmentation Challenge\ndataset demonstrate superiority of our proposed scheme, especially in a setting\nwith a very limited number of annotated data. The code will be available at\nhttps://github.com/charlesyou999648/CVRD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Visual Place Recognition in Dynamics-Invariant Perception Space. (arXiv:2105.07800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07800","description":"<p>Visual place recognition is one of the essential and challenging problems in\nthe fields of robotics. In this letter, we for the first time explore the use\nof multi-modal fusion of semantic and visual modalities in dynamics-invariant\nspace to improve place recognition in dynamic environments. We achieve this by\nfirst designing a novel deep learning architecture to generate the static\nsemantic segmentation and recover the static image directly from the\ncorresponding dynamic image. We then innovatively leverage the\nspatial-pyramid-matching model to encode the static semantic segmentation into\nfeature vectors. In parallel, the static image is encoded using the popular\nBag-of-words model. On the basis of the above multi-modal features, we finally\nmeasure the similarity between the query image and target landmark by the joint\nsimilarity of their semantic and visual codes. Extensive experiments\ndemonstrate the effectiveness and robustness of the proposed approach for place\nrecognition in dynamic environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changyin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09008","description":"<p>In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi-Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung-Yen Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09887","description":"<p>It is difficult to accurately label ambiguous and complex shaped targets\nmanually by binary masks. The weakness of binary mask under-expression is\nhighlighted in medical image segmentation, where blurring is prevalent. In the\ncase of multiple annotations, reaching a consensus for clinicians by binary\nmasks is more challenging. Moreover, these uncertain areas are related to the\nlesions' structure and may contain anatomical information beneficial to\ndiagnosis. However, current studies on uncertainty mainly focus on the\nuncertainty in model training and data labels. None of them investigate the\ninfluence of the ambiguous nature of the lesion itself.Inspired by image\nmatting, this paper introduces alpha matte as a soft mask to represent\nuncertain areas in medical scenes and accordingly puts forward a new\nuncertainty quantification method to fill the gap of uncertainty research for\nlesion structure. In this work, we introduce a new architecture to generate\nbinary masks and alpha mattes in a multitasking framework, which outperforms\nall state-of-the-art matting algorithms compared. The proposed uncertainty map\nis able to highlight the ambiguous regions and a novel multitasking loss\nweighting strategy we presented can improve performance further and demonstrate\ntheir concrete benefits. To fully-evaluate the effectiveness of our proposed\nmethod, we first labelled three medical datasets with alpha matte to address\nthe shortage of available matting datasets in medical scenes and prove the\nalpha matte to be a more efficient labeling method than a binary mask from both\nqualitative and quantitative aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yelin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiufen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation. (arXiv:2107.03358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03358","description":"<p>In this paper, we tackle the problem of novel visual category discovery,\ni.e., grouping unlabelled images from new classes into different semantic\npartitions by leveraging a labelled dataset that contains images from other\ndifferent but relevant categories. This is a more realistic and challenging\nsetting than conventional semi-supervised learning. We propose a two-branch\nlearning framework for this problem, with one branch focusing on local\npart-level information and the other branch focusing on overall\ncharacteristics. To transfer knowledge from the labelled data to the\nunlabelled, we propose using dual ranking statistics on both branches to\ngenerate pseudo labels for training on the unlabelled data. We further\nintroduce a mutual knowledge distillation method to allow information exchange\nand encourage agreement between the two branches for discovering new\ncategories, allowing our model to enjoy the benefits of global and local\nfeatures. We comprehensively evaluate our method on public benchmarks for\ngeneric object classification, as well as the more challenging datasets for\nfine-grained visual recognition, achieving state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04537","description":"<p>With the advent of advancements in deep learning approaches, such as deep\nconvolution neural network, residual neural network, adversarial network; U-Net\narchitectures are most widely utilized in biomedical image segmentation to\naddress the automation in identification and detection of the target regions or\nsub-regions. In recent studies, U-Net based approaches have illustrated\nstate-of-the-art performance in different applications for the development of\ncomputer-aided diagnosis systems for early diagnosis and treatment of diseases\nsuch as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various\nmodalities. This article contributes in presenting the success of these\napproaches by describing the U-Net framework, followed by the comprehensive\nanalysis of the U-Net variants by performing 1) inter-modality, and 2)\nintra-modality categorization to establish better insights into the associated\nchallenges and solutions. Besides, this article also highlights the\ncontribution of U-Net based frameworks in the ongoing pandemic, severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.\nFinally, the strengths and similarities of these U-Net variants are analysed\nalong with the challenges involved in biomedical image segmentation to uncover\npromising future research directions in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00351","description":"<p>A key challenge in the task of human pose and shape estimation is occlusion,\nincluding self-occlusions, object-human occlusions, and inter-person\nocclusions. The lack of diverse and accurate pose and shape training data\nbecomes a major bottleneck, especially for scenes with occlusions in the wild.\nIn this paper, we focus on the estimation of human pose and shape in the case\nof inter-person occlusions, while also handling object-human occlusions and\nself-occlusion. We propose a novel framework that synthesizes occlusion-aware\nsilhouette and 2D keypoints data and directly regress to the SMPL pose and\nshape parameters. A neural 3D mesh renderer is exploited to enable silhouette\nsupervision on the fly, which contributes to great improvements in shape\nestimation. In addition, keypoints-and-silhouette-driven training data in\npanoramic viewpoints are synthesized to compensate for the lack of viewpoint\ndiversity in any existing dataset. Experimental results show that we are among\nthe state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose\nestimation accuracy. The proposed method evidently outperforms the rank-1\nmethod in terms of shape estimation. Top performance is also achieved on SSP-3D\nin terms of shape prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaibing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1\">Masahiro Toyoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution. (arXiv:2108.02348v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02348","description":"<p>For deep learning methods of real-world image super-resolution, the most\ncritical issue is whether the paired low and high resolution images for\ntraining accurately reflect the sampling process of real cameras. Low and high\nresolution (LR$\\sim$HR) image pairs synthesized by existing degradation models\n(e.g., bicubic downsampling) deviate from those in reality; thus the\nsuper-resolution CNN trained by these synthesized LR$\\sim$HR image pairs does\nnot perform well when being applied to real images. To address the problem, we\npropose a novel data acquisition process to shoot a large set of LR$\\sim$HR\nimage pairs using real cameras. The images are displayed on an ultra-high\nquality screen and captured at different resolutions. The resulting LR$\\sim$HR\nimage pairs can be aligned with very high sub-pixel precision by a novel\nspatial-frequency dual-domain registration method, and hence they provide high\nquality training data for the learning task of super-resolution. Moreover, the\ncaptured HR image and the original digital image offer dual references to\nimprove the learning performance. Experimental results show that training a\nsuper-resolution CNN by our LR$\\sim$HR dataset achieves higher image quality\nthan training it by other datasets in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_X/0/1/0/all/0/1\">Xiao Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02508","description":"<p>The advancements in deep learning technologies have produced immense\ncontributions to biomedical image analysis applications. With breast cancer\nbeing the common deadliest disease among women, early detection is the key\nmeans to improve survivability. Medical imaging like ultrasound presents an\nexcellent visual representation of the functioning of the organs; however, for\nany radiologist analysing such scans is challenging and time consuming which\ndelays the diagnosis process. Although various deep learning based approaches\nare proposed that achieved promising results, the present article introduces an\nefficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)\nmodel with minimal training parameters for tumor segmentation using breast\nultrasound imaging to further improve the segmentation performance of varying\ntumor sizes. The RCA-IUnet model follows U-Net topology with residual inception\ndepth-wise separable convolution and hybrid pooling (max pooling and spectral\npooling) layers. In addition, cross-spatial attention filters are added to\nsuppress the irrelevant features and focus on the target structure. The\nsegmentation performance of the proposed model is validated on two publicly\navailable datasets using standard segmentation evaluation metrics, where it\noutperformed the other state-of-the-art segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03227","description":"<p>Bird's-Eye-View (BEV) maps have emerged as one of the most powerful\nrepresentations for scene understanding due to their ability to provide rich\nspatial context while being easy to interpret and process. Such maps have found\nuse in many real-world tasks that extensively rely on accurate scene\nsegmentation as well as object instance identification in the BEV space for\ntheir operation. However, existing segmentation algorithms only predict the\nsemantics in the BEV space, which limits their use in applications where the\nnotion of object instances is also critical. In this work, we present the first\nBEV panoptic segmentation approach for directly predicting dense panoptic\nsegmentation maps in the BEV, given a single monocular image in the frontal\nview (FV). Our architecture follows the top-down paradigm and incorporates a\nnovel dense transformer module consisting of two distinct transformers that\nlearn to independently map vertical and flat regions in the input image from\nthe FV to the BEV. Additionally, we derive a mathematical formulation for the\nsensitivity of the FV-BEV transformation which allows us to intelligently\nweight pixels in the BEV space to account for the varying descriptiveness\nacross the FV image. Extensive evaluations on the KITTI-360 and nuScenes\ndatasets demonstrate that our approach exceeds the state-of-the-art in the PQ\nmetric by 3.61 pp and 4.93 pp respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07851","description":"<p>Existing salient object detection (SOD) methods mainly rely on CNN-based\nU-shaped structures with skip connections to combine the global contexts and\nlocal spatial details that are crucial for locating salient objects and\nrefining object details, respectively. Despite great successes, the ability of\nCNN in learning global contexts is limited. Recently, the vision transformer\nhas achieved revolutionary progress in computer vision owing to its powerful\nmodeling of global dependencies. However, directly applying the transformer to\nSOD is suboptimal because the transformer lacks the ability to learn local\nspatial representations. To this end, this paper explores the combination of\ntransformer and CNN to learn both global and local representations for SOD. We\npropose a transformer-based Asymmetric Bilateral U-Net (ABiU-Net). The\nasymmetric bilateral encoder has a transformer path and a lightweight CNN path,\nwhere the two paths communicate at each encoder stage to learn complementary\nglobal contexts and local spatial details, respectively. The asymmetric\nbilateral decoder also consists of two paths to process features from the\ntransformer and CNN encoder paths, with communication at each decoder stage for\ndecoding coarse salient object locations and find-grained object details,\nrespectively. Such communication between the two encoder/decoder paths enables\nAbiU-Net to learn complementary global and local representations, taking\nadvantage of the natural properties of transformer and CNN, respectively.\nHence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive\nexperiments demonstrate that ABiU-Net performs favorably against previous\nstate-of-the-art SOD methods. The code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04176","description":"<p>Vision transformers (ViTs) have demonstrated impressive performance on a\nseries of computer vision tasks, yet they still suffer from adversarial\nexamples. % crafted in a similar fashion as CNNs. In this paper, we posit that\nadversarial attacks on transformers should be specially tailored for their\narchitecture, jointly considering both patches and self-attention, in order to\nachieve high transferability. More specifically, we introduce a dual attack\nframework, which contains a Pay No Attention (PNA) attack and a PatchOut\nattack, to improve the transferability of adversarial samples across different\nViTs. We show that skipping the gradients of attention during backpropagation\ncan generate adversarial examples with high transferability. In addition,\nadversarial perturbations generated by optimizing randomly sampled subsets of\npatches at each iteration achieve higher attack success rates than attacks\nusing all patches. We evaluate the transferability of attacks on\nstate-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these\nexperiments demonstrate that the proposed dual attack can greatly boost\ntransferability between ViTs and from ViTs to CNNs. In addition, the proposed\nmethod can easily be combined with existing transfer methods to boost\nperformance. Code is available at https://github.com/zhipeng-wei/PNA-PatchOut.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-Guided Contrastive Network for Zero-Shot Object detection. (arXiv:2109.06062v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06062","description":"<p>Zero-shot object detection (ZSD), the task that extends conventional\ndetection models to detecting objects from unseen categories, has emerged as a\nnew challenge in computer vision. Most existing approaches tackle the ZSD task\nwith a strict mapping-transfer strategy, which may lead to suboptimal ZSD\nresults: 1) the learning process of those models ignores the available unseen\nclass information, and thus can be easily biased towards the seen categories;\n2) the original visual feature space is not well-structured and lack of\ndiscriminative information. To address these issues, we develop a novel\nSemantics-Guided Contrastive Network for ZSD, named ContrastZSD, a detection\nframework that first brings contrastive learning mechanism into the realm of\nzero-shot detection. Particularly, ContrastZSD incorporates two\nsemantics-guided contrastive learning subnets that contrast between\nregion-category and region-region pairs respectively. The pairwise contrastive\ntasks take advantage of additional supervision signals derived from both ground\ntruth label and pre-defined class similarity distribution. Under the guidance\nof those explicit semantic supervision, the model can learn more knowledge\nabout unseen categories to avoid the bias problem to seen concepts, while\noptimizing the data structure of visual features to be more discriminative for\nbetter visual-semantic alignment. Extensive experiments are conducted on two\npopular benchmarks for ZSD, i.e., PASCAL VOC and MS COCO. Results show that our\nmethod outperforms the previous state-of-the-art on both ZSD and generalized\nZSD tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Caixia Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11941","description":"<p>Accurately segmenting teeth and identifying the corresponding anatomical\nlandmarks on dental mesh models are essential in computer-aided orthodontic\ntreatment. Manually performing these two tasks is time-consuming, tedious, and,\nmore importantly, highly dependent on orthodontists' experiences due to the\nabnormality and large-scale variance of patients' teeth. Some machine\nlearning-based methods have been designed and applied in the orthodontic field\nto automatically segment dental meshes (e.g., intraoral scans). In contrast,\nthe number of studies on tooth landmark localization is still limited. This\npaper proposes a two-stage framework based on mesh deep learning (called\nTS-MDL) for joint tooth labeling and landmark identification on raw intraoral\nscans. Our TS-MDL first adopts an end-to-end \\emph{i}MeshSegNet method (i.e., a\nvariant of the existing MeshSegNet with both improved accuracy and efficiency)\nto label each tooth on the downsampled scan. Guided by the segmentation\noutputs, our TS-MDL further selects each tooth's region of interest (ROI) on\nthe original mesh to construct a light-weight variant of the pioneering\nPointNet (i.e., PointNet-Reg) for regressing the corresponding landmark\nheatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing\npromising segmentation and localization performance. Specifically,\n\\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice\nsimilarity coefficient (DSC) at $0.964\\pm0.054$, significantly outperforming\nthe original MeshSegNet. In the second stage, PointNet-Reg achieved a mean\nabsolute error (MAE) of $0.597\\pm0.761 \\, mm$ in distances between the\nprediction and ground truth for $66$ landmarks, which is superior compared with\nother networks for landmark detection. All these results suggest the potential\nusage of our TS-MDL in clinical practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tai-Hsien Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastewait_M/0/1/0/all/0/1\">Matthew Pastewait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piers_C/0/1/0/all/0/1\">Christian Piers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chiung-Ying Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenchi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_C/0/1/0/all/0/1\">Christina Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Chang Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Gaussian Heatmap Label Assignment for Arbitrary-Oriented Object Detection. (arXiv:2109.12848v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12848","description":"<p>Recently, many arbitrary-oriented object detection (AOOD) methods have been\nproposed and attracted widespread attention in many fields. However, most of\nthem are based on anchor-boxes or standard Gaussian heatmaps. Such label\nassignment strategy may not only fail to reflect the shape and direction\ncharacteristics of arbitrary-oriented objects, but also have high\nparameter-tuning efforts. In this paper, a novel AOOD method called General\nGaussian Heatmap Label Assignment (GGHL) is proposed. Specifically, an\nanchor-free object-adaptation label assignment (OLA) strategy is presented to\ndefine the positive candidates based on two-dimensional (2-D) oriented Gaussian\nheatmaps, which reflect the shape and direction features of arbitrary-oriented\nobjects. Based on OLA, an oriented-bounding-box (OBB) representation component\n(ORC) is developed to indicate OBBs and adjust the Gaussian center prior\nweights to fit the characteristics of different objects adaptively through\nneural network learning. Moreover, a joint-optimization loss (JOL) with area\nnormalization and dynamic confidence weighting is designed to refine the\nmisalign optimal results of different subtasks. Extensive experiments on public\ndatasets demonstrate that the proposed GGHL improves the AOOD performance with\nlow parameter-tuning and time costs. Furthermore, it is generally applicable to\nmost AOOD methods to improve their performance including lightweight models on\nembedded platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhanchao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiang-Gen Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06850","description":"<p>Recently, Zhang et al.(2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\n$\\ell_\\infty$ robustness by its construction. Despite rigorous theoretical\nguarantees, the model so far can only achieve comparable performance to\nconventional networks. In this paper, we make the following two contributions:\n$\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a\nfundamental advantage in certified robustness over conventional networks (under\ntypical certification approaches); $\\mathrm{(ii)}$ With an improved training\nprocess we are able to significantly boost the certified accuracy of\n$\\ell_\\infty$-distance nets. Our training approach largely alleviates the\noptimization problem that arose in the previous training scheme, in particular,\nthe unexpected large Lipschitz constant due to the use of a crucial trick\ncalled $\\ell_p$-relaxation. The core of our training approach is a novel\nobjective function that combines scaled cross-entropy loss and clipped hinge\nloss with a decaying mixing coefficient. Experiments show that using the\nproposed training strategy, the certified accuracy of $\\ell_\\infty$-distance\nnet can be dramatically improved from 33.30% to 40.06% on CIFAR-10\n($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a\nlarge margin. Our results clearly demonstrate the effectiveness and potential\nof $\\ell_\\infty$-distance net for certified robustness. Codes are available at\nhttps://github.com/zbh2047/L_inf-dist-net-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Du Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08708","description":"<p>Occlusion processing is a key issue in pedestrian attribute recognition\n(PAR). Nevertheless, several existing video-based PAR methods have not yet\nconsidered occlusion handling in depth. In this paper, we formulate finding\nnon-occluded frames as sparsity-based temporal attention of a crowded video. In\nthis manner, a model is guided not to pay attention to the occluded frame.\nHowever, temporal sparsity cannot include a correlation between attributes when\nocclusion occurs. For example, \"boots\" and \"shoe color\" cannot be recognized\nwhen the foot is invisible. To solve the uncorrelated attention issue, we also\npropose a novel group sparsity-based temporal attention module. Group sparsity\nis applied across attention weights in correlated attributes. Thus, attention\nweights in a group are forced to pay attention to the same frames. Experimental\nresults showed that the proposed method achieved a higher F1-score than the\nstate-of-the-art methods on two video-based PAR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geonu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jungchan Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLURP: Side Learning Uncertainty for Regression Problems. (arXiv:2110.11182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11182","description":"<p>It has become critical for deep learning algorithms to quantify their output\nuncertainties to satisfy reliability constraints and provide accurate results.\nUncertainty estimation for regression has received less attention than\nclassification due to the more straightforward standardized output of the\nlatter class of tasks and their high importance. However, regression problems\nare encountered in a wide range of applications in computer vision. We propose\nSLURP, a generic approach for regression uncertainty estimation via a side\nlearner that exploits the output and the intermediate representations generated\nby the main task model. We test SLURP on two critical regression tasks in\ncomputer vision: monocular depth and optical flow estimation. In addition, we\nconduct exhaustive benchmarks comprising transfer to different datasets and the\naddition of aleatoric noise. The results show that our proposal is generic and\nreadily applicable to various regression problems and has a low computational\ncost with respect to existing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugMax: Adversarial Composition of Random Augmentations for Robust Training. (arXiv:2110.13771v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13771","description":"<p>Data augmentation is a simple yet effective way to improve the robustness of\ndeep neural networks (DNNs). Diversity and hardness are two complementary\ndimensions of data augmentation to achieve robustness. For example, AugMix\nexplores random compositions of a diverse set of augmentations to enhance\nbroader coverage, while adversarial training generates adversarially hard\nsamples to spot the weakness. Motivated by this, we propose a data augmentation\nframework, termed AugMax, to unify the two aspects of diversity and hardness.\nAugMax first randomly samples multiple augmentation operators and then learns\nan adversarial mixture of the selected operators. Being a stronger form of data\naugmentation, AugMax leads to a significantly augmented input distribution\nwhich makes model training more challenging. To solve this problem, we further\ndesign a disentangled normalization module, termed DuBIN\n(Dual-Batch-and-Instance Normalization), that disentangles the instance-wise\nfeature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN\nleads to significantly improved out-of-distribution robustness, outperforming\nprior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny\nImageNet-C and ImageNet-C. Codes and pretrained models are available:\nhttps://github.com/VITA-Group/AugMax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (arXiv:2110.14189v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14189","description":"<p>Unsupervised learning has recently made exceptional progress because of the\ndevelopment of more effective contrastive learning methods. However, CNNs are\nprone to depend on low-level features that humans deem non-semantic. This\ndependency has been conjectured to induce a lack of robustness to image\nperturbations or domain shift. In this paper, we show that by generating\ncarefully designed negative samples, contrastive learning can learn more robust\nrepresentations with less dependence on such features. Contrastive learning\nutilizes positive pairs that preserve semantic information while perturbing\nsuperficial features in the training images. Similarly, we propose to generate\nnegative samples in a reversed way, where only the superfluous instead of the\nsemantic features are preserved. We develop two methods, texture-based and\npatch-based augmentations, to generate negative samples. These samples achieve\nbetter generalization, especially under out-of-domain settings. We also analyze\nour method and the generated texture-based samples, showing that texture\nfeatures are indispensable in classifying particular ImageNet classes and\nespecially finer classes. We also show that model bias favors texture and shape\nfeatures differently under different test settings. Our code, trained models,\nand ImageNet-Texture dataset can be found at\nhttps://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shlok Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models. (arXiv:2111.07355v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.07355","description":"<p>Wrist fractures are common cases in hospitals, particularly in emergency\nservices. Physicians need images from various medical devices, and patients\nmedical history and physical examination to diagnose these fractures correctly\nand apply proper treatment. This study aims to perform fracture detection using\ndeep learning on wrist Xray images to assist physicians not specialized in the\nfield, working in emergency services in particular, in diagnosis of fractures.\nFor this purpose, 20 different detection procedures were performed using deep\nlearning based object detection models on dataset of wrist Xray images obtained\nfrom Gazi University Hospital. DCN, Dynamic R_CNN, Faster R_CNN, FSAF, Libra\nR_CNN, PAA, RetinaNet, RegNet and SABL deep learning based object detection\nmodels with various backbones were used herein. To further improve detection\nprocedures in the study, 5 different ensemble models were developed, which were\nlater used to reform an ensemble model to develop a detection model unique to\nour study, titled wrist fracture detection combo (WFD_C). Based on 26 different\nmodels for fracture detection, the highest result of detection was 0.8639\naverage precision (AP50) in WFD_C model developed. This study is supported by\nHuawei Turkey R&amp;D Center within the scope of the ongoing cooperation project\ncoded 071813 among Gazi University, Huawei and Medskor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hardalac_F/0/1/0/all/0/1\">F&#x131;rat Hardala&#xe7;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uysal_F/0/1/0/all/0/1\">Fatih Uysal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peker_O/0/1/0/all/0/1\">Ozan Peker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ciceklidag_M/0/1/0/all/0/1\">Murat &#xc7;i&#xe7;eklida&#x11f;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tolunay_T/0/1/0/all/0/1\">Tolga Tolunay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokgoz_N/0/1/0/all/0/1\">Nil Tokg&#xf6;z</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kutbay_U/0/1/0/all/0/1\">U&#x11f;urhan Kutbay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirciler_B/0/1/0/all/0/1\">Boran Demirciler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mert_F/0/1/0/all/0/1\">Fatih Mert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning. (arXiv:2111.13984v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13984","description":"<p>Optimizing nonconvex (NCVX) problems, especially nonsmooth and constrained\nones, is an essential part of machine learning. However, it can be hard to\nreliably solve such problems without optimization expertise. Existing\ngeneral-purpose NCVX optimization packages are powerful but typically cannot\nhandle nonsmoothness. GRANSO is among the first optimization solvers targeting\ngeneral nonsmooth NCVX problems with nonsmooth constraints, but, as it is\nimplemented in MATLAB and requires the user to provide analytical gradients,\nGRANSO is often not a convenient choice in machine learning (especially deep\nlearning) applications. To greatly lower the technical barrier, we introduce a\nnew software package called NCVX, whose initial release contains the solver\nPyGRANSO, a PyTorch-enabled port of GRANSO incorporating auto-differentiation,\nGPU acceleration, tensor input, and support for new QP solvers. NCVX is built\non freely available and widely used open-source frameworks, and as a highlight,\ncan solve general constrained deep learning problems, the first of its kind.\nNCVX is available at https://ncvx.org, with detailed documentation and numerous\nexamples from machine learning and other fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Buyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tim Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02719","description":"<p>Digitized documents such as scientific articles, tax forms, invoices,\ncontract papers, historic texts are widely used nowadays. These document images\ncould be degraded or damaged due to various reasons including poor lighting\nconditions, shadow, distortions like noise and blur, aging, ink stain,\nbleed-through, watermark, stamp, etc. Document image enhancement plays a\ncrucial role as a pre-processing step in many automated document analysis and\nrecognition tasks such as character recognition. With recent advances in deep\nlearning, many methods are proposed to enhance the quality of these document\nimages. In this paper, we review deep learning-based methods, datasets, and\nmetrics for six main document image enhancement tasks, including binarization,\ndebluring, denoising, defading, watermark removal, and shadow removal. We\nsummarize the recent works for each task and discuss their features,\nchallenges, and limitations. We introduce multiple document image enhancement\ntasks that have received little to no attention, including over and under\nexposure correction, super resolution, and bleed-through removal. We identify\nseveral promising research directions and opportunities for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anvari_Z/0/1/0/all/0/1\">Zahra Anvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models. (arXiv:2112.03916v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.03916","description":"<p>Deep learning has brought the most profound contribution towards biomedical\nimage segmentation to automate the process of delineation in medical imaging.\nTo accomplish such task, the models are required to be trained using huge\namount of annotated or labelled data that highlights the region of interest\nwith a binary mask. However, efficient generation of the annotations for such\nhuge data requires expert biomedical analysts and extensive manual effort. It\nis a tedious and expensive task, while also being vulnerable to human error. To\naddress this problem, a self-supervised learning framework, BT-Unet is proposed\nthat uses the Barlow Twins approach to pre-train the encoder of a U-Net model\nvia redundancy reduction in an unsupervised manner to learn data\nrepresentation. Later, complete network is fine-tuned to perform actual\nsegmentation. The BT-Unet framework can be trained with a limited number of\nannotated samples while having high number of unannotated samples, which is\nmostly the case in real-world problems. This framework is validated over\nmultiple U-Net models over diverse datasets by generating scenarios of a\nlimited number of labelled samples using standard evaluation metrics. With\nexhaustive experiment trials, it is observed that the BT-Unet framework\nenhances the performance of the U-Net models with significant margin under such\ncircumstances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning. (arXiv:2112.04731v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04731","description":"<p>Class Incremental Learning (CIL) aims at learning a multi-class classifier in\na phase-by-phase manner, in which only data of a subset of the classes are\nprovided at each phase. Previous works mainly focus on mitigating forgetting in\nphases after the initial one. However, we find that improving CIL at its\ninitial phase is also a promising direction. Specifically, we experimentally\nshow that directly encouraging CIL Learner at the initial phase to output\nsimilar representations as the model jointly trained on all classes can greatly\nboost the CIL performance. Motivated by this, we study the difference between a\nna\\\"ively-trained initial-phase model and the oracle model. Specifically, since\none major difference between these two models is the number of training\nclasses, we investigate how such difference affects the model representations.\nWe find that, with fewer training classes, the data representations of each\nclass lie in a long and narrow region; with more training classes, the\nrepresentations of each class scatter more uniformly. Inspired by this\nobservation, we propose Class-wise Decorrelation (CwD) that effectively\nregularizes representations of each class to scatter more uniformly, thus\nmimicking the model jointly trained with all classes (i.e., the oracle model).\nOur CwD is simple to implement and easy to plug into existing methods.\nExtensive experiments on various benchmark datasets show that CwD consistently\nand significantly improves the performance of existing state-of-the-art methods\nby around 1\\% to 3\\%. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kuangqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery. (arXiv:2112.10415v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10415","description":"<p>This paper proposes a novel approach to object detection on drone imagery,\nnamely Multi-Proxy Detection Network with Unified Foreground Packing\n(UFPMP-Det). To deal with the numerous instances of very small scales,\ndifferent from the common solution that divides the high-resolution input image\ninto quite a number of chips with low foreground ratios to perform detection on\nthem each, the Unified Foreground Packing (UFP) module is designed, where the\nsub-regions given by a coarse detector are initially merged through clustering\nto suppress background and the resulting ones are subsequently packed into a\nmosaic for a single inference, thus significantly reducing overall time cost.\nFurthermore, to address the more serious confusion between inter-class\nsimilarities and intra-class variations of instances, which deteriorates\ndetection performance but is rarely discussed, the Multi-Proxy Detection\nNetwork (MP-Det) is presented to model object distributions in a fine-grained\nmanner by employing multiple proxy learning, and the proxies are enforced to be\ndiverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport\nloss. By such means, UFPMP-Det largely promotes both the detection accuracy and\nefficiency. Extensive experiments are carried out on the widely used VisDrone\nand UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much\nhigher speed, highlighting its advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yecheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10775","description":"<p>Multiple medical institutions collaboratively training a model using\nfederated learning (FL) has become a promising solution for maximizing the\npotential of data-driven models, yet the non-independent and identically\ndistributed (non-iid) data in medical images is still an outstanding challenge\nin real-world practice. The feature heterogeneity caused by diverse scanners or\nprotocols introduces a drift in the learning process, in both local (client)\nand global (server) optimizations, which harms the convergence as well as model\nperformance. Many previous works have attempted to address the non-iid issue by\ntackling the drift locally or globally, but how to jointly solve the two\nessentially coupled drifts is still unclear. In this work, we concentrate on\nhandling both local and global drifts and introduce a new harmonizing framework\ncalled HarmoFL. First, we propose to mitigate the local update drift by\nnormalizing amplitudes of images transformed into the frequency domain to mimic\na unified imaging setting, in order to generate a harmonized feature space\nacross local clients. Second, based on harmonized features, we design a client\nweight perturbation guiding each local model to reach a flat optimum, where a\nneighborhood area of the local optimal solution has a uniformly low loss.\nWithout any extra communication cost, the perturbation assists the global model\nto optimize towards a converged optimal solution by aggregating several local\nflat optima. We have theoretically analyzed the proposed method and empirically\nconducted extensive experiments on three medical image classification and\nsegmentation tasks, showing that HarmoFL outperforms a set of recent\nstate-of-the-art methods with promising convergence behavior. Code is available\nat https://github.com/med-air/HarmoFL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN. (arXiv:2112.13626v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.13626","description":"<p>Translational brain research using Magnetic Resonance Imaging (MRI) is\nbecoming increasingly popular as animal models are an essential part of\nscientific studies and more ultra-high-field scanners are becoming available.\nSome disadvantages of MRI are the availability of MRI scanners and the time\nrequired for a full scanning session (it usually takes over 30 minutes).\nPrivacy laws and the 3Rs ethics rule also make it difficult to create large\ndatasets for training deep learning models. Generative Adversarial Networks\n(GANs) can perform data augmentation with higher quality than other techniques.\nIn this work, the alpha-GAN architecture is used to test its ability to produce\nrealistic 3D MRI scans of the rat brain. As far as the authors are aware, this\nis the first time that a GAN-based approach has been used for data augmentation\nin preclinical data. The generated scans are evaluated using various\nqualitative and quantitative metrics. A Turing test conducted by 4 experts has\nshown that the generated scans can trick almost any expert. The generated scans\nwere also used to evaluate their impact on the performance of an existing deep\nlearning model developed for segmenting the rat brain into white matter, grey\nmatter and cerebrospinal fluid. The models were compared using the Dice score.\nThe best results for whole brain and white matter segmentation were obtained\nwhen 174 real scans and 348 synthetic scans were used, with improvements of\n0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans\nresulted in improvements of 0.0038 and 0.0764 for grey matter and CSF\nsegmentation, respectively. Thus, by using the proposed new normalisation layer\nand loss functions, it was possible to improve the realism of the generated rat\nMRI scans and it was shown that using the generated data improved the\nsegmentation model more than using the conventional data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_A/0/1/0/all/0/1\">Andr&#xe9; Ferreira</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Magalhaes_R/0/1/0/all/0/1\">Ricardo Magalh&#xe3;es</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Meriaux_S/0/1/0/all/0/1\">S&#xe9;bastien M&#xe9;riaux</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Alves_V/0/1/0/all/0/1\">Victor Alves</a> (1) ((1) Centro Algoritmi, University of Minho, Braga, Portugal, (2) Universit&#xe9; Paris-Saclay, CEA, CNRS, BAOBAB, NeuroSpin, Gif-sur-Yvette, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Visual-Auditory Saliency Detection with Multigranularity Perception. (arXiv:2112.13697v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13697","description":"<p>Thanks to the rapid advances in deep learning techniques and the wide\navailability of large-scale training sets, the performance of video saliency\ndetection models has been improving steadily and significantly. However, deep\nlearning-based visualaudio fixation prediction is still in its infancy. At\npresent, only a few visual-audio sequences have been furnished, with real\nfixations being recorded in real visual-audio environments. Hence, it would be\nneither efficient nor necessary to recollect real fixations under the same\nvisual-audio circumstances. To address this problem, this paper promotes a\nnovel approach in a weakly supervised manner to alleviate the demand of\nlarge-scale training sets for visual-audio model training. By using only the\nvideo category tags, we propose the selective class activation mapping (SCAM)\nand its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former\nfollows a coarse-to-fine strategy to select the most discriminative regions,\nand these regions are usually capable of exhibiting high consistency with the\nreal human-eye fixations. The latter equips the SCAM with an additional\nmulti-granularity perception mechanism, making the whole process more\nconsistent with that of the real human visual system. Moreover, we distill\nknowledge from these regions to obtain complete new spatial-temporal-audio\n(STA) fixation prediction (FP) networks, enabling broad applications in cases\nwhere video tags are not available. Without resorting to any real human-eye\nfixation, the performances of these STA FP networks are comparable to those of\nfully supervised networks. The code and results are publicly available at\nhttps://github.com/guotaowang/STANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Aimin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hong Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15139","description":"<p>Quantized neural networks typically require smaller memory footprints and\nlower computation complexity, which is crucial for efficient deployment.\nHowever, quantization inevitably leads to a distribution divergence from the\noriginal network, which generally degrades the performance. To tackle this\nissue, massive efforts have been made, but most existing approaches lack\nstatistical considerations and depend on several manual configurations. In this\npaper, we present an adaptive-mapping quantization method to learn an optimal\nlatent sub-distribution that is inherent within models and smoothly\napproximated with a concrete Gaussian Mixture (GM). In particular, the network\nweights are projected in compliance with the GM-approximated sub-distribution.\nThis sub-distribution evolves along with the weight update in a co-tuning\nschema guided by the direct task-objective optimization. Sufficient experiments\non image classification and object detection over various modern architectures\ndemonstrate the effectiveness, generalization property, and transferability of\nthe proposed method. Besides, an efficient deployment flow for the mobile CPU\nis developed, achieving up to 7.46$\\times$ inference acceleration on an\nocta-core ARM CPU. Codes are publicly released at\nhttps://github.com/RunpeiDong/DGMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhanhong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengdi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}